{
    "aligned-types-cuda": {
        "/Users/gbolet/hecbench-roofline/src/aligned-types-cuda/main.cu": [
            "__global__ void testKernel(\n          TData *__restrict d_odata,\n    const TData *__restrict d_idata,\n    int numElements\n    )\n{\n  const int pos = blockDim.x * blockIdx.x + threadIdx.x;\n  if (pos < numElements)\n  {\n    d_odata[pos] = d_idata[pos];\n  }\n}"
        ]
    },
    "particlefilter-cuda": {
        "/Users/gbolet/hecbench-roofline/src/particlefilter-cuda/kernel_likelihood.h": [
            "inline __host__ __device__ int4 make_int4(float4 a)\n{\n    return make_int4(int(a.x), int(a.y), int(a.z), int(a.w));\n}\n\ninline __host__ __device__ int4 abs(int4 v)\n{\n    return make_int4(abs(v.x), abs(v.y), abs(v.z), abs(v.w));\n}\n\ninline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\ninline __host__ __device__ float4 expf(float4 v)\n{\n    return make_float4(expf(v.x), expf(v.y), expf(v.z), expf(v.w));\n}\n\ninline __host__ __device__ float4 sqrtf(float4 v)\n{\n    return make_float4(sqrtf(v.x), sqrtf(v.y), sqrtf(v.z), sqrtf(v.w));\n}\n\n__global__ void\nkernel_likelihood (\n    float*__restrict__ arrayX, \n    float*__restrict__ arrayY, \n    const float*__restrict__ xj,\n    const float*__restrict__ yj,\n    int*__restrict__ ind,\n    const int*__restrict__ objxy,\n    float*__restrict__ likelihood,\n    const unsigned char*__restrict__ I,\n    float*__restrict__ weights,\n    int*__restrict__ seed,\n    float*__restrict__ partial_sums,\n    const int Nparticles,\n    const int countOnes,\n    const int IszY,\n    const int Nfr,\n    const int k,\n    const int max_size)\n{\n  __shared__ float weights_local[BLOCK_SIZE];\n\n  int block_id = blockIdx.x; \n  int thread_id = threadIdx.x;\n  int i = block_id * blockDim.x + thread_id;\n  int y;\n  int indX, indY;\n  float u, v;\n\n  if(i < Nparticles){\n    arrayX[i] = xj[i]; \n    arrayY[i] = yj[i]; \n\n    weights[i] = 1.0f / ((float) (Nparticles)); \n    seed[i] = (A*seed[i] + C) % M;\n    u = fabsf(seed[i]/((float)M));\n    seed[i] = (A*seed[i] + C) % M;\n    v = fabsf(seed[i]/((float)M));\n    arrayX[i] += 1.0f + 5.0f*(sqrtf(-2.0f*logf(u))*cosf(2.0f*PI*v));\n\n    seed[i] = (A*seed[i] + C) % M;\n    u = fabsf(seed[i]/((float)M));\n    seed[i] = (A*seed[i] + C) % M;\n    v = fabsf(seed[i]/((float)M));\n    arrayY[i] += -2.0f + 2.0f*(sqrtf(-2.0f*logf(u))*cosf(2.0f*PI*v));\n  }\n\n  __syncthreads();\n\n\n  if(i < Nparticles)\n  {\n    for(y = 0; y < countOnes; y++){\n\n      int iX = arrayX[i];\n      int iY = arrayY[i];\n      int rnd_iX = (arrayX[i] - iX) < .5f ? iX : iX++;\n      int rnd_iY = (arrayY[i] - iY) < .5f ? iY : iY++;\n      indX = rnd_iX + objxy[y*2 + 1];\n      indY = rnd_iY + objxy[y*2];\n\n      ind[i*countOnes + y] = abs(indX*IszY*Nfr + indY*Nfr + k);\n      if(ind[i*countOnes + y] >= max_size)\n        ind[i*countOnes + y] = 0;\n    }\n    float likelihoodSum = 0.0f;\n    for(int x = 0; x < countOnes; x++)\n      likelihoodSum += ((I[ind[i*countOnes + x]] - 100) * (I[ind[i*countOnes + x]] - 100) -\n          (I[ind[i*countOnes + x]] - 228) * (I[ind[i*countOnes + x]] - 228)) / 50.0f;\n    likelihood[i] = likelihoodSum/countOnes-SCALE_FACTOR;\n\n    weights[i] = weights[i] * expf(likelihood[i]); //Donnie Newell - added the missing exponential function call\n\n  }\n\n  weights_local[thread_id] = (i < Nparticles) ? weights[i] : 0.f;\n\n  __syncthreads();\n\n  for(unsigned int s=BLOCK_SIZE/2; s>0; s>>=1)\n  {\n    if(thread_id < s)\n    {\n      weights_local[thread_id] += weights_local[thread_id + s];\n    }\n    __syncthreads();\n  }\n  if(thread_id == 0)\n  {\n    partial_sums[block_id] = weights_local[0];\n  }\n}"
        ],
        "/Users/gbolet/hecbench-roofline/src/particlefilter-cuda/kernel_sum.h": [
            "__global__ void\nkernel_sum (float* partial_sums, const int Nparticles)\n{\n  int x;\n  float sum = 0;\n  int num_blocks = (Nparticles + BLOCK_SIZE - 1) / BLOCK_SIZE;\n  for (x = 0; x < num_blocks; x++) {\n    sum += partial_sums[x];\n  }\n  partial_sums[0] = sum;\n}"
        ],
        "/Users/gbolet/hecbench-roofline/src/particlefilter-cuda/kernel_normalize_weights.h": [
            "inline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\ninline __host__ __device__ float4 sqrtf(float4 v)\n{\n    return make_float4(sqrtf(v.x), sqrtf(v.y), sqrtf(v.z), sqrtf(v.w));\n}\n\n__global__ void\nkernel_normalize_weights (\n    float* __restrict__ weights,\n    const float* __restrict__ partial_sums,\n    float* __restrict__ CDF,\n    float* __restrict__ u,\n    int* __restrict__ seed,\n    const int Nparticles )\n{\n  __shared__ float u1;\n  __shared__ float sumWeights;\n  int local_id = threadIdx.x;\n  int i = blockDim.x * blockIdx.x + threadIdx.x;\n  if(0 == local_id)\n    sumWeights = partial_sums[0];\n  __syncthreads();\n  if(i < Nparticles) {\n    weights[i] = weights[i]/sumWeights;\n  }\n  __syncthreads();\n  if(i == 0) {\n    CDF[0] = weights[0];\n    for(int x = 1; x < Nparticles; x++){\n      CDF[x] = weights[x] + CDF[x-1];\n    }\n\n    seed[i] = (A*seed[i] + C) % M;\n    float p = fabsf(seed[i]/((float)M));\n    seed[i] = (A*seed[i] + C) % M;\n    float q = fabsf(seed[i]/((float)M));\n    u[0] = (1.0f/((float)(Nparticles))) * \n      (sqrtf(-2.0f*log(p))*cosf(2.0f*PI*q));\n    // do this to allow all threads in all blocks to use the same u1\n  }\n  __syncthreads();\n  if(0 == local_id)\n    u1 = u[0];\n\n  __syncthreads();\n  if(i < Nparticles)\n  {\n    u[i] = u1 + i/((float)(Nparticles));\n  }\n}"
        ],
        "/Users/gbolet/hecbench-roofline/src/particlefilter-cuda/kernel_find_index.h": [
            "__global__ void\nkernel_find_index (\n    const float*__restrict__ arrayX,\n    const float*__restrict__ arrayY,\n    const float*__restrict__ CDF,\n    const float*__restrict__ u,\n          float*__restrict__ xj,\n          float*__restrict__ yj,\n    const int Nparticles)\n{\n  int i = blockDim.x * blockIdx.x + threadIdx.x;\n  if(i < Nparticles){\n    int index = -1;\n    int x;\n\n    for(x = 0; x < Nparticles; x++){\n      if(CDF[x] >= u[i]){\n        index = x;\n        break;\n      }\n    }\n    if(index == -1){\n      index = Nparticles-1;\n    }\n\n    xj[i] = arrayX[index];\n    yj[i] = arrayY[index];\n  }\n}"
        ]
    },
    "gmm-cuda": {
        "/Users/gbolet/hecbench-roofline/src/gmm-cuda/gaussian_kernel.cu": [
            "inline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\ninline __host__ __device__ float4 fabs(float4 v)\n{\n    return make_float4(fabs(v.x), fabs(v.y), fabs(v.z), fabs(v.w));\n}\n\n__device__ void invert(float* data, int actualsize, float* log_determinant)  {\n  int maxsize = actualsize;\n  int n = actualsize;\n\n  if(threadIdx.x == 0) {\n    *log_determinant = 0.0;\n\n    // sanity check        \n    if (actualsize == 1) {\n      *log_determinant = logf(data[0]);\n      data[0] = 1.f / data[0];\n    } else {\n\n      for (int i=1; i < actualsize; i++) data[i] /= data[0]; // normalize row 0\n      for (int i=1; i < actualsize; i++)  { \n        for (int j=i; j < actualsize; j++)  { // do a column of L\n          float sum = 0.0;\n          for (int k = 0; k < i; k++)  \n            sum += data[j*maxsize+k] * data[k*maxsize+i];\n          data[j*maxsize+i] -= sum;\n        }\n        if (i == actualsize-1) continue;\n        for (int j=i+1; j < actualsize; j++)  {  // do a row of U\n          float sum = 0.0;\n          for (int k = 0; k < i; k++)\n            sum += data[i*maxsize+k]*data[k*maxsize+j];\n          data[i*maxsize+j] = \n            (data[i*maxsize+j]-sum) / data[i*maxsize+i];\n        }\n      }\n\n      for(int i=0; i<actualsize; i++) {\n        *log_determinant += logf(fabs(data[i*n+i]));\n      }\n\n      for ( int i = 0; i < actualsize; i++ )  // invert L\n        for ( int j = i; j < actualsize; j++ )  {\n          float x = 1.f;\n          if ( i != j ) {\n            x = 0.0;\n            for ( int k = i; k < j; k++ ) \n              x -= data[j*maxsize+k]*data[k*maxsize+i];\n          }\n          data[j*maxsize+i] = x / data[j*maxsize+j];\n        }\n      for ( int i = 0; i < actualsize; i++ )   // invert U\n        for ( int j = i; j < actualsize; j++ )  {\n          if ( i == j ) continue;\n          float sum = 0.0;\n          for ( int k = i; k < j; k++ )\n            sum += data[k*maxsize+j]*( (i==k) ? 1.f : data[i*maxsize+k] );\n          data[i*maxsize+j] = -sum;\n        }\n      for ( int i = 0; i < actualsize; i++ )   // final inversion\n        for ( int j = 0; j < actualsize; j++ )  {\n          float sum = 0.0;\n          for ( int k = ((i>j)?i:j); k < actualsize; k++ )  \n            sum += ((j==k)?1.f:data[j*maxsize+k])*data[k*maxsize+i];\n          data[j*maxsize+i] = sum;\n        }\n    }\n  }\n}\n\n__global__ void\nconstants_kernel(clusters_t* clusters, int num_clusters, int num_dimensions) {\n\n  // compute_constants(clusters,num_clusters,num_dimensions);\n\n  int tid = threadIdx.x;\n  int bid = blockIdx.x;\n  int num_threads = blockDim.x;\n  int num_elements = num_dimensions*num_dimensions;\n\n  __shared__ float determinant_arg; // only one thread computes the inverse so we need a shared argument\n  __shared__ float sum;\n  __shared__ float matrix[NUM_DIMENSIONS*NUM_DIMENSIONS];\n\n  float log_determinant;\n\n\n  // Invert the matrix for every cluster\n\n  // Copy the R matrix into shared memory for doing the matrix inversion\n  for(int i=tid; i<num_elements; i+= num_threads ) {\n    matrix[i] = clusters->R[bid*num_dimensions*num_dimensions+i];\n  }\n\n  __syncthreads(); \n#if DIAG_ONLY\n  if(tid == 0) { \n    determinant_arg = 1.0f;\n    for(int i=0; i < num_dimensions; i++) {\n      determinant_arg *= matrix[i*num_dimensions+i];\n      matrix[i*num_dimensions+i] = 1.0f / matrix[i*num_dimensions+i];\n    }\n    determinant_arg = logf(determinant_arg);\n  }\n#else \n  invert(matrix,num_dimensions,&determinant_arg);\n#endif\n  __syncthreads(); \n  log_determinant = determinant_arg;\n\n  // Copy the matrx from shared memory back into the cluster memory\n  for(int i=tid; i<num_elements; i+= num_threads) {\n    clusters->Rinv[bid*num_dimensions*num_dimensions+i] = matrix[i];\n  }\n\n  __syncthreads();\n\n  // Compute the constant\n  // Equivilent to: log(1/((2*PI)^(M/2)*det(R)^(1/2)))\n  // This constant is used in all E-step likelihood calculations\n  if(tid == 0) {\n    clusters->constant[bid] = -num_dimensions*0.5f*logf(2.0f*PI) - 0.5f*log_determinant;\n  }\n\n  __syncthreads();\n\n  if(bid == 0) {\n    // compute_pi(clusters,num_clusters);\n\n    if(tid == 0) {\n      sum = 0.0;\n      for(int i=0; i<num_clusters; i++) {\n        sum += clusters->N[i];\n      }\n    }\n\n    __syncthreads();\n\n    for(int i = tid; i < num_clusters; i += num_threads) {\n      if(clusters->N[i] < 0.5f) {\n        clusters->pi[tid] = 1e-10;\n      } else {\n        clusters->pi[tid] = clusters->N[i] / sum;\n      }\n    }\n  }\n}",
            "__global__ void\nseed_clusters_kernel( const float* fcs_data, \n    clusters_t* clusters, \n    const int num_dimensions, \n    const int num_clusters, \n    const int num_events) \n{\n  int tid = threadIdx.x;\n  int num_threads = blockDim.x;\n  int row, col;\n  float seed;\n\n  // Number of elements in the covariance matrix\n  int num_elements = num_dimensions*num_dimensions; \n\n  // shared memory\n  __shared__ float means[NUM_DIMENSIONS];\n  __shared__ float avgvar;\n  __shared__ float variances[NUM_DIMENSIONS];\n  __shared__ float total_variance;\n\n  // Compute the means\n  // mvtmeans(fcs_data, num_dimensions, num_events, means);\n\n  if(tid < num_dimensions) {\n    means[tid] = 0.0;\n\n    // Sum up all the values for each dimension\n    for(int i = 0; i < num_events; i++) {\n      means[tid] += fcs_data[i*num_dimensions+tid];\n    }\n\n    // Divide by the # of elements to get the average\n    means[tid] /= (float) num_events;\n  }\n\n  __syncthreads();\n\n  // Compute the average variance\n  // averageVariance(fcs_data, means, num_dimensions, num_events, &avgvar);\n\n  // Compute average variance for each dimension\n  if(tid < num_dimensions) {\n    variances[tid] = 0.0;\n    // Sum up all the variance\n    for(int i = 0; i < num_events; i++) {\n      // variance = (data - mean)^2\n      variances[tid] += (fcs_data[i*num_dimensions + tid])*(fcs_data[i*num_dimensions + tid]);\n    }\n    variances[tid] /= (float) num_events;\n    variances[tid] -= means[tid]*means[tid];\n  }\n\n  __syncthreads();\n\n  if(tid == 0) {\n    total_variance = 0.0;\n    for(int i=0; i<num_dimensions;i++) {\n      total_variance += variances[i];\n    }\n    avgvar = total_variance / (float) num_dimensions;\n  }\n\n  __syncthreads();\n\n  if(num_clusters > 1) {\n    seed = (num_events-1.0f)/(num_clusters-1.0f);\n  } else {\n    seed = 0.0;\n  }\n\n  // Seed the pi, means, and covariances for every cluster\n  for(int c=0; c < num_clusters; c++) {\n    if(tid < num_dimensions) {\n      clusters->means[c*num_dimensions+tid] = fcs_data[((int)(c*seed))*num_dimensions+tid];\n    }\n\n    for(int i=tid; i < num_elements; i+= num_threads) {\n      // Add the average variance divided by a constant, this keeps the cov matrix from becoming singular\n      row = (i) / num_dimensions;\n      col = (i) % num_dimensions;\n\n      if(row == col) {\n        clusters->R[c*num_dimensions*num_dimensions+i] = 1.0f;\n      } else {\n        clusters->R[c*num_dimensions*num_dimensions+i] = 0.0f;\n      }\n    }\n    if(tid == 0) {\n      clusters->pi[c] = 1.0f/((float)num_clusters);\n      clusters->N[c] = ((float) num_events) / ((float)num_clusters);\n      clusters->avgvar[c] = avgvar / COVARIANCE_DYNAMIC_RANGE;\n    }\n  }\n}",
            "__device__ void compute_indices(int num_events, int* start, int* stop) {\n  // Break up the events evenly between the blocks\n  int num_pixels_per_block = num_events / NUM_BLOCKS;\n  // Make sure the events being accessed by the block are aligned to a multiple of 16\n  num_pixels_per_block = num_pixels_per_block - (num_pixels_per_block % 16);\n\n  *start = blockIdx.y * num_pixels_per_block + threadIdx.x;\n\n  // Last block will handle the leftover events\n  if(blockIdx.y == gridDim.y-1) {\n    *stop = num_events;\n  } else { \n    *stop = (blockIdx.y+1) * num_pixels_per_block;\n  }\n}\n\n__global__ void\nestep1(float* data, clusters_t* clusters, int num_dimensions, int num_events) {\n\n  // Cached cluster parameters\n  __shared__ float means[NUM_DIMENSIONS];\n  __shared__ float Rinv[NUM_DIMENSIONS*NUM_DIMENSIONS];\n  float cluster_pi;\n  float constant;\n  const unsigned int tid = threadIdx.x;\n\n  int start_index;\n  int end_index;\n\n  int c = blockIdx.x;\n\n  compute_indices(num_events,&start_index,&end_index);\n\n  float like;\n\n  // This loop computes the expectation of every event into every cluster\n  //\n  // P(k|n) = L(x_n|mu_k,R_k)*P(k) / P(x_n)\n  //\n  // Compute log-likelihood for every cluster for each event\n  // L = constant*exp(-0.5*(x-mu)*Rinv*(x-mu))\n  // log_L = log_constant - 0.5*(x-u)*Rinv*(x-mu)\n  // the constant stored in clusters[c].constant is already the log of the constant\n\n  // copy the means for this cluster into shared memory\n  if(tid < num_dimensions) {\n    means[tid] = clusters->means[c*num_dimensions+tid];\n  }\n\n  // copy the covariance inverse into shared memory\n  for(int i=tid; i < num_dimensions*num_dimensions; i+= NUM_THREADS_ESTEP) {\n    Rinv[i] = clusters->Rinv[c*num_dimensions*num_dimensions+i]; \n  }\n\n  cluster_pi = clusters->pi[c];\n  constant = clusters->constant[c];\n\n  // Sync to wait for all params to be loaded to shared memory\n  __syncthreads();\n\n  for(int event=start_index; event<end_index; event += NUM_THREADS_ESTEP) {\n    like = 0.0f;\n    // this does the loglikelihood calculation\n#if DIAG_ONLY\n    for(int j=0; j<num_dimensions; j++) {\n      like += (data[j*num_events+event]-means[j]) * (data[j*num_events+event]-means[j]) * Rinv[j*num_dimensions+j];\n    }\n#else\n    for(int i=0; i<num_dimensions; i++) {\n      for(int j=0; j<num_dimensions; j++) {\n        like += (data[i*num_events+event]-means[i]) * (data[j*num_events+event]-means[j]) * Rinv[i*num_dimensions+j];\n      }\n    }\n#endif\n    // numerator of the E-step probability computation\n    clusters->memberships[c*num_events+event] = -0.5f * like + constant + logf(cluster_pi);\n  }\n}",
            "inline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\ninline __host__ __device__ float4 expf(float4 v)\n{\n    return make_float4(expf(v.x), expf(v.y), expf(v.z), expf(v.w));\n}\n\ninline __host__ __device__ float4 fmaxf(float4 a, float4 b)\n{\n    return make_float4(fmaxf(a.x,b.x), fmaxf(a.y,b.y), fmaxf(a.z,b.z), fmaxf(a.w,b.w));\n}\n\n__device__ float parallelSum(float* data, const unsigned int ndata) {\n  const unsigned int tid = threadIdx.x;\n  float t;\n\n  __syncthreads();\n\n  // Butterfly sum.  ndata MUST be a power of 2.\n  for(unsigned int bit = ndata >> 1; bit > 0; bit >>= 1) {\n    t = data[tid] + data[tid^bit];  __syncthreads();\n    data[tid] = t;                  __syncthreads();\n  }\n  return data[tid];\n}\n\n__global__ void\nestep2(float* fcs_data, clusters_t* clusters, int num_dimensions, int num_clusters, int num_events, float* likelihood) {\n  float temp;\n  float thread_likelihood = 0.0f;\n  __shared__ float total_likelihoods[NUM_THREADS_ESTEP];\n  float max_likelihood;\n  float denominator_sum;\n\n  // Break up the events evenly between the blocks\n  int num_pixels_per_block = num_events / gridDim.x;\n  // Make sure the events being accessed by the block are aligned to a multiple of 16\n  num_pixels_per_block = num_pixels_per_block - (num_pixels_per_block % 16);\n  int tid = threadIdx.x;\n\n  int start_index;\n  int end_index;\n  start_index = blockIdx.x * num_pixels_per_block + tid;\n\n  // Last block will handle the leftover events\n  if(blockIdx.x == gridDim.x-1) {\n    end_index = num_events;\n  } else {\n    end_index = (blockIdx.x+1) * num_pixels_per_block;\n  }\n\n  total_likelihoods[tid] = 0.0;\n\n  // P(x_n) = sum of likelihoods weighted by P(k) (their probability, cluster[c].pi)\n  //  log(a+b) != log(a) + log(b) so we need to do the log of the sum of the exponentials\n\n  //  For the sake of numerical stability, we first find the max and scale the values\n  //  That way, the maximum value ever going into the exp function is 0 and we avoid overflow\n\n  //  log-sum-exp formula:\n  //  log(sum(exp(x_i)) = max(z) + log(sum(exp(z_i-max(z))))\n  for(int pixel=start_index; pixel<end_index; pixel += NUM_THREADS_ESTEP) {\n    // find the maximum likelihood for this event\n    max_likelihood = clusters->memberships[pixel];\n    for(int c=1; c<num_clusters; c++) {\n      max_likelihood = fmaxf(max_likelihood,clusters->memberships[c*num_events+pixel]);\n    }\n\n    // Compute P(x_n), the denominator of the probability (sum of weighted likelihoods)\n    denominator_sum = 0.0;\n    for(int c=0; c<num_clusters; c++) {\n      temp = expf(clusters->memberships[c*num_events+pixel]-max_likelihood);\n      denominator_sum += temp;\n    }\n    denominator_sum = max_likelihood + logf(denominator_sum);\n    thread_likelihood += denominator_sum;\n\n    // Divide by denominator, also effectively normalize probabilities\n    // exp(log(p) - log(denom)) == p / denom\n    for(int c=0; c<num_clusters; c++) {\n      clusters->memberships[c*num_events+pixel] = expf(clusters->memberships[c*num_events+pixel] - denominator_sum);\n      //printf(\"Probability that pixel #%d is in cluster #%d: %f\\n\",pixel,c,clusters->memberships[c*num_events+pixel]);\n    }\n  }\n\n  total_likelihoods[tid] = thread_likelihood;\n  __syncthreads();\n\n  temp = parallelSum(total_likelihoods,NUM_THREADS_ESTEP);\n  if(tid == 0) {\n    likelihood[blockIdx.x] = temp;\n  }\n}",
            "__device__ float parallelSum(float* data, const unsigned int ndata) {\n  const unsigned int tid = threadIdx.x;\n  float t;\n\n  __syncthreads();\n\n  // Butterfly sum.  ndata MUST be a power of 2.\n  for(unsigned int bit = ndata >> 1; bit > 0; bit >>= 1) {\n    t = data[tid] + data[tid^bit];  __syncthreads();\n    data[tid] = t;                  __syncthreads();\n  }\n  return data[tid];\n}\n\n__global__ void\nmstep_means(float* fcs_data, clusters_t* clusters, int num_dimensions, int num_clusters, int num_events) {\n  // One block per cluster, per dimension:  (M x D) grid of blocks\n  int tid = threadIdx.x;\n  int num_threads = blockDim.x;\n  int c = blockIdx.x; // cluster number\n  int d = blockIdx.y; // dimension number\n\n  __shared__ float temp_sum[NUM_THREADS_MSTEP];\n  float sum = 0.0f;\n\n  for(int event=tid; event < num_events; event+= num_threads) {\n    sum += fcs_data[d*num_events+event]*clusters->memberships[c*num_events+event];\n  }\n  temp_sum[tid] = sum;\n\n  __syncthreads();\n\n  // Reduce partial sums\n  sum = parallelSum(temp_sum,NUM_THREADS_MSTEP);\n  if(tid == 0) {\n    clusters->means[c*num_dimensions+d] = sum;\n  }\n\n  /*if(tid == 0) {\n    for(int i=1; i < num_threads; i++) {\n    temp_sum[0] += temp_sum[i];\n    }\n    clusters->means[c*num_dimensions+d] = temp_sum[0];\n  //clusters->means[c*num_dimensions+d] = temp_sum[0] / clusters->N[c];\n  }*/\n}",
            "__device__ float parallelSum(float* data, const unsigned int ndata) {\n  const unsigned int tid = threadIdx.x;\n  float t;\n\n  __syncthreads();\n\n  // Butterfly sum.  ndata MUST be a power of 2.\n  for(unsigned int bit = ndata >> 1; bit > 0; bit >>= 1) {\n    t = data[tid] + data[tid^bit];  __syncthreads();\n    data[tid] = t;                  __syncthreads();\n  }\n  return data[tid];\n}\n\n__global__ void\nmstep_N(clusters_t* clusters, int num_dimensions, int num_clusters, int num_events) {\n\n  int tid = threadIdx.x;\n  int num_threads = blockDim.x;\n  int c = blockIdx.x;\n\n\n  // Need to store the sum computed by each thread so in the end\n  // a single thread can reduce to get the final sum\n  __shared__ float temp_sums[NUM_THREADS_MSTEP];\n\n  // Compute new N\n  float sum = 0.0f;\n  // Break all the events accross the threads, add up probabilities\n  for(int event=tid; event < num_events; event += num_threads) {\n    sum += clusters->memberships[c*num_events+event];\n  }\n  temp_sums[tid] = sum;\n\n  __syncthreads();\n\n  sum = parallelSum(temp_sums,NUM_THREADS_MSTEP);\n  if(tid == 0) {\n    clusters->N[c] = sum;\n    clusters->pi[c] = sum;\n  }\n\n  // Let the first thread add up all the intermediate sums\n  // Could do a parallel reduction...doubt it's really worth it for so few elements though\n  /*if(tid == 0) {\n    clusters->N[c] = 0.0;\n    for(int j=0; j<num_threads; j++) {\n    clusters->N[c] += temp_sums[j];\n    }\n  //printf(\"clusters[%d].N = %f\\n\",c,clusters[c].N);\n\n  // Set PI to the # of expected items, and then normalize it later\n  clusters->pi[c] = clusters->N[c];\n  }*/\n}",
            "__device__ void compute_row_col(int n, int* row, int* col) {\n  int i = 0;\n  for(int r=0; r < n; r++) {\n    for(int c=0; c <= r; c++) {\n      if(i == blockIdx.y) {  \n        *row = r;\n        *col = c;\n        return;\n      }\n      i++;\n    }\n  }\n}\n\n__device__ float parallelSum(float* data, const unsigned int ndata) {\n  const unsigned int tid = threadIdx.x;\n  float t;\n\n  __syncthreads();\n\n  // Butterfly sum.  ndata MUST be a power of 2.\n  for(unsigned int bit = ndata >> 1; bit > 0; bit >>= 1) {\n    t = data[tid] + data[tid^bit];  __syncthreads();\n    data[tid] = t;                  __syncthreads();\n  }\n  return data[tid];\n}\n\n__global__ void\nmstep_covariance1(float* fcs_data, clusters_t* clusters, int num_dimensions, int num_clusters, int num_events) {\n  int tid = threadIdx.x; // easier variable name for our thread ID\n\n  // Determine what row,col this matrix is handling, also handles the symmetric element\n  int row,col,c;\n  compute_row_col(num_dimensions, &row, &col);\n  //row = blockIdx.y / num_dimensions;\n  //col = blockIdx.y % num_dimensions;\n\n  __syncthreads();\n\n  c = blockIdx.x; // Determines what cluster this block is handling    \n\n  int matrix_index = row * num_dimensions + col;\n\n#if DIAG_ONLY\n  if(row != col) {\n    clusters->R[c*num_dimensions*num_dimensions+matrix_index] = 0.0;\n    matrix_index = col*num_dimensions+row;\n    clusters->R[c*num_dimensions*num_dimensions+matrix_index] = 0.0;\n    return;\n  }\n#endif \n\n  // Store the means in shared memory to speed up the covariance computations\n  __shared__ float means[NUM_DIMENSIONS];\n  // copy the means for this cluster into shared memory\n  if(tid < num_dimensions) {\n    means[tid] = clusters->means[c*num_dimensions+tid];\n  }\n\n  // Sync to wait for all params to be loaded to shared memory\n  __syncthreads();\n\n  __shared__ float temp_sums[NUM_THREADS_MSTEP];\n\n  float cov_sum = 0.0;\n\n  for(int event=tid; event < num_events; event+=NUM_THREADS_MSTEP) {\n    cov_sum += (fcs_data[row*num_events+event]-means[row])*\n      (fcs_data[col*num_events+event]-means[col])*clusters->memberships[c*num_events+event]; \n  }\n  temp_sums[tid] = cov_sum;\n\n  __syncthreads();\n\n  cov_sum = parallelSum(temp_sums,NUM_THREADS_MSTEP);\n\n  if(tid == 0) {\n    clusters->R[c*num_dimensions*num_dimensions+matrix_index] = cov_sum;\n    // Set the symmetric value\n    matrix_index = col*num_dimensions+row;\n    clusters->R[c*num_dimensions*num_dimensions+matrix_index] = cov_sum;\n\n    // Regularize matrix - adds some variance to the diagonal elements\n    // Helps keep covariance matrix non-singular (so it can be inverted)\n    // The amount added is scaled down based on COVARIANCE_DYNAMIC_RANGE constant defined at top of this file\n    if(row == col) {\n      clusters->R[c*num_dimensions*num_dimensions+matrix_index] += clusters->avgvar[c];\n    }\n  }\n}",
            "inline __host__ __device__ uint4 make_uint4(int4 a)\n{\n    return make_uint4(uint(a.x), uint(a.y), uint(a.z), uint(a.w));\n}\n\ninline __host__ __device__ uint4 min(uint4 a, uint4 b)\n{\n    return make_uint4(min(a.x,b.x), min(a.y,b.y), min(a.z,b.z), min(a.w,b.w));\n}\n\n__device__ void compute_row_col(int n, int* row, int* col) {\n  int i = 0;\n  for(int r=0; r < n; r++) {\n    for(int c=0; c <= r; c++) {\n      if(i == blockIdx.y) {  \n        *row = r;\n        *col = c;\n        return;\n      }\n      i++;\n    }\n  }\n}\n\n__device__ float parallelSum(float* data, const unsigned int ndata) {\n  const unsigned int tid = threadIdx.x;\n  float t;\n\n  __syncthreads();\n\n  // Butterfly sum.  ndata MUST be a power of 2.\n  for(unsigned int bit = ndata >> 1; bit > 0; bit >>= 1) {\n    t = data[tid] + data[tid^bit];  __syncthreads();\n    data[tid] = t;                  __syncthreads();\n  }\n  return data[tid];\n}\n\n__global__ void\nmstep_covariance2(float* fcs_data, clusters_t* clusters, int num_dimensions, int num_clusters, int num_events) {\n  int tid = threadIdx.x; // easier variable name for our thread ID\n\n  // Determine what row,col this matrix is handling, also handles the symmetric element\n  int row,col,c1;\n  compute_row_col(num_dimensions, &row, &col);\n\n  __syncthreads();\n\n  c1 = blockIdx.x * NUM_CLUSTERS_PER_BLOCK; // Determines what cluster this block is handling    \n\n#if DIAG_ONLY\n  if(row != col) {\n    clusters->R[c*num_dimensions*num_dimensions+row*num_dimensions+col] = 0.0f;\n    clusters->R[c*num_dimensions*num_dimensions+col*num_dimensions+row] = 0.0f;\n    return;\n  }\n#endif \n\n  // Store the means in shared memory to speed up the covariance computations\n  __shared__ float means_row[NUM_CLUSTERS_PER_BLOCK];\n  __shared__ float means_col[NUM_CLUSTERS_PER_BLOCK];\n\n  //if(tid < NUM_CLUSTERS_PER_BLOCK) {  \n  if ( (tid < min(num_clusters, NUM_CLUSTERS_PER_BLOCK))  // c1 = 0\n      && (c1+tid < num_clusters)) { \n    means_row[tid] = clusters->means[(c1+tid)*num_dimensions+row];\n    means_col[tid] = clusters->means[(c1+tid)*num_dimensions+col];\n  }\n\n  // Sync to wait for all params to be loaded to shared memory\n  __syncthreads();\n\n  // 256 * 6\n  __shared__ float temp_sums[NUM_THREADS_MSTEP*NUM_CLUSTERS_PER_BLOCK];\n\n  float cov_sum1 = 0.0f;\n  float cov_sum2 = 0.0f;\n  float cov_sum3 = 0.0f;\n  float cov_sum4 = 0.0f;\n  float cov_sum5 = 0.0f;\n  float cov_sum6 = 0.0f;\n  float val1,val2;\n\n  for(int c=0; c < NUM_CLUSTERS_PER_BLOCK; c++) {\n    temp_sums[c*NUM_THREADS_MSTEP+tid] = 0.0;\n  } \n\n  for(int event=tid; event < num_events; event+=NUM_THREADS_MSTEP) {\n    val1 = fcs_data[row*num_events+event];\n    val2 = fcs_data[col*num_events+event];\n    cov_sum1 += (val1-means_row[0])*(val2-means_col[0])*clusters->memberships[c1*num_events+event]; \n    cov_sum2 += (val1-means_row[1])*(val2-means_col[1])*clusters->memberships[(c1+1)*num_events+event]; \n    cov_sum3 += (val1-means_row[2])*(val2-means_col[2])*clusters->memberships[(c1+2)*num_events+event]; \n    cov_sum4 += (val1-means_row[3])*(val2-means_col[3])*clusters->memberships[(c1+3)*num_events+event]; \n    cov_sum5 += (val1-means_row[4])*(val2-means_col[4])*clusters->memberships[(c1+4)*num_events+event]; \n    cov_sum6 += (val1-means_row[5])*(val2-means_col[5])*clusters->memberships[(c1+5)*num_events+event]; \n  }\n  temp_sums[0*NUM_THREADS_MSTEP+tid] = cov_sum1;\n  temp_sums[1*NUM_THREADS_MSTEP+tid] = cov_sum2;\n  temp_sums[2*NUM_THREADS_MSTEP+tid] = cov_sum3;\n  temp_sums[3*NUM_THREADS_MSTEP+tid] = cov_sum4;\n  temp_sums[4*NUM_THREADS_MSTEP+tid] = cov_sum5;\n  temp_sums[5*NUM_THREADS_MSTEP+tid] = cov_sum6;\n\n  __syncthreads();\n\n  for(int c=0; c < NUM_CLUSTERS_PER_BLOCK; c++) {\n    temp_sums[c*NUM_THREADS_MSTEP+tid] = parallelSum(&temp_sums[c*NUM_THREADS_MSTEP],NUM_THREADS_MSTEP);\n    __syncthreads();\n  }\n\n  if(tid == 0) {\n    for(int c=0; c < NUM_CLUSTERS_PER_BLOCK && (c+c1) < num_clusters; c++) {\n      int offset = (c+c1)*num_dimensions*num_dimensions;\n      cov_sum1 = temp_sums[c*NUM_THREADS_MSTEP];\n      clusters->R[offset+row*num_dimensions+col] = cov_sum1;\n      // Set the symmetric value\n      clusters->R[offset+col*num_dimensions+row] = cov_sum1;\n\n      // Regularize matrix - adds some variance to the diagonal elements\n      // Helps keep covariance matrix non-singular (so it can be inverted)\n      // The amount added is scaled down based on COVARIANCE_DYNAMIC_RANGE constant defined in gaussian.h\n      if(row == col) {\n        clusters->R[offset+row*num_dimensions+col] += clusters->avgvar[c+c1];\n      }\n    }\n  }\n}"
        ]
    },
    "matrixT-cuda": {
        "/Users/gbolet/hecbench-roofline/src/matrixT-cuda/main.cu": [
            "__global__ void copySharedMem(\n        float *__restrict__ odata,\n  const float *__restrict__ idata,\n  int width, int height)\n{\n  // Handle to thread block group\n  cg::thread_block cta = cg::this_thread_block();\n  __shared__ float tile[TILE_DIM][TILE_DIM];\n\n  int xIndex = blockIdx.x * TILE_DIM + threadIdx.x;\n  int yIndex = blockIdx.y * TILE_DIM + threadIdx.y;\n\n  int index  = xIndex + width*yIndex;\n\n  for (int i=0; i<TILE_DIM; i+=BLOCK_ROWS)\n  {\n    if (xIndex < width && yIndex < height)\n    {\n      tile[threadIdx.y][threadIdx.x] = idata[index];\n    }\n  }\n\n  cg::sync(cta);\n\n  for (int i=0; i<TILE_DIM; i+=BLOCK_ROWS)\n  {\n    if (xIndex < height && yIndex < width)\n    {\n      odata[index] = tile[threadIdx.y][threadIdx.x];\n    }\n  }\n}",
            "__global__ void transposeNaive(\n        float *__restrict__ odata,\n  const float *__restrict__ idata,\n  int width, int height)\n{\n  int xIndex = blockIdx.x * TILE_DIM + threadIdx.x;\n  int yIndex = blockIdx.y * TILE_DIM + threadIdx.y;\n\n  int index_in  = xIndex + width * yIndex;\n  int index_out = yIndex + height * xIndex;\n\n  for (int i=0; i<TILE_DIM; i+=BLOCK_ROWS)\n  {\n    odata[index_out+i] = idata[index_in+i*width];\n  }\n}",
            "__global__ void transposeCoalesced(\n        float *__restrict__ odata,\n  const float *__restrict__ idata,\n  int width, int height)\n{\n  // Handle to thread block group\n  cg::thread_block cta = cg::this_thread_block();\n  __shared__ float tile[TILE_DIM][TILE_DIM];\n\n  int xIndex = blockIdx.x * TILE_DIM + threadIdx.x;\n  int yIndex = blockIdx.y * TILE_DIM + threadIdx.y;\n  int index_in = xIndex + (yIndex)*width;\n\n  xIndex = blockIdx.y * TILE_DIM + threadIdx.x;\n  yIndex = blockIdx.x * TILE_DIM + threadIdx.y;\n  int index_out = xIndex + (yIndex)*height;\n\n  for (int i=0; i<TILE_DIM; i+=BLOCK_ROWS)\n  {\n    tile[threadIdx.y+i][threadIdx.x] = idata[index_in+i*width];\n  }\n\n  cg::sync(cta);\n\n  for (int i=0; i<TILE_DIM; i+=BLOCK_ROWS)\n  {\n    odata[index_out+i*height] = tile[threadIdx.x][threadIdx.y+i];\n  }\n}",
            "__global__ void transposeNoBankConflicts(\n        float *__restrict__ odata,\n  const float *__restrict__ idata,\n  int width, int height)\n{\n  // Handle to thread block group\n  cg::thread_block cta = cg::this_thread_block();\n  __shared__ float tile[TILE_DIM][TILE_DIM+1];\n\n  int xIndex = blockIdx.x * TILE_DIM + threadIdx.x;\n  int yIndex = blockIdx.y * TILE_DIM + threadIdx.y;\n  int index_in = xIndex + (yIndex)*width;\n\n  xIndex = blockIdx.y * TILE_DIM + threadIdx.x;\n  yIndex = blockIdx.x * TILE_DIM + threadIdx.y;\n  int index_out = xIndex + (yIndex)*height;\n\n  for (int i=0; i<TILE_DIM; i+=BLOCK_ROWS)\n  {\n    tile[threadIdx.y+i][threadIdx.x] = idata[index_in+i*width];\n  }\n\n  cg::sync(cta);\n\n  for (int i=0; i<TILE_DIM; i+=BLOCK_ROWS)\n  {\n    odata[index_out+i*height] = tile[threadIdx.x][threadIdx.y+i];\n  }\n}",
            "__global__ void transposeDiagonal(\n        float *__restrict__ odata,\n  const float *__restrict__ idata,\n  int width, int height)\n{\n  // Handle to thread block group\n  cg::thread_block cta = cg::this_thread_block();\n  __shared__ float tile[TILE_DIM][TILE_DIM+1];\n\n  int blockIdx_x, blockIdx_y;\n\n  // do diagonal reordering\n  if (width == height)\n  {\n    blockIdx_y = blockIdx.x;\n    blockIdx_x = (blockIdx.x+blockIdx.y)%gridDim.x;\n  }\n  else\n  {\n    int bid = blockIdx.x + gridDim.x*blockIdx.y;\n    blockIdx_y = bid%gridDim.y;\n    blockIdx_x = ((bid/gridDim.y)+blockIdx_y)%gridDim.x;\n  }\n\n  // from here on the code is same as previous kernel except blockIdx_x replaces blockIdx.x\n  // and similarly for y\n\n  int xIndex = blockIdx_x * TILE_DIM + threadIdx.x;\n  int yIndex = blockIdx_y * TILE_DIM + threadIdx.y;\n  int index_in = xIndex + (yIndex)*width;\n\n  xIndex = blockIdx_y * TILE_DIM + threadIdx.x;\n  yIndex = blockIdx_x * TILE_DIM + threadIdx.y;\n  int index_out = xIndex + (yIndex)*height;\n\n  for (int i=0; i<TILE_DIM; i+=BLOCK_ROWS)\n  {\n    tile[threadIdx.y+i][threadIdx.x] = idata[index_in+i*width];\n  }\n\n  cg::sync(cta);\n\n  for (int i=0; i<TILE_DIM; i+=BLOCK_ROWS)\n  {\n    odata[index_out+i*height] = tile[threadIdx.x][threadIdx.y+i];\n  }\n}",
            "__global__ void transposeFineGrained(\n        float *__restrict__ odata,\n  const float *__restrict__ idata,\n  int width, int height)\n{\n  // Handle to thread block group\n  cg::thread_block cta = cg::this_thread_block();\n  __shared__ float block[TILE_DIM][TILE_DIM+1];\n\n  int xIndex = blockIdx.x * TILE_DIM + threadIdx.x;\n  int yIndex = blockIdx.y * TILE_DIM + threadIdx.y;\n  int index = xIndex + (yIndex)*width;\n\n  for (int i=0; i < TILE_DIM; i += BLOCK_ROWS)\n  {\n    block[threadIdx.y+i][threadIdx.x] = idata[index+i*width];\n  }\n\n  cg::sync(cta);\n\n  for (int i=0; i < TILE_DIM; i += BLOCK_ROWS)\n  {\n    odata[index+i*height] = block[threadIdx.x][threadIdx.y+i];\n  }\n}",
            "__global__ void transposeCoarseGrained(\n        float *__restrict__ odata,\n  const float *__restrict__ idata,\n  int width, int height)\n{\n  // Handle to thread block group\n  cg::thread_block cta = cg::this_thread_block();\n  __shared__ float block[TILE_DIM][TILE_DIM+1];\n\n  int xIndex = blockIdx.x * TILE_DIM + threadIdx.x;\n  int yIndex = blockIdx.y * TILE_DIM + threadIdx.y;\n  int index_in = xIndex + (yIndex)*width;\n\n  xIndex = blockIdx.y * TILE_DIM + threadIdx.x;\n  yIndex = blockIdx.x * TILE_DIM + threadIdx.y;\n  int index_out = xIndex + (yIndex)*height;\n\n  for (int i=0; i<TILE_DIM; i += BLOCK_ROWS)\n  {\n    block[threadIdx.y+i][threadIdx.x] = idata[index_in+i*width];\n  }\n\n  cg::sync(cta);\n\n  for (int i=0; i<TILE_DIM; i += BLOCK_ROWS)\n  {\n    odata[index_out+i*height] = block[threadIdx.y+i][threadIdx.x];\n  }\n}"
        ]
    },
    "btree-cuda": {
        "/Users/gbolet/hecbench-roofline/src/btree-cuda/map/kernels/map_kernels.cu": [
            "__device__ __forceinline__ unsigned lane_id() {\n  return threadIdx.x & 0x1F;\n}\n\n__global__ void insert_keys(uint32_t* __restrict__ d_root,\n                            KeyT* __restrict__ d_keys,\n                            ValueT* __restrict__ d_values,\n                            SizeT num_keys,\n                            AllocatorT allocator) {\n  uint32_t tid = threadIdx.x + blockIdx.x * blockDim.x;\n  uint32_t laneId = lane_id();\n\n  KeyT myKey;\n  ValueT myValue;\n  bool to_insert = false;\n\n  if ((tid - laneId) >= num_keys)\n    return;\n\n  if (tid < num_keys) {\n    myKey = d_keys[tid] + 2;\n    myValue = d_values[tid] + 2;\n    to_insert = true;\n  }\n\n  warps::insertion_unit(to_insert, myKey, myValue, d_root, &allocator);\n}",
            "__device__ __forceinline__ unsigned lane_id() {\n  return threadIdx.x & 0x1F;\n}\n\n__global__ void init_btree(uint32_t* d_root, AllocatorT allocator) {\n  uint32_t laneId = lane_id();\n\n  uint32_t root_id;\n  if (laneId == 0)\n    root_id = allocator.allocate();\n\n  root_id = __shfl_sync(WARP_MASK, root_id, 0);\n\n  *d_root = root_id;\n  uint32_t* tree_root = allocator.getAddressPtr(root_id);\n\n  if (laneId < 2)\n    tree_root[laneId] = 1 - laneId;\n}",
            "__device__ __forceinline__ unsigned lane_id() {\n  return threadIdx.x & 0x1F;\n}\n\n__global__ void search_b_tree(uint32_t* __restrict__ d_root,\n                              KeyT* __restrict__ d_queries,\n                              ValueT* __restrict__ d_results,\n                              SizeT num_queries,\n                              AllocatorT allocator) {\n  uint32_t tid = threadIdx.x + blockIdx.x * blockDim.x;\n  uint32_t laneId = lane_id();\n  if ((tid - laneId) >= num_queries)\n    return;\n\n  uint32_t myQuery = 0;\n  uint32_t myResult = SEARCH_NOT_FOUND;\n  bool to_search = false;\n\n  if (tid < num_queries) {\n    myQuery = d_queries[tid] + 2;\n    to_search = true;\n  }\n\n  warps::search_unit(to_search, laneId, myQuery, myResult, d_root, &allocator);\n\n  if (tid < num_queries)\n    myResult = myResult ? myResult - 2 : myResult;\n  d_results[tid] = myResult;\n}",
            "__device__ __forceinline__ unsigned lane_id() {\n  return threadIdx.x & 0x1F;\n}\n\n__global__ void delete_b_tree(uint32_t* __restrict__ d_root,\n                              KeyT* __restrict__ d_queries,\n                              SizeT num_queries,\n                              AllocatorT allocator) {\n  uint32_t tid = threadIdx.x + blockIdx.x * blockDim.x;\n  uint32_t laneId = lane_id();\n  if ((tid - laneId) >= num_queries)\n    return;\n\n  KeyT myQuery = 0xFFFFFFFF;\n\n  if (tid < uint32_t(num_queries)) {\n    myQuery = d_queries[tid] + 2;\n  }\n\n  warps::delete_unit_bulk(laneId, myQuery, d_root, &allocator);\n}",
            "__device__ __forceinline__ unsigned lane_id() {\n  return threadIdx.x & 0x1F;\n}\n\n__global__ void range_b_tree(uint32_t* __restrict__ d_root,\n                             KeyT* __restrict__ d_queries_lower,\n                             KeyT* __restrict__ d_queries_upper,\n                             ValueT* __restrict__ d_range_results,\n                             SizeT num_queries,\n                             SizeT range_length,\n                             AllocatorT allocator) {\n  uint32_t tid = threadIdx.x + blockIdx.x * blockDim.x;\n  uint32_t laneId = lane_id();\n  if ((tid - laneId) >= num_queries)\n    return;\n\n  uint32_t lower_bound = 0;\n  uint32_t upper_bound = 0;\n  bool to_search = false;\n\n  if (tid < num_queries) {\n    lower_bound = d_queries_lower[tid] + 2;\n    upper_bound = d_queries_upper[tid] + 2;\n    to_search = true;\n  }\n\n  warps::range_unit(laneId,\n                    to_search,\n                    lower_bound,\n                    upper_bound,\n                    d_range_results,\n                    d_root,\n                    range_length,\n                    &allocator);\n}"
        ]
    },
    "interval-cuda": {
        "/Users/gbolet/hecbench-roofline/src/interval-cuda/gpu_interval.h": [
            "#define I complex_t(0.0, 1.0)\n\n\n#define P double\n\n\n#define T ((int)32)\n\n\n#define THREADS ((int)B*T)\n\n\ninline __device__ interval_gpu<T> div_negative(interval_gpu<T> const &x,\n                                               T const &yl) {\n  // assert(yu > 0);\n  if (x.lower() == 0 && x.upper() == 0) return x;\n\n  rounded_arith<T> rnd;\n  typedef interval_gpu<T> I;\n  const T &xl = x.lower();\n  const T &xu = x.upper();\n\n  if (xu < 0)\n    return I(rnd.div_down(xu, yl), rnd.pos_inf());\n  else if (xl < 0)\n    return I(rnd.neg_inf(), rnd.pos_inf());\n  else\n    return I(rnd.neg_inf(), rnd.div_up(xl, yl));\n}\n\ninline __device__ interval_gpu<T> div_non_zero(interval_gpu<T> const &x,\n                                               interval_gpu<T> const &y) {\n  rounded_arith<T> rnd;\n  typedef interval_gpu<T> I;\n  T xl, yl, xu, yu;\n\n  if (y.upper() < 0) {\n    xl = x.upper();\n    xu = x.lower();\n  } else {\n    xl = x.lower();\n    xu = x.upper();\n  }\n\n  if (x.upper() < 0) {\n    yl = y.lower();\n    yu = y.upper();\n  } else if (x.lower() < 0) {\n    if (y.upper() < 0) {\n      yl = y.upper();\n      yu = y.upper();\n    } else {\n      yl = y.lower();\n      yu = y.lower();\n    }\n  } else {\n    yl = y.upper();\n    yu = y.lower();\n  }\n\n  return I(rnd.div_down(xl, yl), rnd.div_up(xu, yu));\n}\n\ninline __device__ interval_gpu<T> div_positive(interval_gpu<T> const &x,\n                                               T const &yu) {\n  // assert(yu > 0);\n  if (x.lower() == 0 && x.upper() == 0) return x;\n\n  rounded_arith<T> rnd;\n  typedef interval_gpu<T> I;\n  const T &xl = x.lower();\n  const T &xu = x.upper();\n\n  if (xu < 0)\n    return I(rnd.neg_inf(), rnd.div_up(xu, yu));\n  else if (xl < 0)\n    return I(rnd.neg_inf(), rnd.pos_inf());\n  else\n    return I(rnd.div_down(xl, yu), rnd.pos_inf());\n}\n\ninline __device__ interval_gpu<T> div_zero_part1(interval_gpu<T> const &x,\n                                                 interval_gpu<T> const &y,\n                                                 bool &b) {\n  if (x.lower() == 0 && x.upper() == 0) {\n    b = false;\n    return x;\n  }\n\n  rounded_arith<T> rnd;\n  typedef interval_gpu<T> I;\n  const T &xl = x.lower();\n  const T &xu = x.upper();\n  const T &yl = y.lower();\n  const T &yu = y.upper();\n\n  if (xu < 0) {\n    b = true;\n    return I(rnd.neg_inf(), rnd.div_up(xu, yu));\n  } else if (xl < 0) {\n    b = false;\n    return I(rnd.neg_inf(), rnd.pos_inf());\n  } else {\n    b = true;\n    return I(rnd.neg_inf(), rnd.div_up(xl, yl));\n  }\n}\n\ninline __device__ interval_gpu<T> div_zero_part2(interval_gpu<T> const &x,\n                                                 interval_gpu<T> const &y) {\n  rounded_arith<T> rnd;\n  typedef interval_gpu<T> I;\n  const T &xl = x.lower();\n  const T &xu = x.upper();\n  const T &yl = y.lower();\n  const T &yu = y.upper();\n\n  if (xu < 0)\n    return I(rnd.div_down(xu, yl), rnd.pos_inf());\n  else\n    return I(rnd.div_down(xl, yu), rnd.pos_inf());\n}\n\ninline __device__ __host__ interval_gpu<T>::interval_gpu(T const &v)\n    : low(v), up(v) {}\n\ninline __host__ __device__ int4 make_int4(float4 a)\n{\n    return make_int4(int(a.x), int(a.y), int(a.z), int(a.w));\n}\n\ninline __host__ __device__ int4 abs(int4 v)\n{\n    return make_int4(abs(v.x), abs(v.y), abs(v.z), abs(v.w));\n}\n\n__device__\nT select (T a, T b, P c) {\n  return c ? b : a;\n}\n\ninline __device__ __host__ bool empty(interval_gpu<T> x) {\n  T hash = x.lower() + x.upper();\n  return (hash != hash);\n}\n\n__inline__ __device__ float f(unsigned int x, unsigned int y, unsigned int z)\n{\n  constexpr float d(2.0f / N);\n  float xf((int(x - Nd2)) * d);//[-1, 1)\n  float yf((int(z - Nd2)) * d);\n  float zf((int(z - Nd2)) * d);\n  return 1.f - 16.f * xf * yf * zf - 4.f * (xf * xf + yf * yf + zf * zf);\n}\n\n__device__\nuint median(uint x1, uint x2, uint x3) {\n\tif (x1 < x2) {\n\t\tif (x2 < x3) {\n\t\t\treturn x2;\n\t\t} else {\n      return select(x1, x3, x1 < x3);\n\t\t}\n\t} else { // x1 >= x2\n\t\tif (x1 < x3) {\n\t\t\treturn x1;\n\t\t} else { // x1 >= x3\n      return select(x2, x3, x2 < x3);\n\t\t}\n\t}\n}\n\ninline __device__ T width(interval_gpu<T> x) {\n  if (empty(x)) return 0;\n\n  rounded_arith<T> rnd;\n  return rnd.sub_up(x.upper(), x.lower());\n}\n\ninline __device__ interval_gpu<T> division_part1(interval_gpu<T> const &x,\n                                                 interval_gpu<T> const &y,\n                                                 bool &b) {\n  b = false;\n\n  if (y.lower() <= 0 && y.upper() >= 0)\n    if (y.lower() != 0)\n      if (y.upper() != 0)\n        return div_zero_part1(x, y, b);\n      else\n        return div_negative(x, y.lower());\n    else if (y.upper() != 0)\n      return div_positive(x, y.upper());\n    else\n      return interval_gpu<T>::empty();\n  else\n    return div_non_zero(x, y);\n}\n\ninline __device__ interval_gpu<T> division_part2(interval_gpu<T> const &x,\n                                                 interval_gpu<T> const &y,\n                                                 bool b = true) {\n  if (!b) return interval_gpu<T>::empty();\n\n  return div_zero_part2(x, y);\n}\n\n__device__ interval_gpu<T> fd(interval_gpu<T> const &x, int thread_id) {\n  typedef interval_gpu<T> I;\n  T alpha = -T(thread_id) / T(THREADS);\n  return I(2) * x + I(alpha - 2);\n}\n\ninline __device__ interval_gpu<T> intersect(interval_gpu<T> const &x,\n                                            interval_gpu<T> const &y) {\n  rounded_arith<T> rnd;\n  T const &l = rnd.max(x.lower(), y.lower());\n  T const &u = rnd.min(x.upper(), y.upper());\n\n  if (l <= u)\n    return interval_gpu<T>(l, u);\n  else\n    return interval_gpu<T>::empty();\n}\n\n__device__ bool is_minimal(interval_gpu<T> const &x, int thread_id) {\n  T const epsilon_x = 1e-6f;\n  T const epsilon_y = 1e-6f;\n  return !empty(x) && (width(x) <= epsilon_x * abs(median(x)) ||\n                       width(f(x, thread_id)) <= epsilon_y);\n}\n\n__device__ bool should_bisect(interval_gpu<T> const &x,\n                              interval_gpu<T> const &x1,\n                              interval_gpu<T> const &x2, T alpha) {\n  T wmax = alpha * width(x);\n  return (!empty(x1) && width(x1) > wmax) || (!empty(x2) && width(x2) > wmax);\n}\n\n__device__ void newton_interval(\n    global_stack<interval_gpu<T>, DEPTH_RESULT, THREADS> &result,\n    interval_gpu<T> const &ix0, int thread_id) {\n  typedef interval_gpu<T> I;\n  int const DEPTH_WORK = 128;\n\n  T const alpha = .99f;  // Threshold before switching to bisection\n\n  // Intervals to be processed\n  local_stack<I, DEPTH_WORK> work;\n\n  // We start with the whole domain\n  I ix = ix0;\n\n  while (true) {\n    // Compute (x - F({x})/F'(ix)) inter ix\n    // -> may yield 0, 1 or 2 intervals\n    T x = median(ix);\n    I iq = f(I(x), thread_id);\n    I id = fd(ix, thread_id);\n\n    bool has_part2;\n    I part1, part2 = I::empty();\n    part1 = division_part1(iq, id, has_part2);\n    part1 = intersect(I(x) - part1, ix);\n\n    if (has_part2) {\n      part2 = division_part2(iq, id);\n      part2 = intersect(I(x) - part2, ix);\n    }\n\n    // Do we have small-enough intervals?\n    if (is_minimal(part1, thread_id)) {\n      result.push(part1);\n      part1 = I::empty();\n    }\n\n    if (has_part2 && is_minimal(part2, thread_id)) {\n      result.push(part2);\n      part2 = I::empty();\n    }\n\n    if (should_bisect(ix, part1, part2, alpha)) {\n      // Not so good improvement\n      // Switch to bisection method for this step\n      part1 = I(ix.lower(), x);\n      part2 = I(x, ix.upper());\n      has_part2 = true;\n    }\n\n    if (!empty(part1)) {\n      // At least 1 solution\n      // We will compute part1 next\n      ix = part1;\n\n      if (has_part2 && !empty(part2)) {\n        // 2 solutions\n        // Save the second solution for later\n        work.push(part2);\n      }\n    } else if (has_part2 && !empty(part2)) {\n      // 1 solution\n      // Work on that next\n      ix = part2;\n    } else {\n      // No solution\n      // Do we still have work to do in the stack?\n      if (work.empty())  // If not, we are done\n        break;\n      else\n        ix = work.pop();  // Otherwise, pick an interval to work on\n    }\n  }\n}\n\n__device__ void newton_interval_naive(\n    global_stack<interval_gpu<T>, DEPTH_RESULT, THREADS> &result,\n    interval_gpu<T> const &ix0, int thread_id) {\n  typedef interval_gpu<T> I;\n  int const DEPTH_WORK = 128;\n  T const alpha = .99f;  // Threshold before switching to bisection\n\n  // Intervals to be processed\n  local_stack<I, DEPTH_WORK> work;\n\n  // We start with the whole domain\n  work.push(ix0);\n\n  while (!work.empty()) {\n    I ix = work.pop();\n\n    if (is_minimal(ix, thread_id)) {\n      result.push(ix);\n    } else {\n      // Compute (x - F({x})/F'(ix)) inter ix\n      // -> may yield 0, 1 or 2 intervals\n      T x = median(ix);\n      I iq = f(I(x), thread_id);\n      I id = fd(ix, thread_id);\n\n      bool has_part2;\n      I part1, part2 = I::empty();\n      part1 = division_part1(iq, id, has_part2);\n      part1 = intersect(I(x) - part1, ix);\n\n      if (has_part2) {\n        part2 = division_part2(iq, id);\n        part2 = intersect(I(x) - part2, ix);\n      }\n\n      if (should_bisect(ix, part1, part2, alpha)) {\n        // Not so good improvement\n        // Switch to bisection method for this step\n        part1 = I(ix.lower(), x);\n        part2 = I(x, ix.upper());\n        has_part2 = true;\n      }\n\n      if (!empty(part1)) {\n        work.push(part1);\n      }\n\n      if (has_part2 && !empty(part2)) {\n        work.push(part2);\n      }\n    }\n  }\n}\n\n__global__\nvoid test_interval_newton(interval_gpu<T> *buffer,\n                          int *nresults,\n                          interval_gpu<T> i,\n                          int implementation_choice)\n{\n  int thread_id = blockIdx.x * BLOCK_SIZE + threadIdx.x;\n  typedef interval_gpu<T> I;\n\n  // Intervals to return\n  global_stack<I, DEPTH_RESULT, THREADS> result(buffer, thread_id);\n\n  switch (implementation_choice) {\n    case 0:\n      newton_interval_naive<T, THREADS>(result, i, thread_id);\n      break;\n\n    case 1:\n      newton_interval<T, THREADS>(result, i, thread_id);\n      break;\n\n    default:\n      newton_interval_naive<T, THREADS>(result, i, thread_id);\n  }\n\n  nresults[thread_id] = result.size();\n}"
        ]
    },
    "debayer-cuda": {
        "/Users/gbolet/hecbench-roofline/src/debayer-cuda/kernel.h": [
            "__global__ void malvar_he_cutler_demosaic (\n  const uint height,\n  const uint width,\n  const uchar *__restrict__ input_image_p,\n  const uint input_image_pitch, \n        uchar *__restrict__ output_image_p,\n  const uint output_image_pitch, \n  const int bayer_pattern )\n{\n  __shared__ LDSPixelT apron[apron_rows * apron_cols];\n\n  const uint tile_col_blocksize = blockDim.x;\n  const uint tile_row_blocksize = blockDim.y;\n  const uint tile_col_block = blockIdx.x;\n  const uint tile_row_block = blockIdx.y;\n  const uint tile_col = threadIdx.x;\n  const uint tile_row = threadIdx.y;\n  const uint g_c = blockDim.x * blockIdx.x + threadIdx.x; \n  const uint g_r = blockDim.y * blockIdx.y + threadIdx.y;\n  const bool valid_pixel_task = (g_r < height) & (g_c < width);\n\n  const uint tile_flat_id = tile_row * tile_cols + tile_col;\n  for(uint apron_fill_task_id = tile_flat_id; apron_fill_task_id < n_apron_fill_tasks; apron_fill_task_id += n_tile_pixels){\n    const uint apron_read_row = apron_fill_task_id / apron_cols;\n    const uint apron_read_col = apron_fill_task_id % apron_cols;\n    const int ag_c = ((int)(apron_read_col + tile_col_block * tile_col_blocksize)) - shalf_ksize;\n    const int ag_r = ((int)(apron_read_row + tile_row_block * tile_row_blocksize)) - shalf_ksize;\n\n    apron[apron_read_row * apron_cols + apron_read_col] = tex2D_at(PixelT, input_image, ag_r, ag_c);\n  }\n\n  __syncthreads();\n\n  //valid tasks read from [half_ksize, (tile_rows|tile_cols) + kernel_size - 1)\n  const uint a_c = tile_col + half_ksize;\n  const uint a_r = tile_row + half_ksize;\n  assert_val(a_c >= half_ksize && a_c < apron_cols - half_ksize, a_c);\n  assert_val(a_r >= half_ksize && a_r < apron_rows - half_ksize, a_r);\n\n  //note the following formulas are col, row convention and uses i,j - this is done to preserve readability with the originating paper\n  const uint i = a_c;\n  const uint j = a_r;\n#define F(_i, _j) apron_pixel((_j), (_i))\n\n  const int Fij = F(i,j);\n  //symmetric 4,2,-1 response - cross\n  const int R1 = (4*F(i, j) + 2*(F(i-1,j) + F(i,j-1) + F(i+1,j) + F(i,j+1)) - \n                    F(i-2,j) - F(i+2,j) - F(i,j-2) - F(i,j+2)) / 8;\n\n  //left-right symmetric response - with .5,1,4,5 - theta\n  const int R2 = (\n      8*(F(i-1,j) + F(i+1,j)) +10*F(i,j) + F(i,j-2) + F(i,j+2)\n      - 2*((F(i-1,j-1) + F(i+1,j-1) + F(i-1,j+1) + F(i+1,j+1)) + F(i-2,j) + F(i+2,j))) / 16;\n\n  //top-bottom symmetric response - with .5,1,4,5 - phi\n  const int R3 = (\n      8*(F(i,j-1) + F(i,j+1)) +10*F(i,j) + F(i-2,j) + F(i+2,j)\n      - 2*((F(i-1,j-1) + F(i+1,j-1) + F(i-1,j+1) + F(i+1,j+1)) + F(i,j-2) + F(i,j+2))) / 16;\n  //symmetric 3/2s response - checker\n  const int R4 = (\n      12*F(i,j) - 3*(F(i-2,j) + F(i+2,j) + F(i,j-2) + F(i,j+2))\n      + 4*(F(i-1,j-1) + F(i+1,j-1) + F(i-1,j+1) + F(i+1,j+1))) / 16;\n\n  const int G_at_red_or_blue = R1;\n  const int R_at_G_in_red = R2;\n  const int B_at_G_in_blue = R2;\n  const int R_at_G_in_blue = R3;\n  const int B_at_G_in_red = R3;\n  const int R_at_B = R4;\n  const int B_at_R = R4;\n\n#undef F\n#undef j\n#undef i\n  //RGGB -> RedXY = (0, 0), GreenXY1 = (1, 0), GreenXY2 = (0, 1), BlueXY = (1, 1)\n  //GRBG -> RedXY = (1, 0), GreenXY1 = (0, 0), GreenXY2 = (1, 1), BlueXY = (0, 1)\n  //GBRG -> RedXY = (0, 1), GreenXY1 = (0, 0), GreenXY2 = (1, 1), BlueXY = (1, 0)\n  //BGGR -> RedXY = (1, 1), GreenXY1 = (1, 0), GreenXY2 = (0, 1), BlueXY = (0, 0)\n  const int r_mod_2 = g_r & 1;\n  const int c_mod_2 = g_c & 1;\n#define is_rggb (bayer_pattern == RGGB)\n#define is_grbg (bayer_pattern == GRBG)\n#define is_gbrg (bayer_pattern == GBRG)\n#define is_bggr (bayer_pattern == BGGR)\n\n  const int red_col = is_grbg | is_bggr;\n  const int red_row = is_gbrg | is_bggr;\n  const int blue_col = 1 - red_col;\n  const int blue_row = 1 - red_row;\n\n  const int in_red_row = r_mod_2 == red_row;\n  const int in_blue_row = r_mod_2 == blue_row;\n  const int is_red_pixel = (r_mod_2 == red_row) & (c_mod_2 == red_col);\n  const int is_blue_pixel = (r_mod_2 == blue_row) & (c_mod_2 == blue_col);\n  const int is_green_pixel = !(is_red_pixel | is_blue_pixel);\n  assert(is_green_pixel + is_blue_pixel + is_red_pixel == 1);\n  assert(in_red_row + in_blue_row == 1);\n\n  //at R locations: R is original\n  //at B locations it is the 3/2s symmetric response\n  //at G in red rows it is the left-right symmmetric with 4s\n  //at G in blue rows it is the top-bottom symmetric with 4s\n  const RGBPixelBaseT R = output_pixel_cast(\n      Fij * is_red_pixel +\n      R_at_B * is_blue_pixel +\n      R_at_G_in_red * (is_green_pixel & in_red_row) +\n      R_at_G_in_blue * (is_green_pixel & in_blue_row)\n      );\n  //at B locations: B is original\n  //at R locations it is the 3/2s symmetric response\n  //at G in red rows it is the top-bottom symmmetric with 4s\n  //at G in blue rows it is the left-right symmetric with 4s\n  const RGBPixelBaseT B = output_pixel_cast(\n      Fij * is_blue_pixel +\n      B_at_R * is_red_pixel +\n      B_at_G_in_red * (is_green_pixel & in_red_row) +\n      B_at_G_in_blue * (is_green_pixel & in_blue_row)\n      );\n  //at G locations: G is original\n  //at R locations: symmetric 4,2,-1\n  //at B locations: symmetric 4,2,-1\n  const RGBPixelBaseT G = output_pixel_cast(Fij * is_green_pixel + G_at_red_or_blue * (!is_green_pixel));\n\n  if(valid_pixel_task){\n    RGBPixelT output;\n#if OUTPUT_CHANNELS == 3 || OUTPUT_CHANNELS == 4\n    output.x = R;\n    output.y = G;\n    output.z = B;\n#if OUTPUT_CHANNELS == 4\n    output.w = ALPHA_VALUE;\n#endif\n#else\n#error \"Unsupported number of output channels\"\n#endif\n    pixel_at(RGBPixelT, output_image, g_r, g_c) = output;\n  }\n}"
        ]
    },
    "cross-cuda": {
        "/Users/gbolet/hecbench-roofline/src/cross-cuda/main.cu": [
            "#define T ((int)32)\n\n\n__global__ void cross_kernel(\n    int numel,\n          T* out,\n    const T* x1,\n    const T* x2,\n    StrideType ostride,\n    StrideType x1stride,\n    StrideType x2stride)\n{\n  for (int i = blockIdx.x * blockDim.x + threadIdx.x;\n           i < numel; i += blockDim.x * gridDim.x) {\n\n    auto* out_row = out + 3*i;\n    const auto* x1_row = x1 + 3*i;\n    const auto* x2_row = x2 + 3*i;\n\n    const T val0 = (x1_row[1 * x1stride] * x2_row[2 * x2stride] -\n                    x1_row[2 * x1stride] * x2_row[1 * x2stride]);\n\n    const T val1 = (x1_row[2 * x1stride] * x2_row[0 * x2stride] -\n                    x1_row[0 * x1stride] * x2_row[2 * x2stride]);\n\n    const T val2 = (x1_row[0 * x1stride] * x2_row[1 * x2stride] -\n                    x1_row[1 * x1stride] * x2_row[0 * x2stride]);\n\n    out_row[0 * ostride] = val0;\n    out_row[1 * ostride] = val1;\n    out_row[2 * ostride] = val2;\n  }\n}",
            "#define T ((int)32)\n\n\n__global__ void cross2_kernel(\n    int numel,\n          T* out,\n    const T* x1,\n    const T* x2,\n    StrideType ostride,\n    StrideType x1stride,\n    StrideType x2stride)\n{\n  for (int i = blockIdx.x * blockDim.x + threadIdx.x;\n           i < numel; i += blockDim.x * gridDim.x) {\n\n    auto* out_row = out + 3*i;\n    const auto* x1_row = x1 + 3*i;\n    const auto* x2_row = x2 + 3*i;\n\n    const T x1_c0 = x1_row[0 * x1stride];\n    const T x1_c1 = x1_row[1 * x1stride];\n    const T x1_c2 = x1_row[2 * x1stride];\n    const T x2_c0 = x2_row[0 * x2stride];\n    const T x2_c1 = x2_row[1 * x2stride];\n    const T x2_c2 = x2_row[2 * x2stride];\n\n    const T val0 = x1_c1 * x2_c2 - x1_c2 * x2_c1 ;\n\n    const T val1 = x1_c2 * x2_c0 - x1_c0 * x2_c2 ;\n\n    const T val2 = x1_c0 * x2_c1 - x1_c1 * x2_c0 ;\n\n    out_row[0 * ostride] = val0;\n    out_row[1 * ostride] = val1;\n    out_row[2 * ostride] = val2;\n  }\n}",
            "#define T ((int)32)\n\n\n__global__ void cross3_kernel(\n    int numel,\n          T* out,\n    const T* x1,\n    const T* x2)\n{\n  for (int i = blockIdx.x * blockDim.x + threadIdx.x;\n           i < numel; i += blockDim.x * gridDim.x) {\n\n    auto* out_row = out + 3*i;\n    const auto* x1_row = x1 + 3*i;\n    const auto* x2_row = x2 + 3*i;\n\n    const T x1_c0 = x1_row[0];\n    const T x1_c1 = x1_row[1];\n    const T x1_c2 = x1_row[2];\n    const T x2_c0 = x2_row[0];\n    const T x2_c1 = x2_row[1];\n    const T x2_c2 = x2_row[2];\n\n    const T val0 = x1_c1 * x2_c2 - x1_c2 * x2_c1 ;\n\n    const T val1 = x1_c2 * x2_c0 - x1_c0 * x2_c2 ;\n\n    const T val2 = x1_c0 * x2_c1 - x1_c1 * x2_c0 ;\n\n    out_row[0] = val0;\n    out_row[1] = val1;\n    out_row[2] = val2;\n  }\n}"
        ]
    },
    "shuffle-cuda": {
        "/Users/gbolet/hecbench-roofline/src/shuffle-cuda/main.cu": [
            "__global__ void bcast_shfl_sg8(const int arg, int *out) {\n  int value = ((threadIdx.x & 0x7) == 0) ? arg : 0;\n  // Synchronize all threads in warp, and get \"value\" from lane 0\n  int out_v = __shfl( value, 0); \n  size_t oi = blockDim.x * blockIdx.x + threadIdx.x;\n  out[oi] = out_v;\n}",
            "__global__ void bcast_shfl_xor_sg8(int *out) {\n  int value = (threadIdx.x & 0x7);\n  for (int mask = 1; mask < 0x7; mask *= 2)\n    value += __shfl_xor(value, mask);\n  size_t oi = blockDim.x * blockIdx.x + threadIdx.x;\n  out[oi] = value;\n}",
            "__global__ void bcast_shfl_sg16(const int arg, int *out) {\n  int value = ((threadIdx.x & 0xf) == 0) ? arg : 0;\n  // Synchronize all threads in warp, and get \"value\" from lane 0\n  int out_v = __shfl( value, 0); \n  size_t oi = blockDim.x * blockIdx.x + threadIdx.x;\n  out[oi] = out_v;\n}",
            "__global__ void bcast_shfl_xor_sg16(int *out) {\n  int value = (threadIdx.x & 0xf);\n  for (int mask = 1; mask < 0xf; mask *= 2)\n    value += __shfl_xor(value, mask);\n  size_t oi = blockDim.x * blockIdx.x + threadIdx.x;\n  out[oi] = value;\n}",
            "__global__ void bcast_shfl_sg32(const int arg, int *out) {\n  int value = ((threadIdx.x & 0x1f) == 0) ? arg : 0;\n  // Synchronize all threads in warp, and get \"value\" from lane 0\n  int out_v = __shfl( value, 0); \n  size_t oi = blockDim.x * blockIdx.x + threadIdx.x;\n  out[oi] = out_v;\n}",
            "__global__ void bcast_shfl_xor_sg32(int *out) {\n  int value = (threadIdx.x & 0x1f);\n  for (int mask = 1; mask < 0x1f; mask *= 2)\n    value += __shfl_xor(value, mask);\n  size_t oi = blockDim.x * blockIdx.x + threadIdx.x;\n  out[oi] = value;\n}",
            "__global__ void transpose_shfl(float* out, const float* in) {\n  unsigned b_start = blockDim.x * blockIdx.x;\n  unsigned b_offs = b_start + threadIdx.x;\n  unsigned s_offs = blockDim.x - threadIdx.x - 1;\n  float val = in[b_offs];\n  out[b_offs] = __shfl(val, s_offs);\n}"
        ]
    },
    "atomicReduction-cuda": {
        "/Users/gbolet/hecbench-roofline/src/atomicReduction-cuda/kernels.h": [
            "__global__ void atomic_reduction(int *in, int* out, int arrayLength) {\n  int sum=0;\n  int idx = blockIdx.x*blockDim.x+threadIdx.x;\n  for(int i= idx;i<arrayLength;i+=blockDim.x*gridDim.x) {\n    sum+=in[i];\n  }\n  atomicAdd(out,sum);\n}",
            "__global__ void atomic_reduction_v2(int *in, int* out, int arrayLength) {\n  int sum=0;\n  int idx = blockIdx.x*blockDim.x+threadIdx.x;\n  for(int i= idx*2;i<arrayLength;i+=blockDim.x*gridDim.x*2) {\n    sum+=in[i] + in[i+1];\n  }\n  atomicAdd(out,sum);\n}",
            "__global__ void atomic_reduction_v4(int *in, int* out, int arrayLength) {\n  int sum=0;\n  int idx = blockIdx.x*blockDim.x+threadIdx.x;\n  for(int i= idx*4;i<arrayLength;i+=blockDim.x*gridDim.x*4) {\n    sum+=in[i] + in[i+1] + in[i+2] + in[i+3];\n  }\n  atomicAdd(out,sum);\n}",
            "__global__ void atomic_reduction_v8(int *in, int* out, int arrayLength) {\n  int sum=0;\n  int idx = blockIdx.x*blockDim.x+threadIdx.x;\n  for(int i= idx*8;i<arrayLength;i+=blockDim.x*gridDim.x*8) {\n    sum+=in[i] + in[i+1] + in[i+2] + in[i+3] +in[i+4] +in[i+5] +in[i+6] +in[i+7];\n  }\n  atomicAdd(out,sum);\n}",
            "__global__ void atomic_reduction_v16(int *in, int* out, int arrayLength) {\n  int sum=0;\n  int idx = blockIdx.x*blockDim.x+threadIdx.x;\n  for(int i= idx*16;i<arrayLength;i+=blockDim.x*gridDim.x*16) {\n    sum+=in[i] + in[i+1] + in[i+2] + in[i+3] +in[i+4] +in[i+5] +in[i+6] +in[i+7]\n      +in[i+8] +in[i+9] +in[i+10] +in[i+11] +in[i+12] +in[i+13] +in[i+14] +in[i+15] ;\n  }\n  atomicAdd(out,sum);\n}"
        ]
    },
    "pso-cuda": {
        "/Users/gbolet/hecbench-roofline/src/pso-cuda/kernel_gpu.cu": [
            "__global__\nvoid kernelUpdateParticle(float *__restrict__ positions,\n                          float *__restrict__ velocities,\n                          const float *__restrict__ pBests,\n                          const float *__restrict__ gBest,\n                          const int p,\n                          const float rp,\n                          const float rg)\n{\n  int i=blockIdx.x*blockDim.x+threadIdx.x;\n  if (i >= p*DIM) return;\n\n  velocities[i]=OMEGA*velocities[i]+\n                c1*rp*(pBests[i]-positions[i])+\n                c2*rg*(gBest[i%DIM]-positions[i]);\n  positions[i]+=velocities[i];\n}",
            "__device__ float fitness_function(float x[])\n{\n  float y1 = F(x[0]);\n  float yn = F(x[DIM-1]);\n  float res = powf(sinf(phi*y1), 2.f) + powf(yn-1, 2.f);\n\n  for(int i = 0; i < DIM-1; i++)\n  {\n    float y = F(x[i]);\n    float yp = F(x[i+1]);\n    res += powf(y-1.f, 2.f) * (1.f + 10.f * powf(sinf(phi*yp), 2.f));\n  }\n\n  return res;\n}\n\n__global__\nvoid kernelUpdatePBest(const float *__restrict__ positions,\n                             float *__restrict__ pBests,\n                             float *__restrict__ gBest,\n                       const int p)\n{\n  int i=blockIdx.x*blockDim.x+threadIdx.x;\n  if (i >= p) return;\n  i = i*DIM;\n\n  float tempParticle1[DIM];\n  float tempParticle2[DIM];\n\n  for(int j=0;j<DIM;j++)\n  {\n    tempParticle1[j]=positions[i+j];\n    tempParticle2[j]=pBests[i+j];\n  }\n\n  if(fitness_function(tempParticle1)<fitness_function(tempParticle2))\n  {\n    for(int j=0;j<DIM;j++)\n      pBests[i+j]=tempParticle1[j];\n\n    if(fitness_function(tempParticle1)<130.f) //fitness_function(gBest))\n    {\n      for(int j=0;j<DIM;j++) {\n        atomicAdd(gBest+j,tempParticle1[j]);\n      }\n    }\n  }\n}"
        ]
    },
    "dense-embedding-cuda": {
        "/Users/gbolet/hecbench-roofline/src/dense-embedding-cuda/main.cu": [
            "#define T ((int)32)\n\n\n__global__ void dense_esuhm(\n    const T* input,\n    const T* dense,\n          T* output,\n    int embedding_dim,\n    const int* offset)\n{\n  const int batch_idx  = blockIdx.x; // each batch is handled by a block\n  const int grain_size = blockDim.x;\n  const int tid = threadIdx.x;\n  const int range = offset[batch_idx + 1] - offset[batch_idx];\n  for (int idx = tid; idx < embedding_dim; idx += grain_size) {\n    const T dense_elem = dense[batch_idx * embedding_dim + idx];\n    for (int nested_idx = idx; nested_idx < range; nested_idx += embedding_dim) {\n      output[offset[batch_idx] + nested_idx] = input[offset[batch_idx] + nested_idx] + dense_elem;\n    }\n  }\n}",
            "#define T ((int)32)\n\n\n__global__ void dense_esuhm2(\n    const T* input,\n    const T* dense,\n          T* output,\n    int embedding_dim,\n    const int* offset)\n{\n  const int batch_idx  = blockIdx.x;\n  const int start = offset[batch_idx];\n  const int range = offset[batch_idx + 1] - start;\n  for (int idx = threadIdx.x; idx < embedding_dim; idx += blockDim.x) {\n    const T dense_elem = dense[batch_idx * embedding_dim + idx];\n    for (int nested_idx = idx; nested_idx < range; nested_idx += embedding_dim) {\n      output[start + nested_idx] = input[start + nested_idx] + dense_elem;\n    }\n  }\n}"
        ]
    },
    "flame-cuda": {
        "/Users/gbolet/hecbench-roofline/src/flame-cuda/kernels.cu": [
            "inline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\ninline __host__ __device__ float4 expf(float4 v)\n{\n    return make_float4(expf(v.x), expf(v.y), expf(v.z), expf(v.w));\n}\n\ninline __host__ __device__ float4 fmodf(float4 a, float4 b)\n{\n    return make_float4(fmodf(a.x, b.x), fmodf(a.y, b.y), fmodf(a.z, b.z), fmodf(a.w, b.w));\n}\n\ninline __host__ __device__ float4 sqrtf(float4 v)\n{\n    return make_float4(sqrtf(v.x), sqrtf(v.y), sqrtf(v.z), sqrtf(v.w));\n}\n\n__device__ void\naffine_transform(float2 &p, const float *params)\n{\n  float2 tmp;\n  tmp.x = params[0] * p.x + params[1] * p.y + params[2];\n  tmp.y = params[3] * p.x + params[4] * p.y + params[5];\n\n  p.x = tmp.x;\n  p.y = tmp.y;\n}\n\n__device__ void\nsierpinski(float2 &p, int idx)\n{\n  p.x = p.x / 2.0f;\n  p.y = p.y / 2.0f;\n  switch(idx % 3) {\n    case 0:\n      break;\n    case 1:\n      p.x += 0.5f;\n      break;\n    case 2:\n      p.y += 0.5f;\n      break;\n  }\n}\n\n__device__ int\nget_function_idx(int idx, const ConstMemParams &params)\n{\n  int func;\n  int start = 0, end = NUM_FUNCTIONS;\n\n  /* use warp size granularity, decreases branch divergence */\n  idx &= ~31;\n\n  for(int i = 0; i < 6; i++) {\n    func = (start + end) / 2;\n    if(params.thread_function_mapping[func] <= idx)\n      start = func + 1;\n    else\n      end = func;\n  }\n\n  return func;\n}\n\n__device__ void\niteration_fractal_flame(float2 &p, \n    float &color,\n    int idx,\n    int threadidx,\n    const float *random_numbers,\n    const ConstMemParams &params)\n{\n\n#define A params.pre_transform_params[idx][0]\n#define B params.pre_transform_params[idx][1]\n#define C params.pre_transform_params[idx][2]\n#define D params.pre_transform_params[idx][3]\n#define E params.pre_transform_params[idx][4]\n#define F params.pre_transform_params[idx][5]\n\n#define RANDOM(i) random_numbers[\\\n  (threadidx + (params.frame_counter << 7) * i)\\\n  & (NUM_RANDOMS - 1)]\n\n  switch(params.enable_sierpinski) {\n    default:\n    case 0:\n      break;\n    case 1:\n      sierpinski(p, threadidx);\n      break;\n    case 2:\n      sierpinski(p, idx);\n      break;\n  }\n\n  affine_transform(p, &params.pre_transform_params[idx][0]);\n\n  float rad_square = p.x * p.x + p.y * p.y,\n        radius     = sqrtf(rad_square),\n        inv_radius = 1.0f / radius,\n        theta      = atan2f(p.x, p.y),\n        phi        = atan2f(p.y, p.x);\n\n  float2 point_out;\n  point_out.x = 0.0f;\n  point_out.y = 0.0f;\n\n  const VariationParameter *vp = &params.variation_parameters[idx][0];\n\n  for(int i = 0; i < VARIATIONS_PER_FUNCTION; i++) {\n    if(fabsf(vp[i].factor) < 0.01f)\n      continue;\n    float2 point;\n    switch(vp[i].idx) {\n      /* {{{ Variations */\n      case 0:\n        point.x = p.x;\n        point.y = p.y;\n        break;\n      case 1:\n        point.x = sinf(p.x);\n        point.y = sinf(p.y);\n        break;\n      case 2:\n        point.x = p.x / rad_square;\n        point.y = p.y / rad_square;\n        break;\n      case 3:\n        point.x = p.x * sinf(rad_square) - p.y * cosf(rad_square);\n        point.y = p.x * cosf(rad_square) + p.y * sinf(rad_square);\n        break;\n      case 4:\n        point.x = (p.x - p.y) * (p.x + p.y) * inv_radius;\n        point.y = 2.0f * p.x * p.y * inv_radius;\n        break;\n      case 5:\n        point.x = theta / M_PI;\n        point.y = radius - 1.0f;\n        break;\n      case 6:\n        point.x = radius * sinf(theta + radius);\n        point.y = radius * cosf(theta - radius);\n        break;\n      case 7:\n        point.x = radius * sinf(theta * radius);\n        point.y = -radius * cosf(theta * radius);\n        break;\n      case 8: {\n          float pi_r = M_PI * radius,\n                theta_pi = theta / M_PI;\n          point.x = theta_pi * sinf(pi_r);\n          point.y = theta_pi * cosf(pi_r);\n          break;\n        }\n      case 9:\n        point.x = radius * (cosf(theta) + sinf(radius));\n        point.y = radius * (sinf(theta) - cosf(radius));\n        break;\n      case 10:\n        point.x = sinf(theta) / radius;\n        point.y = radius * cosf(theta);\n        break;\n      case 11:\n        point.x = sinf(theta) * cosf(radius);\n        point.y = cosf(theta) * sinf(radius);\n        break;\n      case 12: {\n           float p0 = powf(sinf(theta + radius), 3);\n           float p1 = powf(sinf(theta - radius), 3);\n           point.x = radius * (p0 + p1);\n           point.y = radius * (p0 - p1);\n           break;\n         }\n      case 13: {\n           float sqrt_r = sqrtf(radius);\n           float omega = RANDOM(0) > 0.5f ? 0.0f : M_PI;\n           point.x = sqrt_r * cosf(theta / 2.0f + omega);\n           point.y = sqrt_r * sinf(theta / 2.0f + omega);\n           break;\n         }\n      case 14: {\n           switch(((p.x >= 0.0f) << 1) | (p.y >= 0.0f)) {\n             case 3: /* x >= 0, y >= 0 */\n               point.x = p.x;\n               point.y = p.y;\n               break;\n             case 1: /* x < 0, y >= 0 */\n               point.x = 2.0f * p.x;\n               point.y = p.y;\n               break;\n             case 2: /* x >= 0, y < 0 */\n               point.x = p.x;\n               point.y = p.y / 2.0f;\n               break;\n             case 0:\n               point.x = 2.0f * p.x;\n               point.y = p.y / 2.0f;\n               break;\n           }\n           break;\n         }\n      case 15:\n         point.x = p.x + B * sinf(p.y / (C * C));\n         point.y = p.y + E * sinf(p.x / (F * F));\n         break;\n      case 16:\n         point.x = point.y = 2.0f / (radius + 1.0f);\n         point.x *= p.y;\n         point.y *= p.x;\n         break;\n      case 17:\n         point.x = p.x + C * sinf(tanf(3.0f * p.y));\n         point.y = p.y + F * sinf(tanf(3.0f * p.x));\n         break;\n      case 18:\n         point.x = point.y = expf(p.x - 1.0f);\n         point.x *= cosf(M_PI * p.y);\n         point.y *= sinf(M_PI * p.y);\n         break;\n      case 19:\n         point.x = point.y = powf(radius, sinf(theta));\n         point.x *= cosf(theta);\n         point.y *= sinf(theta);\n         break;\n      case 20:\n         point.x = cosf(M_PI * p.x) * coshf(p.y);\n         point.y = -sinf(M_PI * p.x) * sinhf(p.y);\n         break;\n      case 21:\n         point.x = point.y = fmodf(radius + C * C, 2.0f * C * C) - C * C\n           + radius * (1.0f - C * C);\n         point.x *= cosf(theta);\n         point.y *= sinf(theta);\n         break;\n      case 22: {\n           float t = M_PI * C * C;\n           if(fmodf(theta + F, t) > t / 2.0f) {\n             point.x = radius * cosf(theta - t / 2.0f);\n             point.y = radius * sinf(theta - t / 2.0f);\n           }\n           else {\n             point.x = radius * cosf(theta + t / 2.0f);\n             point.y = radius * sinf(theta + t / 2.0f);\n           }\n           break;\n         }\n      case 23:\n      case 24:\n      case 25:\n      case 26:\n         /* TODO */\n         break;\n\n      case 27:\n         point.x = point.y = 2.0f / (radius + 1.0f);\n         point.x *= p.x;\n         point.y *= p.y;\n         break;\n      case 28:\n         point.x = point.y = 4.0f / (radius * radius + 4.0f);\n         point.x *= p.x;\n         point.y *= p.y;\n         break;\n      case 29:\n         point.x = sinf(p.x);\n         point.y = p.y;\n         break;\n      case 30:\n         /* TODO */\n         break;\n      case 31: {\n           float t = RANDOM(0) * 2.0f * M_PI;\n           point.x = RANDOM(1) * point.x * cosf(t);\n           point.y = RANDOM(1) * point.y * sinf(t);\n           break;\n         }\n      case 32:\n      case 33:\n         break;\n      case 34: {\n           float t = RANDOM(0) * 2.0f * M_PI;\n           point.x = RANDOM(1) * cosf(t);\n           point.y = RANDOM(1) * sinf(t);\n           break;\n         }\n      case 35:\n      case 36:\n      case 37:\n      case 38:\n      case 39:\n      case 40:\n         break;\n      case 41: {\n           float t = RANDOM(0) * M_PI * vp[i].factor;\n           point.x = sinf(t);\n           point.y = point.x * point.x / cosf(t);\n           break;\n         }\n      case 42:\n         point.x = sinf(p.x) / cosf(p.y);\n         point.y = tanf(p.y);\n         break;\n      case 43:\n         point.x = RANDOM(0) - 0.5f;\n         point.y = RANDOM(1) - 0.5f;\n         break;\n      case 44:\n         point.x = point.y = vp[i].factor * tanf(RANDOM(0) * M_PI\n             * vp[i].factor) / (radius * radius);\n         point.x *= cosf(p.x);\n         point.y *= sinf(p.y);\n         break;\n      case 45: {\n           float t = RANDOM(0) * radius * vp[i].factor;\n           point.x = p.x * (cosf(t) + sinf(t));\n           point.y = p.x * (cosf(t) - sinf(t));\n           break;\n         }\n      case 46:\n         point.x = p.x;\n         point.y = 1.0f / (vp[i].factor * cosf(radius * vp[i].factor));\n         break;\n      case 47: {\n           float f = RANDOM(0) * radius * vp[i].factor;\n           float t = log10f(f * f) + cosf(f);\n           point.x = p.x * t;\n           point.y = t - M_PI * sinf(f);\n           break;\n         }\n      case 48:\n         point.x = point.y = fabsf(1.0f / (p.x * p.x - p.y * p.y));\n         point.x *= p.x;\n         point.y *= p.y;\n         break;\n         /* }}} end variations */\n    } /* switch */\n\n    point_out.x += vp[i].factor * point.x;\n    point_out.y += vp[i].factor * point.y;\n\n  }\n  const float &col = params.function_colors[idx];\n\n  color = (color + col) / 2.0f;\n\n  p.x = point_out.x;\n  p.y = point_out.y;\n\n  affine_transform(p, &params.post_transform_params[idx][0]);\n\n#undef A\n#undef B\n#undef C\n#undef D\n#undef E\n#undef F\n#undef RANDOM\n\n}\n\n__global__ void\nkernel_initialize(short2 *short_points, \n                  short *colors, \n                  const unsigned short *perms, \n                  const int perm_num,\n                  float2 *start_pos,\n                  const float *random_numbers,\n                  const ConstMemParams params)\n{\n  int idx = blockDim.x * blockIdx.x + threadIdx.x;\n\n  int function = get_function_idx(idx, params);\n\n\n  int perm_idx = perms[NUM_THREADS * perm_num + idx];\n  float2 point = start_pos[perm_idx];\n\n  float color = 0.5f;\n\n  iteration_fractal_flame(point, color, function, idx, random_numbers, params);\n\n  colors[idx] = __float2half_rn(color);\n  short_points[idx].x = __float2half_rn(point.x);\n  short_points[idx].y = __float2half_rn(point.y);\n}",
            "inline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\ninline __host__ __device__ float4 expf(float4 v)\n{\n    return make_float4(expf(v.x), expf(v.y), expf(v.z), expf(v.w));\n}\n\ninline __host__ __device__ float4 fmodf(float4 a, float4 b)\n{\n    return make_float4(fmodf(a.x, b.x), fmodf(a.y, b.y), fmodf(a.z, b.z), fmodf(a.w, b.w));\n}\n\ninline __host__ __device__ float4 sqrtf(float4 v)\n{\n    return make_float4(sqrtf(v.x), sqrtf(v.y), sqrtf(v.z), sqrtf(v.w));\n}\n\n__device__ void\naffine_transform(float2 &p, const float *params)\n{\n  float2 tmp;\n  tmp.x = params[0] * p.x + params[1] * p.y + params[2];\n  tmp.y = params[3] * p.x + params[4] * p.y + params[5];\n\n  p.x = tmp.x;\n  p.y = tmp.y;\n}\n\n__device__ void\nsierpinski(float2 &p, int idx)\n{\n  p.x = p.x / 2.0f;\n  p.y = p.y / 2.0f;\n  switch(idx % 3) {\n    case 0:\n      break;\n    case 1:\n      p.x += 0.5f;\n      break;\n    case 2:\n      p.y += 0.5f;\n      break;\n  }\n}\n\n__device__ int\nget_function_idx(int idx, const ConstMemParams &params)\n{\n  int func;\n  int start = 0, end = NUM_FUNCTIONS;\n\n  /* use warp size granularity, decreases branch divergence */\n  idx &= ~31;\n\n  for(int i = 0; i < 6; i++) {\n    func = (start + end) / 2;\n    if(params.thread_function_mapping[func] <= idx)\n      start = func + 1;\n    else\n      end = func;\n  }\n\n  return func;\n}\n\n__device__ void\niteration_fractal_flame(float2 &p, \n    float &color,\n    int idx,\n    int threadidx,\n    const float *random_numbers,\n    const ConstMemParams &params)\n{\n\n#define A params.pre_transform_params[idx][0]\n#define B params.pre_transform_params[idx][1]\n#define C params.pre_transform_params[idx][2]\n#define D params.pre_transform_params[idx][3]\n#define E params.pre_transform_params[idx][4]\n#define F params.pre_transform_params[idx][5]\n\n#define RANDOM(i) random_numbers[\\\n  (threadidx + (params.frame_counter << 7) * i)\\\n  & (NUM_RANDOMS - 1)]\n\n  switch(params.enable_sierpinski) {\n    default:\n    case 0:\n      break;\n    case 1:\n      sierpinski(p, threadidx);\n      break;\n    case 2:\n      sierpinski(p, idx);\n      break;\n  }\n\n  affine_transform(p, &params.pre_transform_params[idx][0]);\n\n  float rad_square = p.x * p.x + p.y * p.y,\n        radius     = sqrtf(rad_square),\n        inv_radius = 1.0f / radius,\n        theta      = atan2f(p.x, p.y),\n        phi        = atan2f(p.y, p.x);\n\n  float2 point_out;\n  point_out.x = 0.0f;\n  point_out.y = 0.0f;\n\n  const VariationParameter *vp = &params.variation_parameters[idx][0];\n\n  for(int i = 0; i < VARIATIONS_PER_FUNCTION; i++) {\n    if(fabsf(vp[i].factor) < 0.01f)\n      continue;\n    float2 point;\n    switch(vp[i].idx) {\n      /* {{{ Variations */\n      case 0:\n        point.x = p.x;\n        point.y = p.y;\n        break;\n      case 1:\n        point.x = sinf(p.x);\n        point.y = sinf(p.y);\n        break;\n      case 2:\n        point.x = p.x / rad_square;\n        point.y = p.y / rad_square;\n        break;\n      case 3:\n        point.x = p.x * sinf(rad_square) - p.y * cosf(rad_square);\n        point.y = p.x * cosf(rad_square) + p.y * sinf(rad_square);\n        break;\n      case 4:\n        point.x = (p.x - p.y) * (p.x + p.y) * inv_radius;\n        point.y = 2.0f * p.x * p.y * inv_radius;\n        break;\n      case 5:\n        point.x = theta / M_PI;\n        point.y = radius - 1.0f;\n        break;\n      case 6:\n        point.x = radius * sinf(theta + radius);\n        point.y = radius * cosf(theta - radius);\n        break;\n      case 7:\n        point.x = radius * sinf(theta * radius);\n        point.y = -radius * cosf(theta * radius);\n        break;\n      case 8: {\n          float pi_r = M_PI * radius,\n                theta_pi = theta / M_PI;\n          point.x = theta_pi * sinf(pi_r);\n          point.y = theta_pi * cosf(pi_r);\n          break;\n        }\n      case 9:\n        point.x = radius * (cosf(theta) + sinf(radius));\n        point.y = radius * (sinf(theta) - cosf(radius));\n        break;\n      case 10:\n        point.x = sinf(theta) / radius;\n        point.y = radius * cosf(theta);\n        break;\n      case 11:\n        point.x = sinf(theta) * cosf(radius);\n        point.y = cosf(theta) * sinf(radius);\n        break;\n      case 12: {\n           float p0 = powf(sinf(theta + radius), 3);\n           float p1 = powf(sinf(theta - radius), 3);\n           point.x = radius * (p0 + p1);\n           point.y = radius * (p0 - p1);\n           break;\n         }\n      case 13: {\n           float sqrt_r = sqrtf(radius);\n           float omega = RANDOM(0) > 0.5f ? 0.0f : M_PI;\n           point.x = sqrt_r * cosf(theta / 2.0f + omega);\n           point.y = sqrt_r * sinf(theta / 2.0f + omega);\n           break;\n         }\n      case 14: {\n           switch(((p.x >= 0.0f) << 1) | (p.y >= 0.0f)) {\n             case 3: /* x >= 0, y >= 0 */\n               point.x = p.x;\n               point.y = p.y;\n               break;\n             case 1: /* x < 0, y >= 0 */\n               point.x = 2.0f * p.x;\n               point.y = p.y;\n               break;\n             case 2: /* x >= 0, y < 0 */\n               point.x = p.x;\n               point.y = p.y / 2.0f;\n               break;\n             case 0:\n               point.x = 2.0f * p.x;\n               point.y = p.y / 2.0f;\n               break;\n           }\n           break;\n         }\n      case 15:\n         point.x = p.x + B * sinf(p.y / (C * C));\n         point.y = p.y + E * sinf(p.x / (F * F));\n         break;\n      case 16:\n         point.x = point.y = 2.0f / (radius + 1.0f);\n         point.x *= p.y;\n         point.y *= p.x;\n         break;\n      case 17:\n         point.x = p.x + C * sinf(tanf(3.0f * p.y));\n         point.y = p.y + F * sinf(tanf(3.0f * p.x));\n         break;\n      case 18:\n         point.x = point.y = expf(p.x - 1.0f);\n         point.x *= cosf(M_PI * p.y);\n         point.y *= sinf(M_PI * p.y);\n         break;\n      case 19:\n         point.x = point.y = powf(radius, sinf(theta));\n         point.x *= cosf(theta);\n         point.y *= sinf(theta);\n         break;\n      case 20:\n         point.x = cosf(M_PI * p.x) * coshf(p.y);\n         point.y = -sinf(M_PI * p.x) * sinhf(p.y);\n         break;\n      case 21:\n         point.x = point.y = fmodf(radius + C * C, 2.0f * C * C) - C * C\n           + radius * (1.0f - C * C);\n         point.x *= cosf(theta);\n         point.y *= sinf(theta);\n         break;\n      case 22: {\n           float t = M_PI * C * C;\n           if(fmodf(theta + F, t) > t / 2.0f) {\n             point.x = radius * cosf(theta - t / 2.0f);\n             point.y = radius * sinf(theta - t / 2.0f);\n           }\n           else {\n             point.x = radius * cosf(theta + t / 2.0f);\n             point.y = radius * sinf(theta + t / 2.0f);\n           }\n           break;\n         }\n      case 23:\n      case 24:\n      case 25:\n      case 26:\n         /* TODO */\n         break;\n\n      case 27:\n         point.x = point.y = 2.0f / (radius + 1.0f);\n         point.x *= p.x;\n         point.y *= p.y;\n         break;\n      case 28:\n         point.x = point.y = 4.0f / (radius * radius + 4.0f);\n         point.x *= p.x;\n         point.y *= p.y;\n         break;\n      case 29:\n         point.x = sinf(p.x);\n         point.y = p.y;\n         break;\n      case 30:\n         /* TODO */\n         break;\n      case 31: {\n           float t = RANDOM(0) * 2.0f * M_PI;\n           point.x = RANDOM(1) * point.x * cosf(t);\n           point.y = RANDOM(1) * point.y * sinf(t);\n           break;\n         }\n      case 32:\n      case 33:\n         break;\n      case 34: {\n           float t = RANDOM(0) * 2.0f * M_PI;\n           point.x = RANDOM(1) * cosf(t);\n           point.y = RANDOM(1) * sinf(t);\n           break;\n         }\n      case 35:\n      case 36:\n      case 37:\n      case 38:\n      case 39:\n      case 40:\n         break;\n      case 41: {\n           float t = RANDOM(0) * M_PI * vp[i].factor;\n           point.x = sinf(t);\n           point.y = point.x * point.x / cosf(t);\n           break;\n         }\n      case 42:\n         point.x = sinf(p.x) / cosf(p.y);\n         point.y = tanf(p.y);\n         break;\n      case 43:\n         point.x = RANDOM(0) - 0.5f;\n         point.y = RANDOM(1) - 0.5f;\n         break;\n      case 44:\n         point.x = point.y = vp[i].factor * tanf(RANDOM(0) * M_PI\n             * vp[i].factor) / (radius * radius);\n         point.x *= cosf(p.x);\n         point.y *= sinf(p.y);\n         break;\n      case 45: {\n           float t = RANDOM(0) * radius * vp[i].factor;\n           point.x = p.x * (cosf(t) + sinf(t));\n           point.y = p.x * (cosf(t) - sinf(t));\n           break;\n         }\n      case 46:\n         point.x = p.x;\n         point.y = 1.0f / (vp[i].factor * cosf(radius * vp[i].factor));\n         break;\n      case 47: {\n           float f = RANDOM(0) * radius * vp[i].factor;\n           float t = log10f(f * f) + cosf(f);\n           point.x = p.x * t;\n           point.y = t - M_PI * sinf(f);\n           break;\n         }\n      case 48:\n         point.x = point.y = fabsf(1.0f / (p.x * p.x - p.y * p.y));\n         point.x *= p.x;\n         point.y *= p.y;\n         break;\n         /* }}} end variations */\n    } /* switch */\n\n    point_out.x += vp[i].factor * point.x;\n    point_out.y += vp[i].factor * point.y;\n\n  }\n  const float &col = params.function_colors[idx];\n\n  color = (color + col) / 2.0f;\n\n  p.x = point_out.x;\n  p.y = point_out.y;\n\n  affine_transform(p, &params.post_transform_params[idx][0]);\n\n#undef A\n#undef B\n#undef C\n#undef D\n#undef E\n#undef F\n#undef RANDOM\n\n}\n\n__global__ void\nkernel_iterate(short2 *short_points, \n               short *colors, \n               const unsigned short *perms,\n               const int perm_num,\n               const float *random_numbers,\n               const ConstMemParams params)\n{\n  int idx = blockDim.x * blockIdx.x + threadIdx.x;\n\n  int function = get_function_idx(idx, params);\n\n  float2 point;\n  float color;\n\n  int perm_idx = perms[NUM_THREADS * perm_num + idx];\n\n  color = __half2float(colors[perm_idx]);\n  point.x = __half2float(short_points[perm_idx].x);\n  point.y = __half2float(short_points[perm_idx].y);\n\n  iteration_fractal_flame(point, color, function, idx, random_numbers, params);\n\n  colors[perm_idx] = __float2half_rn(color);\n  short_points[perm_idx].x = __float2half_rn(point.x);\n  short_points[perm_idx].y = __float2half_rn(point.y);\n}",
            "inline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\ninline __host__ __device__ float4 expf(float4 v)\n{\n    return make_float4(expf(v.x), expf(v.y), expf(v.z), expf(v.w));\n}\n\ninline __host__ __device__ float4 fmodf(float4 a, float4 b)\n{\n    return make_float4(fmodf(a.x, b.x), fmodf(a.y, b.y), fmodf(a.z, b.z), fmodf(a.w, b.w));\n}\n\ninline __host__ __device__ float4 sqrtf(float4 v)\n{\n    return make_float4(sqrtf(v.x), sqrtf(v.y), sqrtf(v.z), sqrtf(v.w));\n}\n\n__device__ void\naffine_transform(float2 &p, const float *params)\n{\n  float2 tmp;\n  tmp.x = params[0] * p.x + params[1] * p.y + params[2];\n  tmp.y = params[3] * p.x + params[4] * p.y + params[5];\n\n  p.x = tmp.x;\n  p.y = tmp.y;\n}\n\n__device__ void\nsierpinski(float2 &p, int idx)\n{\n  p.x = p.x / 2.0f;\n  p.y = p.y / 2.0f;\n  switch(idx % 3) {\n    case 0:\n      break;\n    case 1:\n      p.x += 0.5f;\n      break;\n    case 2:\n      p.y += 0.5f;\n      break;\n  }\n}\n\n__device__ int\nget_function_idx(int idx, const ConstMemParams &params)\n{\n  int func;\n  int start = 0, end = NUM_FUNCTIONS;\n\n  /* use warp size granularity, decreases branch divergence */\n  idx &= ~31;\n\n  for(int i = 0; i < 6; i++) {\n    func = (start + end) / 2;\n    if(params.thread_function_mapping[func] <= idx)\n      start = func + 1;\n    else\n      end = func;\n  }\n\n  return func;\n}\n\n__device__ void\niteration_fractal_flame(float2 &p, \n    float &color,\n    int idx,\n    int threadidx,\n    const float *random_numbers,\n    const ConstMemParams &params)\n{\n\n#define A params.pre_transform_params[idx][0]\n#define B params.pre_transform_params[idx][1]\n#define C params.pre_transform_params[idx][2]\n#define D params.pre_transform_params[idx][3]\n#define E params.pre_transform_params[idx][4]\n#define F params.pre_transform_params[idx][5]\n\n#define RANDOM(i) random_numbers[\\\n  (threadidx + (params.frame_counter << 7) * i)\\\n  & (NUM_RANDOMS - 1)]\n\n  switch(params.enable_sierpinski) {\n    default:\n    case 0:\n      break;\n    case 1:\n      sierpinski(p, threadidx);\n      break;\n    case 2:\n      sierpinski(p, idx);\n      break;\n  }\n\n  affine_transform(p, &params.pre_transform_params[idx][0]);\n\n  float rad_square = p.x * p.x + p.y * p.y,\n        radius     = sqrtf(rad_square),\n        inv_radius = 1.0f / radius,\n        theta      = atan2f(p.x, p.y),\n        phi        = atan2f(p.y, p.x);\n\n  float2 point_out;\n  point_out.x = 0.0f;\n  point_out.y = 0.0f;\n\n  const VariationParameter *vp = &params.variation_parameters[idx][0];\n\n  for(int i = 0; i < VARIATIONS_PER_FUNCTION; i++) {\n    if(fabsf(vp[i].factor) < 0.01f)\n      continue;\n    float2 point;\n    switch(vp[i].idx) {\n      /* {{{ Variations */\n      case 0:\n        point.x = p.x;\n        point.y = p.y;\n        break;\n      case 1:\n        point.x = sinf(p.x);\n        point.y = sinf(p.y);\n        break;\n      case 2:\n        point.x = p.x / rad_square;\n        point.y = p.y / rad_square;\n        break;\n      case 3:\n        point.x = p.x * sinf(rad_square) - p.y * cosf(rad_square);\n        point.y = p.x * cosf(rad_square) + p.y * sinf(rad_square);\n        break;\n      case 4:\n        point.x = (p.x - p.y) * (p.x + p.y) * inv_radius;\n        point.y = 2.0f * p.x * p.y * inv_radius;\n        break;\n      case 5:\n        point.x = theta / M_PI;\n        point.y = radius - 1.0f;\n        break;\n      case 6:\n        point.x = radius * sinf(theta + radius);\n        point.y = radius * cosf(theta - radius);\n        break;\n      case 7:\n        point.x = radius * sinf(theta * radius);\n        point.y = -radius * cosf(theta * radius);\n        break;\n      case 8: {\n          float pi_r = M_PI * radius,\n                theta_pi = theta / M_PI;\n          point.x = theta_pi * sinf(pi_r);\n          point.y = theta_pi * cosf(pi_r);\n          break;\n        }\n      case 9:\n        point.x = radius * (cosf(theta) + sinf(radius));\n        point.y = radius * (sinf(theta) - cosf(radius));\n        break;\n      case 10:\n        point.x = sinf(theta) / radius;\n        point.y = radius * cosf(theta);\n        break;\n      case 11:\n        point.x = sinf(theta) * cosf(radius);\n        point.y = cosf(theta) * sinf(radius);\n        break;\n      case 12: {\n           float p0 = powf(sinf(theta + radius), 3);\n           float p1 = powf(sinf(theta - radius), 3);\n           point.x = radius * (p0 + p1);\n           point.y = radius * (p0 - p1);\n           break;\n         }\n      case 13: {\n           float sqrt_r = sqrtf(radius);\n           float omega = RANDOM(0) > 0.5f ? 0.0f : M_PI;\n           point.x = sqrt_r * cosf(theta / 2.0f + omega);\n           point.y = sqrt_r * sinf(theta / 2.0f + omega);\n           break;\n         }\n      case 14: {\n           switch(((p.x >= 0.0f) << 1) | (p.y >= 0.0f)) {\n             case 3: /* x >= 0, y >= 0 */\n               point.x = p.x;\n               point.y = p.y;\n               break;\n             case 1: /* x < 0, y >= 0 */\n               point.x = 2.0f * p.x;\n               point.y = p.y;\n               break;\n             case 2: /* x >= 0, y < 0 */\n               point.x = p.x;\n               point.y = p.y / 2.0f;\n               break;\n             case 0:\n               point.x = 2.0f * p.x;\n               point.y = p.y / 2.0f;\n               break;\n           }\n           break;\n         }\n      case 15:\n         point.x = p.x + B * sinf(p.y / (C * C));\n         point.y = p.y + E * sinf(p.x / (F * F));\n         break;\n      case 16:\n         point.x = point.y = 2.0f / (radius + 1.0f);\n         point.x *= p.y;\n         point.y *= p.x;\n         break;\n      case 17:\n         point.x = p.x + C * sinf(tanf(3.0f * p.y));\n         point.y = p.y + F * sinf(tanf(3.0f * p.x));\n         break;\n      case 18:\n         point.x = point.y = expf(p.x - 1.0f);\n         point.x *= cosf(M_PI * p.y);\n         point.y *= sinf(M_PI * p.y);\n         break;\n      case 19:\n         point.x = point.y = powf(radius, sinf(theta));\n         point.x *= cosf(theta);\n         point.y *= sinf(theta);\n         break;\n      case 20:\n         point.x = cosf(M_PI * p.x) * coshf(p.y);\n         point.y = -sinf(M_PI * p.x) * sinhf(p.y);\n         break;\n      case 21:\n         point.x = point.y = fmodf(radius + C * C, 2.0f * C * C) - C * C\n           + radius * (1.0f - C * C);\n         point.x *= cosf(theta);\n         point.y *= sinf(theta);\n         break;\n      case 22: {\n           float t = M_PI * C * C;\n           if(fmodf(theta + F, t) > t / 2.0f) {\n             point.x = radius * cosf(theta - t / 2.0f);\n             point.y = radius * sinf(theta - t / 2.0f);\n           }\n           else {\n             point.x = radius * cosf(theta + t / 2.0f);\n             point.y = radius * sinf(theta + t / 2.0f);\n           }\n           break;\n         }\n      case 23:\n      case 24:\n      case 25:\n      case 26:\n         /* TODO */\n         break;\n\n      case 27:\n         point.x = point.y = 2.0f / (radius + 1.0f);\n         point.x *= p.x;\n         point.y *= p.y;\n         break;\n      case 28:\n         point.x = point.y = 4.0f / (radius * radius + 4.0f);\n         point.x *= p.x;\n         point.y *= p.y;\n         break;\n      case 29:\n         point.x = sinf(p.x);\n         point.y = p.y;\n         break;\n      case 30:\n         /* TODO */\n         break;\n      case 31: {\n           float t = RANDOM(0) * 2.0f * M_PI;\n           point.x = RANDOM(1) * point.x * cosf(t);\n           point.y = RANDOM(1) * point.y * sinf(t);\n           break;\n         }\n      case 32:\n      case 33:\n         break;\n      case 34: {\n           float t = RANDOM(0) * 2.0f * M_PI;\n           point.x = RANDOM(1) * cosf(t);\n           point.y = RANDOM(1) * sinf(t);\n           break;\n         }\n      case 35:\n      case 36:\n      case 37:\n      case 38:\n      case 39:\n      case 40:\n         break;\n      case 41: {\n           float t = RANDOM(0) * M_PI * vp[i].factor;\n           point.x = sinf(t);\n           point.y = point.x * point.x / cosf(t);\n           break;\n         }\n      case 42:\n         point.x = sinf(p.x) / cosf(p.y);\n         point.y = tanf(p.y);\n         break;\n      case 43:\n         point.x = RANDOM(0) - 0.5f;\n         point.y = RANDOM(1) - 0.5f;\n         break;\n      case 44:\n         point.x = point.y = vp[i].factor * tanf(RANDOM(0) * M_PI\n             * vp[i].factor) / (radius * radius);\n         point.x *= cosf(p.x);\n         point.y *= sinf(p.y);\n         break;\n      case 45: {\n           float t = RANDOM(0) * radius * vp[i].factor;\n           point.x = p.x * (cosf(t) + sinf(t));\n           point.y = p.x * (cosf(t) - sinf(t));\n           break;\n         }\n      case 46:\n         point.x = p.x;\n         point.y = 1.0f / (vp[i].factor * cosf(radius * vp[i].factor));\n         break;\n      case 47: {\n           float f = RANDOM(0) * radius * vp[i].factor;\n           float t = log10f(f * f) + cosf(f);\n           point.x = p.x * t;\n           point.y = t - M_PI * sinf(f);\n           break;\n         }\n      case 48:\n         point.x = point.y = fabsf(1.0f / (p.x * p.x - p.y * p.y));\n         point.x *= p.x;\n         point.y *= p.y;\n         break;\n         /* }}} end variations */\n    } /* switch */\n\n    point_out.x += vp[i].factor * point.x;\n    point_out.y += vp[i].factor * point.y;\n\n  }\n  const float &col = params.function_colors[idx];\n\n  color = (color + col) / 2.0f;\n\n  p.x = point_out.x;\n  p.y = point_out.y;\n\n  affine_transform(p, &params.post_transform_params[idx][0]);\n\n#undef A\n#undef B\n#undef C\n#undef D\n#undef E\n#undef F\n#undef RANDOM\n\n}\n\n__global__ void\nkernel_generate_points(float3 *vertices,\n                       short2 *short_points, \n                       short *colors, \n                       const unsigned short *perms, \n                       const int perm_num,\n                       const float *random_numbers,\n                       const ConstMemParams params)\n{\n  int idx = blockDim.x * blockIdx.x + threadIdx.x;\n\n  int function = get_function_idx(idx, params);\n\n  float2 point;\n  float color;\n\n  for(int i = 0; i < NUM_POINTS_PER_THREAD; i++) {\n    int perm_idx = perms[((perm_num + i) % NUM_PERMUTATIONS) *\n      NUM_THREADS + idx];\n\n    short2 _p = short_points[perm_idx];\n    point.x = __half2float(_p.x);\n    point.y = __half2float(_p.y);\n    color = __half2float(colors[perm_idx]);\n\n    iteration_fractal_flame(point, color, function, idx, random_numbers, params);\n\n    vertices[idx + i * NUM_THREADS].x = point.x;\n    vertices[idx + i * NUM_THREADS].y = point.y;\n    vertices[idx + i * NUM_THREADS].z = color;\n  }\n}"
        ]
    },
    "flip-cuda": {
        "/Users/gbolet/hecbench-roofline/src/flip-cuda/main.cu": [
            "__global__ void flip_kernel(\n    const scalar_t* in_tensor,\n          scalar_t* out_tensor,\n    int64_t  n,\n    const int64_t* flip_dims,\n    const int64_t  flip_dims_size,\n    const int64_t* strides,\n    const int64_t* strides_contiguous,\n    const int64_t* shape,\n    const int64_t  total_dims)\n{\n  int64_t linear_index = blockIdx.x * blockDim.x + threadIdx.x;\n\n  if (linear_index >= n) return;\n\n  int64_t cur_indices = linear_index;\n  int64_t rem = 0;\n  int64_t dst_offset = 0;\n\n  for (int64_t i = 0; i < total_dims; i++) {\n    int64_t temp = cur_indices;\n    cur_indices = cur_indices / strides_contiguous[i];\n    rem = temp - cur_indices * strides_contiguous[i];\n    for (int64_t j = 0; j < flip_dims_size; j++) {\n      // flip the indices if it is in flip_dims\n      if (i == flip_dims[j]) {\n        cur_indices = shape[i] - 1 - cur_indices;\n      }\n    }\n    dst_offset += cur_indices * strides[i];\n    cur_indices = rem;\n  }\n  out_tensor[linear_index] = in_tensor[dst_offset];\n}"
        ]
    },
    "gelu-cuda": {
        "/Users/gbolet/hecbench-roofline/src/gelu-cuda/main.cu": [
            "__global__ void gelu_bias_loop(__half* src, const __half* bias, int width, int height)\n{\n  int x     = blockIdx.x;  // seq length\n  int y     = threadIdx.x * 2;\n  int batch = blockIdx.y;\n\n  if (x < height) {\n    int    index = batch * width * height + x * width;\n    half2  v_src;\n    half2  v_bias;\n    half2  v;\n    float2 t;\n    for (; y < width; y = y + blockDim.x * 2) {\n      v_bias = ((half2*)bias)[y >> 1];\n      v_src  = ((half2*)src)[(index + y) >> 1];\n      v      = __hadd2(v_src, v_bias);\n      t      = __half22float2(v);\n      t.x    = (0.5f * t.x * (1.0f + tanhf(0.79788456f * (t.x + 0.044715f * t.x * t.x * t.x))));\n      t.y    = (0.5f * t.y * (1.0f + tanhf(0.79788456f * (t.y + 0.044715f * t.y * t.y * t.y))));\n\n      ((half2*)src)[(index + y) >> 1] = __float22half2_rn(t);\n    }\n  }\n}"
        ]
    },
    "entropy-cuda": {
        "/Users/gbolet/hecbench-roofline/src/entropy-cuda/main.cu": [
            "__global__ \nvoid entropy(\n        float *__restrict__ d_entropy,\n    const char*__restrict__ d_val, \n    int height, int width)\n{\n  const int x = threadIdx.x + blockIdx.x * blockDim.x;\n  const int y = threadIdx.y + blockIdx.y * blockDim.y;\n\n  // value of matrix element ranges from 0 inclusive to 16 exclusive\n  char count[16];\n  for (int i = 0; i < 16; i++) count[i] = 0;\n\n  // total number of valid elements\n  char total = 0;\n\n  // 5x5 window\n  for(int dy = -2; dy <= 2; dy++) {\n    for(int dx = -2; dx <= 2; dx++) {\n      int xx = x + dx;\n      int yy = y + dy;\n      if(xx >= 0 && yy >= 0 && yy < height && xx < width) {\n        count[d_val[yy * width + xx]]++;\n        total++;\n      }\n    }\n  }\n\n  float entropy = 0;\n  if (total < 1) {\n    total = 1;\n  } else {\n    for(int k = 0; k < 16; k++) {\n      float p = __fdividef((float)count[k], (float)total);\n      entropy -= p * log2f(p);\n    }\n  }\n\n  if(y < height && x < width) d_entropy[y * width + x] = entropy;\n}",
            "__global__ void entropy_opt(\n       float *__restrict__ d_entropy,\n  const  char*__restrict__ d_val, \n  const float*__restrict__ d_logTable,\n  int m, int n)\n{\n  __shared__ int sd_count[16][bsize_y*bsize_x];\n\n  const int x = threadIdx.x + blockIdx.x * blockDim.x;\n  const int y = threadIdx.y + blockIdx.y * blockDim.y;\n  const int idx = threadIdx.y*bsize_x + threadIdx.x;\n\n  for(int i = 0; i < 16;i++) sd_count[i][idx] = 0;\n\n  char total = 0;\n  for(int dy = -2; dy <= 2; dy++) {\n    for(int dx = -2; dx <= 2; dx++) {\n      int xx = x + dx,\n          yy = y + dy;\n\n      if(xx >= 0 && yy >= 0 && yy < m && xx < n) {\n        sd_count[d_val[yy*n+xx]][idx]++;\n        total++;\n      }\n    }\n  }\n\n  float entropy = 0;\n  for(int k = 0; k < 16; k++)\n    entropy -= d_logTable[sd_count[k][idx]];\n  \n  entropy = entropy / total + log2f(total);\n  if(y < m && x < n) d_entropy[y*n+x] = entropy;\n}"
        ]
    },
    "simpleSpmv-cuda": {
        "/Users/gbolet/hecbench-roofline/src/simpleSpmv-cuda/kernels.cu": [
            "#define REAL DOUBLE\n\n\n__global__ void mv_dense(const size_t num_rows, const REAL* matrix, const REAL* x, REAL* y)\n{\n  size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < num_rows) {\n    REAL temp = 0;\n    for (size_t j = 0; j < num_rows; j++) {\n      if (matrix[i * num_rows + j] != (REAL)0) \n        temp += matrix[i * num_rows + j] * x[j];\n    }\n    y[i] = temp;\n  }\n}",
            "#define REAL DOUBLE\n\n\n__global__ void mv_csr(const size_t num_rows,\n                       const size_t *row_indices,\n                       const size_t *col_indices,\n                       const REAL *values,\n                       const REAL *x,\n                             REAL *y)\n{\n  size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < num_rows) {\n    size_t row_start = row_indices[i];\n    size_t row_end = row_indices[i+1];\n\n    REAL temp = 0;\n    for(size_t n = row_start; n < row_end; n++){\n      temp += values[n] * x[col_indices[n]];\n    }\n    y[i] = temp;\n  }\n}",
            "#define REAL DOUBLE\n\n\n__global__ void vector_mv_csr(const size_t num_rows,\n                              const size_t *row_indices,\n                              const size_t *col_indices,\n                              const REAL *values,\n                              const REAL *x,\n                                    REAL *y)\n{\n  size_t m = blockIdx.x * blockDim.y + threadIdx.y;\n  if (m < num_rows) {\n    size_t row_start = row_indices[m];\n    size_t row_end = row_indices[m+1];\n\n    REAL temp = 0;\n    for(size_t n = row_start + threadIdx.x; n < row_end; n += BS){\n      temp += values[n] * x[col_indices[n]];\n    }\n    #pragma unroll\n    for (int i = BS >> 1; i > 0; i >>= 1)\n      temp += __shfl_down_sync(0xFFFFFFFF, temp, i, BS);\n\n    y[m] = temp;\n  }\n}"
        ]
    },
    "bfs-cuda": {
        "/Users/gbolet/hecbench-roofline/src/bfs-cuda/bfs.cu": [
            "__global__ void\nKernel(const Node* __restrict__ d_graph_nodes, \n       const int* __restrict__ d_graph_edges,\n       char* __restrict__ d_graph_mask,\n       char* __restrict__ d_updatind_graph_mask,\n       const char *__restrict__ d_graph_visited,\n       int* __restrict__ d_cost,\n       const int no_of_nodes) \n{\n  int tid = blockIdx.x * blockDim.x + threadIdx.x;\n  if( tid<no_of_nodes && d_graph_mask[tid])\n  {\n    d_graph_mask[tid]=0;\n    const int num_edges = d_graph_nodes[tid].no_of_edges;\n    const int starting = d_graph_nodes[tid].starting;\n\n    for(int i=starting; i<(num_edges + starting); i++)\n    {\n      int id = d_graph_edges[i];\n      if(!d_graph_visited[id])\n      {\n        d_cost[id]=d_cost[tid]+1;\n        d_updatind_graph_mask[id]=1;\n      }\n    }\n  }\n}",
            "__global__ void\nKernel2(char* __restrict__ d_graph_mask,\n        char* __restrict__ d_updatind_graph_mask,\n        char* __restrict__ d_graph_visited,\n        char* __restrict__ d_over,\n        const int no_of_nodes)\n{\n  int tid = blockIdx.x * blockDim.x + threadIdx.x;\n  if( tid<no_of_nodes && d_updatind_graph_mask[tid])\n  {\n    d_graph_mask[tid]=1;\n    d_graph_visited[tid]=1;\n    *d_over=1;\n    d_updatind_graph_mask[tid]=0;\n  }\n}"
        ]
    },
    "fft-cuda": {
        "/Users/gbolet/hecbench-roofline/src/fft-cuda/ifft1D_512.h": [
            "#define T ((int)32)\n\n\n#define T2 double2\n\n\n__host__ __device__\nT2 cmplx_mul( T2 a, T2 b ) { return (T2){ a.x*b.x-a.y*b.y, a.x*b.y+a.y*b.x }; }\n\n__host__ __device__\nT2 exp_i( T phi ) {\n  return (T2){ cos(phi), sin(phi) };\n}\n\n__global__ void ifft1D_512 (T2* work) \n{\n  __shared__  T smem[8*8*9];\n\n  int tid = threadIdx.x;\n  int gid = blockIdx.x * 512 + tid;\n  int hi = tid>>3;\n  int lo = tid&7;\n  T2 data[8];\n  //__local T smem[8*8*9];\n  const int reversed[] = {0,4,2,6,1,5,3,7};\n\n  // starting index of data to/from global memory\n  for( int i = 0; i < 8; i++ ) data[i] = work[gid+i*64];\n\n  IFFT8( data );\n\n  //itwiddle8( data, tid, 512 );\n  for( int j = 1; j < 8; j++ )\n      data[j] = cmplx_mul(data[j] , exp_i(((T)2*(T)M_PI*reversed[j]/(T)512)*(tid)) );\n\n  //transpose(data, &smem[hi*8+lo], 66, &smem[lo*66+hi], 8, 0xf);\n  for( int i = 0; i < 8; i++ ) smem[hi*8+lo+i*66] = data[reversed[i]].x;\n  __syncthreads(); \n  for( int i = 0; i < 8; i++ ) data[i].x = smem[lo*66+hi+i*8]; \n  __syncthreads(); \n  for( int i = 0; i < 8; i++ ) smem[hi*8+lo+i*66] = data[reversed[i]].y;\n  __syncthreads(); \n  for( int i = 0; i < 8; i++ ) data[i].y= smem[lo*66+hi+i*8]; \n  __syncthreads(); \n\n  IFFT8( data );\n\n  //itwiddle8( data, hi, 64 );\n  for( int j = 1; j < 8; j++ )\n      data[j] = cmplx_mul(data[j] , exp_i(((T)2*(T)M_PI*reversed[j]/(T)64)*hi) );\n\n\n  //transpose(data, &smem[hi*8+lo], 8*9, &smem[hi*8*9+lo], 8, 0xE);\n  for( int i = 0; i < 8; i++ ) smem[hi*8+lo+i*72] = data[reversed[i]].x;\n  __syncthreads(); \n  for( int i = 0; i < 8; i++ ) data[i].x = smem[hi*72+lo+i*8]; \n  __syncthreads(); \n  for( int i = 0; i < 8; i++ ) smem[hi*8+lo+i*72] = data[reversed[i]].y;\n  __syncthreads(); \n  for( int i = 0; i < 8; i++ ) data[i].y= smem[hi*72+lo+i*8]; \n\n  IFFT8( data );\n\n  for( int i = 0; i < 8; i++ ) {\n      data[i].x = data[i].x/(T)512;\n      data[i].y = data[i].y/(T)512;\n  }\n\n  //globalStores8(data, work, 64);\n  for( int i = 0; i < 8; i++ )\n    work[gid+i*64] = data[reversed[i]];\n}"
        ],
        "/Users/gbolet/hecbench-roofline/src/fft-cuda/fft1D_512.h": [
            "#define T ((int)32)\n\n\n#define T2 double2\n\n\n__host__ __device__\nT2 cmplx_mul( T2 a, T2 b ) { return (T2){ a.x*b.x-a.y*b.y, a.x*b.y+a.y*b.x }; }\n\n__host__ __device__\nT2 exp_i( T phi ) {\n  return (T2){ cos(phi), sin(phi) };\n}\n\n__global__ void fft1D_512 (T2* work) \n{\n  __shared__  T smem[8*8*9];\n\n  int tid = threadIdx.x;\n  int gid = blockIdx.x * 512 + tid;\n  int hi = tid>>3;\n  int lo = tid&7;\n  T2 data[8];\n  //__local T smem[8*8*9];\n  const int reversed[] = {0,4,2,6,1,5,3,7};\n\n  // starting index of data to/from global memory\n  // globalLoads8(data, work, 64)\n  for( int i = 0; i < 8; i++ ) data[i] = work[gid+i*64];\n\n  FFT8( data );\n\n  //twiddle8( data, tid, 512 );\n  for( int j = 1; j < 8; j++ ){                                       \n      data[j] = cmplx_mul( data[j],exp_i(((T)-2*(T)M_PI*reversed[j]/(T)512)*tid) ); \n  }                                                                   \n\n  //transpose(data, &smem[hi*8+lo], 66, &smem[lo*66+hi], 8, 0xf);\n  for( int i = 0; i < 8; i++ ) smem[hi*8+lo+i*66] = data[reversed[i]].x;\n  __syncthreads(); \n  for( int i = 0; i < 8; i++ ) data[i].x = smem[lo*66+hi+i*8]; \n  __syncthreads(); \n  for( int i = 0; i < 8; i++ ) smem[hi*8+lo+i*66] = data[reversed[i]].y;\n  __syncthreads(); \n  for( int i = 0; i < 8; i++ ) data[i].y= smem[lo*66+hi+i*8]; \n  __syncthreads(); \n\n  FFT8( data );\n\n  //twiddle8( data, hi, 64 );\n  for( int j = 1; j < 8; j++ ){                                       \n      data[j] = cmplx_mul( data[j],exp_i(((T)-2*(T)M_PI*reversed[j]/(T)64)*hi) ); \n  }                                                                   \n\n  //transpose(data, &smem[hi*8+lo], 8*9, &smem[hi*8*9+lo], 8, 0xE);\n  for( int i = 0; i < 8; i++ ) smem[hi*8+lo+i*72] = data[reversed[i]].x;\n  __syncthreads(); \n  for( int i = 0; i < 8; i++ ) data[i].x = smem[hi*72+lo+i*8]; \n  __syncthreads(); \n  for( int i = 0; i < 8; i++ ) smem[hi*8+lo+i*72] = data[reversed[i]].y;\n  __syncthreads(); \n  for( int i = 0; i < 8; i++ ) data[i].y= smem[hi*72+lo+i*8]; \n\n  FFT8( data );\n\n  //globalStores8(data, work, 64);\n  for( int i = 0; i < 8; i++ )\n    work[gid+i*64] = data[reversed[i]];\n}"
        ]
    },
    "mcpr-cuda": {
        "/Users/gbolet/hecbench-roofline/src/mcpr-cuda/kernels.h": [
            "__global__ void compute_probs(\n  const double* __restrict__ alphas,\n  const double* __restrict__ rands,\n        double* __restrict__ probs,\n  int n, int K, int M)\n{\n  // assign overall id/index of the thread = id of row\n  int i = blockIdx.x * blockDim.x + threadIdx.x; \n\n  if(i < n) {\n    double maxval;    \n    int m, k;\n    int maxind;\n    double M_d = (double) M; \n    double w[21]; // w[K]\n\n    for(k = 0; k < K; ++k){   // initialize probs (though already done on CPU)\n      probs[i*K + k] = 0.0;\n    }\n\n    // core computations\n    for(m = 0; m < M; ++m){   // loop over Monte Carlo iterations\n      for(k = 0; k < K; ++k){  // generate W ~ N(alpha, 1)\n        w[k] = alphas[i*K + k] + rands[m*K + k];\n      }\n\n      // determine which category has max W\n      maxind = K-1;\n      maxval = w[K-1];\n      for(k = 0; k < (K-1); ++k){\n        if(w[k] > maxval){\n          maxind = k;\n          maxval = w[k];\n        } \n      }\n      probs[i*K + maxind] += 1.0;\n    }\n\n    // compute final proportions\n    for(k = 0; k < K; ++k) {\n      probs[i*K + k] /= M_d;\n    }\n  }\n}",
            "__global__ void compute_probs_unitStrides(\n  const double* __restrict__ alphas,\n  const double* __restrict__ rands,\n        double* __restrict__ probs,\n  int n, int K, int M)\n{\n  // assign overall id/index of the thread = id of row\n  int i = blockIdx.x * blockDim.x + threadIdx.x; \n\n  if(i < n) {\n    double maxval;    \n    int m, k;\n    int maxind;\n    double M_d = (double) M; \n    double w[21]; // w[K]\n\n    for(k = 0; k < K; ++k){  // initialize probs (though already done on CPU)\n      probs[k*n + i] = 0.0;\n    }\n\n    // core computations\n    for(m = 0; m < M; ++m){    // loop over Monte Carlo iterations\n      for(k = 0; k < K; ++k){  // generate W ~ N(alpha, 1)\n        // with +i we now have unit strides in inner loop\n        w[k] = alphas[k*n + i] + rands[k*M + m];\n      }\n\n      // determine which category has max W\n      maxind = K-1;\n      maxval = w[K-1];\n      for(k = 0; k < (K-1); ++k){\n        if(w[k] > maxval){\n          maxind = k;\n          maxval = w[k];\n        } \n      }\n      probs[maxind*n + i] += 1.0;\n    }\n\n    // compute final proportions\n    for(k = 0; k < K; ++k) {\n      // unit strides\n      probs[k*n + i] /= M_d;\n    }\n  }\n}",
            "__global__ void compute_probs_unitStrides_sharedMem(\n  const double* __restrict__ alphas,\n  const double* __restrict__ rands,\n        double* __restrict__ probs,\n  int n, int K, int M)\n{\n  // assign overall id/index of the thread = id of row\n  int i = blockIdx.x * blockDim.x + threadIdx.x; \n  if (i >= n) return;\n\n  int threads_per_block = blockDim.x; \n\n  // set up shared memory: half for probs and half for w\n  extern __shared__ double shared[];\n  double* probs_shared = shared;\n\n  // shared mem is one big block, so need to index into latter portion of it to use for w\n  double* w = &shared[K*threads_per_block];\n\n  double maxval;    \n  int m, k;\n  int maxind;\n  double M_d = (double) M; \n\n  // initialize shared memory probs\n  for(k = 0; k < K; ++k) {\n    probs_shared[k*threads_per_block + threadIdx.x] = 0.0;\n  }\n\n  // core computation\n  for(m = 0; m < M; ++m){     // loop over Monte Carlo iterations \n    for(k = 0; k < K; ++k){   // generate W ~ N(alpha, 1)\n      w[k*threads_per_block + threadIdx.x] = alphas[k*n + i] + rands[k*M + m];\n    }\n    maxind = K-1;\n    maxval = w[(K-1)*threads_per_block + threadIdx.x];\n    for(k = 0; k < (K-1); ++k){\n      if(w[k*threads_per_block + threadIdx.x] > maxval){\n        maxind = k;\n        maxval = w[k*threads_per_block + threadIdx.x];\n      } \n    }\n    probs_shared[maxind*threads_per_block + threadIdx.x] += 1.0;\n  }\n\n  for(k = 0; k < K; ++k) {\n    probs_shared[k*threads_per_block + threadIdx.x] /= M_d;\n  }\n\n  // copy to device memory so can be returned to CPU\n  for(k = 0; k < K; ++k) {\n    probs[k*n + i] = probs_shared[k*threads_per_block + threadIdx.x];\n  }\n}"
        ]
    },
    "radixsort-cuda": {
        "/Users/gbolet/hecbench-roofline/src/radixsort-cuda/RadixSort_kernels.cu": [
            "__device__\nunsigned int scanwarp(unsigned int val, volatile unsigned int* sData, const int maxlevel)\n{\n  // The following is the same as 2 * RadixSort::WARP_SIZE * warpId + threadInWarp = \n  // 64*(threadIdx.x >> 5) + (threadIdx.x & (RadixSort::WARP_SIZE - 1))\n  int localId = threadIdx.x;\n  int idx = 2 * localId - (localId & (WARP_SIZE - 1));\n  sData[idx] = 0;\n  idx += WARP_SIZE;\n  sData[idx] = val;     \n\n  if (0 <= maxlevel) { sData[idx] += sData[idx - 1]; }\n  if (1 <= maxlevel) { sData[idx] += sData[idx - 2]; }\n  if (2 <= maxlevel) { sData[idx] += sData[idx - 4]; }\n  if (3 <= maxlevel) { sData[idx] += sData[idx - 8]; }\n  if (4 <= maxlevel) { sData[idx] += sData[idx -16]; }\n\n  return sData[idx] - val;  // convert inclusive -> exclusive\n}\n\n__device__\nuint4 scan4(const uint4 idata, unsigned int* ptr)\n{    \n\n  unsigned int idx = threadIdx.x;\n\n  uint4 val4 = idata;\n  unsigned int sum[3];\n  sum[0] = val4.x;\n  sum[1] = val4.y + sum[0];\n  sum[2] = val4.z + sum[1];\n\n  unsigned int val = val4.w + sum[2];\n\n  val = scanwarp(val, ptr, 4);\n  __syncthreads();\n\n  if ((idx & (WARP_SIZE - 1)) == WARP_SIZE - 1)\n  {\n    ptr[idx >> 5] = val + val4.w + sum[2];\n  }\n  __syncthreads();\n\n  if (idx < WARP_SIZE)\n    ptr[idx] = scanwarp(ptr[idx], ptr, 2);\n\n  __syncthreads();\n\n  val += ptr[idx >> 5];\n\n  val4.x = val;\n  val4.y = val + sum[0];\n  val4.z = val + sum[1];\n  val4.w = val + sum[2];\n\n  return val4;\n}\n\n__device__\nuint4 rank4(const uint4 preds, unsigned int* sMem, unsigned int* numtrue)\n{\n  int localId = threadIdx.x;\n  int localSize = blockDim.x;\n\n  uint4 address = scan4(preds, sMem);\n\n  if (localId == localSize - 1) \n  {\n    numtrue[0] = address.w + preds.w;\n  }\n  __syncthreads();\n\n  uint4 rank;\n  int idx = localId*4;\n  rank.x = (preds.x) ? address.x : numtrue[0] + idx - address.x;\n  rank.y = (preds.y) ? address.y : numtrue[0] + idx + 1 - address.y;\n  rank.z = (preds.z) ? address.z : numtrue[0] + idx + 2 - address.z;\n  rank.w = (preds.w) ? address.w : numtrue[0] + idx + 3 - address.w;\n\n  return rank;\n}\n\n__global__ void radixSortBlocksKeysK(\n   const unsigned int*__restrict__ keysIn,\n         unsigned int*__restrict__ keysOut,\n   const unsigned int nbits,\n   const unsigned int startbit)\n{\n  int globalId = blockIdx.x * blockDim.x + threadIdx.x;\n  __shared__ unsigned int numtrue[1];\n  __shared__ unsigned int sMem[4*CTA_SIZE];\n\n  uint4 key = reinterpret_cast<const uint4*>(keysIn)[globalId];\n\n  __syncthreads();\n\n  // radixSortBlockKeysOnly(&key, nbits, startbit, sMem, numtrue);\n  int localId = threadIdx.x;\n  int localSize = blockDim.x;\n\n  for(unsigned int shift = startbit; shift < (startbit + nbits); ++shift)\n  {\n    uint4 lsb;\n    lsb.x = !((key.x >> shift) & 0x1);\n    lsb.y = !((key.y >> shift) & 0x1);\n    lsb.z = !((key.z >> shift) & 0x1);\n    lsb.w = !((key.w >> shift) & 0x1);\n\n    uint4 r;\n\n    r = rank4(lsb, sMem, numtrue);\n\n    // This arithmetic strides the ranks across 4 CTA_SIZE regions\n    sMem[(r.x & 3) * localSize + (r.x >> 2)] = key.x;\n    sMem[(r.y & 3) * localSize + (r.y >> 2)] = key.y;\n    sMem[(r.z & 3) * localSize + (r.z >> 2)] = key.z;\n    sMem[(r.w & 3) * localSize + (r.w >> 2)] = key.w;\n    __syncthreads();\n\n    // The above allows us to read without 4-way bank conflicts:\n    key.x = sMem[localId];\n    key.y = sMem[localId +     localSize];\n    key.z = sMem[localId + 2 * localSize];\n    key.w = sMem[localId + 3 * localSize];\n\n    __syncthreads();\n  }\n\n  //keysOut[globalId] = key;\n  reinterpret_cast<uint4*>(keysOut)[globalId] = key;  \n}",
            "__global__ void findRadixOffsetsK(\n    const unsigned int*__restrict__ keys,\n          unsigned int*__restrict__ counters,\n          unsigned int*__restrict__ blockOffsets,\n    const unsigned int startbit,\n    const unsigned int totalBlocks)\n{\n  __shared__ unsigned int  sStartPointers[16];\n  __shared__ unsigned int  sRadix1[2*CTA_SIZE];\n\n  unsigned int groupId = blockIdx.x;\n  unsigned int localId = threadIdx.x;\n  unsigned int groupSize = blockDim.x;\n  unsigned int globalId = groupId * groupSize + localId;\n\n  uint2 radix2 = reinterpret_cast<const uint2*>(keys)[globalId];\n\n  sRadix1[2 * localId]     = (radix2.x >> startbit) & 0xF;\n  sRadix1[2 * localId + 1] = (radix2.y >> startbit) & 0xF;\n\n  // Finds the position where the sRadix1 entries differ and stores start \n  // index for each radix.\n  if(localId < 16) \n  {\n    sStartPointers[localId] = 0; \n  }\n  __syncthreads();\n\n  if((localId > 0) && (sRadix1[localId] != sRadix1[localId - 1]) ) \n  {\n    sStartPointers[sRadix1[localId]] = localId;\n  }\n  if(sRadix1[localId + groupSize] != sRadix1[localId + groupSize - 1]) \n  {\n    sStartPointers[sRadix1[localId + groupSize]] = localId + groupSize;\n  }\n  __syncthreads();\n\n  if(localId < 16) \n  {\n    blockOffsets[groupId*16 + localId] = sStartPointers[localId];\n  }\n  __syncthreads();\n\n  // Compute the sizes of each block.\n  if((localId > 0) && (sRadix1[localId] != sRadix1[localId - 1]) ) \n  {\n    sStartPointers[sRadix1[localId - 1]] = \n      localId - sStartPointers[sRadix1[localId - 1]];\n  }\n  if(sRadix1[localId + groupSize] != sRadix1[localId + groupSize - 1] ) \n  {\n    sStartPointers[sRadix1[localId + groupSize - 1]] = \n      localId + groupSize - sStartPointers[sRadix1[localId + groupSize - 1]];\n  }\n\n  if(localId == groupSize - 1) \n  {\n    sStartPointers[sRadix1[2 * groupSize - 1]] = \n      2 * groupSize - sStartPointers[sRadix1[2 * groupSize - 1]];\n  }\n  __syncthreads();\n\n  if(localId < 16) \n  {\n    counters[localId * totalBlocks + groupId] = sStartPointers[localId];\n  }\n}",
            "__global__ void reorderDataKeysOnlyK(\n          unsigned int*__restrict__ outKeys,\n    const unsigned int*__restrict__ keys,\n          unsigned int*__restrict__ blockOffsets,\n    const unsigned int*__restrict__ offsets,\n    const unsigned int startbit,\n    const unsigned int numElements,\n    const unsigned int totalBlocks)\n{\n  __shared__ unsigned int sOffsets[16];\n  __shared__ unsigned int sBlockOffsets[16];\n  __shared__ uint2 sKeys2[CTA_SIZE];\n\n  unsigned int *sKeys1 = (unsigned int*)sKeys2;\n\n  unsigned int groupId = blockIdx.x;\n  unsigned int localId = threadIdx.x;\n  unsigned int groupSize = blockDim.x;\n  unsigned int globalId = groupId * groupSize + localId;\n \n  sKeys2[localId] = reinterpret_cast<const uint2*>(keys)[globalId];\n\n  if(localId < 16)\n  {\n    sOffsets[localId]      = offsets[localId * totalBlocks + groupId];\n    sBlockOffsets[localId] = blockOffsets[groupId * 16 + localId];\n  }\n  __syncthreads();\n\n  unsigned int radix = (sKeys1[localId] >> startbit) & 0xF;\n  unsigned int globalOffset = sOffsets[radix] + localId - sBlockOffsets[radix];\n\n  if (globalOffset < numElements)\n  {\n    outKeys[globalOffset]   = sKeys1[localId];\n  }\n\n  radix = (sKeys1[localId + groupSize] >> startbit) & 0xF;\n  globalOffset = sOffsets[radix] + localId + groupSize - sBlockOffsets[radix];\n\n  if (globalOffset < numElements)\n  {\n    outKeys[globalOffset]   = sKeys1[localId + groupSize];\n  }\n}"
        ],
        "/Users/gbolet/hecbench-roofline/src/radixsort-cuda/Scan_kernels.cu": [
            "inline __host__ __device__ uint4 make_uint4(int4 a)\n{\n    return make_uint4(uint(a.x), uint(a.y), uint(a.z), uint(a.w));\n}\n\n__device__\ninline unsigned int scan1Inclusive(const unsigned int idata, \n                                   unsigned int* l_Data, const unsigned int size)\n{\n  if(size > WARP_SIZE){\n    //Bottom-level inclusive warp scan\n    unsigned int warpResult = warpScanInclusive(idata, l_Data, WARP_SIZE);\n\n    //Save top elements of each warp for exclusive warp scan\n    //sync to wait for warp scans to complete (because l_Data is being overwritten)\n    __syncthreads();\n\n    int lid = threadIdx.x;\n    if( (lid & (WARP_SIZE - 1)) == (WARP_SIZE - 1) )\n      l_Data[lid >> LOG2_WARP_SIZE] = warpResult;\n\n    //wait for warp scans to complete\n    __syncthreads();\n    if( lid < (WORKGROUP_SIZE / WARP_SIZE) ){\n      //grab top warp elements\n      unsigned int val = l_Data[lid] ;\n      //calculate exclsive scan and write back to shared memory\n      l_Data[lid] = warpScanExclusive(val, l_Data, size >> LOG2_WARP_SIZE);\n    }\n\n    //return updated warp scans with exclusive scan results\n    __syncthreads();\n    return warpResult + l_Data[lid >> LOG2_WARP_SIZE];\n  }else{\n    return warpScanInclusive(idata, l_Data, size);\n  }\n}\n\n__device__\ninline uint4 scan4Inclusive(uint4 data4, \n                            unsigned int* l_Data, const unsigned int size){\n  //Level-0 inclusive scan\n  data4.y += data4.x;\n  data4.z += data4.y;\n  data4.w += data4.z;\n\n  //Level-1 exclusive scan\n  unsigned int val = scan1Inclusive(data4.w, l_Data, size / 4) - data4.w;\n\n  return (data4 + make_uint4(val));\n}\n\n__device__\ninline uint4 scan4Exclusive(uint4 data4, \n                            unsigned int* l_Data, const unsigned int size)\n{\n  return scan4Inclusive(data4, l_Data, size) - data4;\n}\n\n__global__  void scanExclusiveLocal1K(\n            unsigned int*__restrict__ d_Dst,\n      const unsigned int*__restrict__ d_Src,\n      const unsigned int size)\n{\n    __shared__ unsigned int l_Data[2 * WORKGROUP_SIZE];\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n\n    //Load data\n    uint4 idata4 = reinterpret_cast<const uint4*>(d_Src)[i];\n\n    //Calculate exclusive scan\n    uint4 odata4 = scan4Exclusive(idata4, l_Data, size);\n\n    //Write back\n    reinterpret_cast<uint4*>(d_Dst)[i] = odata4;\n}",
            "__device__\ninline unsigned int scan1Inclusive(const unsigned int idata, \n                                   unsigned int* l_Data, const unsigned int size)\n{\n  if(size > WARP_SIZE){\n    //Bottom-level inclusive warp scan\n    unsigned int warpResult = warpScanInclusive(idata, l_Data, WARP_SIZE);\n\n    //Save top elements of each warp for exclusive warp scan\n    //sync to wait for warp scans to complete (because l_Data is being overwritten)\n    __syncthreads();\n\n    int lid = threadIdx.x;\n    if( (lid & (WARP_SIZE - 1)) == (WARP_SIZE - 1) )\n      l_Data[lid >> LOG2_WARP_SIZE] = warpResult;\n\n    //wait for warp scans to complete\n    __syncthreads();\n    if( lid < (WORKGROUP_SIZE / WARP_SIZE) ){\n      //grab top warp elements\n      unsigned int val = l_Data[lid] ;\n      //calculate exclsive scan and write back to shared memory\n      l_Data[lid] = warpScanExclusive(val, l_Data, size >> LOG2_WARP_SIZE);\n    }\n\n    //return updated warp scans with exclusive scan results\n    __syncthreads();\n    return warpResult + l_Data[lid >> LOG2_WARP_SIZE];\n  }else{\n    return warpScanInclusive(idata, l_Data, size);\n  }\n}\n\n__device__\ninline unsigned int scan1Exclusive(const unsigned int idata, \n                                   unsigned int* l_Data, const unsigned int size){\n  return scan1Inclusive(idata, l_Data, size) - idata;\n}\n\n__global__ void scanExclusiveLocal2K(\n            unsigned int*__restrict__ d_Buf,\n            unsigned int*__restrict__ d_Dst,\n      const unsigned int*__restrict__ d_Src,\n      const unsigned int N,\n      const unsigned int arrayLength)\n{\n    //Load top elements\n    //Convert results of bottom-level scan back to inclusive\n    //Skip loads and stores for inactive work-items of the work-group with highest index(pos >= N)\n\n    __shared__ unsigned int l_Data[2 * WORKGROUP_SIZE];\n    unsigned int data = 0;\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if(i < N)\n      data = d_Dst[(4 * WORKGROUP_SIZE - 1) + (4 * WORKGROUP_SIZE) * i] + \n             d_Src[(4 * WORKGROUP_SIZE - 1) + (4 * WORKGROUP_SIZE) * i];\n\n    //Compute\n    unsigned int odata = scan1Exclusive(data, l_Data, arrayLength);\n\n    //Avoid out-of-bound access\n    if(i < N) d_Buf[i] = odata;\n}",
            "inline __host__ __device__ uint4 make_uint4(int4 a)\n{\n    return make_uint4(uint(a.x), uint(a.y), uint(a.z), uint(a.w));\n}\n\n__global__ void uniformUpdateK(\n      unsigned int*__restrict__ d_Data,\n      unsigned int*__restrict__ d_Buf)\n{\n    __shared__ unsigned int buf[1];\n\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    uint4 data4 = reinterpret_cast<uint4*>(d_Data)[i];\n    if(threadIdx.x == 0)\n      buf[0] = d_Buf[blockIdx.x];\n\n    __syncthreads();\n    data4 += make_uint4(buf[0]);\n\n    reinterpret_cast<uint4*>(d_Data)[i] = data4;\n}"
        ]
    },
    "rainflow-cuda": {
        "/Users/gbolet/hecbench-roofline/src/rainflow-cuda/main.cu": [
            "inline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\ninline __host__ __device__ float4 fabs(float4 v)\n{\n    return make_float4(fabs(v.x), fabs(v.y), fabs(v.z), fabs(v.w));\n}\n\n__device__\nvoid Extrema(const double* history, const int history_length, double *result, int& result_length)\n{\n  result[0] = history[0];\n\n  int eidx = 0;\n  for (int i = 1; i < history_length - 1; i++)\n    if ((history[i] > result[eidx] && history[i] > history[i + 1]) ||\n        (history[i] < result[eidx] && history[i] < history[i + 1]))\n      result[++eidx] = history[i];\n\n  result[++eidx] = history[history_length - 1];\n  result_length = eidx + 1;\n}\n\n__device__\nvoid Execute(const double* history, const int history_length,\n             double *extrema, int* points, double3 *results,\n             int *results_length )\n{\n  int extrema_length = 0;\n  Extrema(history, history_length, extrema, extrema_length);\n\n  int pidx = -1, eidx = -1, ridx = -1;\n\n  for (int i = 0; i < extrema_length; i++)\n  {\n    points[++pidx] = ++eidx;\n    double xRange, yRange;\n    while (pidx >= 2 && (xRange = fabs(extrema[points[pidx - 1]] - extrema[points[pidx]]))\n           >= (yRange = fabs(extrema[points[pidx - 2]] - extrema[points[pidx - 1]])))\n    {\n      double yMean = 0.5 * (extrema[points[pidx - 2]] + extrema[points[pidx - 1]]);\n\n      if (pidx == 2)\n      {\n        results[++ridx] = make_double3( 0.5, yRange, yMean );\n        points[0] = points[1];\n        points[1] = points[2];\n        pidx = 1;\n      }\n      else\n      {\n        results[++ridx] = make_double3( 1.0, yRange, yMean );\n        points[pidx - 2] = points[pidx];\n        pidx -= 2;\n      }\n    }\n  }\n\n  for (int i = 0; i <= pidx - 1; i++)\n  {\n    double range = fabs(extrema[points[i]] - extrema[points[i + 1]]);\n    double mean = 0.5 * (extrema[points[i]] + extrema[points[i + 1]]);\n    results[++ridx] = make_double3 ( 0.5, range, mean );\n  }\n\n  *results_length = ridx + 1;\n}\n\n__global__\nvoid rainflow_count(const double *__restrict__ history,\n                    const int *__restrict__ history_lengths,\n                    double *__restrict__ extrema,\n                       int *__restrict__  points,\n                    double3 *__restrict__ results,\n                    int *__restrict__ result_length,\n                    const int num_history )\n{\n  int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i >= num_history) return;\n\n  const int offset = history_lengths[i];\n  const int history_length = history_lengths[i+1] - offset;\n  Execute(history + offset, \n          history_length,\n          extrema + offset,\n          points + offset,\n          results + offset,\n          result_length + i);\n}"
        ]
    },
    "relu-cuda": {
        "/Users/gbolet/hecbench-roofline/src/relu-cuda/main.cu": [
            "__global__\nvoid ReluGrad_impl1(const half*__restrict__ gradient,\n                    const half*__restrict__ feature,\n                          half*__restrict__ backprop,\n                    const int count)\n{\n  int half2_count = count >> 1;\n  int index = blockIdx.x * blockDim.x + threadIdx.x;\n  const int total_device_threads = gridDim.x * blockDim.x;\n\n  while (index < half2_count) {\n    // The fast branch.\n    // One half2, two fp16, is fetched and processed at a time.\n    half2 gradient_h2 = reinterpret_cast<const half2*>(gradient)[index];\n    half2 feature_h2 = reinterpret_cast<const half2*>(feature)[index];\n    half2* p_backprop_h2 = reinterpret_cast<half2*>(backprop) + index;\n\n    // Assume half2 primitives are available.\n    const half2 kZero_h2 = __float2half2_rn(0.f);\n    // mask = (feature > 0)\n    half2 mask_h2 = __hgt2(feature_h2, kZero_h2);\n    // backprop = mask * gradient\n    half2 backprop_h2 = __hmul2(mask_h2, gradient_h2);\n\n    // Write back the result.\n    *p_backprop_h2 = backprop_h2;\n\n    index += total_device_threads;\n  }\n\n  if ((count & 0x1) == 1 && index == half2_count) {\n    // If the total number of the elements is odd, process the last element.\n    half grad_h = gradient[count - 1];\n    half feature_h = feature[count - 1];\n\n    float grad_f = static_cast<float>(grad_h);\n    float feature_f = static_cast<float>(feature_h);\n    float backprop_f = (feature_f > 0) ? grad_f : 0;\n\n    half backprop_h(backprop_f);\n    backprop[count - 1] = backprop_h;\n  }\n}",
            "__global__\nvoid ReluGrad_impl2(const half* __restrict__ gradient,\n                    const half* __restrict__ feature,\n                          half* __restrict__ backprop,\n                    const int count)\n{\n  int half8_count = count / VectorSize;\n  int index = blockIdx.x * blockDim.x + threadIdx.x;\n\n  if (index < half8_count) {\n    float4 gradient_h8 = reinterpret_cast<const float4*>(gradient)[index];\n    float4 feature_h8 = reinterpret_cast<const float4*>(feature)[index];\n    float4* p_backprop_h8 = reinterpret_cast<float4*>(backprop) + index;\n\n    half2 *gradient_h2 = reinterpret_cast<half2*>(&gradient_h8);\n    half2 *feature_h2 = reinterpret_cast<half2*>(&feature_h8);\n    float4 backprop_h8;\n    half2* p_backprop_h2 = reinterpret_cast<half2*>(&backprop_h8);\n\n    // Assume half2 primitives are available.\n    const half2 kZero_h2 = __float2half2_rn(0.f);\n\n    for (int i = 0; i < VectorSize / 2; i++) {\n      // mask = (feature > 0)\n      half2 mask_h2 = __hgt2(feature_h2[i], kZero_h2);\n      // backprop = mask * gradient\n      half2 backprop_h2 = __hmul2(mask_h2, gradient_h2[i]);\n      p_backprop_h2[i] = backprop_h2;\n    }\n    // Write back the result.\n    *p_backprop_h8 = backprop_h8;\n  }\n\n  int remaining_count = (count % VectorSize);\n\n  if (index < remaining_count) {\n    // Use first threads to process the remaining elements.\n    half grad_h = gradient[half8_count * VectorSize + index];\n    half feature_h = feature[half8_count * VectorSize + index];\n\n    float grad_f = static_cast<float>(grad_h);\n    float feature_f = static_cast<float>(feature_h);\n    float backprop_f = (feature_f > 0) ? grad_f : 0;\n\n    half backprop_h(backprop_f);\n    backprop[half8_count * VectorSize + index] = backprop_h;\n  }\n}",
            "inline __host__ __device__ uint4 make_uint4(int4 a)\n{\n    return make_uint4(uint(a.x), uint(a.y), uint(a.z), uint(a.w));\n}\n\ninline __host__ __device__ uint4 max(uint4 a, uint4 b)\n{\n    return make_uint4(max(a.x,b.x), max(a.y,b.y), max(a.z,b.z), max(a.w,b.w));\n}\n\n__global__\nvoid Relu_impl1(int count, const int* input, int* output)\n{\n  int index = blockIdx.x * blockDim.x + threadIdx.x;\n  if (index < count) {\n    uint4 v;\n    char4 b;\n    b.x = input[index] & 0xFF;\n    b.y = (input[index] >> 8) & 0xFF;\n    b.z = (input[index] >> 16) & 0xFF;\n    b.w = (input[index] >> 24) & 0xFF;\n    v.x = max(b.x, 0);\n    v.y = max(b.y, 0);\n    v.z = max(b.z, 0);\n    v.w = max(b.w, 0);\n    output[index] = v.w << 24 | v.z << 16 | v.y << 8 | v.x;\n  }\n}",
            "__global__\nvoid Relu_impl2(int count, const int* input, int* output)\n{\n  int index = blockIdx.x * blockDim.x + threadIdx.x;\n  if (index < count) {\n    output[index] = __vmaxs4(input[index], 0);\n  }\n}"
        ]
    },
    "openmp-cuda": {
        "/Users/gbolet/hecbench-roofline/src/openmp-cuda/main.cu": [
            "__global__ void addConstant(int *g_a, const int b, const int repeat) {\n  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  for (int i = 0; i < repeat; i++)\n    g_a[idx] += i % b;\n}"
        ]
    },
    "md-cuda": {
        "/Users/gbolet/hecbench-roofline/src/md-cuda/main.cu": [
            "#define FORCEVECTYPE double4\n\n\n#define FPTYPE double\n\n\n#define POSVECTYPE double4\n\n\n__global__ void md (\n  const POSVECTYPE* __restrict__ position,\n        FORCEVECTYPE* __restrict__ force,\n  const int* __restrict__ neighborList, \n  const int nAtom,\n  const int maxNeighbors, \n  const FPTYPE lj1_t,\n  const FPTYPE lj2_t,\n  const FPTYPE cutsq_t )\n{\n  const uint idx = blockDim.x * blockIdx.x + threadIdx.x;\n  if (idx >= nAtom) return;\n\n  POSVECTYPE ipos = position[idx];\n  FORCEVECTYPE f = zero;\n\n  int j = 0;\n  while (j < maxNeighbors)\n  {\n    int jidx = neighborList[j*nAtom + idx];\n\n    // Uncoalesced read\n    POSVECTYPE jpos = position[jidx];\n\n    // Calculate distance\n    FPTYPE delx = ipos.x - jpos.x;\n    FPTYPE dely = ipos.y - jpos.y;\n    FPTYPE delz = ipos.z - jpos.z;\n    FPTYPE r2inv = delx*delx + dely*dely + delz*delz;\n\n    // If distance is less than cutoff, calculate force\n    if (r2inv > 0 && r2inv < cutsq_t)\n    {\n      r2inv = (FPTYPE)1.0 / r2inv;\n      FPTYPE r6inv = r2inv * r2inv * r2inv;\n      FPTYPE forceC = r2inv * r6inv * (lj1_t * r6inv - lj2_t);\n\n      f.x += delx * forceC;\n      f.y += dely * forceC;\n      f.z += delz * forceC;\n    }\n    j++;\n  }\n  force[idx] = f;\n}"
        ]
    },
    "lrn-cuda": {
        "/Users/gbolet/hecbench-roofline/src/lrn-cuda/kernels.h": [
            "inline __host__ __device__ uint4 make_uint4(int4 a)\n{\n    return make_uint4(uint(a.x), uint(a.y), uint(a.z), uint(a.w));\n}\n\ninline __host__ __device__ uint4 max(uint4 a, uint4 b)\n{\n    return make_uint4(max(a.x,b.x), max(a.y,b.y), max(a.z,b.z), max(a.w,b.w));\n}\n\ninline __host__ __device__ uint4 min(uint4 a, uint4 b)\n{\n    return make_uint4(min(a.x,b.x), min(a.y,b.y), min(a.z,b.z), min(a.w,b.w));\n}\n\ninline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\ninline __host__ __device__ float4 sqrtf(float4 v)\n{\n    return make_float4(sqrtf(v.x), sqrtf(v.y), sqrtf(v.z), sqrtf(v.w));\n}\n\n__global__ void ker(const float * cormat, float * upper,int n)\n{\n  size_t idx = blockDim.x*blockIdx.x+threadIdx.x;\n  if (idx < (size_t)n * n) {\n    size_t i = idx/n;\n    size_t j = idx%n;\n    if(i<j)\n    {\n      size_t t = n * i - i * (i+1) / 2 + j - i - 1;\n      //printf(\"(%lu, %lu) %lu %lu\\n\", i, j, t, i*n+j);\n      upper[t]=cormat[j*n+i];\n    }\n  }\n}\n\n__global__\nvoid lrn_fwd_kernel(\n    const float* __restrict__ src_, \n          float* __restrict__ dst_,\n    int64_t N_,\n    int64_t C_, \n    int64_t D_, \n    int64_t H_, \n    int64_t W_, \n    int64_t stride_mb_, \n    int64_t ndims_, \n    int64_t wk_size_, \n    int64_t size_, \n    float alpha_, \n    float beta_, \n    float k_)\n{\n  for (int64_t idx = blockIdx.x * blockDim.x + threadIdx.x;\n               idx < wk_size_ ; idx += blockDim.x * gridDim.x) {\n\n    auto data_off = [=](int64_t mb, int64_t c, int64_t d, int64_t h, int64_t w) {\n      int64_t tag = 0;\n      switch (tag) {\n        case 0 : return mb * stride_mb_ + c * H_ * W_ + h * W_ + w;\n        case 1 : return mb * stride_mb_ + h * W_ * C_ + w * C_ + c;\n        default:\n           return (int64_t)1;\n      }\n    };\n\n    auto ker = [=](int64_t mb, int64_t oc, int64_t od, int64_t oh, int64_t ow) {\n      float sum = 0;\n      const int64_t half_size = (size_ - 1) / 2;\n      bool across_channel = 1;\n      if (across_channel) {\n        const int64_t c_st = max(oc - half_size + 0, (int64_t)0);\n        const int64_t c_en = min(oc + half_size + 1, C_);\n\n        for (int64_t c = c_st; c < c_en; ++c) {\n          const auto s_off = data_off(mb, c, od, oh, ow);\n          const auto s = src_[s_off];\n          sum+=s*s;\n        }\n      } else {\n        int64_t d_st = max(od - half_size + 0, (int64_t)0);\n        int64_t d_en = min(od + half_size + 1, D_);\n        int64_t h_st = max(oh - half_size + 0, (int64_t)0);\n        int64_t h_en = min(oh + half_size + 1, H_);\n        int64_t w_st = max(ow - half_size + 0, (int64_t)0);\n        int64_t w_en = min(ow + half_size + 1, W_);\n        for (int64_t d = d_st; d < d_en; ++d) {\n          for (int64_t h = h_st; h < h_en; ++h) {\n            for (int64_t w = w_st; w < w_en; ++w) {\n              const auto s_off = data_off(mb, oc, d, h, w);\n              const auto s = src_[s_off];\n              sum+=s*s;\n            }\n          }\n        }\n      }\n      sum = k_ + alpha_ * sum / size_;\n      const auto s_off = data_off(mb, oc, od, oh, ow);\n      const auto s = src_[s_off];\n      return (s * sqrtf(1.0f / (sqrtf(sum) * sum)));\n    };\n\n    auto Operation = [=]( int64_t mb, int64_t c, int64_t d, int64_t h, int64_t w) {\n      bool channel = 0;\n      if(channel) {\n        const int64_t off = mb * stride_mb_ + h * W_ * C_ + w * C_ + c;\n        auto val = ker(mb, c, 0, h, w);\n        dst_[off] = val;\n      }   \n      else {\n        const int64_t off = data_off(mb, c, d, h, w);\n        auto val = ker(mb, c, d, h, w);\n        dst_[off] = val;\n      }\n    };\n\n    int64_t n = (idx / (C_ * D_ * H_ * W_)) % N_;\n    int64_t c = (idx / (D_ * H_ * W_)) % C_;\n    int64_t d = (idx / (H_ * W_)) % D_;\n    int64_t h = (idx / (W_)) % H_;\n    int64_t w = (idx / (1)) % W_;\n\n    Operation(n, c, d, h, w);\n  }\n}",
            "inline __host__ __device__ uint4 make_uint4(int4 a)\n{\n    return make_uint4(uint(a.x), uint(a.y), uint(a.z), uint(a.w));\n}\n\ninline __host__ __device__ uint4 max(uint4 a, uint4 b)\n{\n    return make_uint4(max(a.x,b.x), max(a.y,b.y), max(a.z,b.z), max(a.w,b.w));\n}\n\ninline __host__ __device__ uint4 min(uint4 a, uint4 b)\n{\n    return make_uint4(min(a.x,b.x), min(a.y,b.y), min(a.z,b.z), min(a.w,b.w));\n}\n\ninline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\ninline __host__ __device__ float4 sqrtf(float4 v)\n{\n    return make_float4(sqrtf(v.x), sqrtf(v.y), sqrtf(v.z), sqrtf(v.w));\n}\n\n__global__ void ker(const float * cormat, float * upper,int n)\n{\n  size_t idx = blockDim.x*blockIdx.x+threadIdx.x;\n  if (idx < (size_t)n * n) {\n    size_t i = idx/n;\n    size_t j = idx%n;\n    if(i<j)\n    {\n      size_t t = n * i - i * (i+1) / 2 + j - i - 1;\n      //printf(\"(%lu, %lu) %lu %lu\\n\", i, j, t, i*n+j);\n      upper[t]=cormat[j*n+i];\n    }\n  }\n}\n\n__global__\nvoid  lrn_bwd_kernel(\n    const float* __restrict__ src_, \n          float* __restrict__ dst_,\n          float* __restrict__ diff_src_mem_, \n    int64_t N_,\n    int64_t C_, \n    int64_t D_, \n    int64_t H_, \n    int64_t W_, \n    int64_t stride_mb_, \n    int64_t ndims_, \n    int64_t wk_size_, \n    int64_t size_, \n    float alpha_, \n    float beta_, \n    float k_)\n{\n  for (int64_t idx = blockIdx.x * blockDim.x + threadIdx.x;\n               idx < wk_size_; idx += blockDim.x * gridDim.x) {\n\n    auto data_off = [=](int64_t mb, int64_t c, int64_t d, int64_t h, int64_t w) {\n      int64_t tag = 0;\n      switch (tag) {\n        case 0 : return mb * stride_mb_ + c * H_ * W_ + h * W_ + w;\n        case 1 : return mb * stride_mb_ + h * W_ * C_ + w * C_ + c;\n        default:\n           return (int64_t)1;\n      }\n    };\n\n    auto get_omega = [=](int64_t mb, int64_t oc, int64_t od, int64_t oh, int64_t ow) {\n      auto sum = 0;\n      const int64_t half_size = (size_ - 1) / 2;\n      bool across_channel = 1;\n      if (across_channel) {\n        const int64_t c_st = max(oc - half_size + 0, (int64_t)0);\n        const int64_t c_en = min(oc + half_size + 1, C_);\n\n        for (int64_t c = c_st; c < c_en; ++c) {\n          const auto s_off = data_off(mb, c, od, oh, ow);\n          const auto s = src_[s_off];\n          sum += s * s;\n        }\n      } else {\n        int64_t d_st = max(od - half_size + 0, (int64_t)0);\n        int64_t d_en = min(od + half_size + 1, D_);\n        int64_t h_st = max(oh - half_size + 0, (int64_t)0);\n        int64_t h_en = min(oh + half_size + 1, H_);\n        int64_t w_st = max(ow - half_size + 0, (int64_t)0);\n        int64_t w_en = min(ow + half_size + 1, W_);\n        for (int64_t d = d_st; d < d_en; ++d)\n          for (int64_t h = h_st; h < h_en; ++h)\n            for (int64_t w = w_st; w < w_en; ++w) {\n              const auto s_off = data_off(mb, oc, d, h, w);\n              const auto s = src_[s_off];\n              sum += s * s;\n            }\n      }\n      return (k_ + alpha_ * sum / size_);\n    };\n\n    auto ker = [=](int64_t mb, int64_t oc, int64_t od, int64_t oh, int64_t ow) {\n      float A = 0, B = 0;\n      const int64_t half_size = (size_ - 1) / 2;\n      bool across_channel = 1;\n      if (across_channel) {\n        const int64_t c_st = max(oc - half_size + 0, (int64_t)0);\n        const int64_t c_en = min(oc + half_size + 1, C_);\n\n        for (int64_t c = c_st; c < c_en; ++c) {\n          const auto off = data_off(mb, c, od, oh, ow);\n          const auto omega = get_omega(mb, c, od, oh, ow);\n          const auto omega_in_beta\n            = sqrtf(1.0f / (sqrtf(omega) * omega));\n\n          const auto dst_val = dst_[off];\n          const auto tmp = omega_in_beta * dst_val;\n          if (c == oc) A = tmp;\n          const auto src_val = src_[off];\n          B += (src_val * tmp / omega);\n        }\n      } else {\n        int64_t d_st = max(od - half_size + 0, (int64_t)0);\n        int64_t d_en = min(od + half_size + 1, D_);\n        int64_t h_st = max(oh - half_size + 0, (int64_t)0);\n        int64_t h_en = min(oh + half_size + 1, H_);\n        int64_t w_st = max(ow - half_size + 0, (int64_t)0);\n        int64_t w_en = min(ow + half_size + 1, W_);\n        for (int64_t d = d_st; d < d_en; ++d)\n          for (int64_t h = h_st; h < h_en; ++h)\n            for (int64_t w = w_st; w < w_en; ++w) {\n              const auto off = data_off(mb, oc, d, h, w);\n              const auto omega = get_omega(mb, oc, d, h, w);\n              const auto omega_in_beta\n                = sqrtf(1.0f / (sqrtf(omega) * omega));\n\n              const auto dst_val = dst_[off];\n              const auto tmp\n                = omega_in_beta * dst_val;\n              if (d == od && h == oh && w == ow) A = tmp;\n              const auto src_val = src_[off];\n              B += (src_val * tmp / omega);\n            }\n      }\n      const auto off = data_off(mb, oc, od, oh, ow);\n      const auto src_val = src_[off];\n      B *= (2.0f * alpha_ * beta_ * src_val / size_);\n      return (A - B);\n    };\n\n    auto Operation = [=]( int64_t mb, int64_t c, int64_t d, int64_t h, int64_t w) {\n      bool channel = 0;\n      if(channel) {\n        const int64_t off = mb * stride_mb_ + h * W_ * C_ + w * C_ + c;\n        auto val = ker(mb, c, 0, h, w);\n        dst_[off] = val;\n      }   \n      else {\n        const int64_t off = data_off(mb, c, d, h, w);\n        auto val = ker(mb, c, d, h, w);\n        diff_src_mem_[off] = val;\n      }\n    };\n\n    auto n = (idx / (C_ * D_ * H_ * W_)) % N_;\n    auto c = (idx / (D_ * H_ * W_)) % C_;\n    auto d = (idx / (H_ * W_)) % D_;\n    auto h = (idx / (W_)) % H_;\n    auto w = (idx / (1)) % W_;\n\n    Operation(n, c, d, h, w);\n  }\n}"
        ]
    },
    "unfold-cuda": {
        "/Users/gbolet/hecbench-roofline/src/unfold-cuda/main.cu": [
            "__inline__ __device__ float f(unsigned int x, unsigned int y, unsigned int z)\n{\n  constexpr float d(2.0f / N);\n  float xf((int(x - Nd2)) * d);//[-1, 1)\n  float yf((int(z - Nd2)) * d);\n  float zf((int(z - Nd2)) * d);\n  return 1.f - 16.f * xf * yf * zf - 4.f * (xf * xf + yf * yf + zf * zf);\n}\n\n__global__\nvoid unfold_backward_elementwise_kernel(int total_n_elems, func_t f) {\n  constexpr int total_work_block = n_threads * n_elems_per_thread;\n  int idx = total_work_block * blockIdx.x + threadIdx.x;\n  #pragma unroll\n  for (int i = 0; i < n_elems_per_thread; ++i) {\n    if (idx < total_n_elems) {\n      f(idx);\n      idx += n_threads;\n    }\n  }\n}"
        ]
    },
    "bm3d-cuda": {
        "/Users/gbolet/hecbench-roofline/src/bm3d-cuda/blockmatching.cu": [
            "#define T ((int)32)\n\n\n__device__ __inline__ T L2p2(const T i1, const T i2)\n{\n  T diff = i1 - i2;\n  return diff*diff;\n}\n\n__device__\nvoid add_to_matched_image(\n    uint *stack,         //IN/OUT: Stack of N patches matched to current reference patch\n    uchar *num_patches_in_stack,//IN/OUT: Number of patches in stack\n    const uint value,       //IN: [..DIFF(ushort)..|..LOC_Y(sbyte)..|..LOC_X(sbyte)..]\n    const Params & params    //IN: Denoising parameters\n    )\n{\n  //stack[*num_patches_in_stack-1] is most similar (lowest number)\n  int k;\n\n  uchar num = (*num_patches_in_stack);\n  if (num < params.N) //add new value\n  {\n    k = num++;\n    while(k > 0 && value > stack[k-1])\n    {\n      stack[k] = stack[k-1];\n      --k;\n    }\n\n    stack[k] = value;\n    *num_patches_in_stack = num;\n  }\n  else if (value >= stack[0]) \n    return;  \n  else //delete highest value and add new\n  {\n    k = 1;\n    while (k < params.N && value < stack[k])\n    {\n      stack[k-1] = stack[k];\n      k++;\n    }\n    stack[k-1] = value;\n  }\n}\n\n__device__ __inline__ uint flp2 (uint x)\n{\n  return (0x80000000u >> __clz(x));\n}\n\n__global__\nvoid block_matching(\n    const  uchar* __restrict image, //IN: Original image\n    ushort* __restrict g_stacks,         //OUT: For each reference patch contains addresses of similar patches (patch is adressed by top left corner) [..LOC_Y(sbyte)..|..LOC_X(sbyte)..]\n    uint* __restrict g_num_patches_in_stack,  //OUT: For each reference patch contains number of similar patches\n    const uint2 image_dim,      //IN: Image dimensions\n    const uint2 stacks_dim,      //IN: Size of area, where reference patches could be located\n    const Params params,      //IN: Denoising parameters\n    const uint2 start_point)    //IN: Address of the top-left reference patch of a batch\n{\n  //One block is processing warpSize patches (because each warp is computing distance of same warpSize patches from different displaced patches)\n  int tid = threadIdx.x % warpSize;\n  int wid = threadIdx.x / warpSize;\n  int num_warps = blockDim.x/warpSize;\n\n  //p_block denotes reference rectangle on which current cuda block is computing\n  uint p_rectangle_width = ((warpSize-1) * params.p) + params.k;\n  uint p_rectangle_start = start_point.x + blockIdx.x * warpSize * params.p;\n\n  //Shared arrays\n  extern __shared__ uint s_data[];\n  uint *s_diff = s_data; //SIZE: p_rectangle_width*num_warps\n  uint *s_stacks = &s_data[p_rectangle_width*num_warps]; //SIZE: params.N*num_warps*warpSize\n  uchar *s_patches_in_stack = (uchar*)&s_data[num_warps*(p_rectangle_width + params.N*warpSize)]; //SIZE: num_warps*warpSize\n  uchar *s_image_p = (uchar*)&s_patches_in_stack[num_warps*warpSize]; //SIZE: p_rectangle_width*params.k\n\n  s_diff += idx2(0, wid, p_rectangle_width);\n\n  //Initialize s_patches_in_stack to zero\n  s_patches_in_stack[ idx2(tid, wid, warpSize) ] = 0;\n\n  int2 p; //Address of reference patch\n  int2 q; //Address of patch against which the difference is computed\n\n  p.x = p_rectangle_start + (tid*params.p);\n  p.y = start_point.y + (blockIdx.y*params.p);\n\n  //Ensure, that the bottom most patches will be taken as reference patches regardless the p parameter.\n  if (p.y >= stacks_dim.y && p.y < stacks_dim.y + params.p - 1)\n    p.y = stacks_dim.y - 1;\n  else if (p.y >= stacks_dim.y) return;\n\n  //Ensure, that the right most patches will be taken as reference patches regardless the p parameter.\n  uint inner_p_x = tid*params.p;\n  if (p.x >= stacks_dim.x && p.x < stacks_dim.x + params.p - 1)\n  {\n    inner_p_x -= (p.x - (stacks_dim.x - 1));\n    p.x = stacks_dim.x - 1;\n  }\n\n  //Load reference patches needed by actual block to shared memory\n  for(int i = threadIdx.x; i < p_rectangle_width*params.k; i+=blockDim.x)\n  {\n    int sx = i % p_rectangle_width;\n    int sy = i / p_rectangle_width;\n    if (p_rectangle_start+sx >= image_dim.x) continue;\n    s_image_p[i] = image[idx2(p_rectangle_start+sx,p.y+sy,image_dim.x)];\n  }\n\n  __syncthreads();\n\n  //scale difference so that it can fit ushort\n  uint shift = (__clz(params.Tn) < 16u) ? 16u - (uint)__clz(params.Tn) : 0;\n\n\n  //Ensure that displaced patch coordinates (q) will be positive\n  int2 from;\n  from.y = (p.y - (int)params.n < 0) ? -p.y : -(int)params.n;\n  from.x = (((int)p_rectangle_start) - (int)params.n < 0) ? -((int)p_rectangle_start) : -(int)params.n;\n  from.x += wid;\n\n  //For each displacement (x,y) in n neighbourhood\n  for(int y = from.y; y <= (int)params.n; ++y)\n  {\n    q.y = p.y + y;\n    if (q.y >= stacks_dim.y) break;\n\n    for(int x = from.x; x <= (int)params.n; x += num_warps)\n    {\n      //Reference patch is always the most similar to itself (there is no need to copute it)\n      if (x == 0 && y == 0) continue; \n\n      //Each warp is computing the same patch with slightly different displacement.\n      //Compute distance of reference patch p from current patch q which is dispaced by (x+tid,y)\n\n      //q_block denotes displaced rectangle which is processed by the current warp\n      uint q_rectangle_start = p_rectangle_start + x;\n      q.x = q_rectangle_start + inner_p_x;\n\n      //Compute distance for each column of reference patch\n      for(uint i = tid; i < p_rectangle_width && p_rectangle_start+i < image_dim.x && \n                        q_rectangle_start+i < image_dim.x; i+=warpSize)\n      {\n        uint dist = 0;\n        for(uint iy = 0; iy < params.k; ++iy)\n        {\n          dist += L2p2((int)s_image_p[ idx2(i, iy, p_rectangle_width) ], \n                       (int)image[ idx2(q_rectangle_start+i, q.y+iy, image_dim.x) ]);\n        }\n        s_diff[i] = dist;\n      }\n\n      if (p.x >= stacks_dim.x || q.x >= stacks_dim.x) continue;\n\n      //Sum column distances to obtain patch distance\n      uint diff = 0;\n      for (uint i = 0; i < params.k; ++i) \n        diff += s_diff[inner_p_x + i];\n\n      //Distance threshold\n      if(diff < params.Tn)\n      {\n        uint loc_y = (uint)((q.y - p.y) & 0xFF); //relative location y (-127 to 127)\n        uint loc_x = (uint)((q.x - p.x) & 0xFF); //relative location x (-127 to 127)\n        diff >>= shift;\n        diff <<= 16u; // [..DIFF(ushort)..|..LOC_Y(sbyte)..|..LOC_X(sbyte)..]\n        diff |= (loc_y << 8u);\n        diff |= loc_x;\n\n        //Add current patch to s_stacks\n        add_to_matched_image( \n            &s_stacks[ params.N * idx2(tid, wid, warpSize) ],\n            &s_patches_in_stack[ idx2(tid, wid, warpSize) ],\n            diff,\n            params\n            );\n      }\n    }\n  }\n\n  __syncthreads();\n\n  uint batch_size = gridDim.x*warpSize;\n  uint block_address_x = blockIdx.x*warpSize+tid;\n\n  if (wid > 0) return;\n  //Select N most similar patches for each reference patch from stacks in shared memory and save them to global memory\n  //Each thread represents one reference patch \n  //Each thread will find N most similar blocks in num_warps stacks (which were computed by different warps) and save them into global memory\n  //In shared memory the most similar patch is at the end, in global memory the order does not matter\n  //DEV: performance impact cca 8%\n  if (p.x >= stacks_dim.x) return;\n\n  int j;\n  for (j = 0; j < params.N; ++j)\n  {\n    uint count = 0;\n    uint minIdx = 0;\n    uint minVal = 0xFFFFFFFF; //INF\n\n    //Finds patch with minimal value of remaining\n    for (int i = minIdx; i < num_warps; ++i)\n    {\n      count = (uint)s_patches_in_stack[ idx2(tid, i, warpSize) ];\n      if (count == 0) continue;\n\n      uint newMinVal = s_stacks[ idx3(count-1,tid,i,params.N,warpSize) ];\n      if (newMinVal < minVal)\n      {\n        minVal = newMinVal;\n        minIdx = i;\n      }\n    }\n    if (minVal == 0xFFFFFFFF) break; //All stacks are empty\n\n    //Remove patch from shared stack\n    s_patches_in_stack[ idx2(tid, minIdx, warpSize) ]--;\n\n    //Adds patch to stack in global memory\n    g_stacks[idx3(j, block_address_x, blockIdx.y, params.N, batch_size)] = (ushort)(minVal & 0xFFFF);\n  }\n  //Save to the global memory the number of similar patches rounded to the nearest lower power of two\n  g_num_patches_in_stack[ idx2(block_address_x ,blockIdx.y, batch_size) ] = flp2((uint)j+1)-1;\n}"
        ],
        "/Users/gbolet/hecbench-roofline/src/bm3d-cuda/filtering.cu": [
            "__device__ inline void get_block_addresses(\n  const uint2 & start_point,    //IN: first reference patch of a batch\n  const uint & patch_stack_size,  //IN: maximal size of a 3D group\n  const uint2 & stacks_dim,    //IN: Size of area, where reference patches could be located\n  const Params & params,      //IN: Denoising parameters\n  uint2 & outer_address,      //OUT: Coordinetes of reference patch in the image\n  uint & start_idx)        //OUT: Address of a first element of the 3D group in stacks array\n{\n  //One block handles one patch_stack, data are in array one after one.\n  start_idx = patch_stack_size * idx2(blockIdx.x,blockIdx.y,gridDim.x);\n  \n  outer_address.x = start_point.x + (blockIdx.x * params.p);\n  outer_address.y = start_point.y + (blockIdx.y * params.p);\n\n  //Ensure, that the bottom most patches will be taken as reference patches regardless the p parameter.\n  if (outer_address.y >= stacks_dim.y && outer_address.y < stacks_dim.y + params.p - 1)\n    outer_address.y = stacks_dim.y - 1;\n  //Ensure, that the right most patches will be taken as reference patches regardless the p parameter.\n  if (outer_address.x >= stacks_dim.x && outer_address.x < stacks_dim.x + params.p - 1)\n    outer_address.x = stacks_dim.x - 1;\n}\n\n__global__\nvoid get_block(\n    const uint2 start_point,                       //IN: first reference patch of a batch\n    const uchar* __restrict image,                 //IN: image\n    const ushort* __restrict stacks,               //IN: array of adresses of similar patches\n    const uint* __restrict g_num_patches_in_stack, //IN: numbers of patches in 3D groups\n    float* __restrict patch_stack,                 //OUT: assembled 3D groups\n    const uint2 image_dim,                         //IN: image dimensions\n    const uint2 stacks_dim,                        //IN: dimensions limiting addresses of reference patches\n    const Params params)                           //IN: denoising parameters\n{\n  \n  \n  uint startidx;\n  uint2 outer_address;\n  get_block_addresses(start_point,  params.k*params.k*(params.N+1), stacks_dim, params, outer_address, startidx);\n\n  if (outer_address.x >= stacks_dim.x || outer_address.y >= stacks_dim.y) return;\n  \n  patch_stack += startidx;\n  \n  const ushort* z_ptr = &stacks[ idx3(0, blockIdx.x, blockIdx.y, params.N,  gridDim.x) ];\n\n  uint num_patches = g_num_patches_in_stack[ idx2(blockIdx.x, blockIdx.y, gridDim.x) ];\n  \n  patch_stack[ idx3(threadIdx.x, threadIdx.y, 0, params.k, params.k) ] = (float)(image[ idx2(outer_address.x+threadIdx.x, outer_address.y+threadIdx.y, image_dim.x)]);\n  for(uint i = 0; i < num_patches; ++i)\n  {\n    int x = (int)((signed char)(z_ptr[i] & 0xFF));\n    int y = (int)((signed char)((z_ptr[i] >> 8) & 0xFF));\n    patch_stack[ idx3(threadIdx.x, threadIdx.y, i+1, params.k, params.k) ] = (float)(image[ idx2(outer_address.x+x+threadIdx.x, outer_address.y+y+threadIdx.y, image_dim.x)]);\n  }\n}",
            "#define T ((int)32)\n\n\n__inline__ __device__ T warpReduceSum(T val)\n{\n#pragma unroll\n  for (int mask = 16; mask > 0; mask >>= 1)\n    val += __shfl_xor_sync(FINAL_MASK, val, mask, 32);\n  return val;\n}\n\n__device__ __inline__ uint ilog2(IntType n)\n{\n  uint l;\n  for (l = 0; n; n >>= 1, ++l);\n  return l;\n}\n\n__global__ \nvoid rotate (const int n, const float angle, const float3 w, float3 *d)\n{\n  int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i >= n) return;\n\n  float s, c;\n  sincosf(angle, &s,&c);\n  \n  const float3 p = d[i];\n  const float mc = 1.f - c;\n\n  // Rodrigues' formula:\n  float m1 = c+(w.x)*(w.x)*(mc);\n  float m2 = (w.z)*s+(w.x)*(w.y)*(mc);\n  float m3 =-(w.y)*s+(w.x)*(w.z)*(mc);\n  \n  float m4 =-(w.z)*s+(w.x)*(w.y)*(mc);\n  float m5 = c+(w.y)*(w.y)*(mc);\n  float m6 = (w.x)*s+(w.y)*(w.z)*(mc);\n  \n  float m7 = (w.y)*s+(w.x)*(w.z)*(mc);\n  float m8 =-(w.x)*s+(w.y)*(w.z)*(mc);\n  float m9 = c+(w.z)*(w.z)*(mc);\n\n  float ox = p.x*m1 + p.y*m2 + p.z*m3;\n  float oy = p.x*m4 + p.y*m5 + p.z*m6;\n  float oz = p.x*m7 + p.y*m8 + p.z*m9;\n  d[i] = {ox, oy, oz};\n}\n\ninline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\ninline __host__ __device__ float4 sqrtf(float4 v)\n{\n    return make_float4(sqrtf(v.x), sqrtf(v.y), sqrtf(v.z), sqrtf(v.w));\n}\n\n__inline__ __device__ T blockReduceSum(T val)\n{\n  static __shared__ T shared[32];\n  int lane = threadIdx.x & 0x1f;\n  int wid = threadIdx.x >> 5;\n\n  val = warpReduceSum<T>(val);\n\n  if (lane == 0)\n    shared[wid] = val;\n\n  __syncthreads();\n\n  // Modify from blockDim.x << 5 to blockDim.x / 32. to prevent\n  // blockDim.x is not divided by 32\n  val = (threadIdx.x < (blockDim.x / 32.f)) ? shared[lane] : (T)(0.0f);\n  val = warpReduceSum<T>(val);\n\n  return val;\n}\n\n__device__ __inline__ void fwht(T *data, uint n)\n{\n  unsigned l2 = ilog2(n) - 1;\n  for ( uint i = 0; i < l2; ++i )\n  {\n    for (uint j = 0; j < n; j += (1 << (i + 1)))\n    for (uint k = 0; k < (uint)(1 << i); ++k)\n      rotate(data[j + k], data[j + k + (uint)(1 << i)]);\n  }\n}\n\n__device__ inline void get_block_addresses(\n  const uint2 & start_point,    //IN: first reference patch of a batch\n  const uint & patch_stack_size,  //IN: maximal size of a 3D group\n  const uint2 & stacks_dim,    //IN: Size of area, where reference patches could be located\n  const Params & params,      //IN: Denoising parameters\n  uint2 & outer_address,      //OUT: Coordinetes of reference patch in the image\n  uint & start_idx)        //OUT: Address of a first element of the 3D group in stacks array\n{\n  //One block handles one patch_stack, data are in array one after one.\n  start_idx = patch_stack_size * idx2(blockIdx.x,blockIdx.y,gridDim.x);\n  \n  outer_address.x = start_point.x + (blockIdx.x * params.p);\n  outer_address.y = start_point.y + (blockIdx.y * params.p);\n\n  //Ensure, that the bottom most patches will be taken as reference patches regardless the p parameter.\n  if (outer_address.y >= stacks_dim.y && outer_address.y < stacks_dim.y + params.p - 1)\n    outer_address.y = stacks_dim.y - 1;\n  //Ensure, that the right most patches will be taken as reference patches regardless the p parameter.\n  if (outer_address.x >= stacks_dim.x && outer_address.x < stacks_dim.x + params.p - 1)\n    outer_address.x = stacks_dim.x - 1;\n}\n\n__global__\nvoid hard_treshold_block(\n  const uint2 start_point,                        //IN: first reference patch of a batch\n  float* __restrict patch_stack,                  //IN/OUT: 3D groups with thransfomed patches\n  float* __restrict  w_P,                         //OUT: weight of each 3D group\n  const uint* __restrict g_num_patches_in_stack,  //IN: numbers of patches in 3D groups\n  uint2 stacks_dim,                               //IN: dimensions limiting addresses of reference patches\n  const Params params,                            //IN: denoising parameters\n  const uint sigma                                //IN: noise variance\n)\n{\n  extern __shared__ float data[];  \n\n  int paramN = params.N+1;\n  uint tcount = blockDim.x*blockDim.y;\n  uint tid = idx2(threadIdx.x, threadIdx.y, blockDim.x);\n  uint patch_stack_size = tcount * paramN;\n\n  uint startidx;\n  uint2 outer_address;\n  get_block_addresses(start_point, patch_stack_size, stacks_dim, params, outer_address, startidx);\n  \n  if (outer_address.x >= stacks_dim.x || outer_address.y >= stacks_dim.y) return;\n\n  uint num_patches = g_num_patches_in_stack[ idx2(blockIdx.x, blockIdx.y, gridDim.x) ]+1; //+1 for the reference patch.\n  float* s_patch_stack = data + (tid * (num_patches+1)); //+1 for avoiding bank conflicts //TODO:sometimes\n  patch_stack = patch_stack + startidx + tid;\n    \n  //Load to the shared memory\n  for(uint i = 0; i < num_patches; ++i)\n    s_patch_stack[i] = patch_stack[ i*tcount ];  \n\n  //1D Transform\n  fwht(s_patch_stack, num_patches);\n  \n  //Hard-thresholding + counting of nonzero coefficients\n  uint nonzero = 0;\n  float threshold = params.L3D * sqrtf((float)(num_patches * sigma));\n  for(int i = 0; i < num_patches; ++i)\n  {\n    if (fabsf(s_patch_stack[ i ]) < threshold)\n    {\n      s_patch_stack[ i ] = 0.0f;\n    }\n    else \n      ++nonzero;\n  }\n  \n  //Inverse 1D Transform\n  fwht(s_patch_stack, num_patches);\n  \n  //Normalize and save to global memory\n  for (uint i = 0; i < num_patches; ++i)\n  {\n    patch_stack[ i*tcount ] = s_patch_stack[i] / num_patches;\n  }\n  \n  //Reuse the shared memory for 32 partial sums\n  __syncthreads();\n  uint* shared = (uint*)data;\n  //Sum the number of non-zero coefficients for a 3D group\n  nonzero = blockReduceSum<uint>(shared, nonzero, tid, tcount);\n  \n  //Save the weight of a 3D group (1/nonzero coefficients)\n  if (tid == 0)\n  {\n    if (nonzero < 1) nonzero = 1;\n    w_P[ idx2(blockIdx.x, blockIdx.y, gridDim.x ) ] = 1.0f/(float)nonzero;\n  }\n}",
            "__device__ inline void get_block_addresses(\n  const uint2 & start_point,    //IN: first reference patch of a batch\n  const uint & patch_stack_size,  //IN: maximal size of a 3D group\n  const uint2 & stacks_dim,    //IN: Size of area, where reference patches could be located\n  const Params & params,      //IN: Denoising parameters\n  uint2 & outer_address,      //OUT: Coordinetes of reference patch in the image\n  uint & start_idx)        //OUT: Address of a first element of the 3D group in stacks array\n{\n  //One block handles one patch_stack, data are in array one after one.\n  start_idx = patch_stack_size * idx2(blockIdx.x,blockIdx.y,gridDim.x);\n  \n  outer_address.x = start_point.x + (blockIdx.x * params.p);\n  outer_address.y = start_point.y + (blockIdx.y * params.p);\n\n  //Ensure, that the bottom most patches will be taken as reference patches regardless the p parameter.\n  if (outer_address.y >= stacks_dim.y && outer_address.y < stacks_dim.y + params.p - 1)\n    outer_address.y = stacks_dim.y - 1;\n  //Ensure, that the right most patches will be taken as reference patches regardless the p parameter.\n  if (outer_address.x >= stacks_dim.x && outer_address.x < stacks_dim.x + params.p - 1)\n    outer_address.x = stacks_dim.x - 1;\n}\n\n__global__\nvoid aggregate_block(\n  const uint2 start_point,                        //IN: first reference patch of a batch\n  const float* __restrict patch_stack,            //IN: 3D groups with thransfomed patches\n  const float* __restrict w_P,                    //IN: weight for each 3D group\n  const ushort* __restrict stacks,                //IN: array of adresses of similar patches\n  const float* __restrict kaiser_window,          //IN: kaiser window\n  float* __restrict numerator,                    //IN/OUT: numerator aggregation buffer (have to be initialized to 0)\n  float* __restrict denominator,                  //IN/OUT: denominator aggregation buffer (have to be initialized to 0)\n  const uint* __restrict g_num_patches_in_stack,  //IN: numbers of patches in 3D groups\n  const uint2 image_dim,                          //IN: image dimensions\n  const uint2 stacks_dim,                         //IN: dimensions limiting addresses of reference patches\n  const Params params                             //IN: denoising parameters\n)\n{    \n  uint startidx;\n  uint2 outer_address;\n  get_block_addresses(start_point, params.k*params.k*(params.N+1), stacks_dim, params, outer_address, startidx);\n  \n  if (outer_address.x >= stacks_dim.x || outer_address.y >= stacks_dim.y) return;\n\n  patch_stack += startidx;\n\n  uint num_patches = g_num_patches_in_stack[ idx2(blockIdx.x, blockIdx.y, gridDim.x) ]+1;\n\n  float wp = w_P[ idx2(blockIdx.x, blockIdx.y, gridDim.x ) ];\n  \n  const ushort* z_ptr = &stacks[ idx3(0, blockIdx.x, blockIdx.y, params.N,  gridDim.x) ];\n\n  float kaiser_value = kaiser_window[ idx2(threadIdx.x, threadIdx.y, params.k) ];\n\n  for(uint z = 0; z < num_patches; ++z)\n  {\n    int x = 0;\n    int y = 0;\n    if (z > 0) {\n      x = (int)((signed char)(z_ptr[z-1] & 0xFF));\n      y = (int)((signed char)((z_ptr[z-1] >> 8) & 0xFF));\n    }\n\n    float value = ( patch_stack[ idx3(threadIdx.x, threadIdx.y, z, params.k, params.k) ]);\n    int idx = idx2(outer_address.x + x + threadIdx.x, outer_address.y + y + threadIdx.y, image_dim.x);\n    atomicAdd(numerator + idx, value * kaiser_value * wp);\n    atomicAdd(denominator + idx, kaiser_value * wp);\n  }\n}",
            "__global__\nvoid aggregate_final(\n  const float* __restrict numerator,    //IN: numerator aggregation buffer\n  const float* __restrict denominator,  //IN: denominator aggregation buffer\n  const uint2 image_dim,                //IN: image dimensions\n  uchar*__restrict result)              //OUT: image estimate\n{\n  uint idx = blockIdx.x * blockDim.x + threadIdx.x;\n  uint idy = blockIdx.y * blockDim.y + threadIdx.y;\n  if (idx >= image_dim.x || idy >= image_dim.y) return;\n\n  int value = lrintf(numerator[ idx2(idx,idy,image_dim.x) ] / denominator[ idx2(idx,idy,image_dim.x) ] );\n  if (value < 0) value = 0;\n  if (value > 255) value = 255;\n  result[ idx2(idx,idy,image_dim.x) ] = (uchar)value;\n}"
        ],
        "/Users/gbolet/hecbench-roofline/src/bm3d-cuda/dct8x8.cu": [
            "__device__ void InplaceDCTvector(float *Vect0, int Step)\n{\n    float *Vect1 = Vect0 + Step;\n    float *Vect2 = Vect1 + Step;\n    float *Vect3 = Vect2 + Step;\n    float *Vect4 = Vect3 + Step;\n    float *Vect5 = Vect4 + Step;\n    float *Vect6 = Vect5 + Step;\n    float *Vect7 = Vect6 + Step;\n\n    float X07P = (*Vect0) + (*Vect7);\n    float X16P = (*Vect1) + (*Vect6);\n    float X25P = (*Vect2) + (*Vect5);\n    float X34P = (*Vect3) + (*Vect4);\n\n    float X07M = (*Vect0) - (*Vect7);\n    float X61M = (*Vect6) - (*Vect1);\n    float X25M = (*Vect2) - (*Vect5);\n    float X43M = (*Vect4) - (*Vect3);\n\n    float X07P34PP = X07P + X34P;\n    float X07P34PM = X07P - X34P;\n    float X16P25PP = X16P + X25P;\n    float X16P25PM = X16P - X25P;\n\n    (*Vect0) = C_norm * (X07P34PP + X16P25PP);\n    (*Vect2) = C_norm * (C_b * X07P34PM + C_e * X16P25PM);\n    (*Vect4) = C_norm * (X07P34PP - X16P25PP);\n    (*Vect6) = C_norm * (C_e * X07P34PM - C_b * X16P25PM);\n\n    (*Vect1) = C_norm * (C_a * X07M - C_c * X61M + C_d * X25M - C_f * X43M);\n    (*Vect3) = C_norm * (C_c * X07M + C_f * X61M - C_a * X25M + C_d * X43M);\n    (*Vect5) = C_norm * (C_d * X07M + C_a * X61M + C_f * X25M - C_c * X43M);\n    (*Vect7) = C_norm * (C_f * X07M + C_d * X61M + C_c * X25M + C_a * X43M);\n}\n\n__global__ void DCT2D8x8(float *__restrict dst, const float *__restrict src, const uint size)\n{\n  __shared__ float block[KER2_BLOCK_HEIGHT * KER2_SMEMBLOCK_STRIDE];\n\n  if (blockIdx.x * KER2_BLOCK_HEIGHT * KER2_BLOCK_WIDTH + (threadIdx.y+1) * BLOCK_SIZE*BLOCK_SIZE-1 >= size) return;\n\n  int offset = threadIdx.y * (BLOCK_SIZE*BLOCK_SIZE) + threadIdx.x;\n\n  //Get macro-block address\n  src += blockIdx.x * KER2_BLOCK_HEIGHT * KER2_BLOCK_WIDTH;\n  dst += blockIdx.x * KER2_BLOCK_HEIGHT * KER2_BLOCK_WIDTH;\n  \n  //8x1 blocks in one macro-block (threadIdx.y - index of block inside the macro-block)\n  //Get the first element of the column in the block with index threadIdx.y\n  src += offset;\n  dst += offset;\n  \n  float *bl_ptr = block + offset;\n\n#pragma unroll\n\n  for (unsigned int i = 0; i < BLOCK_SIZE; i++)\n    bl_ptr[i * BLOCK_SIZE] = src[i * BLOCK_SIZE]; //Load column to the shared mem\n\n  //process rows\n  InplaceDCTvector(bl_ptr - threadIdx.x + BLOCK_SIZE * threadIdx.x, 1);\n\n  //process columns\n  InplaceDCTvector(bl_ptr, BLOCK_SIZE);\n\n  for (unsigned int i = 0; i < BLOCK_SIZE; i++)\n    dst[i * BLOCK_SIZE] = bl_ptr[i * BLOCK_SIZE];\n}",
            "__device__ void InplaceIDCTvector(float *Vect0, int Step)\n{\n    float *Vect1 = Vect0 + Step;\n    float *Vect2 = Vect1 + Step;\n    float *Vect3 = Vect2 + Step;\n    float *Vect4 = Vect3 + Step;\n    float *Vect5 = Vect4 + Step;\n    float *Vect6 = Vect5 + Step;\n    float *Vect7 = Vect6 + Step;\n\n    float Y04P   = (*Vect0) + (*Vect4);\n    float Y2b6eP = C_b * (*Vect2) + C_e * (*Vect6);\n\n    float Y04P2b6ePP = Y04P + Y2b6eP;\n    float Y04P2b6ePM = Y04P - Y2b6eP;\n    float Y7f1aP3c5dPP = C_f * (*Vect7) + C_a * (*Vect1) + C_c * (*Vect3) + C_d * (*Vect5);\n    float Y7a1fM3d5cMP = C_a * (*Vect7) - C_f * (*Vect1) + C_d * (*Vect3) - C_c * (*Vect5);\n\n    float Y04M   = (*Vect0) - (*Vect4);\n    float Y2e6bM = C_e * (*Vect2) - C_b * (*Vect6);\n\n    float Y04M2e6bMP = Y04M + Y2e6bM;\n    float Y04M2e6bMM = Y04M - Y2e6bM;\n    float Y1c7dM3f5aPM = C_c * (*Vect1) - C_d * (*Vect7) - C_f * (*Vect3) - C_a * (*Vect5);\n    float Y1d7cP3a5fMM = C_d * (*Vect1) + C_c * (*Vect7) - C_a * (*Vect3) + C_f * (*Vect5);\n\n    (*Vect0) = C_norm * (Y04P2b6ePP + Y7f1aP3c5dPP);\n    (*Vect7) = C_norm * (Y04P2b6ePP - Y7f1aP3c5dPP);\n    (*Vect4) = C_norm * (Y04P2b6ePM + Y7a1fM3d5cMP);\n    (*Vect3) = C_norm * (Y04P2b6ePM - Y7a1fM3d5cMP);\n\n    (*Vect1) = C_norm * (Y04M2e6bMP + Y1c7dM3f5aPM);\n    (*Vect5) = C_norm * (Y04M2e6bMM - Y1d7cP3a5fMM);\n    (*Vect2) = C_norm * (Y04M2e6bMM + Y1d7cP3a5fMM);\n    (*Vect6) = C_norm * (Y04M2e6bMP - Y1c7dM3f5aPM);\n}\n\n__global__ void IDCT2D8x8(float *__restrict dst, const float *__restrict src, const uint size)\n{\n  __shared__ float block[KER2_BLOCK_HEIGHT * KER2_SMEMBLOCK_STRIDE];\n\n  if (blockIdx.x * KER2_BLOCK_HEIGHT * KER2_BLOCK_WIDTH + (threadIdx.y+1) * BLOCK_SIZE*BLOCK_SIZE-1 >= size) return;\n\n  int offset = threadIdx.y * (BLOCK_SIZE*BLOCK_SIZE) + threadIdx.x;\n  \n  src += blockIdx.x * KER2_BLOCK_HEIGHT * KER2_BLOCK_WIDTH; \n  dst += blockIdx.x * KER2_BLOCK_HEIGHT * KER2_BLOCK_WIDTH; \n  \n  src += offset;\n  dst += offset;\n  \n  float *bl_ptr = block + offset;\n\n#pragma unroll\n\n  for (unsigned int i = 0; i < BLOCK_SIZE; i++)\n    bl_ptr[i * BLOCK_SIZE] = src[i * BLOCK_SIZE];\n\n  //process rows\n  InplaceIDCTvector(bl_ptr - threadIdx.x + BLOCK_SIZE * threadIdx.x, 1);\n\n  //process columns\n      InplaceIDCTvector(bl_ptr, BLOCK_SIZE);    \n\n  for (unsigned int i = 0; i < BLOCK_SIZE; i++)\n    dst[i * BLOCK_SIZE] = bl_ptr[i * BLOCK_SIZE];\n}"
        ]
    },
    "dxtc2-cuda": {
        "/Users/gbolet/hecbench-roofline/src/dxtc2-cuda/main.cu": [
            "#define T ((int)32)\n\n\ninline __host__ __device__ float3 make_float3(uint3 a)\n{\n    return make_float3(float(a.x), float(a.y), float(a.z));\n}\n\ninline __device__ float3 roundAndExpand(float3 v, ushort *w) {\n  v.x = rintf(__saturatef(v.x) * 31.0f);\n  v.y = rintf(__saturatef(v.y) * 63.0f);\n  v.z = rintf(__saturatef(v.z) * 31.0f);\n\n  *w = ((ushort)v.x << 11) | ((ushort)v.y << 5) | (ushort)v.z;\n  v.x *= 0.03227752766457f;  // approximate integer bit expansion.\n  v.y *= 0.01583151765563f;\n  v.z *= 0.03227752766457f;\n  return v;\n}\n\nstatic __device__ float evalPermutation3(const float3 *colors, uint permutation,\n                                         ushort *start, ushort *end,\n                                         float3 color_sum) {\n// Compute endpoints using least squares.\n#if USE_TABLES\n  float3 alphax_sum = make_float3(0.0f, 0.0f, 0.0f);\n\n  int akku = 0;\n\n  // Compute alpha & beta for this permutation.\n  for (int i = 0; i < 16; i++) {\n    const uint bits = permutation >> (2 * i);\n\n    alphax_sum += alphaTable3[bits & 3] * colors[i];\n    akku += prods3[bits & 3];\n  }\n\n  float alpha2_sum = float(akku >> 16);\n  float beta2_sum = float((akku >> 8) & 0xff);\n  float alphabeta_sum = float((akku >> 0) & 0xff);\n  float3 betax_sum = (4.0f * color_sum) - alphax_sum;\n#else\n  float alpha2_sum = 0.0f;\n  float beta2_sum = 0.0f;\n  float alphabeta_sum = 0.0f;\n  float3 alphax_sum = make_float3(0.0f, 0.0f, 0.0f);\n\n  // Compute alpha & beta for this permutation.\n  for (int i = 0; i < 16; i++) {\n    const uint bits = permutation >> (2 * i);\n\n    float beta = (bits & 1);\n\n    if (bits & 2) {\n      beta = 0.5f;\n    }\n\n    float alpha = 1.0f - beta;\n\n    alpha2_sum += alpha * alpha;\n    beta2_sum += beta * beta;\n    alphabeta_sum += alpha * beta;\n    alphax_sum += alpha * colors[i];\n  }\n\n  float3 betax_sum = color_sum - alphax_sum;\n#endif\n\n  const float factor =\n      1.0f / (alpha2_sum * beta2_sum - alphabeta_sum * alphabeta_sum);\n\n  float3 a = (alphax_sum * beta2_sum - betax_sum * alphabeta_sum) * factor;\n  float3 b = (betax_sum * alpha2_sum - alphax_sum * alphabeta_sum) * factor;\n\n  // Round a, b to the closest 5-6-5 color and expand...\n  a = roundAndExpand(a, start);\n  b = roundAndExpand(b, end);\n\n  // compute the error\n  float3 e = a * a * alpha2_sum + b * b * beta2_sum +\n             2.0f * (a * b * alphabeta_sum - a * alphax_sum - b * betax_sum);\n\n  return (0.25f) * (e.x + e.y + e.z);\n}\n\nstatic __device__ float evalPermutation4(const float3 *colors, uint permutation,\n                                         ushort *start, ushort *end,\n                                         float3 color_sum) {\n// Compute endpoints using least squares.\n#if USE_TABLES\n  float3 alphax_sum = make_float3(0.0f, 0.0f, 0.0f);\n\n  int akku = 0;\n\n  // Compute alpha & beta for this permutation.\n  for (int i = 0; i < 16; i++) {\n    const uint bits = permutation >> (2 * i);\n\n    alphax_sum += alphaTable4[bits & 3] * colors[i];\n    akku += prods4[bits & 3];\n  }\n\n  float alpha2_sum = float(akku >> 16);\n  float beta2_sum = float((akku >> 8) & 0xff);\n  float alphabeta_sum = float((akku >> 0) & 0xff);\n  float3 betax_sum = (9.0f * color_sum) - alphax_sum;\n#else\n  float alpha2_sum = 0.0f;\n  float beta2_sum = 0.0f;\n  float alphabeta_sum = 0.0f;\n  float3 alphax_sum = make_float3(0.0f, 0.0f, 0.0f);\n\n  // Compute alpha & beta for this permutation.\n  for (int i = 0; i < 16; i++) {\n    const uint bits = permutation >> (2 * i);\n\n    float beta = (bits & 1);\n\n    if (bits & 2) {\n      beta = (1 + beta) * (1.0f / 3.0f);\n    }\n\n    float alpha = 1.0f - beta;\n\n    alpha2_sum += alpha * alpha;\n    beta2_sum += beta * beta;\n    alphabeta_sum += alpha * beta;\n    alphax_sum += alpha * colors[i];\n  }\n\n  float3 betax_sum = color_sum - alphax_sum;\n#endif\n\n  // alpha2, beta2, alphabeta and factor could be precomputed for each\n  // permutation, but it's faster to recompute them.\n  const float factor =\n      1.0f / (alpha2_sum * beta2_sum - alphabeta_sum * alphabeta_sum);\n\n  float3 a = (alphax_sum * beta2_sum - betax_sum * alphabeta_sum) * factor;\n  float3 b = (betax_sum * alpha2_sum - alphax_sum * alphabeta_sum) * factor;\n\n  // Round a, b to the closest 5-6-5 color and expand...\n  a = roundAndExpand(a, start);\n  b = roundAndExpand(b, end);\n\n  // compute the error\n  float3 e = a * a * alpha2_sum + b * b * beta2_sum +\n             2.0f * (a * b * alphabeta_sum - a * alphax_sum - b * betax_sum);\n\n  return (0.111111111111f) * (e.x + e.y + e.z);\n}\n\n__device__ inline void swap(T &a, T &b) {\n  T tmp = a;\n  a = b;\n  b = tmp;\n}\n\ninline __host__ __device__ uint4 make_uint4(int4 a)\n{\n    return make_uint4(uint(a.x), uint(a.y), uint(a.z), uint(a.w));\n}\n\ninline __host__ __device__ uint4 max(uint4 a, uint4 b)\n{\n    return make_uint4(max(a.x,b.x), max(a.y,b.y), max(a.z,b.z), max(a.w,b.w));\n}\n\ninline __device__ __host__ float3 firstEigenVector(float matrix[6]) {\n  // 8 iterations seems to be more than enough.\n\n  float3 v = make_float3(1.0f, 1.0f, 1.0f);\n\n  for (int i = 0; i < 8; i++) {\n    float x = v.x * matrix[0] + v.y * matrix[1] + v.z * matrix[2];\n    float y = v.x * matrix[1] + v.y * matrix[3] + v.z * matrix[4];\n    float z = v.x * matrix[2] + v.y * matrix[4] + v.z * matrix[5];\n    float m = max(max(x, y), z);\n    float iv = 1.0f / m;\n    v = make_float3(x * iv, y * iv, z * iv);\n  }\n\n  return v;\n}\n\ninline __device__ float3 bestFitLine(const float3 *colors, float3 color_sum)\n{\n  // Compute covariance matrix of the given colors.\n  const int idx = threadIdx.x;\n\n  float3 diff = colors[idx] - color_sum * (1.0f / 16.0f);\n\n  // @@ Eliminate two-way bank conflicts here.\n  // @@ It seems that doing that and unrolling the reduction doesn't help...\n  __shared__ float covariance[16 * 6];\n\n  covariance[6 * idx + 0] = diff.x * diff.x;  // 0, 6, 12, 2, 8, 14, 4, 10, 0\n  covariance[6 * idx + 1] = diff.x * diff.y;\n  covariance[6 * idx + 2] = diff.x * diff.z;\n  covariance[6 * idx + 3] = diff.y * diff.y;\n  covariance[6 * idx + 4] = diff.y * diff.z;\n  covariance[6 * idx + 5] = diff.z * diff.z;\n\n  for (int d = 8; d > 0; d >>= 1) {\n    if (idx < d) {\n      covariance[6 * idx + 0] += covariance[6 * (idx + d) + 0];\n      covariance[6 * idx + 1] += covariance[6 * (idx + d) + 1];\n      covariance[6 * idx + 2] += covariance[6 * (idx + d) + 2];\n      covariance[6 * idx + 3] += covariance[6 * (idx + d) + 3];\n      covariance[6 * idx + 4] += covariance[6 * (idx + d) + 4];\n      covariance[6 * idx + 5] += covariance[6 * (idx + d) + 5];\n    }\n  }\n\n  // Compute first eigen vector.\n  return firstEigenVector(covariance);\n}\n\ninline __device__ void colorSums(const float3 *colors, float3 *sums) {\n  const int idx = threadIdx.x;\n\n  sums[idx] = colors[idx];\n  sums[idx] += sums[idx ^ 8];\n  sums[idx] += sums[idx ^ 4];\n  sums[idx] += sums[idx ^ 2];\n  sums[idx] += sums[idx ^ 1];\n}\n\n__device__ void sortColors(const float *values, int *ranks) {\n  const int tid = threadIdx.x;\n\n  int rank = 0;\n\n#pragma unroll\n\n  for (int i = 0; i < 16; i++) {\n    rank += (values[i] < values[tid]);\n  }\n\n  ranks[tid] = rank;\n\n  // Resolve elements with the same index.\n  for (int i = 0; i < 15; i++) {\n    if (tid > i && ranks[tid] == ranks[i]) {\n      ++ranks[tid];\n    }\n  }\n}\n\n__device__ void evalAllPermutations(const float3 *colors,\n                                    const uint *permutations, ushort &bestStart,\n                                    ushort &bestEnd, uint &bestPermutation,\n                                    float *errors, float3 color_sum,\n                                    cg::thread_block cta) {\n  const int idx = threadIdx.x;\n\n  float bestError = FLT_MAX;\n\n  __shared__ uint s_permutations[160];\n\n  for (int i = 0; i < 16; i++) {\n    int pidx = idx + NUM_THREADS * i;\n\n    if (pidx >= 992) {\n      break;\n    }\n\n    ushort start, end;\n    uint permutation = permutations[pidx];\n\n    if (pidx < 160) {\n      s_permutations[pidx] = permutation;\n    }\n\n    float error =\n        evalPermutation4(colors, permutation, &start, &end, color_sum);\n\n    if (error < bestError) {\n      bestError = error;\n      bestPermutation = permutation;\n      bestStart = start;\n      bestEnd = end;\n    }\n  }\n\n  if (bestStart < bestEnd) {\n    swap(bestEnd, bestStart);\n    bestPermutation ^= 0x55555555;  // Flip indices.\n  }\n\n  cg::sync(cta);  // Sync here to ensure s_permutations is valid going forward\n\n  for (int i = 0; i < 3; i++) {\n    int pidx = idx + NUM_THREADS * i;\n\n    if (pidx >= 160) {\n      break;\n    }\n\n    ushort start, end;\n    uint permutation = s_permutations[pidx];\n    float error =\n        evalPermutation3(colors, permutation, &start, &end, color_sum);\n\n    if (error < bestError) {\n      bestError = error;\n      bestPermutation = permutation;\n      bestStart = start;\n      bestEnd = end;\n\n      if (bestStart > bestEnd) {\n        swap(bestEnd, bestStart);\n        bestPermutation ^=\n            (~bestPermutation >> 1) & 0x55555555;  // Flip indices.\n      }\n    }\n  }\n\n  errors[idx] = bestError;\n}\n\n__device__ int findMinError(float *errors, cg::thread_block cta) {\n  const int idx = threadIdx.x;\n  __shared__ int indices[NUM_THREADS];\n  indices[idx] = idx;\n\n  cg::sync(cta);\n\n  for (int d = NUM_THREADS / 2; d > 0; d >>= 1) {\n    float err0 = errors[idx];\n    float err1 = (idx + d) < NUM_THREADS ? errors[idx + d] : FLT_MAX;\n    int index1 = (idx + d) < NUM_THREADS ? indices[idx + d] : 0;\n\n    cg::sync(cta);\n\n    if (err1 < err0) {\n      errors[idx] = err1;\n      indices[idx] = index1;\n    }\n\n    cg::sync(cta);\n  }\n\n  return indices[0];\n}\n\n__device__ void loadColorBlock(const uint *image, float3 colors[16],\n                               float3 sums[16], int xrefs[16], int blockOffset)\n{\n  const int bid = blockIdx.x + blockOffset;\n  const int idx = threadIdx.x;\n\n  __shared__ float dps[16];\n\n  float3 tmp;\n\n  if (idx < 16) {\n    // Read color and copy to shared mem.\n    uint c = image[(bid)*16 + idx];\n\n    colors[idx].x = ((c >> 0) & 0xFF) * (1.0f / 255.0f);\n    colors[idx].y = ((c >> 8) & 0xFF) * (1.0f / 255.0f);\n    colors[idx].z = ((c >> 16) & 0xFF) * (1.0f / 255.0f);\n\n    // Sort colors along the best fit line.\n    colorSums(colors, sums);\n\n    float3 axis = bestFitLine(colors, sums[0]);\n\n    dps[idx] = colors[idx].x * axis.x + \n               colors[idx].y * axis.y +\n               colors[idx].z * axis.z;\n\n    sortColors(dps, xrefs);\n\n    tmp = colors[idx];\n\n    colors[xrefs[idx]] = tmp;\n  }\n}\n\n__device__ void saveBlockDXT1(ushort start, ushort end, uint permutation,\n                              int xrefs[16], uint2 *result, int blockOffset) {\n  const int bid = blockIdx.x + blockOffset;\n\n  if (start == end) {\n    permutation = 0;\n  }\n\n  // Reorder permutation.\n  uint indices = 0;\n\n  for (int i = 0; i < 16; i++) {\n    int ref = xrefs[i];\n    indices |= ((permutation >> (2 * ref)) & 3) << (2 * i);\n  }\n\n  // Write endpoints.\n  result[bid].x = (end << 16) | start;\n\n  // Write palette indices.\n  result[bid].y = indices;\n}\n\n__global__ void compress(const uint *permutations, const uint *image,\n                         uint2 *result, int blockOffset) {\n  // Handle to thread block group\n  cg::thread_block cta = cg::this_thread_block();\n\n  const int idx = threadIdx.x;\n\n  __shared__ float3 colors[16];\n  __shared__ float3 sums[16];\n  __shared__ int xrefs[16];\n\n  loadColorBlock(image, colors, sums, xrefs, blockOffset);\n\n  cg::sync(cta);\n\n  ushort bestStart, bestEnd;\n  uint bestPermutation;\n\n  __shared__ float errors[NUM_THREADS];\n\n  evalAllPermutations(colors, permutations, bestStart, bestEnd, bestPermutation,\n                      errors, sums[0], cta);\n\n  // Use a parallel reduction to find minimum error.\n  const int minIdx = findMinError(errors, cta);\n\n  cg::sync(cta);\n\n  // Only write the result of the winner thread.\n  if (idx == minIdx) {\n    saveBlockDXT1(bestStart, bestEnd, bestPermutation, xrefs, result,\n                  blockOffset);\n  }\n}"
        ]
    },
    "warpexchange-cuda": {
        "/Users/gbolet/hecbench-roofline/src/warpexchange-cuda/main.cu": [
            "__global__ void k(const int *d, int *o, const int n)\n{\n  const int bid = blockIdx.x;\n  const int tid = threadIdx.x;\n  const int dim = gridDim.x;\n  constexpr int block_threads = BLOCK_SIZE;\n  constexpr int warps_per_block = block_threads / warp_threads;\n  const int warp_id = tid / warp_threads;\n\n  typedef WarpExchange<int, items_per_thread, warp_threads> WarpExchangeT;\n  typedef BlockLoad<int, block_threads, items_per_thread> LoadInteger;\n  typedef BlockStore<int, block_threads, items_per_thread> StoreInteger;\n\n  // Allocate shared memory\n  __shared__ typename WarpExchangeT::TempStorage temp_storage[warps_per_block];\n  __shared__ typename LoadInteger::TempStorage loadi;\n  __shared__ typename StoreInteger::TempStorage storei;\n\n  // Obtain a segment of consecutive items that are blocked across threads\n  int thread_data[items_per_thread];\n\n  const int n_full = (NUM_BLOCK*(n/NUM_BLOCK)) + (n % NUM_BLOCK == 0 ? 0 : NUM_BLOCK);\n  const int base_idx = (bid * NUM_BLOCK);\n  for (unsigned int i = base_idx; i < n_full; i += dim*NUM_BLOCK) {\n    unsigned int valid_items = n - i > NUM_BLOCK ? NUM_BLOCK : n - i;\n    LoadInteger(loadi).Load(&(d[i]), thread_data, valid_items);\n\n    // Collectively exchange data into a striped arrangement across threads\n    WarpExchangeT(temp_storage[warp_id]).BlockedToStriped(thread_data, thread_data);\n\n    StoreInteger(storei).Store(&(o[i]), thread_data, valid_items);\n  }\n}"
        ]
    },
    "heartwall-cuda": {
        "/Users/gbolet/hecbench-roofline/src/heartwall-cuda/kernel/kernel.h": [
            "#define fp float\n\n\ninline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\ninline __host__ __device__ float4 fabs(float4 v)\n{\n    return make_float4(fabs(v.x), fabs(v.y), fabs(v.z), fabs(v.w));\n}\n\n__global__ void hw (\n  const int frame_no,\n  const params_common d_common,\n  const fp* d_frame,\n  int* d_endoRow,\n  int* d_endoCol,\n  int* d_tEndoRowLoc,\n  int* d_tEndoColLoc,\n  int* d_epiRow,\n  int* d_epiCol,\n  int* d_tEpiRowLoc,\n  int* d_tEpiColLoc,\n  fp* d_endoT,\n  fp* d_epiT,\n  fp* d_in2,\n  fp* d_conv,\n  fp* d_in2_pad_cumv,\n  fp* d_in2_pad_cumv_sel,\n  fp* d_in2_sub_cumh,\n  fp* d_in2_sub_cumh_sel,\n  fp* d_in2_sub2,\n  fp* d_in2_sqr,\n  fp* d_in2_sqr_sub2,\n  fp* d_in_sqr,\n  fp* d_tMask,\n  fp* d_mask_conv,\n  fp* d_in_mod_temp,\n  fp* d_in_partial_sum,\n  fp* d_in_sqr_partial_sum,\n  fp* d_par_max_val,\n  fp* d_par_max_coo,\n  fp* d_in_final_sum,\n  fp* d_in_sqr_final_sum,\n  fp* d_denomT\n#ifdef TEST_CHECKSUM\n  , fp* checksum\n#endif\n)\n{\n\n\t// __global fp* d_in;\n\tint rot_row;\n\tint rot_col;\n\tint in2_rowlow;\n\tint in2_collow;\n\tint ic;\n\tint jc;\n\tint jp1;\n\tint ja1, ja2;\n\tint ip1;\n\tint ia1, ia2;\n\tint ja, jb;\n\tint ia, ib;\n\tfp s;\n\tint i;\n\tint j;\n\tint row;\n\tint col;\n\tint ori_row;\n\tint ori_col;\n\tint position;\n\tfp sum;\n\tint pos_ori;\n\tfp temp;\n\tfp temp2;\n\tint location;\n\tint cent;\n\tint tMask_row; \n\tint tMask_col;\n\tfp largest_value_current = 0;\n\tfp largest_value = 0;\n\tint largest_coordinate_current = 0;\n\tint largest_coordinate = 0;\n\tfp fin_max_val = 0;\n\tint fin_max_coo = 0;\n\tint largest_row;\n\tint largest_col;\n\tint offset_row;\n\tint offset_col;\n\tfp mean;\n\tfp mean_sqr;\n\tfp variance;\n\tfp deviation;\n\tint pointer;\n\tint ori_pointer;\n\tint loc_pointer;\n\n\n\t//======================================================================================================================================================150\n\t//\tBLOCK/THREAD IDs\n\t//======================================================================================================================================================150\n\n  int bx = blockIdx.x;  // get current horizontal block index (0-n)\n  int tx = threadIdx.x;\t// get current horizontal thread index (0-n)\n\tint ei_new;\n\n\t//======================================================================================================================================================150\n\t//\tUNIQUE STRUCTURE RECONSTRUCTED HERE\n\t//======================================================================================================================================================150\n\n\t// common\n\n\t// offsets for either endo or epi points (separate arrays for endo and epi points)\n\tint d_unique_point_no = bx < d_common.endoPoints ? bx : bx-d_common.endoPoints;\n\n\tint* d_unique_d_Row = bx < d_common.endoPoints ? d_endoRow: d_epiRow;\n\tint* d_unique_d_Col = bx < d_common.endoPoints ? d_endoCol: d_epiCol;\n\tint* d_unique_d_tRowLoc = bx < d_common.endoPoints ? d_tEndoRowLoc: d_tEpiRowLoc;\n\tint* d_unique_d_tColLoc = bx < d_common.endoPoints ? d_tEndoColLoc: d_tEpiColLoc;\n\tfp*  d_in = bx < d_common.endoPoints ? &d_endoT[d_unique_point_no * d_common.in_elem] :\n                                               &d_epiT[d_unique_point_no * d_common.in_elem] ;\n  \n\n\t// offsets for all points (one array for all points)\n\tfp*  d_unique_d_in2 = &d_in2[bx*d_common.in2_elem];\n\tfp*  d_unique_d_conv = &d_conv[bx*d_common.conv_elem];\n\tfp*  d_unique_d_in2_pad_cumv = &d_in2_pad_cumv[bx*d_common.in2_pad_cumv_elem];\n\tfp*  d_unique_d_in2_pad_cumv_sel = &d_in2_pad_cumv_sel[bx*d_common.in2_pad_cumv_sel_elem];\n\tfp*  d_unique_d_in2_sub_cumh = &d_in2_sub_cumh[bx*d_common.in2_sub_cumh_elem];\n\tfp*  d_unique_d_in2_sub_cumh_sel = &d_in2_sub_cumh_sel[bx*d_common.in2_sub_cumh_sel_elem];\n\tfp*  d_unique_d_in2_sub2 = &d_in2_sub2[bx*d_common.in2_sub2_elem];\n\tfp*  d_unique_d_in2_sqr = &d_in2_sqr[bx*d_common.in2_sqr_elem];\n\tfp*  d_unique_d_in2_sqr_sub2 = &d_in2_sqr_sub2[bx*d_common.in2_sqr_sub2_elem];\n\tfp*  d_unique_d_in_sqr = &d_in_sqr[bx*d_common.in_sqr_elem];\n\tfp*  d_unique_d_tMask = &d_tMask[bx*d_common.tMask_elem];\n\tfp*  d_unique_d_mask_conv = &d_mask_conv[bx*d_common.mask_conv_elem];\n\n\t// used to be local\n\tfp*  d_unique_d_in_mod_temp = &d_in_mod_temp[bx*d_common.in_elem];\n\tfp*  d_unique_d_in_partial_sum = &d_in_partial_sum[bx*d_common.in_cols];\n\tfp*  d_unique_d_in_sqr_partial_sum = &d_in_sqr_partial_sum[bx*d_common.in_sqr_rows];\n\tfp*  d_unique_d_par_max_val = &d_par_max_val[bx*d_common.mask_conv_rows];\n\tfp*  d_unique_d_par_max_coo = &d_par_max_coo[bx*d_common.mask_conv_rows];\n\n\tfp*  d_unique_d_in_final_sum = &d_in_final_sum[bx];\n\tfp*  d_unique_d_in_sqr_final_sum = &d_in_sqr_final_sum[bx];\n\tfp*  d_unique_d_denomT = &d_denomT[bx];\n\n\t//======================================================================================================================================================150\n\t//\tEND\n\t//======================================================================================================================================================150\n\n\t//======================================================================================================================================================150\n\t//\tInitialize checksum\n\t//======================================================================================================================================================150\n#ifdef TEST_CHECKSUM\n\tif(bx==0 && tx==0){\n\n\t\tfor(i=0; i<CHECK; i++){\n\t\t\tchecksum[i] = 0;\n\t\t}\n\n\t}\n#endif\n\t//======================================================================================================================================================150\n\t//\tINITIAL COORDINATE AND TEMPLATE UPDATE\n\t//======================================================================================================================================================150\n\n\t// generate templates based on the first frame only\n\tif(frame_no == 0){\n\n\t\t//====================================================================================================100\n\t\t//\tUPDATE ROW LOC AND COL LOC\n\t\t//====================================================================================================100\n\n\t\t// uptade temporary endo/epi row/col coordinates (in each block corresponding to point, narrow work to one thread)\n\t\tei_new = tx;\n\t\tif(ei_new == 0){\n\n\t\t\t// update temporary row/col coordinates\n\t\t\tpointer = d_unique_point_no*d_common.no_frames+frame_no;\n\t\t\td_unique_d_tRowLoc[pointer] = d_unique_d_Row[d_unique_point_no];\n\t\t\td_unique_d_tColLoc[pointer] = d_unique_d_Col[d_unique_point_no];\n\n\t\t}\n\n\t\t//====================================================================================================100\n\t\t//\tCREATE TEMPLATES\n\t\t//====================================================================================================100\n\n\t\t// work\n\t\tei_new = tx;\n\t\twhile(ei_new < d_common.in_elem){\n\n\t\t\t// figure out row/col location in new matrix\n\t\t\trow = (ei_new+1) % d_common.in_rows - 1;\t\t\t\t\t\t\t\t\t\t\t\t// (0-n) row\n\t\t\tcol = (ei_new+1) / d_common.in_rows + 1 - 1;\t\t\t\t\t\t\t\t\t\t\t// (0-n) column\n\t\t\tif((ei_new+1) % d_common.in_rows == 0){\n\t\t\t\trow = d_common.in_rows - 1;\n\t\t\t\tcol = col-1;\n\t\t\t}\n\n\t\t\t// figure out row/col location in corresponding new template area in image and give to every thread (get top left corner and progress down and right)\n\t\t\tori_row = d_unique_d_Row[d_unique_point_no] - 25 + row - 1;\n\t\t\tori_col = d_unique_d_Col[d_unique_point_no] - 25 + col - 1;\n\t\t\tori_pointer = ori_col*d_common.frame_rows+ori_row;\n\n\t\t\t// update template\n\t\t\td_in[col*d_common.in_rows+row] = d_frame[ori_pointer];\n\n\t\t\t// go for second round\n\t\t\tei_new = ei_new + NUMBER_THREADS;\n\n\t\t}\n\n\t\t//====================================================================================================100\n\t\t//\tSYNCHRONIZE THREADS\n\t\t//====================================================================================================100\n\n\t\t__syncthreads();\n\n\t\t//====================================================================================================100\n\t\t//\tchecksum\n\t\t//====================================================================================================100\n#ifdef TEST_CHECKSUM\n\t\tif(bx==0 && tx==0){\n\t\t\tfor(i=0; i<d_common.in_elem; i++){\n\t\t\t\tchecksum[0] = checksum[0]+d_in[i];\n\t\t\t}\n\t\t}\n\n\t\t//====================================================================================================100\n\t\t//\tSYNCHRONIZE THREADS\n\t\t//====================================================================================================100\n\n\t\t__syncthreads();\n#endif\n\t\t//====================================================================================================100\n\t\t//\tEnd\n\t\t//====================================================================================================100\n\n\t}\n\n\t//======================================================================================================================================================150\n\t//\tPROCESS POINTS\n\t//======================================================================================================================================================150\n\n\t// process points in all frames except for the first one\n\tif(frame_no != 0){\n\n\t\t//====================================================================================================100\n\t\t//\tInitialize frame-specific variables\n\t\t//====================================================================================================100\n\n\t\t//====================================================================================================100\n\t\t//\tSELECTION\n\t\t//====================================================================================================100\n\n\t\tin2_rowlow = d_unique_d_Row[d_unique_point_no] - d_common.sSize;\t\t\t\t\t\t\t\t\t\t\t\t\t// (1 to n+1)\n\t\tin2_collow = d_unique_d_Col[d_unique_point_no] - d_common.sSize;\n\n\t\t// work\n\t\tei_new = tx;\n\t\twhile(ei_new < d_common.in2_elem){\n\n\t\t\t// figure out row/col location in new matrix\n\t\t\trow = (ei_new+1) % d_common.in2_rows - 1;\t\t\t\t\t\t\t\t\t\t\t\t// (0-n) row\n\t\t\tcol = (ei_new+1) / d_common.in2_rows + 1 - 1;\t\t\t\t\t\t\t\t\t\t\t// (0-n) column\n\t\t\tif((ei_new+1) % d_common.in2_rows == 0){\n\t\t\t\trow = d_common.in2_rows - 1;\n\t\t\t\tcol = col-1;\n\t\t\t}\n\n\t\t\t// figure out corresponding location in old matrix and copy values to new matrix\n\t\t\tori_row = row + in2_rowlow - 1;\n\t\t\tori_col = col + in2_collow - 1;\n\t\t\td_unique_d_in2[ei_new] = d_frame[ori_col*d_common.frame_rows+ori_row];\n\n\t\t\t// go for second round\n\t\t\tei_new = ei_new + NUMBER_THREADS;\n\n\t\t}\n\n\t\t//====================================================================================================100\n\t\t//\tSYNCHRONIZE THREADS\n\t\t//====================================================================================================100\n\n\t\t__syncthreads();\n\n\t\t//====================================================================================================100\n\t\t//\tchecksum\n\t\t//====================================================================================================100\n#ifdef TEST_CHECKSUM\n\t\tif(bx==0 && tx==0){\n\t\t\tfor(i=0; i<d_common.in2_elem; i++){\n\t\t\t\tchecksum[1] = checksum[1]+d_unique_d_in2[i];\n\t\t\t}\n\t\t}\n\n\t\t//====================================================================================================100\n\t\t//\tSYNCHRONIZE THREADS\n\t\t//====================================================================================================100\n\n\t\t__syncthreads();\n#endif\n\t\t//====================================================================================================100\n\t\t//\tCONVOLUTION\n\t\t//====================================================================================================100\n\n\t\t//==================================================50\n\t\t//\tROTATION\n\t\t//==================================================50\n\n\t\t// work\n\t\tei_new = tx;\n\t\twhile(ei_new < d_common.in_elem){\n\t\t// while(ei_new < 1){\n\n\t\t\t// figure out row/col location in padded array\n\t\t\trow = (ei_new+1) % d_common.in_rows - 1;\t\t\t\t\t\t\t\t\t\t\t\t// (0-n) row\n\t\t\tcol = (ei_new+1) / d_common.in_rows + 1 - 1;\t\t\t\t\t\t\t\t\t\t\t// (0-n) column\n\t\t\tif((ei_new+1) % d_common.in_rows == 0){\n\t\t\t\trow = d_common.in_rows - 1;\n\t\t\t\tcol = col-1;\n\t\t\t}\n\n\t\t\t// execution\n\t\t\trot_row = (d_common.in_rows-1) - row;\n\t\t\trot_col = (d_common.in_rows-1) - col;\n\t\t\td_unique_d_in_mod_temp[ei_new] = d_in[rot_col*d_common.in_rows+rot_row];\n\n\t\t\t// go for second round\n\t\t\tei_new = ei_new + NUMBER_THREADS;\n\n\t\t}\n\n\t\t//==================================================50\n\t\t//\tSYNCHRONIZE THREADS\n\t\t//==================================================50\n\n\t\t__syncthreads();\n\n\t\t//==================================================50\n\t\t//\tchecksum\n\t\t//==================================================50\n#ifdef TEST_CHECKSUM\n\t\tif(bx==0 && tx==0){\n\t\t\tfor(i=0; i<d_common.in_elem; i++){\n\t\t\t\tchecksum[2] = checksum[2]+d_unique_d_in_mod_temp[i];\n\t\t\t}\n\t\t}\n\n\t\t//==================================================50\n\t\t//\tSYNCHRONIZE THREADS\n\t\t//==================================================50\n\n\t\t__syncthreads();\n#endif\n\t\t//==================================================50\n\t\t//\tACTUAL CONVOLUTION\n\t\t//==================================================50\n\n\t\t// work\n\t\tei_new = tx;\n\t\twhile(ei_new < d_common.conv_elem){\n\n\t\t\t// figure out row/col location in array\n\t\t\tic = (ei_new+1) % d_common.conv_rows;\t\t\t\t\t\t\t\t\t\t\t\t// (1-n)\n\t\t\tjc = (ei_new+1) / d_common.conv_rows + 1;\t\t\t\t\t\t\t\t\t\t\t// (1-n)\n\t\t\tif((ei_new+1) % d_common.conv_rows == 0){\n\t\t\t\tic = d_common.conv_rows;\n\t\t\t\tjc = jc-1;\n\t\t\t}\n\n\t\t\t//\n\t\t\tj = jc + d_common.joffset;\n\t\t\tjp1 = j + 1;\n\t\t\tif(d_common.in2_cols < jp1){\n\t\t\t\tja1 = jp1 - d_common.in2_cols;\n\t\t\t}\n\t\t\telse{\n\t\t\t\tja1 = 1;\n\t\t\t}\n\t\t\tif(d_common.in_cols < j){\n\t\t\t\tja2 = d_common.in_cols;\n\t\t\t}\n\t\t\telse{\n\t\t\t\tja2 = j;\n\t\t\t}\n\n\t\t\ti = ic + d_common.ioffset;\n\t\t\tip1 = i + 1;\n\t\t\t\n\t\t\tif(d_common.in2_rows < ip1){\n\t\t\t\tia1 = ip1 - d_common.in2_rows;\n\t\t\t}\n\t\t\telse{\n\t\t\t\tia1 = 1;\n\t\t\t}\n\t\t\tif(d_common.in_rows < i){\n\t\t\t\tia2 = d_common.in_rows;\n\t\t\t}\n\t\t\telse{\n\t\t\t\tia2 = i;\n\t\t\t}\n\n\t\t\ts = 0;\n\n\t\t\tfor(ja=ja1; ja<=ja2; ja++){\n\t\t\t\tjb = jp1 - ja;\n\t\t\t\tfor(ia=ia1; ia<=ia2; ia++){\n\t\t\t\t\tib = ip1 - ia;\n\t\t\t\t\ts = s + d_unique_d_in_mod_temp[d_common.in_rows*(ja-1)+ia-1] * d_unique_d_in2[d_common.in2_rows*(jb-1)+ib-1];\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t//d_unique_d_conv[d_common.conv_rows*(jc-1)+ic-1] = s;\n\t\t\td_unique_d_conv[ei_new] = s;\n\n\t\t\t// go for second round\n\t\t\tei_new = ei_new + NUMBER_THREADS;\n\n\t\t}\n\n\t\t//==================================================50\n\t\t//\tSYNCHRONIZE THREADS\n\t\t//==================================================50\n\n\t\t__syncthreads();\n\n\t\t//==================================================50\n\t\t//\tchecksum\n\t\t//==================================================50\n#ifdef TEST_CHECKSUM\n\t\tif(bx==0 && tx==0){\n\t\t\tfor(i=0; i<d_common.conv_elem; i++){\n\t\t\t\tchecksum[3] = checksum[3]+d_unique_d_conv[i];\n\t\t\t}\n\t\t}\n\n\t\t//==================================================50\n\t\t//\tSYNCHRONIZE THREADS\n\t\t//==================================================50\n\n\t\t__syncthreads();\n#endif\n\t\t//==================================================50\n\t\t//\tEnd\n\t\t//==================================================50\n\n\t\t//====================================================================================================100\n\t\t// \tCUMULATIVE SUM\t(LOCAL)\n\t\t//====================================================================================================100\n\n\t\t//==================================================50\n\t\t//\tPADD ARRAY\n\t\t//==================================================50\n\n\t\t// work\n\t\tei_new = tx;\n\t\twhile(ei_new < d_common.in2_pad_cumv_elem){\n\n\t\t\t// figure out row/col location in padded array\n\t\t\trow = (ei_new+1) % d_common.in2_pad_cumv_rows - 1;\t\t\t\t\t\t\t\t\t\t\t\t// (0-n) row\n\t\t\tcol = (ei_new+1) / d_common.in2_pad_cumv_rows + 1 - 1;\t\t\t\t\t\t\t\t\t\t\t// (0-n) column\n\t\t\tif((ei_new+1) % d_common.in2_pad_cumv_rows == 0){\n\t\t\t\trow = d_common.in2_pad_cumv_rows - 1;\n\t\t\t\tcol = col-1;\n\t\t\t}\n\n\t\t\t// execution\n\t\t\tif(\trow > (d_common.in2_pad_add_rows-1) &&\t\t\t\t\t\t\t\t\t\t\t\t\t\t// do if has numbers in original array\n\t\t\t\trow < (d_common.in2_pad_add_rows+d_common.in2_rows) && \n\t\t\t\tcol > (d_common.in2_pad_add_cols-1) && \n\t\t\t\tcol < (d_common.in2_pad_add_cols+d_common.in2_cols)){\n\t\t\t\tori_row = row - d_common.in2_pad_add_rows;\n\t\t\t\tori_col = col - d_common.in2_pad_add_cols;\n\t\t\t\td_unique_d_in2_pad_cumv[ei_new] = d_unique_d_in2[ori_col*d_common.in2_rows+ori_row];\n\t\t\t}\n\t\t\telse{\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t// do if otherwise\n\t\t\t\td_unique_d_in2_pad_cumv[ei_new] = 0;\n\t\t\t}\n\n\t\t\t// go for second round\n\t\t\tei_new = ei_new + NUMBER_THREADS;\n\n\t\t}\n\n\t\t//==================================================50\n\t\t//\tSYNCHRONIZE THREADS\n\t\t//==================================================50\n\n\t\t__syncthreads();\n\n\t\t//==================================================50\n\t\t//\tchecksum\n\t\t//==================================================50\n#ifdef TEST_CHECKSUM\n\t\tif(bx==0 && tx==0){\n\t\t\tfor(i=0; i<d_common.in2_pad_cumv_elem; i++){\n\t\t\t\tchecksum[4] = checksum[4]+d_unique_d_in2_pad_cumv[i];\n\t\t\t}\n\t\t}\n\n\t\t//==================================================50\n\t\t//\tSYNCHRONIZE THREADS\n\t\t//==================================================50\n\n\t\t__syncthreads();\n#endif\n\t\t//==================================================50\n\t\t//\tVERTICAL CUMULATIVE SUM\n\t\t//==================================================50\n\n\t\t//work\n\t\tei_new = tx;\n\t\twhile(ei_new < d_common.in2_pad_cumv_cols){\n\n\t\t\t// figure out column position\n\t\t\tpos_ori = ei_new*d_common.in2_pad_cumv_rows;\n\n\t\t\t// variables\n\t\t\tsum = 0;\n\t\t\t\n\t\t\t// loop through all rows\n\t\t\tfor(position = pos_ori; position < pos_ori+d_common.in2_pad_cumv_rows; position = position + 1){\n\t\t\t\td_unique_d_in2_pad_cumv[position] = d_unique_d_in2_pad_cumv[position] + sum;\n\t\t\t\tsum = d_unique_d_in2_pad_cumv[position];\n\t\t\t}\n\n\t\t\t// go for second round\n\t\t\tei_new = ei_new + NUMBER_THREADS;\n\n\t\t}\n\n\t\t//==================================================50\n\t\t//\tSYNCHRONIZE THREADS\n\t\t//==================================================50\n\n\t\t__syncthreads();\n\n\t\t//==================================================50\n\t\t//\tchecksum\n\t\t//==================================================50\n#ifdef TEST_CHECKSUM\n\t\tif(bx==0 && tx==0){\n\t\t\tfor(i=0; i<d_common.in2_pad_cumv_cols; i++){\n\t\t\t\tchecksum[5] = checksum[5]+d_unique_d_in2_pad_cumv[i];\n\t\t\t}\n\t\t}\n\n\t\t//==================================================50\n\t\t//\tSYNCHRONIZE THREADS\n\t\t//==================================================50\n\n\t\t__syncthreads();\n#endif\n\t\t//==================================================50\n\t\t//\tSELECTION\n\t\t//==================================================50\n\n\t\t// work\n\t\tei_new = tx;\n\t\twhile(ei_new < d_common.in2_pad_cumv_sel_elem){\n\n\t\t\t// figure out row/col location in new matrix\n\t\t\trow = (ei_new+1) % d_common.in2_pad_cumv_sel_rows - 1;\t\t\t\t\t\t\t\t\t\t\t\t// (0-n) row\n\t\t\tcol = (ei_new+1) / d_common.in2_pad_cumv_sel_rows + 1 - 1;\t\t\t\t\t\t\t\t\t\t\t// (0-n) column\n\t\t\tif((ei_new+1) % d_common.in2_pad_cumv_sel_rows == 0){\n\t\t\t\trow = d_common.in2_pad_cumv_sel_rows - 1;\n\t\t\t\tcol = col-1;\n\t\t\t}\n\n\t\t\t// figure out corresponding location in old matrix and copy values to new matrix\n\t\t\tori_row = row + d_common.in2_pad_cumv_sel_rowlow - 1;\n\t\t\tori_col = col + d_common.in2_pad_cumv_sel_collow - 1;\n\t\t\td_unique_d_in2_pad_cumv_sel[ei_new] = d_unique_d_in2_pad_cumv[ori_col*d_common.in2_pad_cumv_rows+ori_row];\n\n\t\t\t// go for second round\n\t\t\tei_new = ei_new + NUMBER_THREADS;\n\n\t\t}\n\n\t\t//==================================================50\n\t\t//\tSYNCHRONIZE THREADS\n\t\t//==================================================50\n\n\t\t__syncthreads();\n\n\t\t//==================================================50\n\t\t//\tchecksum\n\t\t//==================================================50\n#ifdef TEST_CHECKSUM\n\t\tif(bx==0 && tx==0){\n\t\t\tfor(i=0; i<d_common.in2_pad_cumv_sel_elem; i++){\n\t\t\t\tchecksum[6] = checksum[6]+d_unique_d_in2_pad_cumv_sel[i];\n\t\t\t}\n\t\t}\n\n\t\t//==================================================50\n\t\t//\tSYNCHRONIZE THREADS\n\t\t//==================================================50\n\n\t\t__syncthreads();\n#endif\n\t\t//==================================================50\n\t\t//\tSELECTION 2\n\t\t//==================================================50\n\n\t\t// work\n\t\tei_new = tx;\n\t\twhile(ei_new < d_common.in2_sub_cumh_elem){\n\n\t\t\t// figure out row/col location in new matrix\n\t\t\trow = (ei_new+1) % d_common.in2_sub_cumh_rows - 1;\t\t\t\t\t\t\t\t\t\t\t\t// (0-n) row\n\t\t\tcol = (ei_new+1) / d_common.in2_sub_cumh_rows + 1 - 1;\t\t\t\t\t\t\t\t\t\t\t// (0-n) column\n\t\t\tif((ei_new+1) % d_common.in2_sub_cumh_rows == 0){\n\t\t\t\trow = d_common.in2_sub_cumh_rows - 1;\n\t\t\t\tcol = col-1;\n\t\t\t}\n\n\t\t\t// figure out corresponding location in old matrix and copy values to new matrix\n\t\t\tori_row = row + d_common.in2_pad_cumv_sel2_rowlow - 1;\n\t\t\tori_col = col + d_common.in2_pad_cumv_sel2_collow - 1;\n\t\t\td_unique_d_in2_sub_cumh[ei_new] = d_unique_d_in2_pad_cumv[ori_col*d_common.in2_pad_cumv_rows+ori_row];\n\n\t\t\t// go for second round\n\t\t\tei_new = ei_new + NUMBER_THREADS;\n\n\t\t}\n\n\t\t//==================================================50\n\t\t//\tSYNCHRONIZE THREADS\n\t\t//==================================================50\n\n\t\t__syncthreads();\n\n\t\t//==================================================50\n\t\t//\tchecksum\n\t\t//==================================================50\n#ifdef TEST_CHECKSUM\n\t\tif(bx==0 && tx==0){\n\t\t\tfor(i=0; i<d_common.in2_sub_cumh_elem; i++){\n\t\t\t\tchecksum[7] = checksum[7]+d_unique_d_in2_sub_cumh[i];\n\t\t\t}\n\t\t}\n\t\t//==================================================50\n\t\t//\tSYNCHRONIZE THREADS\n\t\t//==================================================50\n\n\t\t__syncthreads();\n#endif\n\n\t\t//==================================================50\n\t\t//\tSUBTRACTION\n\t\t//==================================================50\n\n\t\t// work\n\t\tei_new = tx;\n\t\twhile(ei_new < d_common.in2_sub_cumh_elem){\n\n\t\t\t// subtract\n\t\t\td_unique_d_in2_sub_cumh[ei_new] = d_unique_d_in2_pad_cumv_sel[ei_new] - d_unique_d_in2_sub_cumh[ei_new];\n\n\t\t\t// go for second round\n\t\t\tei_new = ei_new + NUMBER_THREADS;\n\n\t\t}\n\n\t\t//==================================================50\n\t\t//\tSYNCHRONIZE THREADS\n\t\t//==================================================50\n\n\t\t__syncthreads();\n\n\t\t//==================================================50\n\t\t//\tchecksum\n\t\t//==================================================50\n#ifdef TEST_CHECKSUM\n\t\tif(bx==0 && tx==0){\n\t\t\tfor(i=0; i<d_common.in2_sub_cumh_elem; i++){\n\t\t\t\tchecksum[8] = checksum[8]+d_unique_d_in2_sub_cumh[i];\n\t\t\t}\n\t\t}\n\n\t\t//==================================================50\n\t\t//\tSYNCHRONIZE THREADS\n\t\t//==================================================50\n\n\t\t__syncthreads();\n#endif\n\t\t//==================================================50\n\t\t//\tHORIZONTAL CUMULATIVE SUM\n\t\t//==================================================50\n\n\t\t// work\n\t\tei_new = tx;\n\t\twhile(ei_new < d_common.in2_sub_cumh_rows){\n\n\t\t\t// figure out row position\n\t\t\tpos_ori = ei_new;\n\n\t\t\t// variables\n\t\t\tsum = 0;\n\n\t\t\t// loop through all rows\n\t\t\tfor(position = pos_ori; position < pos_ori+d_common.in2_sub_cumh_elem; position = position + d_common.in2_sub_cumh_rows){\n\t\t\t\td_unique_d_in2_sub_cumh[position] = d_unique_d_in2_sub_cumh[position] + sum;\n\t\t\t\tsum = d_unique_d_in2_sub_cumh[position];\n\t\t\t}\n\n\t\t\t// go for second round\n\t\t\tei_new = ei_new + NUMBER_THREADS;\n\n\t\t}\n\n\t\t//==================================================50\n\t\t//\tSYNCHRONIZE THREADS\n\t\t//==================================================50\n\n\t\t__syncthreads();\n\n\t\t//==================================================50\n\t\t//\tchecksum\n\t\t//==================================================50\n#ifdef TEST_CHECKSUM\n\t\tif(bx==0 && tx==0){\n\t\t\tfor(i=0; i<d_common.in2_sub_cumh_elem; i++){\n\t\t\t\tchecksum[9] = checksum[9]+d_unique_d_in2_sub_cumh[i];\n\t\t\t}\n\t\t}\n\n\t\t//==================================================50\n\t\t//\tSYNCHRONIZE THREADS\n\t\t//==================================================50\n\n\t\t__syncthreads();\n#endif\n\t\t//==================================================50\n\t\t//\tSELECTION\n\t\t//==================================================50\n\n\t\t// work\n\t\tei_new = tx;\n\t\twhile(ei_new < d_common.in2_sub_cumh_sel_elem){\n\n\t\t\t// figure out row/col location in new matrix\n\t\t\trow = (ei_new+1) % d_common.in2_sub_cumh_sel_rows - 1;\t\t\t\t\t\t\t\t\t\t\t\t// (0-n) row\n\t\t\tcol = (ei_new+1) / d_common.in2_sub_cumh_sel_rows + 1 - 1;\t\t\t\t\t\t\t\t\t\t\t// (0-n) column\n\t\t\tif((ei_new+1) % d_common.in2_sub_cumh_sel_rows == 0){\n\t\t\t\trow = d_common.in2_sub_cumh_sel_rows - 1;\n\t\t\t\tcol = col - 1;\n\t\t\t}\n\n\t\t\t// figure out corresponding location in old matrix and copy values to new matrix\n\t\t\tori_row = row + d_common.in2_sub_cumh_sel_rowlow - 1;\n\t\t\tori_col = col + d_common.in2_sub_cumh_sel_collow - 1;\n\t\t\td_unique_d_in2_sub_cumh_sel[ei_new] = d_unique_d_in2_sub_cumh[ori_col*d_common.in2_sub_cumh_rows+ori_row];\n\n\t\t\t// go for second round\n\t\t\tei_new = ei_new + NUMBER_THREADS;\n\n\t\t}\n\n\t\t//==================================================50\n\t\t//\tSYNCHRONIZE THREADS\n\t\t//==================================================50\n\n\t\t__syncthreads();\n\n\t\t//==================================================50\n\t\t//\tchecksum\n\t\t//==================================================50\n#ifdef TEST_CHECKSUM\n\t\tif(bx==0 && tx==0){\n\t\t\tfor(i=0; i<d_common.in2_sub_cumh_sel_elem; i++){\n\t\t\t\tchecksum[10] = checksum[10]+d_unique_d_in2_sub_cumh_sel[i];\n\t\t\t}\n\t\t}\n\n\t\t//==================================================50\n\t\t//\tSYNCHRONIZE THREADS\n\t\t//==================================================50\n\n\t\t__syncthreads();\n#endif\n\t\t//==================================================50\n\t\t//\tSELECTION 2\n\t\t//==================================================50\n\n\t\t// work\n\t\tei_new = tx;\n\t\twhile(ei_new < d_common.in2_sub2_elem){\n\n\t\t\t// figure out row/col location in new matrix\n\t\t\trow = (ei_new+1) % d_common.in2_sub2_rows - 1;\t\t\t\t\t\t\t\t\t\t\t\t// (0-n) row\n\t\t\tcol = (ei_new+1) / d_common.in2_sub2_rows + 1 - 1;\t\t\t\t\t\t\t\t\t\t\t// (0-n) column\n\t\t\tif((ei_new+1) % d_common.in2_sub2_rows == 0){\n\t\t\t\trow = d_common.in2_sub2_rows - 1;\n\t\t\t\tcol = col-1;\n\t\t\t}\n\n\t\t\t// figure out corresponding location in old matrix and copy values to new matrix\n\t\t\tori_row = row + d_common.in2_sub_cumh_sel2_rowlow - 1;\n\t\t\tori_col = col + d_common.in2_sub_cumh_sel2_collow - 1;\n\t\t\td_unique_d_in2_sub2[ei_new] = d_unique_d_in2_sub_cumh[ori_col*d_common.in2_sub_cumh_rows+ori_row];\n\n\t\t\t// go for second round\n\t\t\tei_new = ei_new + NUMBER_THREADS;\n\n\t\t}\n\n\t\t//==================================================50\n\t\t//\tSYNCHRONIZE THREADS\n\t\t//==================================================50\n\n\t\t__syncthreads();\n\n\t\t//==================================================50\n\t\t//\tchecksum\n\t\t//==================================================50\n#ifdef TEST_CHECKSUM\n\t\tif(bx==0 && tx==0){\n\t\t\tfor(i=0; i<d_common.in2_sub2_elem; i++){\n\t\t\t\tchecksum[11] = checksum[11]+d_unique_d_in2_sub2[i];\n\t\t\t}\n\t\t}\n\n\t\t//==================================================50\n\t\t//\tSYNCHRONIZE THREADS\n\t\t//==================================================50\n\n\t\t__syncthreads();\n#endif\n\t\t//==================================================50\n\t\t//\tSUBTRACTION\n\t\t//==================================================50\n\n\t\t// work\n\t\tei_new = tx;\n\t\twhile(ei_new < d_common.in2_sub2_elem){\n\n\t\t\t// subtract\n\t\t\td_unique_d_in2_sub2[ei_new] = d_unique_d_in2_sub_cumh_sel[ei_new] - d_unique_d_in2_sub2[ei_new];\n\n\t\t\t// go for second round\n\t\t\tei_new = ei_new + NUMBER_THREADS;\n\n\t\t}\n\n\t\t//==================================================50\n\t\t//\tSYNCHRONIZE THREADS\n\t\t//==================================================50\n\n\t\t__syncthreads();\n\n\t\t//==================================================50\n\t\t//\tchecksum\n\t\t//==================================================50\n#ifdef TEST_CHECKSUM\n\t\tif(bx==0 && tx==0){\n\t\t\tfor(i=0; i<d_common.in2_sub2_elem; i++){\n\t\t\t\tchecksum[12] = checksum[12]+d_unique_d_in2_sub2[i];\n\t\t\t}\n\t\t}\n\n\t\t//==================================================50\n\t\t//\tSYNCHRONIZE THREADS\n\t\t//==================================================50\n\n\t\t__syncthreads();\n#endif\n\t\t//==================================================50\n\t\t//\tEnd\n\t\t//==================================================50\n\n\t\t//====================================================================================================100\n\t\t//\tCUMULATIVE SUM 2\n\t\t//====================================================================================================100\n\n\t\t//==================================================50\n\t\t//\tMULTIPLICATION\n\t\t//==================================================50\n\n\t\t// work\n\t\tei_new = tx;\n\t\twhile(ei_new < d_common.in2_sqr_elem){\n\n\t\t\ttemp = d_unique_d_in2[ei_new];\n\t\t\td_unique_d_in2_sqr[ei_new] = temp * temp;\n\n\t\t\t// go for second round\n\t\t\tei_new = ei_new + NUMBER_THREADS;\n\n\t\t}\n\n\t\t//==================================================50\n\t\t//\tSYNCHRONIZE THREADS\n\t\t//==================================================50\n\n\t\t__syncthreads();\n\n\t\t//==================================================50\n\t\t//\tchecksum\n\t\t//==================================================50\n#ifdef TEST_CHECKSUM\n\t\tif(bx==0 && tx==0){\n\t\t\tfor(i=0; i<d_common.in2_sqr_elem; i++){\n\t\t\t\tchecksum[13] = checksum[13]+d_unique_d_in2_sqr[i];\n\t\t\t}\n\t\t}\n\n\t\t//==================================================50\n\t\t//\tSYNCHRONIZE THREADS\n\t\t//==================================================50\n\n\t\t__syncthreads();\n#endif\n\t\t//==================================================50\n\t\t//\tPAD ARRAY, VERTICAL CUMULATIVE SUM\n\t\t//==================================================50\n\n\t\t//==================================================50\n\t\t//\tPAD ARRAY\n\t\t//==================================================50\n\n\t\t// work\n\t\tei_new = tx;\n\t\twhile(ei_new < d_common.in2_pad_cumv_elem){\n\n\t\t\t// figure out row/col location in padded array\n\t\t\trow = (ei_new+1) % d_common.in2_pad_cumv_rows - 1;\t\t\t\t\t\t\t\t\t\t\t\t// (0-n) row\n\t\t\tcol = (ei_new+1) / d_common.in2_pad_cumv_rows + 1 - 1;\t\t\t\t\t\t\t\t\t\t\t// (0-n) column\n\t\t\tif((ei_new+1) % d_common.in2_pad_cumv_rows == 0){\n\t\t\t\trow = d_common.in2_pad_cumv_rows - 1;\n\t\t\t\tcol = col-1;\n\t\t\t}\n\n\t\t\t// execution\n\t\t\tif(\trow > (d_common.in2_pad_add_rows-1) &&\t\t\t\t\t\t\t\t\t\t\t\t\t// do if has numbers in original array\n\t\t\t\trow < (d_common.in2_pad_add_rows+d_common.in2_sqr_rows) && \n\t\t\t\tcol > (d_common.in2_pad_add_cols-1) && \n\t\t\t\tcol < (d_common.in2_pad_add_cols+d_common.in2_sqr_cols)){\n\t\t\t\tori_row = row - d_common.in2_pad_add_rows;\n\t\t\t\tori_col = col - d_common.in2_pad_add_cols;\n\t\t\t\td_unique_d_in2_pad_cumv[ei_new] = d_unique_d_in2_sqr[ori_col*d_common.in2_sqr_rows+ori_row];\n\t\t\t}\n\t\t\telse{\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t// do if otherwise\n\t\t\t\td_unique_d_in2_pad_cumv[ei_new] = 0;\n\t\t\t}\n\n\t\t\t// go for second round\n\t\t\tei_new = ei_new + NUMBER_THREADS;\n\n\t\t}\n\n\t\t//==================================================50\n\t\t//\tSYNCHRONIZE THREADS\n\t\t//==================================================50\n\n\t\t__syncthreads();\n\n\t\t//==================================================50\n\t\t//\tchecksum\n\t\t//==================================================50\n#ifdef TEST_CHECKSUM\n\t\tif(bx==0 && tx==0){\n\t\t\tfor(i=0; i<d_common.in2_pad_cumv_elem; i++){\n\t\t\t\tchecksum[14] = checksum[14]+d_unique_d_in2_pad_cumv[i];\n\t\t\t}\n\t\t}\n\n\t\t//==================================================50\n\t\t//\tSYNCHRONIZE THREADS\n\t\t//==================================================50\n\n\t\t__syncthreads();\n#endif\n\t\t//==================================================50\n\t\t//\tVERTICAL CUMULATIVE SUM\n\t\t//==================================================50\n\n\t\t//work\n\t\tei_new = tx;\n\t\twhile(ei_new < d_common.in2_pad_cumv_cols){\n\n\t\t\t// figure out column position\n\t\t\tpos_ori = ei_new*d_common.in2_pad_cumv_rows;\n\n\t\t\t// variables\n\t\t\tsum = 0;\n\t\t\t\n\t\t\t// loop through all rows\n\t\t\tfor(position = pos_ori; position < pos_ori+d_common.in2_pad_cumv_rows; position = position + 1){\n\t\t\t\td_unique_d_in2_pad_cumv[position] = d_unique_d_in2_pad_cumv[position] + sum;\n\t\t\t\tsum = d_unique_d_in2_pad_cumv[position];\n\t\t\t}\n\n\t\t\t// go for second round\n\t\t\tei_new = ei_new + NUMBER_THREADS;\n\n\t\t}\n\n\t\t//==================================================50\n\t\t//\tSYNCHRONIZE THREADS\n\t\t//==================================================50\n\n\t\t__syncthreads();\n\n\t\t//==================================================50\n\t\t//\tchecksum\n\t\t//==================================================50\n#ifdef TEST_CHECKSUM\n\t\tif(bx==0 && tx==0){\n\t\t\tfor(i=0; i<d_common.in2_pad_cumv_elem; i++){\n\t\t\t\tchecksum[15] = checksum[15]+d_unique_d_in2_pad_cumv[i];\n\t\t\t}\n\t\t}\n\n\t\t//==================================================50\n\t\t//\tSYNCHRONIZE THREADS\n\t\t//==================================================50\n\n\t\t__syncthreads();\n#endif\n\t\t//==================================================50\n\t\t//\tSELECTION\n\t\t//==================================================50\n\n\t\t// work\n\t\tei_new = tx;\n\t\twhile(ei_new < d_common.in2_pad_cumv_sel_elem){\n\n\t\t\t// figure out row/col location in new matrix\n\t\t\trow = (ei_new+1) % d_common.in2_pad_cumv_sel_rows - 1;\t\t\t\t\t\t\t\t\t\t\t\t// (0-n) row\n\t\t\tcol = (ei_new+1) / d_common.in2_pad_cumv_sel_rows + 1 - 1;\t\t\t\t\t\t\t\t\t\t\t// (0-n) column\n\t\t\tif((ei_new+1) % d_common.in2_pad_cumv_sel_rows == 0){\n\t\t\t\trow = d_common.in2_pad_cumv_sel_rows - 1;\n\t\t\t\tcol = col-1;\n\t\t\t}\n\n\t\t\t// figure out corresponding location in old matrix and copy values to new matrix\n\t\t\tori_row = row + d_common.in2_pad_cumv_sel_rowlow - 1;\n\t\t\tori_col = col + d_common.in2_pad_cumv_sel_collow - 1;\n\t\t\td_unique_d_in2_pad_cumv_sel[ei_new] = d_unique_d_in2_pad_cumv[ori_col*d_common.in2_pad_cumv_rows+ori_row];\n\n\t\t\t// go for second round\n\t\t\tei_new = ei_new + NUMBER_THREADS;\n\n\t\t}\n\n\t\t//==================================================50\n\t\t//\tSYNCHRONIZE THREADS\n\t\t//==================================================50\n\n\t\t__syncthreads();\n\n\t\t//==================================================50\n\t\t//\tchecksum\n\t\t//==================================================50\n#ifdef TEST_CHECKSUM\n\t\tif(bx==0 && tx==0){\n\t\t\tfor(i=0; i<d_common.in2_pad_cumv_sel_elem; i++){\n\t\t\t\tchecksum[16] = checksum[16]+d_unique_d_in2_pad_cumv_sel[i];\n\t\t\t}\n\t\t}\n\n\t\t//==================================================50\n\t\t//\tSYNCHRONIZE THREADS\n\t\t//==================================================50\n\n\t\t__syncthreads();\n#endif\n\t\t//==================================================50\n\t\t//\tSELECTION 2\n\t\t//==================================================50\n\n\t\t// work\n\t\tei_new = tx;\n\t\twhile(ei_new < d_common.in2_sub_cumh_elem){\n\n\t\t\t// figure out row/col location in new matrix\n\t\t\trow = (ei_new+1) % d_common.in2_sub_cumh_rows - 1;\t\t\t\t\t\t\t\t\t\t\t\t// (0-n) row\n\t\t\tcol = (ei_new+1) / d_common.in2_sub_cumh_rows + 1 - 1;\t\t\t\t\t\t\t\t\t\t\t// (0-n) column\n\t\t\tif((ei_new+1) % d_common.in2_sub_cumh_rows == 0){\n\t\t\t\trow = d_common.in2_sub_cumh_rows - 1;\n\t\t\t\tcol = col-1;\n\t\t\t}\n\n\t\t\t// figure out corresponding location in old matrix and copy values to new matrix\n\t\t\tori_row = row + d_common.in2_pad_cumv_sel2_rowlow - 1;\n\t\t\tori_col = col + d_common.in2_pad_cumv_sel2_collow - 1;\n\t\t\td_unique_d_in2_sub_cumh[ei_new] = d_unique_d_in2_pad_cumv[ori_col*d_common.in2_pad_cumv_rows+ori_row];\n\n\t\t\t// go for second round\n\t\t\tei_new = ei_new + NUMBER_THREADS;\n\n\t\t}\n\n\t\t//==================================================50\n\t\t//\tSYNCHRONIZE THREADS\n\t\t//==================================================50\n\n\t\t__syncthreads();\n\n\t\t//==================================================50\n\t\t//\tchecksum\n\t\t//==================================================50\n#ifdef TEST_CHECKSUM\n\t\tif(bx==0 && tx==0){\n\t\t\tfor(i=0; i<d_common.in2_sub_cumh_elem; i++){\n\t\t\t\tchecksum[17] = checksum[17]+d_unique_d_in2_sub_cumh[i];\n\t\t\t}\n\t\t}\n\n\t\t//==================================================50\n\t\t//\tSYNCHRONIZE THREADS\n\t\t//==================================================50\n\n\t\t__syncthreads();\n#endif\n\t\t//==================================================50\n\t\t//\tSUBTRACTION\n\t\t//==================================================50\n\n\t\t// work\n\t\tei_new = tx;\n\t\twhile(ei_new < d_common.in2_sub_cumh_elem){\n\n\t\t\t// subtract\n\t\t\td_unique_d_in2_sub_cumh[ei_new] = d_unique_d_in2_pad_cumv_sel[ei_new] - d_unique_d_in2_sub_cumh[ei_new];\n\n\t\t\t// go for second round\n\t\t\tei_new = ei_new + NUMBER_THREADS;\n\n\t\t}\n\n\t\t//==================================================50\n\t\t//\tSYNCHRONIZE THREADS\n\t\t//==================================================50\n\n\t\t__syncthreads();\n\n\t\t//==================================================50\n\t\t//\tchecksum\n\t\t//==================================================50\n#ifdef TEST_CHECKSUM\n\t\tif(bx==0 && tx==0){\n\t\t\tfor(i=0; i<d_common.in2_sub_cumh_elem; i++){\n\t\t\t\tchecksum[18] = checksum[18]+d_unique_d_in2_sub_cumh[i];\n\t\t\t}\n\t\t}\n\n\t\t//==================================================50\n\t\t//\tSYNCHRONIZE THREADS\n\t\t//==================================================50\n\n\t\t__syncthreads();\n#endif\n\t\t//==================================================50\n\t\t//\tHORIZONTAL CUMULATIVE SUM\n\t\t//==================================================50\n\n\t\t// work\n\t\tei_new = tx;\n\t\twhile(ei_new < d_common.in2_sub_cumh_rows){\n\n\t\t\t// figure out row position\n\t\t\tpos_ori = ei_new;\n\n\t\t\t// variables\n\t\t\tsum = 0;\n\n\t\t\t// loop through all rows\n\t\t\tfor(position = pos_ori; position < pos_ori+d_common.in2_sub_cumh_elem; position = position + d_common.in2_sub_cumh_rows){\n\t\t\t\td_unique_d_in2_sub_cumh[position] = d_unique_d_in2_sub_cumh[position] + sum;\n\t\t\t\tsum = d_unique_d_in2_sub_cumh[position];\n\t\t\t}\n\n\t\t\t// go for second round\n\t\t\tei_new = ei_new + NUMBER_THREADS;\n\n\t\t}\n\n\t\t//==================================================50\n\t\t//\tSYNCHRONIZE THREADS\n\t\t//==================================================50\n\n\t\t__syncthreads();\n\n\t\t//==================================================50\n\t\t//\tchecksum\n\t\t//==================================================50\n#ifdef TEST_CHECKSUM\n\t\tif(bx==0 && tx==0){\n\t\t\tfor(i=0; i<d_common.in2_sub_cumh_rows; i++){\n\t\t\t\tchecksum[19] = checksum[19]+d_unique_d_in2_sub_cumh[i];\n\t\t\t}\n\t\t}\n\n\t\t//==================================================50\n\t\t//\tSYNCHRONIZE THREADS\n\t\t//==================================================50\n\n\t\t__syncthreads();\n#endif\n\t\t//==================================================50\n\t\t//\tSELECTION\n\t\t//==================================================50\n\n\t\t// work\n\t\tei_new = tx;\n\t\twhile(ei_new < d_common.in2_sub_cumh_sel_elem){\n\n\t\t\t// figure out row/col location in new matrix\n\t\t\trow = (ei_new+1) % d_common.in2_sub_cumh_sel_rows - 1;\t\t\t\t\t\t\t\t\t\t\t\t// (0-n) row\n\t\t\tcol = (ei_new+1) / d_common.in2_sub_cumh_sel_rows + 1 - 1;\t\t\t\t\t\t\t\t\t\t\t// (0-n) column\n\t\t\tif((ei_new+1) % d_common.in2_sub_cumh_sel_rows == 0){\n\t\t\t\trow = d_common.in2_sub_cumh_sel_rows - 1;\n\t\t\t\tcol = col - 1;\n\t\t\t}\n\n\t\t\t// figure out corresponding location in old matrix and copy values to new matrix\n\t\t\tori_row = row + d_common.in2_sub_cumh_sel_rowlow - 1;\n\t\t\tori_col = col + d_common.in2_sub_cumh_sel_collow - 1;\n\t\t\td_unique_d_in2_sub_cumh_sel[ei_new] = d_unique_d_in2_sub_cumh[ori_col*d_common.in2_sub_cumh_rows+ori_row];\n\n\t\t\t// go for second round\n\t\t\tei_new = ei_new + NUMBER_THREADS;\n\n\t\t}\n\n\t\t//==================================================50\n\t\t//\tSYNCHRONIZE THREADS\n\t\t//==================================================50\n\n\t\t__syncthreads();\n\n\t\t//==================================================50\n\t\t//\tchecksum\n\t\t//==================================================50\n#ifdef TEST_CHECKSUM\n\t\tif(bx==0 && tx==0){\n\t\t\tfor(i=0; i<d_common.in2_sub_cumh_sel_elem; i++){\n\t\t\t\tchecksum[20] = checksum[20]+d_unique_d_in2_sub_cumh_sel[i];\n\t\t\t}\n\t\t}\n\n\t\t//==================================================50\n\t\t//\tSYNCHRONIZE THREADS\n\t\t//==================================================50\n\n\t\t__syncthreads();\n#endif\n\t\t//==================================================50\n\t\t//\tSELECTION 2\n\t\t//==================================================50\n\n\t\t// work\n\t\tei_new = tx;\n\t\twhile(ei_new < d_common.in2_sub2_elem){\n\n\t\t\t// figure out row/col location in new matrix\n\t\t\trow = (ei_new+1) % d_common.in2_sub2_rows - 1;\t\t\t\t\t\t\t\t\t\t\t\t// (0-n) row\n\t\t\tcol = (ei_new+1) / d_common.in2_sub2_rows + 1 - 1;\t\t\t\t\t\t\t\t\t\t\t// (0-n) column\n\t\t\tif((ei_new+1) % d_common.in2_sub2_rows == 0){\n\t\t\t\trow = d_common.in2_sub2_rows - 1;\n\t\t\t\tcol = col-1;\n\t\t\t}\n\n\t\t\t// figure out corresponding location in old matrix and copy values to new matrix\n\t\t\tori_row = row + d_common.in2_sub_cumh_sel2_rowlow - 1;\n\t\t\tori_col = col + d_common.in2_sub_cumh_sel2_collow - 1;\n\t\t\td_unique_d_in2_sqr_sub2[ei_new] = d_unique_d_in2_sub_cumh[ori_col*d_common.in2_sub_cumh_rows+ori_row];\n\n\t\t\t// go for second round\n\t\t\tei_new = ei_new + NUMBER_THREADS;\n\n\t\t}\n\n\t\t//==================================================50\n\t\t//\tSYNCHRONIZE THREADS\n\t\t//==================================================50\n\n\t\t__syncthreads();\n\n\t\t//==================================================50\n\t\t//\tchecksum\n\t\t//==================================================50\n#ifdef TEST_CHECKSUM\n\t\tif(bx==0 && tx==0){\n\t\t\tfor(i=0; i<d_common.in2_sub2_elem; i++){\n\t\t\t\tchecksum[21] = checksum[21]+d_unique_d_in2_sqr_sub2[i];\n\t\t\t}\n\t\t}\n\n\t\t//==================================================50\n\t\t//\tSYNCHRONIZE THREADS\n\t\t//==================================================50\n\n\t\t__syncthreads();\n#endif\n\t\t//==================================================50\n\t\t//\tSUBTRACTION\n\t\t//==================================================50\n\n\t\t// work\n\t\tei_new = tx;\n\t\twhile(ei_new < d_common.in2_sub2_elem){\n\n\t\t\t// subtract\n\t\t\td_unique_d_in2_sqr_sub2[ei_new] = d_unique_d_in2_sub_cumh_sel[ei_new] - d_unique_d_in2_sqr_sub2[ei_new];\n\n\t\t\t// go for second round\n\t\t\tei_new = ei_new + NUMBER_THREADS;\n\n\t\t}\n\n\t\t//==================================================50\n\t\t//\tSYNCHRONIZE THREADS\n\t\t//==================================================50\n\n\t\t__syncthreads();\n\n\t\t//==================================================50\n\t\t//\tchecksum\n\t\t//==================================================50\n#ifdef TEST_CHECKSUM\n\t\tif(bx==0 && tx==0){\n\t\t\tfor(i=0; i<d_common.in2_sub2_elem; i++){\n\t\t\t\tchecksum[22] = checksum[22]+d_unique_d_in2_sqr_sub2[i];\n\t\t\t}\n\t\t}\n\n\t\t//==================================================50\n\t\t//\tSYNCHRONIZE THREADS\n\t\t//==================================================50\n\n\t\t__syncthreads();\n#endif\n\t\t//==================================================50\n\t\t//\tEnd\n\t\t//==================================================50\n\n\t\t//====================================================================================================100\n\t\t//\tFINAL\n\t\t//====================================================================================================100\n\n\t\t//==================================================50\n\t\t//\tDENOMINATOR A\t\tSAVE RESULT IN CUMULATIVE SUM A2\n\t\t//==================================================50\n\n\t\t// work\n\t\tei_new = tx;\n\t\twhile(ei_new < d_common.in2_sub2_elem){\n\n\t\t\ttemp = d_unique_d_in2_sub2[ei_new];\n\t\t\ttemp2 = d_unique_d_in2_sqr_sub2[ei_new] - (temp * temp / d_common.in_elem);\n\t\t\tif(temp2 < 0){\n\t\t\t\ttemp2 = 0;\n\t\t\t}\n\t\t\td_unique_d_in2_sqr_sub2[ei_new] = sqrt(temp2);\n\t\t\t\n\n\t\t\t// go for second round\n\t\t\tei_new = ei_new + NUMBER_THREADS;\n\n\t\t}\n\n\t\t//==================================================50\n\t\t//\tSYNCHRONIZE THREADS\n\t\t//==================================================50\n\n\t\t__syncthreads();\n\n\t\t//==================================================50\n\t\t//\tchecksum\n\t\t//==================================================50\n#ifdef TEST_CHECKSUM\n\t\tif(bx==0 && tx==0){\n\t\t\tfor(i=0; i<d_common.in2_sub2_elem; i++){\n\t\t\t\tchecksum[23] = checksum[23]+d_unique_d_in2_sqr_sub2[i];\n\t\t\t}\n\t\t}\n\n\t\t//==================================================50\n\t\t//\tSYNCHRONIZE THREADS\n\t\t//==================================================50\n\n\t\t__syncthreads();\n#endif\n\t\t//==================================================50\n\t\t//\tMULTIPLICATION\n\t\t//==================================================50\n\n\t\t// work\n\t\tei_new = tx;\n\t\twhile(ei_new < d_common.in_sqr_elem){\n\n\t\t\ttemp = d_in[ei_new];\n\t\t\td_unique_d_in_sqr[ei_new] = temp * temp;\n\n\t\t\t// go for second round\n\t\t\tei_new = ei_new + NUMBER_THREADS;\n\n\t\t}\n\n\t\t//==================================================50\n\t\t//\tSYNCHRONIZE THREADS\n\t\t//==================================================50\n\n\t\t__syncthreads();\n\n\t\t//==================================================50\n\t\t//\tchecksum\n\t\t//==================================================50\n#ifdef TEST_CHECKSUM\n\t\tif(bx==0 && tx==0){\n\t\t\tfor(i=0; i<d_common.in_sqr_elem; i++){\n\t\t\t\tchecksum[24] = checksum[24]+d_unique_d_in_sqr[i];\n\t\t\t}\n\t\t}\n\n\t\t//==================================================50\n\t\t//\tSYNCHRONIZE THREADS\n\t\t//==================================================50\n\n\t\t__syncthreads();\n#endif\n\t\t//==================================================50\n\t\t//\tIN SUM\n\t\t//==================================================50\n\n\t\t// work\n\t\tei_new = tx;\n\t\twhile(ei_new < d_common.in_cols){\n\n\t\t\tsum = 0;\n\t\t\tfor(i = 0; i < d_common.in_rows; i++){\n\n\t\t\t\tsum = sum + d_in[ei_new*d_common.in_rows+i];\n\n\t\t\t}\n\t\t\td_unique_d_in_partial_sum[ei_new] = sum;\n\n\t\t\t// go for second round\n\t\t\tei_new = ei_new + NUMBER_THREADS;\n\n\t\t}\n\n\t\t//==================================================50\n\t\t//\tSYNCHRONIZE THREADS\n\t\t//==================================================50\n\n\t\t__syncthreads();\n\n\t\t//==================================================50\n\t\t//\tchecksum\n\t\t//==================================================50\n#ifdef TEST_CHECKSUM\n\t\tif(bx==0 && tx==0){\n\t\t\tfor(i=0; i<d_common.in_cols; i++){\n\t\t\t\tchecksum[25] = checksum[25]+d_unique_d_in_partial_sum[i];\n\t\t\t}\n\t\t}\n\n\t\t//==================================================50\n\t\t//\tSYNCHRONIZE THREADS\n\t\t//==================================================50\n\n\t\t__syncthreads();\n#endif\n\t\t//==================================================50\n\t\t//\tIN_SQR SUM\n\t\t//==================================================50\n\n\t\tei_new = tx;\n\t\twhile(ei_new < d_common.in_sqr_rows){\n\t\t\t\t\n\t\t\tsum = 0;\n\t\t\tfor(i = 0; i < d_common.in_sqr_cols; i++){\n\n\t\t\t\tsum = sum + d_unique_d_in_sqr[ei_new+d_common.in_sqr_rows*i];\n\n\t\t\t}\n\t\t\td_unique_d_in_sqr_partial_sum[ei_new] = sum;\n\n\t\t\t// go for second round\n\t\t\tei_new = ei_new + NUMBER_THREADS;\n\n\t\t}\n\n\t\t//==================================================50\n\t\t//\tSYNCHRONIZE THREADS\n\t\t//==================================================50\n\n\t\t__syncthreads();\n\n\t\t//==================================================50\n\t\t//\tchecksum\n\t\t//==================================================50\n#ifdef TEST_CHECKSUM\n\t\tif(bx==0 && tx==0){\n\t\t\tfor(i=0; i<d_common.in_sqr_rows; i++){\n\t\t\t\tchecksum[26] = checksum[26]+d_unique_d_in_sqr_partial_sum[i];\n\t\t\t}\n\t\t}\n\n\t\t//==================================================50\n\t\t//\tSYNCHRONIZE THREADS\n\t\t//==================================================50\n\n\t\t__syncthreads();\n#endif\n\t\t//==================================================50\n\t\t//\tFINAL SUMMATION\n\t\t//==================================================50\n\n\t\tif(tx == 0){\n\n\t\t\td_unique_d_in_final_sum[0] = 0;\n\t\t\tfor(i = 0; i<d_common.in_cols; i++){\n\t\t\t\t// in_final_sum = in_final_sum + d_unique_d_in_partial_sum[i];\n\t\t\t\td_unique_d_in_final_sum[0] = d_unique_d_in_final_sum[0] + d_unique_d_in_partial_sum[i];\n\t\t\t}\n\n\t\t}else if(tx == 1){\n\n\t\t\td_unique_d_in_sqr_final_sum[0] = 0;\n\t\t\tfor(i = 0; i<d_common.in_sqr_cols; i++){\n\t\t\t\t// in_sqr_final_sum = in_sqr_final_sum + d_unique_d_in_sqr_partial_sum[i];\n\t\t\t\td_unique_d_in_sqr_final_sum[0] = d_unique_d_in_sqr_final_sum[0] + d_unique_d_in_sqr_partial_sum[i];\n\t\t\t}\n\n\t\t}\n\n\t\t//==================================================50\n\t\t//\tSYNCHRONIZE THREADS\n\t\t//==================================================50\n\n\t\t__syncthreads();\n\n\t\t//==================================================50\n\t\t//\tchecksum\n\t\t//==================================================50\n#ifdef TEST_CHECKSUM\n\t\tif(bx==0 && tx==0){\n\t\t\tchecksum[27] = checksum[27]+d_unique_d_in_final_sum[0]+d_unique_d_in_sqr_final_sum[0];\n\t\t}\n\n\t\t//==================================================50\n\t\t//\tSYNCHRONIZE THREADS\n\t\t//==================================================50\n\n\t\t__syncthreads();\n#endif\n\t\t//==================================================50\n\t\t//\tDENOMINATOR T\n\t\t//==================================================50\n\n\t\tif(tx == 0){\n\n\t\t\t// mean = in_final_sum / d_common.in_elem;\t\t\t\t\t\t\t\t\t\t\t\t\t// gets mean (average) value of element in ROI\n\t\t\tmean = d_unique_d_in_final_sum[0] / d_common.in_elem;\t\t\t\t\t\t\t\t\t\t\t\t\t// gets mean (average) value of element in ROI\n\t\t\tmean_sqr = mean * mean;\n\t\t\t// variance  = (in_sqr_final_sum / d_common.in_elem) - mean_sqr;\t\t\t\t\t\t\t// gets variance of ROI\n\t\t\tvariance  = (d_unique_d_in_sqr_final_sum[0] / d_common.in_elem) - mean_sqr;\t\t\t\t\t\t\t// gets variance of ROI\n\t\t\tdeviation = sqrt(variance);\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t// gets standard deviation of ROI\n\n\t\t\t// denomT = sqrt((float)(d_common.in_elem-1))*deviation;\n\t\t\td_unique_d_denomT[0] = sqrt((float)(d_common.in_elem-1))*deviation;\n\n\t\t}\n\n\t\t//==================================================50\n\t\t//\tSYNCHRONIZE THREADS\n\t\t//==================================================50\n\n\t\t__syncthreads();\n\n\t\t//==================================================50\n\t\t//\tchecksum\n\t\t//==================================================50\n#ifdef TEST_CHECKSUM\n\t\tif(bx==0 && tx==0){\n\t\t\tchecksum[28] = checksum[28]+d_unique_d_denomT[i];\n\t\t}\n\n\t\t//==================================================50\n\t\t//\tSYNCHRONIZE THREADS\n\t\t//==================================================50\n\n\t\t__syncthreads();\n#endif\n\t\t//==================================================50\n\t\t//\tDENOMINATOR\t\tSAVE RESULT IN CUMULATIVE SUM A2\n\t\t//==================================================50\n\n\t\t// work\n\t\tei_new = tx;\n\t\twhile(ei_new < d_common.in2_sub2_elem){\n\n\t\t\t// d_unique_d_in2_sqr_sub2[ei_new] = d_unique_d_in2_sqr_sub2[ei_new] * denomT;\n\t\t\td_unique_d_in2_sqr_sub2[ei_new] = d_unique_d_in2_sqr_sub2[ei_new] * d_unique_d_denomT[0];\n\n\t\t\t// go for second round\n\t\t\tei_new = ei_new + NUMBER_THREADS;\n\n\t\t}\n\n\t\t//==================================================50\n\t\t//\tSYNCHRONIZE THREADS\n\t\t//==================================================50\n\n\t\t__syncthreads();\n\n\t\t//==================================================50\n\t\t//\tchecksum\n\t\t//==================================================50\n#ifdef TEST_CHECKSUM\n\t\tif(bx==0 && tx==0){\n\t\t\tfor(i=0; i<d_common.in2_sub2_elem; i++){\n\t\t\t\tchecksum[29] = checksum[29]+d_unique_d_in2_sqr_sub2[i];\n\t\t\t}\n\t\t}\n\n\t\t//==================================================50\n\t\t//\tSYNCHRONIZE THREADS\n\t\t//==================================================50\n\n\t\t__syncthreads();\n#endif\n\t\t//==================================================50\n\t\t//\tNUMERATOR\tSAVE RESULT IN CONVOLUTION\n\t\t//==================================================50\n\n\t\t// work\n\t\tei_new = tx;\n\t\twhile(ei_new < d_common.conv_elem){\n\n\t\t\t// d_unique_d_conv[ei_new] = d_unique_d_conv[ei_new] - d_unique_d_in2_sub2[ei_new] * in_final_sum / d_common.in_elem;\n\t\t\td_unique_d_conv[ei_new] = d_unique_d_conv[ei_new] - d_unique_d_in2_sub2[ei_new] * d_unique_d_in_final_sum[0] / d_common.in_elem;\n\n\t\t\t// go for second round\n\t\t\tei_new = ei_new + NUMBER_THREADS;\n\n\t\t}\n\n\t\t//==================================================50\n\t\t//\tSYNCHRONIZE THREADS\n\t\t//==================================================50\n\n\t\t__syncthreads();\n\n\t\t//==================================================50\n\t\t//\tchecksum\n\t\t//==================================================50\n#ifdef TEST_CHECKSUM\n\t\tif(bx==0 && tx==0){\n\t\t\tfor(i=0; i<d_common.conv_elem; i++){\n\t\t\t\tchecksum[30] = checksum[30]+d_unique_d_conv[i];\n\t\t\t}\n\t\t}\n\n\t\t//==================================================50\n\t\t//\tSYNCHRONIZE THREADS\n\t\t//==================================================50\n\n\t\t__syncthreads();\n#endif\n\t\t//==================================================50\n\t\t//\tCORRELATION\tSAVE RESULT IN CUMULATIVE SUM A2\n\t\t//==================================================50\n\n\t\t// work\n\t\tei_new = tx;\n\t\twhile(ei_new < d_common.in2_sub2_elem){\n\n\t\t\td_unique_d_in2_sqr_sub2[ei_new] = d_unique_d_conv[ei_new] / d_unique_d_in2_sqr_sub2[ei_new];\n\n\t\t\t// go for second round\n\t\t\tei_new = ei_new + NUMBER_THREADS;\n\n\t\t}\n\n\n\n\t\t//==================================================50\n\t\t//\tSYNCHRONIZE THREADS\n\t\t//==================================================50\n\n\t\t__syncthreads();\n\n\t\t//==================================================50\n\t\t//\tchecksum\n\t\t//==================================================50\n#ifdef TEST_CHECKSUM\n\t\tif(bx==0 && tx==0){\n\t\t\tfor(i=0; i<d_common.in2_sub2_elem; i++){\n\t\t\t\tchecksum[31] = checksum[31]+d_unique_d_in2_sqr_sub2[i];\n\t\t\t}\n\t\t}\n\n\t\t//==================================================50\n\t\t//\tSYNCHRONIZE THREADS\n\t\t//==================================================50\n\n\t\t__syncthreads();\n#endif\n\t\t//==================================================50\n\t\t//\tEnd\n\t\t//==================================================50\n\n\t\t//====================================================================================================100\n\t\t//\tTEMPLATE MASK CREATE\n\t\t//====================================================================================================100\n\n\t\tcent = d_common.sSize + d_common.tSize + 1;\n\t\tif(frame_no == 0){\n\t\t\ttMask_row = cent + d_unique_d_Row[d_unique_point_no] - d_unique_d_Row[d_unique_point_no] - 1;\n\t\t\ttMask_col = cent + d_unique_d_Col[d_unique_point_no] - d_unique_d_Col[d_unique_point_no] - 1;\n\t\t}\n\t\telse{\n\t\t\tpointer = d_unique_point_no*d_common.no_frames+frame_no-1;\n\t\t\ttMask_row = cent + d_unique_d_tRowLoc[pointer] - d_unique_d_Row[d_unique_point_no] - 1;\n\t\t\ttMask_col = cent + d_unique_d_tColLoc[pointer] - d_unique_d_Col[d_unique_point_no] - 1;\n\t\t}\n\n\t\t//work\n\t\tei_new = tx;\n\t\twhile(ei_new < d_common.tMask_elem){\n\n\t\t\tlocation = tMask_col*d_common.tMask_rows + tMask_row;\n\n\t\t\tif(ei_new==location){\n\t\t\t\td_unique_d_tMask[ei_new] = 1;\n\t\t\t}\n\t\t\telse{\n\t\t\t\td_unique_d_tMask[ei_new] = 0;\n\t\t\t}\n\n\t\t\t//go for second round\n\t\t\tei_new = ei_new + NUMBER_THREADS;\n\n\t\t}\n\n\t\t//==================================================50\n\t\t//\tSYNCHRONIZE THREADS\n\t\t//==================================================50\n\n\t\t__syncthreads();\n\n\t\t//==================================================50\n\t\t//\tchecksum\n\t\t//==================================================50\n#ifdef TEST_CHECKSUM\n\t\tif(bx==0 && tx==0){\n\t\t\tfor(i=0; i<d_common.tMask_elem; i++){\n\t\t\t\tchecksum[32] = checksum[32]+d_unique_d_tMask[i];\n\t\t\t}\n\t\t}\n\n\t\t//==================================================50\n\t\t//\tSYNCHRONIZE THREADS\n\t\t//==================================================50\n\n\t\t__syncthreads();\n#endif\n\t\t//==================================================50\n\t\t//\tEnd\n\t\t//==================================================50\n\n\t\t//====================================================================================================100\n\t\t//\tMASK CONVOLUTION\n\t\t//====================================================================================================100\n\n\t\t// work\n\t\tei_new = tx;\n\t\twhile(ei_new < d_common.mask_conv_elem){\n\n\t\t\t// figure out row/col location in array\n\t\t\tic = (ei_new+1) % d_common.mask_conv_rows;\t\t\t\t\t\t\t\t\t\t\t\t// (1-n)\n\t\t\tjc = (ei_new+1) / d_common.mask_conv_rows + 1;\t\t\t\t\t\t\t\t\t\t\t// (1-n)\n\t\t\tif((ei_new+1) % d_common.mask_conv_rows == 0){\n\t\t\t\tic = d_common.mask_conv_rows;\n\t\t\t\tjc = jc-1;\n\t\t\t}\n\n\t\t\t//\n\t\t\tj = jc + d_common.mask_conv_joffset;\n\t\t\tjp1 = j + 1;\n\t\t\tif(d_common.mask_cols < jp1){\n\t\t\t\tja1 = jp1 - d_common.mask_cols;\n\t\t\t}\n\t\t\telse{\n\t\t\t\tja1 = 1;\n\t\t\t}\n\t\t\tif(d_common.tMask_cols < j){\n\t\t\t\tja2 = d_common.tMask_cols;\n\t\t\t}\n\t\t\telse{\n\t\t\t\tja2 = j;\n\t\t\t}\n\n\t\t\ti = ic + d_common.mask_conv_ioffset;\n\t\t\tip1 = i + 1;\n\t\t\t\n\t\t\tif(d_common.mask_rows < ip1){\n\t\t\t\tia1 = ip1 - d_common.mask_rows;\n\t\t\t}\n\t\t\telse{\n\t\t\t\tia1 = 1;\n\t\t\t}\n\t\t\tif(d_common.tMask_rows < i){\n\t\t\t\tia2 = d_common.tMask_rows;\n\t\t\t}\n\t\t\telse{\n\t\t\t\tia2 = i;\n\t\t\t}\n\n\t\t\ts = 0;\n\n\t\t\tfor(ja=ja1; ja<=ja2; ja++){\n\t\t\t\tjb = jp1 - ja;\n\t\t\t\tfor(ia=ia1; ia<=ia2; ia++){\n\t\t\t\t\tib = ip1 - ia;\n\t\t\t\t\ts = s + d_unique_d_tMask[d_common.tMask_rows*(ja-1)+ia-1] * 1;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t// //d_unique_d_mask_conv[d_common.mask_conv_rows*(jc-1)+ic-1] = s;\n\t\t\td_unique_d_mask_conv[ei_new] = d_unique_d_in2_sqr_sub2[ei_new] * s;\n\n\t\t\t// go for second round\n\t\t\tei_new = ei_new + NUMBER_THREADS;\n\n\t\t}\n\n\t\t//==================================================50\n\t\t//\tSYNCHRONIZE THREADS\n\t\t//==================================================50\n\n\t\t__syncthreads();\n\n\t\t//==================================================50\n\t\t//\tchecksum\n\t\t//==================================================50\n#ifdef TEST_CHECKSUM\n\t\tif(bx==0 && tx==0){\n\t\t\tfor(i=0; i<d_common.mask_conv_elem; i++){\n\t\t\t\tchecksum[33] = checksum[33]+d_unique_d_mask_conv[i];\n\t\t\t}\n\t\t}\n\n\t\t//==================================================50\n\t\t//\tSYNCHRONIZE THREADS\n\t\t//==================================================50\n\n\t\t__syncthreads();\n#endif\n\t\t//==================================================50\n\t\t//\tEnd\n\t\t//==================================================50\n\n\t\t//====================================================================================================100\n\t\t//\tMAXIMUM VALUE\n\t\t//====================================================================================================100\n\n\t\t//==================================================50\n\t\t//\tINITIAL SEARCH\n\t\t//==================================================50\n\n\t\tei_new = tx;\n\t\twhile(ei_new < d_common.mask_conv_rows){\n\n\t\t\tfor(i=0; i<d_common.mask_conv_cols; i++){\n\t\t\t\tlargest_coordinate_current = ei_new*d_common.mask_conv_rows+i;\n\t\t\t\tlargest_value_current = fabs(d_unique_d_mask_conv[largest_coordinate_current]);\n\t\t\t\tif(largest_value_current > largest_value){\n\t\t\t\t\tlargest_coordinate = largest_coordinate_current;\n\t\t\t\t\tlargest_value = largest_value_current;\n\t\t\t\t}\n\t\t\t}\n\t\t\td_unique_d_par_max_coo[ei_new] = largest_coordinate;\n\t\t\td_unique_d_par_max_val[ei_new] = largest_value;\n\n\t\t\t// go for second round\n\t\t\tei_new = ei_new + NUMBER_THREADS;\n\n\t\t}\n\n\t\t//==================================================50\n\t\t//\tSYNCHRONIZE THREADS\n\t\t//==================================================50\n\n\t\t__syncthreads();\n\n\t\t//==================================================50\n\t\t//\tchecksum\n\t\t//==================================================50\n#ifdef TEST_CHECKSUM\n\t\tif(bx==0 && tx==0){\n\t\t\tfor(i=0; i<d_common.mask_conv_rows; i++){\n\t\t\t\tchecksum[34] = checksum[34]+d_unique_d_par_max_coo[i]+d_unique_d_par_max_val[i];\n\t\t\t}\n\t\t}\n\n\t\t//==================================================50\n\t\t//\tSYNCHRONIZE THREADS\n\t\t//==================================================50\n\n\t\t__syncthreads();\n#endif\n\t\t//==================================================50\n\t\t//\tFINAL SEARCH\n\t\t//==================================================50\n\n\t\tif(tx == 0){\n\n\t\t\tfor(i = 0; i < d_common.mask_conv_rows; i++){\n\t\t\t\tif(d_unique_d_par_max_val[i] > fin_max_val){\n\t\t\t\t\tfin_max_val = d_unique_d_par_max_val[i];\n\t\t\t\t\tfin_max_coo = d_unique_d_par_max_coo[i];\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t// convert coordinate to row/col form\n\t\t\tlargest_row = (fin_max_coo+1) % d_common.mask_conv_rows - 1;\t\t\t\t\t\t\t\t\t\t\t// (0-n) row\n\t\t\tlargest_col = (fin_max_coo+1) / d_common.mask_conv_rows;\t\t\t\t\t\t\t\t\t\t\t\t// (0-n) column\n\t\t\tif((fin_max_coo+1) % d_common.mask_conv_rows == 0){\n\t\t\t\tlargest_row = d_common.mask_conv_rows - 1;\n\t\t\t\tlargest_col = largest_col - 1;\n\t\t\t}\n\n\t\t\t// calculate offset\n\t\t\tlargest_row = largest_row + 1;\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t// compensate to match MATLAB format (1-n)\n\t\t\tlargest_col = largest_col + 1;\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t// compensate to match MATLAB format (1-n)\n\t\t\toffset_row = largest_row - d_common.in_rows - (d_common.sSize - d_common.tSize);\n\t\t\toffset_col = largest_col - d_common.in_cols - (d_common.sSize - d_common.tSize);\n\t\t\tpointer = d_unique_point_no*d_common.no_frames+frame_no;\n\t\t\td_unique_d_tRowLoc[pointer] = d_unique_d_Row[d_unique_point_no] + offset_row;\n\t\t\td_unique_d_tColLoc[pointer] = d_unique_d_Col[d_unique_point_no] + offset_col;\n\n\t\t}\n\n\t\t//==================================================50\n\t\t//\tSYNCHRONIZE THREADS\n\t\t//==================================================50\n\n\t\t__syncthreads();\n\n\t\t//==================================================50\n\t\t//\tchecksum\n\t\t//==================================================50\n#ifdef TEST_CHECKSUM\n\t\tif(bx==0 && tx==0){\n\t\t\tchecksum[35] = checksum[35]+d_unique_d_tRowLoc[pointer]+d_unique_d_tColLoc[pointer];\n\t\t}\n\n\t\t//==================================================50\n\t\t//\tSYNCHRONIZE THREADS\n\t\t//==================================================50\n\n\t\t__syncthreads();\n#endif\n\t\t//==================================================50\n\t\t//\tEnd\n\t\t//==================================================50\n\n\t\t//====================================================================================================100\n\t\t//\tEnd\n\t\t//====================================================================================================100\n\n\t}\n\n\t//======================================================================================================================================================150\n\t//\tPERIODIC COORDINATE AND TEMPLATE UPDATE\n\t//======================================================================================================================================================150\n\n\tif(frame_no != 0 && (frame_no)%10 == 0){\n\n\n\t\t//====================================================================================================100\n\t\t// if the last frame in the bath, update template\n\t\t//====================================================================================================100\n\n\t\t// update coordinate\n\t\tloc_pointer = d_unique_point_no*d_common.no_frames+frame_no;\n\n\t\td_unique_d_Row[d_unique_point_no] = d_unique_d_tRowLoc[loc_pointer];\n\t\td_unique_d_Col[d_unique_point_no] = d_unique_d_tColLoc[loc_pointer];\n\n\t\t// work\n\t\tei_new = tx;\n\t\twhile(ei_new < d_common.in_elem){\n\n\t\t\t// figure out row/col location in new matrix\n\t\t\trow = (ei_new+1) % d_common.in_rows - 1;\t\t\t\t\t\t\t\t\t\t\t\t// (0-n) row\n\t\t\tcol = (ei_new+1) / d_common.in_rows + 1 - 1;\t\t\t\t\t\t\t\t\t\t\t// (0-n) column\n\t\t\tif((ei_new+1) % d_common.in_rows == 0){\n\t\t\t\trow = d_common.in_rows - 1;\n\t\t\t\tcol = col-1;\n\t\t\t}\n\n\t\t\t// figure out row/col location in corresponding new template area in image and give to every thread (get top left corner and progress down and right)\n\t\t\tori_row = d_unique_d_Row[d_unique_point_no] - 25 + row - 1;\n\t\t\tori_col = d_unique_d_Col[d_unique_point_no] - 25 + col - 1;\n\t\t\tori_pointer = ori_col*d_common.frame_rows+ori_row;\n\n\t\t\t// update template\n\t\t\td_in[ei_new] = d_common.alpha*d_in[ei_new] + (1-d_common.alpha)*d_frame[ori_pointer];\n\n\t\t\t// go for second round\n\t\t\tei_new = ei_new + NUMBER_THREADS;\n\n\t\t}\n\n\t\t//==================================================50\n\t\t//\tSYNCHRONIZE THREADS\n\t\t//==================================================50\n\n\t\t__syncthreads();\n\n\t\t//==================================================50\n\t\t//\tchecksum\n\t\t//==================================================50\n#ifdef TEST_CHECKSUM\n\t\tif(bx==0 && tx==0){\n\t\t\tfor(i=0; i<d_common.in_elem; i++){\n\t\t\t\tchecksum[36] = checksum[36]+d_in[i];\n\t\t\t}\n\t\t}\n\n\t\t//==================================================50\n\t\t//\tSYNCHRONIZE THREADS\n\t\t//==================================================50\n\n\t\t__syncthreads();\n#endif\n\t\t//==================================================50\n\t\t//\tEnd\n\t\t//==================================================50\n\n\t}\n}"
        ]
    },
    "doh-cuda": {
        "/Users/gbolet/hecbench-roofline/src/doh-cuda/main.cu": [
            "inline __host__ __device__ uint4 make_uint4(int4 a)\n{\n    return make_uint4(uint(a.x), uint(a.y), uint(a.z), uint(a.w));\n}\n\ninline __host__ __device__ uint4 max(uint4 a, uint4 b)\n{\n    return make_uint4(max(a.x,b.x), max(a.y,b.y), max(a.z,b.z), max(a.w,b.w));\n}\n\n__device__\ninline int _clip(const int x, const int low, const int high)\n{\n  if (x > high)\n    return high;\n  else if (x < low)\n    return low;\n  else\n    return x;\n}\n\n__device__\ninline IMAGE_T _integ(const IMAGE_T * img,\n                      const INT_T img_rows,\n                      const INT_T img_cols,\n                      int r,\n                      int c,\n                      const int rl,\n                      const int cl)\n{\n  r = _clip(r, 0, img_rows - 1);\n  c = _clip(c, 0, img_cols - 1);\n\n  const int r2 = _clip(r + rl, 0, img_rows - 1);\n  const int c2 = _clip(c + cl, 0, img_cols - 1);\n\n  IMAGE_T ans = img[r * img_cols + c] + img[r2 * img_cols + c2] -\n                img[r * img_cols + c2] - img[r2 * img_cols + c];\n\n  return max((IMAGE_T)0, ans);\n}\n\n__global__\nvoid hessian_matrix_det(const IMAGE_T* img,\n                        const INT_T img_rows,\n                        const INT_T img_cols,\n                        const IMAGE_T sigma,\n                        IMAGE_T* out)\n{\n  int tid = blockDim.x * blockIdx.x + threadIdx.x;\n\n  if (tid >= img_rows*img_cols) return;\n\n  const int r = tid / img_cols;\n  const int c = tid % img_cols;\n\n  int size = (int)((IMAGE_T)3.0 * sigma);\n\n  const int b = (size - 1) / 2 + 1;\n  const int l = size / 3;\n  const int w = size;\n\n  const IMAGE_T w_i = (IMAGE_T)1.0 / (size * size);\n\n  const IMAGE_T tl = _integ(img, img_rows, img_cols, r - l, c - l, l, l); // top left\n  const IMAGE_T br = _integ(img, img_rows, img_cols, r + 1, c + 1, l, l); // bottom right\n  const IMAGE_T bl = _integ(img, img_rows, img_cols, r - l, c + 1, l, l); // bottom left\n  const IMAGE_T tr = _integ(img, img_rows, img_cols, r + 1, c - l, l, l); // top right\n\n  IMAGE_T dxy = bl + tr - tl - br;\n  dxy = -dxy * w_i;\n\n  IMAGE_T mid = _integ(img, img_rows, img_cols, r - l + 1, c - l, 2 * l - 1, w);  // middle box\n  IMAGE_T side = _integ(img, img_rows, img_cols, r - l + 1, c - l / 2, 2 * l - 1, l);  // sides\n\n  IMAGE_T dxx = mid - (IMAGE_T)3 * side;\n  dxx = -dxx * w_i;\n\n  mid = _integ(img, img_rows, img_cols, r - l, c - b + 1, w, 2 * b - 1);\n  side = _integ(img, img_rows, img_cols, r - b / 2, c - b + 1, b, 2 * b - 1);\n\n  IMAGE_T dyy = mid - (IMAGE_T)3 * side;\n  dyy = -dyy * w_i;\n\n  out[tid] = (dxx * dyy - (IMAGE_T)0.81 * (dxy * dxy));\n}"
        ]
    },
    "ert-cuda": {
        "/Users/gbolet/hecbench-roofline/src/ert-cuda/kernel.h": [
            "#define T ((int)32)\n\n\n__global__ void block_stride(uint32_t ntrials, uint32_t nsize, T *__restrict__ A)\n{\n  uint32_t total_thr    = gridDim.x * blockDim.x;\n  uint32_t elem_per_thr = (nsize + (total_thr - 1)) / total_thr;\n\n  uint32_t start_idx  = blockIdx.x * blockDim.x + threadIdx.x;\n  uint32_t end_idx    = start_idx + elem_per_thr * total_thr;\n  uint32_t stride_idx = total_thr;\n\n  if (start_idx > nsize) {\n    start_idx = nsize;\n  }\n\n  if (end_idx > nsize) {\n    end_idx = nsize;\n  }\n\n  // A needs to be initilized to -1 coming in\n  // And with alpha=2 and beta=1, A=-1 is preserved upon return\n  T alpha, const_beta;\n  alpha      = 2.0;\n  const_beta = 1.0;\n\n  uint32_t i, j;\n  for (j = 0; j < ntrials; ++j) {\n    for (i = start_idx; i < end_idx; i += stride_idx) {\n      T beta = const_beta;\n      /* add 1+2+4+8+16+32+64+128+256+512+1024 flops */\n      KERNEL1(beta, A[i], alpha);\n      KERNEL2(beta, A[i], alpha);\n      REP2(KERNEL2(beta, A[i], alpha));\n      REP4(KERNEL2(beta, A[i], alpha));\n      REP8(KERNEL2(beta, A[i], alpha));\n      REP16(KERNEL2(beta, A[i], alpha));\n      REP32(KERNEL2(beta, A[i], alpha));\n      REP64(KERNEL2(beta, A[i], alpha));\n      REP128(KERNEL2(beta, A[i], alpha));\n      REP256(KERNEL2(beta, A[i], alpha));\n      REP512(KERNEL2(beta, A[i], alpha));\n      A[i] = -beta;\n    }\n  }\n}"
        ]
    },
    "pointwise-cuda": {
        "/Users/gbolet/hecbench-roofline/src/pointwise-cuda/main.cu": [
            "inline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\ninline __host__ __device__ float4 expf(float4 v)\n{\n    return make_float4(expf(v.x), expf(v.y), expf(v.z), expf(v.w));\n}\n\n__forceinline__ __device__ float sigmoidf(float in) {\n  return 1.f / (1.f + expf(-in));  \n}\n\n__global__ \nvoid elementWise_fp(int hiddenSize, int miniBatch,\n    const float *__restrict__ tmp_h, \n    const float *__restrict__ tmp_i, \n    const float *__restrict__ bias,\n    float *__restrict__ linearGates,\n    float *__restrict__ h_out,\n    float *__restrict__ i_out,\n    const float *__restrict__ c_in,\n    float *__restrict__ c_out)\n{\n  int index = blockIdx.x * blockDim.x + threadIdx.x;\n  int numElements = miniBatch * hiddenSize;\n\n  if (index >= numElements) return;\n\n  int batch = index / hiddenSize;\n  int gateIndex = (index % hiddenSize) + 4 * batch * hiddenSize;   \n\n  float g[4];\n\n  for (int i = 0; i < 4; i++) {\n    g[i] = tmp_i[i * hiddenSize + gateIndex] + tmp_h[i * hiddenSize + gateIndex];\n    g[i] += bias[i * hiddenSize + index % hiddenSize] + bias[(i + 4) * hiddenSize + index % hiddenSize];\n    linearGates[gateIndex + i * hiddenSize] = g[i];\n  }   \n\n  float in_gate     = sigmoidf(g[0]);\n  float forget_gate = sigmoidf(g[1]);\n  float in_gate2    = tanhf(g[2]);\n  float out_gate    = sigmoidf(g[3]);\n\n  float val = (forget_gate * c_in[index]) + (in_gate * in_gate2);\n\n  c_out[index] = val;\n\n  val = out_gate * tanhf(val);                                   \n\n  h_out[index] = val;\n  i_out[index] = val;\n}"
        ]
    },
    "hungarian-cuda": {
        "/Users/gbolet/hecbench-roofline/src/hungarian-cuda/main.cu": [
            "__device__ int column_of_star_at_row[nrows];\n\n__device__ int cover_column[ncols];\n\n__device__ int cover_row[nrows];\n\n__device__ int row_of_star_at_column[ncols];\n\n__global__ void init()\n{\n  int i = blockDim.x * blockIdx.x + threadIdx.x;\n  // initializations\n  //for step 2\n  if (i < nrows){\n    cover_row[i] = 0;\n    column_of_star_at_row[i] = -1;\n  }\n  if (i < ncols){\n    cover_column[i] = 0;\n    row_of_star_at_column[i] = -1;\n  }\n}",
            "__device__ data min_in_rows[nrows];\n\n__device__ data slack[nrows*ncols];\n\ninline __host__ __device__ uint4 make_uint4(int4 a)\n{\n    return make_uint4(uint(a.x), uint(a.y), uint(a.z), uint(a.w));\n}\n\ninline __host__ __device__ uint4 min(uint4 a, uint4 b)\n{\n    return make_uint4(min(a.x,b.x), min(a.y,b.y), min(a.z,b.z), min(a.w,b.w));\n}\n\n__device__ void min_in_rows_warp_reduce(volatile data* sdata, int tid) {\n  if (n_threads_reduction >= 64 && n_rows_per_block < 64) sdata[tid] = min(sdata[tid], sdata[tid + 32]);\n  if (n_threads_reduction >= 32 && n_rows_per_block < 32) sdata[tid] = min(sdata[tid], sdata[tid + 16]);\n  if (n_threads_reduction >= 16 && n_rows_per_block < 16) sdata[tid] = min(sdata[tid], sdata[tid + 8]);\n  if (n_threads_reduction >= 8 && n_rows_per_block < 8) sdata[tid] = min(sdata[tid], sdata[tid + 4]);\n  if (n_threads_reduction >= 4 && n_rows_per_block < 4) sdata[tid] = min(sdata[tid], sdata[tid + 2]);\n  if (n_threads_reduction >= 2 && n_rows_per_block < 2) sdata[tid] = min(sdata[tid], sdata[tid + 1]);\n}\n\n__global__ void calc_min_in_rows()\n{\n  __shared__ data sdata[n_threads_reduction];    // One temporary result for each thread.\n\n  unsigned int tid = threadIdx.x;\n  unsigned int bid = blockIdx.x;\n  // One gets the line and column from the blockID and threadID.\n  unsigned int l = bid * n_rows_per_block + tid % n_rows_per_block;\n  unsigned int c = tid / n_rows_per_block;\n  unsigned int i = c * nrows + l;\n  const unsigned int gridSize = n_threads_reduction * n_blocks_reduction;\n  data thread_min = MAX_DATA;\n\n  while (i < n * n) {\n    thread_min = min(thread_min, slack[i]);\n    i += gridSize;  // go to the next piece of the matrix...\n    // gridSize = 2^k * n, so that each thread always processes the same line or column\n  }\n  sdata[tid] = thread_min;\n\n  __syncthreads();\n  if (n_threads_reduction >= 1024 && n_rows_per_block < 1024) {if (tid < 512) { sdata[tid] = min(sdata[tid], sdata[tid + 512]); } __syncthreads(); }\n  if (n_threads_reduction >= 512 && n_rows_per_block < 512) { if (tid < 256) { sdata[tid] = min(sdata[tid], sdata[tid + 256]); } __syncthreads(); }\n  if (n_threads_reduction >= 256 && n_rows_per_block < 256) { if (tid < 128) { sdata[tid] = min(sdata[tid], sdata[tid + 128]); } __syncthreads(); }\n  if (n_threads_reduction >= 128 && n_rows_per_block < 128) { if (tid <  64) { sdata[tid] = min(sdata[tid], sdata[tid + 64]); } __syncthreads(); }\n  if (tid < 32) min_in_rows_warp_reduce(sdata, tid);\n  if (tid < n_rows_per_block) min_in_rows[bid*n_rows_per_block + tid] = sdata[tid];\n}",
            "__device__ data min_in_cols[ncols];\n\n__device__ data slack[nrows*ncols];\n\ninline __host__ __device__ uint4 make_uint4(int4 a)\n{\n    return make_uint4(uint(a.x), uint(a.y), uint(a.z), uint(a.w));\n}\n\ninline __host__ __device__ uint4 min(uint4 a, uint4 b)\n{\n    return make_uint4(min(a.x,b.x), min(a.y,b.y), min(a.z,b.z), min(a.w,b.w));\n}\n\n__device__ void min_in_cols_warp_reduce(volatile data* sdata, int tid) {\n  if (n_threads_reduction >= 64 && n_cols_per_block < 64) sdata[tid] = min(sdata[tid], sdata[tid + 32]);\n  if (n_threads_reduction >= 32 && n_cols_per_block < 32) sdata[tid] = min(sdata[tid], sdata[tid + 16]);\n  if (n_threads_reduction >= 16 && n_cols_per_block < 16) sdata[tid] = min(sdata[tid], sdata[tid + 8]);\n  if (n_threads_reduction >= 8 && n_cols_per_block < 8) sdata[tid] = min(sdata[tid], sdata[tid + 4]);\n  if (n_threads_reduction >= 4 && n_cols_per_block < 4) sdata[tid] = min(sdata[tid], sdata[tid + 2]);\n  if (n_threads_reduction >= 2 && n_cols_per_block < 2) sdata[tid] = min(sdata[tid], sdata[tid + 1]);\n}\n\n__global__ void calc_min_in_cols()\n{\n  __shared__ data sdata[n_threads_reduction];    // One temporary result for each thread\n\n  unsigned int tid = threadIdx.x;\n  unsigned int bid = blockIdx.x;\n  // One gets the line and column from the blockID and threadID.\n  unsigned int c = bid * n_cols_per_block + tid % n_cols_per_block;\n  unsigned int l = tid / n_cols_per_block;\n  const unsigned int gridSize = n_threads_reduction * n_blocks_reduction;\n  data thread_min = MAX_DATA;\n\n  while (l < n) {\n    unsigned int i = c * nrows + l;\n    thread_min = min(thread_min, slack[i]);\n    l += gridSize / n;  // go to the next piece of the matrix...\n    // gridSize = 2^k * n, so that each thread always processes the same line or column\n  }\n  sdata[tid] = thread_min;\n\n  __syncthreads();\n  if (n_threads_reduction >= 1024 && n_cols_per_block < 1024) {\n    if (tid < 512) { sdata[tid] = min(sdata[tid], sdata[tid + 512]); } __syncthreads(); }\n  if (n_threads_reduction >= 512 && n_cols_per_block < 512) {\n    if (tid < 256) { sdata[tid] = min(sdata[tid], sdata[tid + 256]); } __syncthreads(); }\n  if (n_threads_reduction >= 256 && n_cols_per_block < 256) {\n    if (tid < 128) { sdata[tid] = min(sdata[tid], sdata[tid + 128]); } __syncthreads(); }\n  if (n_threads_reduction >= 128 && n_cols_per_block < 128) {\n    if (tid <  64) { sdata[tid] = min(sdata[tid], sdata[tid + 64]); } __syncthreads(); }\n  if (tid < 32) min_in_cols_warp_reduce(sdata, tid);\n  if (tid < n_cols_per_block) min_in_cols[bid*n_cols_per_block + tid] = sdata[tid];\n}",
            "__device__ data min_in_rows[nrows];\n\n__device__ data slack[nrows*ncols];\n\n__global__ void step_1_row_sub()\n{\n  int i = blockDim.x * blockIdx.x + threadIdx.x;\n  int l = i & row_mask;\n  slack[i] = slack[i] - min_in_rows[l];  // subtract the minimum in row from that row\n}",
            "__device__ data min_in_cols[ncols];\n\n__device__ data slack[nrows*ncols];\n\n__device__ int zeros_size_b[n_blocks_step_4];\n\n__global__ void step_1_col_sub()\n{\n  int i = blockDim.x * blockIdx.x + threadIdx.x;\n  int c = i >> log2_n;\n  slack[i] = slack[i] - min_in_cols[c]; // subtract the minimum in row from that row\n\n  if (i == 0) zeros_size = 0;\n  if (i < n_blocks_step_4) zeros_size_b[i] = 0;\n}",
            "__device__ data slack[nrows*ncols];\n\n__device__ int zeros[nrows*ncols];\n\n__device__ int zeros_size_b[n_blocks_step_4];\n\n__global__ void compress_matrix(){\n  int i = blockDim.x * blockIdx.x + threadIdx.x;\n\n  if (slack[i] == 0) {\n    atomicAdd(&zeros_size, 1);\n    int b = i >> log2_data_block_size;\n    int i0 = i & ~(data_block_size - 1);    // == b << log2_data_block_size\n    int j = atomicAdd(zeros_size_b + b, 1);\n    zeros[i0 + j] = i;\n  }\n}",
            "__device__ int column_of_star_at_row[nrows];\n\n__device__ int cover_column[ncols];\n\n__device__ int cover_row[nrows];\n\n__device__ int row_of_star_at_column[ncols];\n\n__device__ int zeros[nrows*ncols];\n\n__device__ int zeros_size_b[n_blocks_step_4];\n\n__global__ void step_2()\n{\n  int i = threadIdx.x;\n  int b = blockIdx.x;\n  __shared__ bool repeat;\n  __shared__ bool s_repeat_kernel;\n\n  if (i == 0) s_repeat_kernel = false;\n\n  do {\n    __syncthreads();\n    if (i == 0) repeat = false;\n    __syncthreads();\n\n    for (int j = i; j < zeros_size_b[b]; j += blockDim.x)\n    {\n      int z = zeros[(b << log2_data_block_size) + j];\n      int l = z & row_mask;\n      int c = z >> log2_n;\n\n      if (cover_row[l] == 0 && cover_column[c] == 0) {\n        // thread trys to get the line\n        if (!atomicExch((int *)&(cover_row[l]), 1)){\n          // only one thread gets the line\n          if (!atomicExch((int *)&(cover_column[c]), 1)){\n            // only one thread gets the column\n            row_of_star_at_column[c] = l;\n            column_of_star_at_row[l] = c;\n          }\n          else {\n            cover_row[l] = 0;\n            repeat = true;\n            s_repeat_kernel = true;\n          }\n        }\n      }\n    }\n    __syncthreads();\n  } while (repeat);\n\n  if (s_repeat_kernel) repeat_kernel = true;\n}",
            "__device__ int cover_column[ncols];\n\n__device__ int cover_row[nrows];\n\n__global__ void step_3ini()\n{\n  int i = blockDim.x * blockIdx.x + threadIdx.x;\n  cover_row[i] = 0;\n  cover_column[i] = 0;\n  if (i == 0) n_matches = 0;\n}",
            "__device__ int cover_column[ncols];\n\n__device__ int row_of_star_at_column[ncols];\n\n__global__ void step_3()\n{\n  int i = blockDim.x * blockIdx.x + threadIdx.x;\n  if (row_of_star_at_column[i]>=0)\n  {\n    cover_column[i] = 1;\n    atomicAdd((int*)&n_matches, 1);\n  }\n}",
            "__device__ int column_of_prime_at_row[nrows];\n\n__device__ int row_of_green_at_column[ncols];\n\n__global__ void step_4_init()\n{\n  int i = blockDim.x * blockIdx.x + threadIdx.x;\n  column_of_prime_at_row[i] = -1;\n  row_of_green_at_column[i] = -1;\n}",
            "__device__ int column_of_prime_at_row[nrows];\n\n__device__ int column_of_star_at_row[nrows];\n\n__device__ int cover_column[ncols];\n\n__device__ int cover_row[nrows];\n\n__device__ int zeros[nrows*ncols];\n\n__device__ int zeros_size_b[n_blocks_step_4];\n\n__global__ void step_4() {\n  __shared__  bool s_found;\n  __shared__  bool s_goto_5;\n  __shared__  bool s_repeat_kernel;\n  volatile int *v_cover_row = cover_row;\n  volatile int *v_cover_column = cover_column;\n\n  int i = threadIdx.x;\n  int b = blockIdx.x;\n\n  if (i == 0) {\n    s_repeat_kernel = false;\n    s_goto_5 = false;\n  }\n\n  do {\n    __syncthreads();\n    if (i == 0) s_found = false;\n    __syncthreads();\n\n    for (int j = i; j < zeros_size_b[b]; j += blockDim.x)\n    {\n      int z = zeros[(b << log2_data_block_size) + j];\n      int l = z & row_mask;\n      int c = z >> log2_n;\n      int c1 = column_of_star_at_row[l];\n\n      for (int n = 0; n < 10; n++) {\n\n        if (!v_cover_column[c] && !v_cover_row[l]) {\n          s_found = true; s_repeat_kernel = true;\n          column_of_prime_at_row[l] = c;\n\n          if (c1 >= 0) {\n            v_cover_row[l] = 1;\n            __threadfence();\n            v_cover_column[c1] = 0;\n          }\n          else {\n            s_goto_5 = true;\n          }\n        }\n      } // for(int n\n\n    } // for(int j\n    __syncthreads();\n  } while (s_found && !s_goto_5);\n\n  if (i == 0 && s_repeat_kernel) repeat_kernel = true;\n  if (i == 0 && s_goto_5) goto_5 = true;\n}",
            "__device__ int column_of_prime_at_row[nrows];\n\n__device__ int column_of_star_at_row[nrows];\n\n__device__ int row_of_green_at_column[ncols];\n\n__device__ int row_of_star_at_column[ncols];\n\n__global__ void step_5a()\n{\n  int i = blockDim.x * blockIdx.x + threadIdx.x;\n\n  int r_Z0, c_Z0;\n\n  c_Z0 = column_of_prime_at_row[i];\n  if (c_Z0 >= 0 && column_of_star_at_row[i] < 0) {\n    row_of_green_at_column[c_Z0] = i;\n\n    while ((r_Z0 = row_of_star_at_column[c_Z0]) >= 0) {\n      c_Z0 = column_of_prime_at_row[r_Z0];\n      row_of_green_at_column[c_Z0] = r_Z0;\n    }\n  }\n}",
            "__device__ int column_of_star_at_row[nrows];\n\n__device__ int row_of_green_at_column[ncols];\n\n__device__ int row_of_star_at_column[ncols];\n\n__global__ void step_5b()\n{\n  int j = blockDim.x * blockIdx.x + threadIdx.x;\n\n  int r_Z0, c_Z0, c_Z2;\n\n  r_Z0 = row_of_green_at_column[j];\n\n  if (r_Z0 >= 0 && row_of_star_at_column[j] < 0) {\n\n    c_Z2 = column_of_star_at_row[r_Z0];\n\n    column_of_star_at_row[r_Z0] = j;\n    row_of_star_at_column[j] = r_Z0;\n\n    while (c_Z2 >= 0) {\n      r_Z0 = row_of_green_at_column[c_Z2];  // row of Z2\n      c_Z0 = c_Z2;              // col of Z2\n      c_Z2 = column_of_star_at_row[r_Z0];    // col of Z4\n\n      // star Z2\n      column_of_star_at_row[r_Z0] = c_Z0;\n      row_of_star_at_column[c_Z0] = r_Z0;\n    }\n  }\n}",
            "__device__ data slack[nrows*ncols];\n\n__device__ int cover_column[ncols];\n\n__device__ int cover_row[nrows];\n\n__device__ int zeros_size_b[n_blocks_step_4];\n\n__global__ void step_6_add_sub()\n{\n  // STEP 6:\n  //  /*STEP 6: Add the minimum uncovered value to every element of each covered\n  //  row, and subtract it from every element of each uncovered column.\n  //  Return to Step 4 without altering any stars, primes, or covered lines. */\n  int i = blockDim.x * blockIdx.x + threadIdx.x;\n  int l = i & row_mask;\n  int c = i >> log2_n;\n  if (cover_row[l] == 1 && cover_column[c] == 1)\n    slack[i] += d_min_in_mat;\n  if (cover_row[l] == 0 && cover_column[c] == 0)\n    slack[i] -= d_min_in_mat;\n\n  if (i == 0) zeros_size = 0;\n  if (i < n_blocks_step_4) zeros_size_b[i] = 0;\n}",
            "__device__ data d_min_in_mat_vect[n_blocks_reduction];\n\n__device__ data slack[nrows*ncols];\n\n__device__ int cover_column[ncols];\n\n__device__ int cover_row[nrows];\n\ninline __host__ __device__ uint4 make_uint4(int4 a)\n{\n    return make_uint4(uint(a.x), uint(a.y), uint(a.z), uint(a.w));\n}\n\ninline __host__ __device__ uint4 min(uint4 a, uint4 b)\n{\n    return make_uint4(min(a.x,b.x), min(a.y,b.y), min(a.z,b.z), min(a.w,b.w));\n}\n\n__device__ void min_warp_reduce(volatile data* sdata, int tid) {\n  if (blockSize >= 64) sdata[tid] = min(sdata[tid], sdata[tid + 32]);\n  if (blockSize >= 32) sdata[tid] = min(sdata[tid], sdata[tid + 16]);\n  if (blockSize >= 16) sdata[tid] = min(sdata[tid], sdata[tid + 8]);\n  if (blockSize >= 8) sdata[tid] = min(sdata[tid], sdata[tid + 4]);\n  if (blockSize >= 4) sdata[tid] = min(sdata[tid], sdata[tid + 2]);\n  if (blockSize >= 2) sdata[tid] = min(sdata[tid], sdata[tid + 1]);\n}\n\n__device__ void min_reduce1(volatile data *g_idata, volatile data *g_odata, unsigned int n)\n{\n  unsigned int tid = threadIdx.x;\n  unsigned int i = blockIdx.x*(blockSize * 2) + tid;\n  unsigned int gridSize = blockSize * 2 * gridDim.x;\n  sdata[tid] = MAX_DATA;\n\n  while (i < n) {\n    int i1 = i;\n    int i2 = i + blockSize;\n    int l1 = i1 & row_mask;\n    int c1 = i1 >> log2_n; \n    data g1;\n    if (cover_row[l1] == 1 || cover_column[c1] == 1) g1 = MAX_DATA;\n    else g1 = g_idata[i1];\n    int l2 = i2 & row_mask;\n    int c2 = i2 >> log2_n;\n    data g2;\n    if (cover_row[l2] == 1 || cover_column[c2] == 1) g2 = MAX_DATA;\n    else g2 = g_idata[i2];\n    sdata[tid] = min(sdata[tid], min(g1, g2));\n    i += gridSize;\n  }\n\n  __syncthreads();\n  if (blockSize >= 1024) { if (tid < 512) { sdata[tid] = min(sdata[tid], sdata[tid + 512]); } __syncthreads(); }\n  if (blockSize >= 512) { if (tid < 256) { sdata[tid] = min(sdata[tid], sdata[tid + 256]); } __syncthreads(); }\n  if (blockSize >= 256) { if (tid < 128) { sdata[tid] = min(sdata[tid], sdata[tid + 128]); } __syncthreads(); }\n  if (blockSize >= 128) { if (tid <  64) { sdata[tid] = min(sdata[tid], sdata[tid + 64]); } __syncthreads(); }\n  if (tid < 32) min_warp_reduce<blockSize>(sdata, tid);\n  if (tid == 0) g_odata[blockIdx.x] = sdata[0];\n}\n\n__global__ void min_reduce_kernel1() {\n  min_reduce1<n_threads_reduction>(slack, d_min_in_mat_vect, nrows*ncols);\n}",
            "__device__ data d_min_in_mat_vect[n_blocks_reduction];\n\ninline __host__ __device__ uint4 make_uint4(int4 a)\n{\n    return make_uint4(uint(a.x), uint(a.y), uint(a.z), uint(a.w));\n}\n\ninline __host__ __device__ uint4 min(uint4 a, uint4 b)\n{\n    return make_uint4(min(a.x,b.x), min(a.y,b.y), min(a.z,b.z), min(a.w,b.w));\n}\n\n__device__ void min_warp_reduce(volatile data* sdata, int tid) {\n  if (blockSize >= 64) sdata[tid] = min(sdata[tid], sdata[tid + 32]);\n  if (blockSize >= 32) sdata[tid] = min(sdata[tid], sdata[tid + 16]);\n  if (blockSize >= 16) sdata[tid] = min(sdata[tid], sdata[tid + 8]);\n  if (blockSize >= 8) sdata[tid] = min(sdata[tid], sdata[tid + 4]);\n  if (blockSize >= 4) sdata[tid] = min(sdata[tid], sdata[tid + 2]);\n  if (blockSize >= 2) sdata[tid] = min(sdata[tid], sdata[tid + 1]);\n}\n\n__device__ void min_reduce2(volatile data *g_idata, volatile data *g_odata, unsigned int n)\n{\n  unsigned int tid = threadIdx.x;\n  unsigned int i = blockIdx.x*(blockSize * 2) + tid;\n\n  sdata[tid] = min(g_idata[i], g_idata[i + blockSize]);\n  __syncthreads();\n  if (blockSize >= 1024) { if (tid < 512) { sdata[tid] = min(sdata[tid], sdata[tid + 512]); } __syncthreads(); }\n  if (blockSize >= 512) { if (tid < 256) { sdata[tid] = min(sdata[tid], sdata[tid + 256]); } __syncthreads(); }\n  if (blockSize >= 256) { if (tid < 128) { sdata[tid] = min(sdata[tid], sdata[tid + 128]); } __syncthreads(); }\n  if (blockSize >= 128) { if (tid <  64) { sdata[tid] = min(sdata[tid], sdata[tid + 64]); } __syncthreads(); }\n  if (tid < 32) min_warp_reduce<blockSize>(sdata, tid);\n  if (tid == 0) g_odata[blockIdx.x] = sdata[0];\n}\n\n__global__ void min_reduce_kernel2() {\n  min_reduce2<n_threads_reduction / 2>(d_min_in_mat_vect, &d_min_in_mat, n_blocks_reduction);\n}"
        ]
    },
    "testSNAP-cuda": {
        "/Users/gbolet/hecbench-roofline/src/testSNAP-cuda/main.cu": [
            "#define COMPLEX 1\n\n\n__global__ void reset_ulisttot(COMPLEX *ulisttot, const int ulisttot_size) \n{\n  int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < ulisttot_size) ulisttot[i] = {0.0, 0.0};\n}",
            "#define COMPLEX 1\n\n\n__global__ void set_ulisttot(\n    COMPLEX *__restrict__ ulisttot,\n    const int*__restrict__ idxu_block, \n    const int num_atoms,\n    const int twojmax,\n    const double wself) \n{\n  int natom = blockIdx.x * blockDim.x + threadIdx.x;\n  if (natom < num_atoms) \n    for (int j = 0; j <= twojmax; j++) {\n      int jju = idxu_block[j];\n      for (int ma = 0; ma <= j; ma++) {\n        ulisttot[INDEX_2D(natom, jju)] = { wself, 0.0 };\n        jju += j + 2;\n      }\n    }\n}",
            "#define COMPLEX 1\n\n\n__device__\ndouble compute_sfac(double r, double rcut, const int switch_flag)\n{\n  if (switch_flag == 0)\n    return 1.0;\n  if (switch_flag == 1) {\n    if (r <= rmin0)\n      return 1.0;\n    else if (r > rcut)\n      return 0.0;\n    else {\n      double rcutfac = MY_PI / (rcut - rmin0);\n      return 0.5 * (cos((r - rmin0) * rcutfac) + 1.0);\n    }\n  }\n  return 0.0;\n}\n\n__global__ void update_ulisttot(\n    const double*__restrict__ rij, \n    const double*__restrict__ rcutij,\n    const double*__restrict__ wj, \n    const int*__restrict__ ulist_parity, \n    const int*__restrict__ idxu_block, \n    const double*__restrict__ rootpqarray, \n    COMPLEX *__restrict__ ulist, \n    COMPLEX *__restrict__ ulisttot, \n    const int num_atoms,\n    const int num_nbor,\n    const int switch_flag, \n    const int twojmax, \n    const int jdimpq)\n{\n\n  int natom = blockIdx.x * blockDim.x + threadIdx.x;\n  int nbor = blockIdx.y * blockDim.y + threadIdx.y;\n\n  if (natom < num_atoms && nbor < num_nbor) {\n    double x = rij[ULIST_INDEX(natom, nbor, 0)];\n    double y = rij[ULIST_INDEX(natom, nbor, 1)];\n    double z = rij[ULIST_INDEX(natom, nbor, 2)];\n    double rsq = x * x + y * y + z * z;\n    double r = sqrt(rsq);\n\n    double theta0 = (r - rmin0) * rfac0 * MY_PI / (rcutij[INDEX_2D(natom, nbor)] - rmin0);\n    double z0 = r / tan(theta0);\n\n    double rootpq;\n    int jju, jjup;\n\n    // compute Cayley-Klein parameters for unit quaternion\n\n    double r0inv = 1.0 / sqrt(r * r + z0 * z0);\n    double a_r = r0inv * z0;\n    double a_i = -r0inv * z;\n    double b_r = r0inv * y;\n    double b_i = -r0inv * x;\n\n    double sfac;\n\n    sfac = compute_sfac(r, rcutij[INDEX_2D(natom, nbor)], switch_flag);\n    sfac *= wj[INDEX_2D(natom, nbor)];\n\n    // Recursion relations\n    // VMK Section 4.8.2\n\n    //   u[j,ma,mb] = Sqrt((j-ma)/(j-mb)) a* u[j-1,ma,mb]\n    //               -Sqrt((ma)/(j-mb)) b* u[j-1,ma-1,mb]\n\n    //   u[j,ma,mb] = Sqrt((j-ma)/(mb)) b u[j-1,ma,mb-1]\n    //                Sqrt((ma)/(mb)) a u[j-1,ma-1,mb-1]\n\n    // initialize first entry\n    // initialize top row of each layer to zero\n    ulist[ULIST_INDEX(natom, nbor, 0)].re = 1.0;\n    ulist[ULIST_INDEX(natom, nbor, 0)].im = 0.0;\n\n    // skip over right half of each uarray\n    jju = 1;\n    for (int j = 1; j <= twojmax; j++) {\n      int deljju = j + 1;\n      for (int mb = 0; 2 * mb <= j; mb++) {\n        ulist[ULIST_INDEX(natom, nbor, jju)].re = 0.0;\n        ulist[ULIST_INDEX(natom, nbor, jju)].im = 0.0;\n        jju += deljju;\n      }\n      int ncolhalf = deljju / 2;\n      jju += deljju * ncolhalf;\n    }\n\n    jju = 1;\n    jjup = 0;\n    for (int j = 1; j <= twojmax; j++) {\n      int deljju = j + 1;\n      int deljjup = j;\n      int mb_max = (j + 1) / 2;\n      int ma_max = j;\n      int m_max = ma_max * mb_max;\n\n      // fill in left side of matrix layer from previous layer\n      for (int m_iter = 0; m_iter < m_max; ++m_iter) {\n        int mb = m_iter / ma_max;\n        int ma = m_iter % ma_max;\n        double up_r = ulist[ULIST_INDEX(natom, nbor, jjup)].re;\n        double up_i = ulist[ULIST_INDEX(natom, nbor, jjup)].im;\n\n        rootpq = rootpqarray[ROOTPQ_INDEX(j - ma, j - mb)];\n        ulist[ULIST_INDEX(natom, nbor, jju)].re += rootpq * (a_r * up_r + a_i * up_i);\n        ulist[ULIST_INDEX(natom, nbor, jju)].im += rootpq * (a_r * up_i - a_i * up_r);\n\n        rootpq = rootpqarray[ROOTPQ_INDEX(ma + 1, j - mb)];\n        ulist[ULIST_INDEX(natom, nbor, jju+1)].re = -rootpq * (b_r * up_r + b_i * up_i);\n        ulist[ULIST_INDEX(natom, nbor, jju+1)].im = -rootpq * (b_r * up_i - b_i * up_r);\n\n        // assign middle column i.e. mb+1\n\n        if (2 * (mb + 1) == j) {\n          rootpq = rootpqarray[ROOTPQ_INDEX(j - ma, mb + 1)];\n          ulist[ULIST_INDEX(natom, nbor, jju+deljju)].re += rootpq * (b_r * up_r - b_i * up_i);\n          ulist[ULIST_INDEX(natom, nbor, jju+deljju)].im += rootpq * (b_r * up_i + b_i * up_r);\n\n          rootpq = rootpqarray[ROOTPQ_INDEX(ma + 1, mb + 1)];\n          ulist[ULIST_INDEX(natom, nbor, jju+deljju+1)].re = rootpq * (a_r * up_r - a_i * up_i);\n          ulist[ULIST_INDEX(natom, nbor, jju+deljju+1)].im = rootpq * (a_r * up_i + a_i * up_r);\n        }\n\n        jju++;\n        jjup++;\n\n        if (ma == ma_max - 1)\n          jju++;\n      }\n\n      // copy left side to right side with inversion symmetry VMK 4.4(2)\n      // u[ma-j][mb-j] = (-1)^(ma-mb)*Conj([u[ma][mb])\n      // dependence on idxu_block could be removed\n      // renamed counters b/c can not modify jju, jjup\n      int jjui = idxu_block[j];\n      int jjuip = jjui + (j + 1) * (j + 1) - 1;\n      for (int mb = 0; 2 * mb < j; mb++) {\n        for (int ma = 0; ma <= j; ma++) {\n          ulist[ULIST_INDEX(natom, nbor, jjuip)].re = ulist_parity[jjui] * ulist[ULIST_INDEX(natom, nbor, jjui)].re;\n          ulist[ULIST_INDEX(natom, nbor, jjuip)].im = ulist_parity[jjui] * -ulist[ULIST_INDEX(natom, nbor, jjui)].im;\n          jjui++;\n          jjuip--;\n        }\n      }\n\n      // skip middle and right half cols\n      // b/c no longer using idxu_block\n      if (j % 2 == 0)\n        jju += deljju;\n      int ncolhalf = deljju / 2;\n      jju += deljju * ncolhalf;\n      int ncolhalfp = deljjup / 2;\n      jjup += deljjup * ncolhalfp;\n    }\n\n    sfac = compute_sfac(r, rcutij[INDEX_2D(natom, nbor)], switch_flag);\n    sfac *= wj[INDEX_2D(natom, nbor)];\n\n    for (int j = 0; j <= twojmax; j++) {\n      int jju = idxu_block[j];\n      for (int mb = 0; mb <= j; mb++)\n        for (int ma = 0; ma <= j; ma++) {\n          atomicAdd(&(ulisttot[INDEX_2D(natom, jju)].re), sfac * ulist[ULIST_INDEX(natom, nbor, jju)].re);\n          atomicAdd(&(ulisttot[INDEX_2D(natom, jju)].im), sfac * ulist[ULIST_INDEX(natom, nbor, jju)].im);\n          jju++;\n        }\n    }\n  }\n}",
            "#define COMPLEX 1\n\n\n__global__ void reset_ylist(COMPLEX *ylist, const int ylist_size) \n{\n  int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < ylist_size) ylist[i] = {0.0, 0.0};\n}",
            "#define COMPLEX 1\n\n\n__global__ void compute_yi (\n    const int*__restrict__ idxz,\n    const double*__restrict__ idxzbeta,\n    const double*__restrict__ cglist,\n    const int*__restrict__ idxcg_block,\n    const int*__restrict__ idxu_block,\n    const int*__restrict__ idxdu_block,\n    const COMPLEX*__restrict__ ulisttot,\n          COMPLEX*__restrict__ ylist,\n    const int num_atoms,\n    const int idxz_max,\n    const int jdim) \n{\n  int natom = blockIdx.x * blockDim.x + threadIdx.x;\n  int jjz = blockIdx.y * blockDim.y + threadIdx.y;\n  if (jjz < idxz_max && natom < num_atoms) {\n    const int j1 = idxz[IDXZ_INDEX(jjz, 0)];\n    const int j2 = idxz[IDXZ_INDEX(jjz, 1)];\n    const int j = idxz[IDXZ_INDEX(jjz, 2)];\n    const int ma1min = idxz[IDXZ_INDEX(jjz, 3)];\n    const int ma2max = idxz[IDXZ_INDEX(jjz, 4)];\n    const int na = idxz[IDXZ_INDEX(jjz, 5)];\n    const int mb1min = idxz[IDXZ_INDEX(jjz, 6)];\n    const int mb2max = idxz[IDXZ_INDEX(jjz, 7)];\n    const int nb = idxz[IDXZ_INDEX(jjz, 8)];\n\n    const double betaj = idxzbeta[jjz];\n\n    const double* cgblock = cglist + idxcg_block[j1 + jdim*j2 + jdim*jdim*j];\n\n    int mb = (2 * (mb1min + mb2max) - j1 - j2 + j) / 2;\n    int ma = (2 * (ma1min + ma2max) - j1 - j2 + j) / 2;\n    const int jjdu = idxdu_block[j] + (j + 1) * mb + ma;\n\n    int jju1 = idxu_block[j1] + (j1 + 1) * mb1min;\n    int jju2 = idxu_block[j2] + (j2 + 1) * mb2max;\n    int icgb = mb1min * (j2 + 1) + mb2max;\n\n    double ztmp_r = 0.0;\n    double ztmp_i = 0.0;\n\n    // loop over columns of u1 and corresponding\n    // columns of u2 satisfying Clebsch-Gordan constraint\n    //      2*mb-j = 2*mb1-j1 + 2*mb2-j2\n\n    for (int ib = 0; ib < nb; ib++) {\n\n      double suma1_r = 0.0;\n      double suma1_i = 0.0;\n\n      int ma1 = ma1min;\n      int ma2 = ma2max;\n      int icga = ma1min * (j2 + 1) + ma2max;\n\n      // loop over elements of row u1[mb1] and corresponding elements\n      // of row u2[mb2] satisfying Clebsch-Gordan constraint\n      //      2*ma-j = 2*ma1-j1 + 2*ma2-j2\n\n      for (int ia = 0; ia < na; ia++) {\n        suma1_r += cgblock[icga] *\n          (ulisttot[INDEX_2D(natom, jju1 + ma1)].re * ulisttot[INDEX_2D(natom, jju2 + ma2)].re -\n           ulisttot[INDEX_2D(natom, jju1 + ma1)].im * ulisttot[INDEX_2D(natom, jju2 + ma2)].im);\n\n        suma1_i += cgblock[icga] *\n          (ulisttot[INDEX_2D(natom, jju1 + ma1)].re * ulisttot[INDEX_2D(natom, jju2 + ma2)].im +\n           ulisttot[INDEX_2D(natom, jju1 + ma1)].im * ulisttot[INDEX_2D(natom, jju2 + ma2)].re);\n\n        ma1++;\n        ma2--;\n        icga += j2;\n      } // end loop over ia\n\n      ztmp_r += cgblock[icgb] * suma1_r;\n      ztmp_i += cgblock[icgb] * suma1_i;\n      jju1 += j1 + 1;\n      jju2 -= j2 + 1;\n      icgb += j2;\n    } // end loop over ib\n\n    // apply z(j1,j2,j,ma,mb) to unique element of y(j)\n\n    atomicAdd(&(ylist[INDEX_2D(natom, jjdu)].re), betaj * ztmp_r);\n    atomicAdd(&(ylist[INDEX_2D(natom, jjdu)].im), betaj * ztmp_i);\n\n  } // end jjz and natom loop\n}",
            "#define COMPLEX 1\n\n\n__device__\ndouble compute_dsfac(double r, double rcut, const int switch_flag)\n{\n  if (switch_flag == 0)\n    return 0.0;\n  if (switch_flag == 1) {\n    if (r <= rmin0)\n      return 0.0;\n    else if (r > rcut)\n      return 0.0;\n    else {\n      double rcutfac = MY_PI / (rcut - rmin0);\n      return -0.5 * sin((r - rmin0) * rcutfac) * rcutfac;\n    }\n  }\n  return 0.0;\n}\n\n__device__\ndouble compute_sfac(double r, double rcut, const int switch_flag)\n{\n  if (switch_flag == 0)\n    return 1.0;\n  if (switch_flag == 1) {\n    if (r <= rmin0)\n      return 1.0;\n    else if (r > rcut)\n      return 0.0;\n    else {\n      double rcutfac = MY_PI / (rcut - rmin0);\n      return 0.5 * (cos((r - rmin0) * rcutfac) + 1.0);\n    }\n  }\n  return 0.0;\n}\n\n__device__\nvoid compute_duarray(const int natom,\n                     const int nbor,\n                     const int num_atoms,\n                     const int num_nbor,\n                     const int twojmax,\n                     const int idxdu_max,\n                     const int jdimpq,\n                     const int switch_flag,\n                     const double x,\n                     const double y,\n                     const double z,\n                     const double z0,\n                     const double r,\n                     const double dz0dr,\n                     const double wj_in,\n                     const double rcut,\n                     const double* rootpqarray,\n                     const COMPLEX* ulist,\n                     COMPLEX* dulist)\n{\n  double r0inv;\n  double a_r, a_i, b_r, b_i;\n  double da_r[3], da_i[3], db_r[3], db_i[3];\n  double dz0[3], dr0inv[3], dr0invdr;\n  double rootpq;\n  int jju, jjup, jjdu, jjdup;\n\n  double rinv = 1.0 / r;\n  double ux = x * rinv;\n  double uy = y * rinv;\n  double uz = z * rinv;\n\n  r0inv = 1.0 / sqrt(r * r + z0 * z0);\n  a_r = z0 * r0inv;\n  a_i = -z * r0inv;\n  b_r = y * r0inv;\n  b_i = -x * r0inv;\n\n  dr0invdr = -pow(r0inv, 3.0) * (r + z0 * dz0dr);\n\n  dr0inv[0] = dr0invdr * ux;\n  dr0inv[1] = dr0invdr * uy;\n  dr0inv[2] = dr0invdr * uz;\n\n  dz0[0] = dz0dr * ux;\n  dz0[1] = dz0dr * uy;\n  dz0[2] = dz0dr * uz;\n\n  for (int k = 0; k < 3; k++) {\n    da_r[k] = dz0[k] * r0inv + z0 * dr0inv[k];\n    da_i[k] = -z * dr0inv[k];\n  }\n\n  da_i[2] += -r0inv;\n\n  for (int k = 0; k < 3; k++) {\n    db_r[k] = y * dr0inv[k];\n    db_i[k] = -x * dr0inv[k];\n  }\n\n  db_i[0] += -r0inv;\n  db_r[1] += r0inv;\n\n  for (int k = 0; k < 3; ++k)\n    dulist[DULIST_INDEX(natom, nbor, 0, k)] = { 0.0, 0.0 };\n\n  jju = 1;\n  jjdu = 1;\n  for (int j = 1; j <= twojmax; j++) {\n    int deljju = j + 1;\n    for (int mb = 0; 2 * mb <= j; mb++) {\n\n      for (int k = 0; k < 3; ++k)\n        dulist[DULIST_INDEX(natom, nbor, jjdu, k)] = { 0.0, 0.0 };\n\n      jju += deljju;\n      jjdu += deljju;\n    }\n    int ncolhalf = deljju / 2;\n    jju += deljju * ncolhalf;\n  }\n\n  jju = 1;\n  jjdu = 1;\n  jjup = 0;\n  jjdup = 0;\n  for (int j = 1; j <= twojmax; j++) {\n    int deljju = j + 1;\n    int deljjup = j;\n\n    for (int mb = 0; 2 * mb < j; mb++) {\n\n      for (int ma = 0; ma < j; ma++) {\n\n        double up_r = ulist[ULIST_INDEX(natom, nbor, jjup)].re;\n        double up_i = ulist[ULIST_INDEX(natom, nbor, jjup)].im;\n\n        rootpq = rootpqarray[ROOTPQ_INDEX(j - ma, j - mb)];\n        for (int k = 0; k < 3; k++) {\n          dulist[DULIST_INDEX(natom, nbor, jjdu, k)].re +=\n            rootpq * (da_r[k] * up_r + da_i[k] * up_i +\n                      a_r * dulist[DULIST_INDEX(natom, nbor, jjdup, k)].re +\n                      a_i * dulist[DULIST_INDEX(natom, nbor, jjdup, k)].im);\n          dulist[DULIST_INDEX(natom, nbor, jjdu, k)].im +=\n            rootpq * (da_r[k] * up_i - da_i[k] * up_r +\n                      a_r * dulist[DULIST_INDEX(natom, nbor, jjdup, k)].im -\n                      a_i * dulist[DULIST_INDEX(natom, nbor, jjdup, k)].re);\n        }\n\n        rootpq = rootpqarray[ROOTPQ_INDEX(ma + 1, j - mb)];\n        for (int k = 0; k < 3; k++) {\n          dulist[DULIST_INDEX(natom, nbor, jjdu + 1, k)].re =\n            -rootpq * (db_r[k] * up_r + db_i[k] * up_i +\n                       b_r * dulist[DULIST_INDEX(natom, nbor, jjdup, k)].re +\n                       b_i * dulist[DULIST_INDEX(natom, nbor, jjdup, k)].im);\n          dulist[DULIST_INDEX(natom, nbor, jjdu + 1, k)].im =\n            -rootpq * (db_r[k] * up_i - db_i[k] * up_r +\n                       b_r * dulist[DULIST_INDEX(natom, nbor, jjdup, k)].im -\n                       b_i * dulist[DULIST_INDEX(natom, nbor, jjdup, k)].re);\n        }\n\n        // assign middle column i.e. mb+1\n\n        if (2 * (mb + 1) == j) {\n          rootpq = rootpqarray[ROOTPQ_INDEX(j - ma, mb + 1)];\n          for (int k = 0; k < 3; k++) {\n            dulist[DULIST_INDEX(natom, nbor, jjdu + deljju, k)].re +=\n              rootpq * (db_r[k] * up_r - db_i[k] * up_i +\n                        b_r * dulist[DULIST_INDEX(natom, nbor, jjdup, k)].re -\n                        b_i * dulist[DULIST_INDEX(natom, nbor, jjdup, k)].im);\n            dulist[DULIST_INDEX(natom, nbor, jjdu + deljju, k)].im +=\n              rootpq * (db_r[k] * up_i + db_i[k] * up_r +\n                        b_r * dulist[DULIST_INDEX(natom, nbor, jjdup, k)].im +\n                        b_i * dulist[DULIST_INDEX(natom, nbor, jjdup, k)].re);\n          }\n\n          rootpq = rootpqarray[ROOTPQ_INDEX(ma + 1, mb + 1)];\n          for (int k = 0; k < 3; k++) {\n            dulist[DULIST_INDEX(natom, nbor, jjdu + 1 + deljju, k)].re =\n              rootpq * (da_r[k] * up_r - da_i[k] * up_i +\n                        a_r * dulist[DULIST_INDEX(natom, nbor, jjdup, k)].re -\n                        a_i * dulist[DULIST_INDEX(natom, nbor, jjdup, k)].im);\n            dulist[DULIST_INDEX(natom, nbor, jjdu + 1 + deljju, k)].im =\n              rootpq * (da_r[k] * up_i + da_i[k] * up_r +\n                        a_r * dulist[DULIST_INDEX(natom, nbor, jjdup, k)].im +\n                        a_i * dulist[DULIST_INDEX(natom, nbor, jjdup, k)].re);\n          }\n        }\n\n        jju++;\n        jjup++;\n        jjdu++;\n        jjdup++;\n      }\n      jju++;\n      jjdu++;\n    }\n    if (j % 2 == 0) {\n      jju += deljju;\n      jjdu += deljju;\n    }\n    int ncolhalf = deljju / 2;\n    jju += deljju * ncolhalf;\n    int ncolhalfp = deljjup / 2;\n    jjup += deljjup * ncolhalfp;\n  }\n\n  double sfac = compute_sfac(r, rcut, switch_flag);\n  double dsfac = compute_dsfac(r, rcut, switch_flag);\n\n  sfac *= wj_in;\n  dsfac *= wj_in;\n  jju = 0;\n  jjdu = 0;\n  for (int j = 0; j <= twojmax; j++) {\n    int deljju = j + 1;\n    for (int mb = 0; 2 * mb <= j; mb++)\n      for (int ma = 0; ma <= j; ma++) {\n        dulist[DULIST_INDEX(natom, nbor, jjdu, 0)].re =\n          dsfac * ulist[ULIST_INDEX(natom, nbor, jju)].re * ux +\n          sfac * dulist[DULIST_INDEX(natom, nbor, jjdu, 0)].re;\n        dulist[DULIST_INDEX(natom, nbor, jjdu, 0)].im =\n          dsfac * ulist[ULIST_INDEX(natom, nbor, jju)].im * ux +\n          sfac * dulist[DULIST_INDEX(natom, nbor, jjdu, 0)].im;\n        dulist[DULIST_INDEX(natom, nbor, jjdu, 1)].re =\n          dsfac * ulist[ULIST_INDEX(natom, nbor, jju)].re * uy +\n          sfac * dulist[DULIST_INDEX(natom, nbor, jjdu, 1)].re;\n        dulist[DULIST_INDEX(natom, nbor, jjdu, 1)].im =\n          dsfac * ulist[ULIST_INDEX(natom, nbor, jju)].im * uy +\n          sfac * dulist[DULIST_INDEX(natom, nbor, jjdu, 1)].im;\n        dulist[DULIST_INDEX(natom, nbor, jjdu, 2)].re =\n          dsfac * ulist[ULIST_INDEX(natom, nbor, jju)].re * uz +\n          sfac * dulist[DULIST_INDEX(natom, nbor, jjdu, 2)].re;\n        dulist[DULIST_INDEX(natom, nbor, jjdu, 2)].im =\n          dsfac * ulist[ULIST_INDEX(natom, nbor, jju)].im * uz +\n          sfac * dulist[DULIST_INDEX(natom, nbor, jjdu, 2)].im;\n        jju++;\n        jjdu++;\n      }\n    int ncolhalf = deljju / 2;\n    jju += deljju * ncolhalf;\n  }\n}\n\n__global__ void compute_duidrj (\n    const double *__restrict__ wj,\n    const double *__restrict__ rij,\n    const double *__restrict__ rcutij,\n    const double*__restrict__ rootpqarray,\n    const COMPLEX*__restrict__ ulist,\n          COMPLEX*__restrict__ dulist,\n    const int num_atoms,\n    const int num_nbor,\n    const int twojmax,\n    const int idxdu_max,\n    const int jdimpq,\n    const int switch_flag)\n{\n  int natom = blockIdx.x * blockDim.x + threadIdx.x;\n  int nbor = blockIdx.y * blockDim.y + threadIdx.y;\n  if (natom < num_atoms && nbor < num_nbor) {\n    double wj_in = wj[INDEX_2D(natom, nbor)];\n    double rcut = rcutij[INDEX_2D(natom, nbor)];\n\n    double x = rij[ULIST_INDEX(natom, nbor, 0)];\n    double y = rij[ULIST_INDEX(natom, nbor, 1)];\n    double z = rij[ULIST_INDEX(natom, nbor, 2)];\n    double rsq = x * x + y * y + z * z;\n    double r = sqrt(rsq);\n    double rscale0 = rfac0 * MY_PI / (rcut - rmin0);\n    double theta0 = (r - rmin0) * rscale0;\n    double cs = cos(theta0);\n    double sn = sin(theta0);\n    double z0 = r * cs / sn;\n    double dz0dr = z0 / r - (r * rscale0) * (rsq + z0 * z0) / rsq;\n\n    compute_duarray(natom, nbor, num_atoms, num_nbor, \n        twojmax, idxdu_max, jdimpq, switch_flag,\n        x, y, z, z0, r, dz0dr, wj_in, rcut,\n        rootpqarray,\n        ulist,\n        dulist);\n  }\n}",
            "#define COMPLEX 1\n\n\n__global__ void compute_deidrj(\n    const int*__restrict__ idxdu_block,\n    const COMPLEX*__restrict__ dulist,\n    const COMPLEX*__restrict__ ylist,\n    double*__restrict__ dedr,\n    const int num_atoms,\n    const int num_nbor,\n    const int twojmax,\n    const int idxdu_max)\n{\n  int natom = blockIdx.x * blockDim.x + threadIdx.x;\n  int nbor = blockIdx.y * blockDim.y + threadIdx.y;\n  if (natom < num_atoms && nbor < num_nbor) {\n    for (int k = 0; k < 3; k++)\n      dedr[ULIST_INDEX(natom, nbor, k)] = 0.0;\n\n    for (int j = 0; j <= twojmax; j++) {\n      int jjdu = idxdu_block[j];\n\n      for (int mb = 0; 2 * mb < j; mb++)\n        for (int ma = 0; ma <= j; ma++) {\n\n          double jjjmambyarray_r = ylist[INDEX_2D(natom, jjdu)].re;\n          double jjjmambyarray_i = ylist[INDEX_2D(natom, jjdu)].im;\n\n          for (int k = 0; k < 3; k++)\n            dedr[ULIST_INDEX(natom, nbor, k)] +=\n              dulist[DULIST_INDEX(natom, nbor, jjdu, k)].re * jjjmambyarray_r +\n              dulist[DULIST_INDEX(natom, nbor, jjdu, k)].im * jjjmambyarray_i;\n          jjdu++;\n        } // end loop over ma mb\n\n      // For j even, handle middle column\n\n      if (j % 2 == 0) {\n\n        int mb = j / 2;\n        for (int ma = 0; ma < mb; ma++) {\n          double jjjmambyarray_r = ylist[INDEX_2D(natom, jjdu)].re;\n          double jjjmambyarray_i = ylist[INDEX_2D(natom, jjdu)].im;\n\n          for (int k = 0; k < 3; k++)\n            dedr[ULIST_INDEX(natom, nbor, k)] +=\n              dulist[DULIST_INDEX(natom, nbor, jjdu, k)].re * jjjmambyarray_r +\n              dulist[DULIST_INDEX(natom, nbor, jjdu, k)].im * jjjmambyarray_i;\n          jjdu++;\n        }\n\n        double jjjmambyarray_r = ylist[INDEX_2D(natom, jjdu)].re;\n        double jjjmambyarray_i = ylist[INDEX_2D(natom, jjdu)].im;\n\n        for (int k = 0; k < 3; k++)\n          dedr[ULIST_INDEX(natom, nbor, k)] +=\n            (dulist[DULIST_INDEX(natom, nbor, jjdu, k)].re * jjjmambyarray_r +\n             dulist[DULIST_INDEX(natom, nbor, jjdu, k)].im * jjjmambyarray_i) *\n            0.5;\n        jjdu++;\n\n      } // end if jeven\n\n    } // end loop over j\n\n    for (int k = 0; k < 3; k++)\n      dedr[ULIST_INDEX(natom, nbor, k)] *= 2.0;\n  }\n}"
        ]
    },
    "crossEntropy-cuda": {
        "/Users/gbolet/hecbench-roofline/src/crossEntropy-cuda/main.cu": [
            "__inline__ __host__ __device__\nfloat h2f (__half x) { return __half2float(x); }\n\n__global__ void loss_bwd<__half, __half> (\n    const __half* __restrict__ log_softmax,\n    const __half* __restrict__ grad_output,\n    const __half* __restrict__ grad_output_neg,\n    const int64_t* __restrict__ target,\n    const __half* __restrict__ weight,\n    const int64_t* __restrict__ mask,\n          __half* __restrict__ grad_predict)\n{\n  int local_id_x = threadIdx.x;\n  int group_id_bs = blockIdx.y;\n  int group_id_x = blockIdx.x;\n\n  int linear_x_id = group_id_x * threadX + local_id_x;\n\n  if (linear_x_id >= H) return;\n\n  int offset2d = group_id_bs * H + linear_x_id;\n  int idx = target[offset2d];\n  int sum_offset = group_id_bs * W * H + idx * H + linear_x_id;\n\n  __half tmp_grad;\n  if (mask[offset2d])\n    tmp_grad = __hneg(__hadd(grad_output[offset2d] , grad_output_neg[offset2d]));\n  else\n    tmp_grad = __hneg(grad_output[offset2d]);\n\n  tmp_grad = __hmul(tmp_grad , weight[offset2d]);\n\n  float sum_value = h2f(__hmul(tmp_grad , log_softmax[sum_offset]));\n\n  #pragma unroll\n  for (int i = 0; i < W; ++i) {\n    int in_offset = group_id_bs * W * H + i * H + linear_x_id;\n    float tmp_sfm = h2f(hexp(log_softmax[in_offset])) * sum_value;\n    float res = 0.f;\n    if (i == idx) {\n      res = h2f(tmp_grad) - tmp_sfm;\n    }\n    else {\n      res = -tmp_sfm;\n    }\n    grad_predict[in_offset] = res;\n  }\n}"
        ]
    },
    "wlcpow-cuda": {
        "/Users/gbolet/hecbench-roofline/src/wlcpow-cuda/main.cu": [
            "#define N\t\t\t\t\t  6\n\n\n#define T ((int)32)\n\n\ninline __host__ __device__ uint4 make_uint4(int4 a)\n{\n    return make_uint4(uint(a.x), uint(a.y), uint(a.z), uint(a.w));\n}\n\ninline __host__ __device__ uint4 max(uint4 a, uint4 b)\n{\n    return make_uint4(max(a.x,b.x), max(a.y,b.y), max(a.z,b.z), max(a.w,b.w));\n}\n\ninline __host__ __device__ uint4 min(uint4 a, uint4 b)\n{\n    return make_uint4(min(a.x,b.x), min(a.y,b.y), min(a.z,b.z), min(a.w,b.w));\n}\n\ninline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\ninline __host__ __device__ float4 sqrtf(float4 v)\n{\n    return make_float4(sqrtf(v.x), sqrtf(v.y), sqrtf(v.z), sqrtf(v.w));\n}\n\n__inline__ __device__\nvoid __TEA_core<0>( uint &v0, uint &v1, uint sum ) {}\n\n__inline__ __device__\nT bound( T x, T lower, T upper )\n{\n  return max( lower, min( x, upper ) );\n}\n\n__inline__ __device__\nfloat gaussian_TEA_fast( bool pred, int u, int v )\n{\n  uint v0 =  pred ? u : v;\n  uint v1 = !pred ? u : v;\n  __TEA_core<N>( v0, v1 );\n  float f = sinpif( int( v0 ) * float(_2_TO_MINUS_31) );\n  float r = sqrtf( -2.0f * float(_LN_2) * log2f( v1 * float(_2_TO_MINUS_32) ) );\n  return bound( r * f, -4.0f, 4.0f );\n}\n\n__inline__ __device__\nfloat minimum_image( float dr, float p )\n{\n  float p_half = p * 0.5f;\n  return dr + ( dr > -p_half ? ( dr < p_half ? 0.f : -p ) : p );\n}\n\n__global__ \nvoid bond_wlcpowallvisc(\n             r64* __restrict__ force_x,\n             r64* __restrict__ force_y,\n             r64* __restrict__ force_z,\n    const float4* __restrict__ coord_merged,\n    const float4* __restrict__ veloc,\n    const int*  __restrict__ nbond,\n    const int2* __restrict__ bonds,\n    const r64* __restrict__ bond_r0,\n    const r32* __restrict__ temp_global,\n    const r32* __restrict__ r0_global,\n    const r32* __restrict__ mu_targ_global,\n    const r32* __restrict__ qp_global,\n    const r32* __restrict__ gamc_global,\n    const r32* __restrict__ gamt_global,\n    const r32* __restrict__ sigc_global,\n    const r32* __restrict__ sigt_global,\n    const float3 period,\n    const int padding,\n    const int n_type,\n    const int n_local )\n{\n  extern __shared__ r32 shared_data[];\n  r32* temp    = &shared_data[0];\n  r32* r0      = &shared_data[1*(n_type+1)];\n  r32* mu_targ = &shared_data[2*(n_type+1)];\n  r32* qp      = &shared_data[3*(n_type+1)];\n  r32* gamc    = &shared_data[4*(n_type+1)];\n  r32* gamt    = &shared_data[5*(n_type+1)];\n  r32* sigc    = &shared_data[6*(n_type+1)];\n  r32* sigt    = &shared_data[7*(n_type+1)];\n\n  for ( int i = threadIdx.x; i < n_type + 1; i += blockDim.x ) {\n    temp[i]    = temp_global[i];\n    r0[i]      = r0_global[i];\n    mu_targ[i] = mu_targ_global[i];\n    qp[i]      = qp_global[i];\n    gamc[i]    = gamc_global[i];\n    gamt[i]    = gamt_global[i];\n    sigc[i]    = sigc_global[i];\n    sigt[i]    = sigt_global[i];\n  }\n  __syncthreads();\n\n  for( int i = blockIdx.x * blockDim.x + threadIdx.x;\n           i < n_local ; i += gridDim.x * blockDim.x ) {\n    int n = nbond[i];\n    float4 coord1 = coord_merged[i];\n    float4 veloc1 = veloc[i];\n    r32 fxi = 0.f, fyi = 0.f, fzi = 0.f;\n\n    for( int p = 0; p < n; p++ ) {\n      int j = bonds[ i + p*padding ].x;\n      int type = bonds[ i + p*padding ].y;\n      float4 coord2 = coord_merged[j];\n      r32 delx = minimum_image( coord1.x - coord2.x, period.x );\n      r32 dely = minimum_image( coord1.y - coord2.y, period.y );\n      r32 delz = minimum_image( coord1.z - coord2.z, period.z );\n      float4 veloc2 = veloc[j];\n      r32 dvx = veloc1.x - veloc2.x;\n      r32 dvy = veloc1.y - veloc2.y;\n      r32 dvz = veloc1.z - veloc2.z;\n\n      r32 l0 = bond_r0[ i + p*padding ];\n      r32 ra = sqrtf(delx*delx + dely*dely + delz*delz);\n      r32 lmax = l0*r0[type];\n      r32 rr = 1.0f/r0[type];\n      r32 sr = (1.0f-rr)*(1.0f-rr);\n      r32 kph = powf(l0,qp[type])*temp[type]*(0.25f/sr-0.25f+rr);\n      // mu is described in the papers\n      r32 mu = 0.433f*(   // 0.25 * sqrt(3)\n\t       temp[type]*(-0.25f/sr + 0.25f + \n               0.5f*rr/(sr*(1.0f-rr)))/(lmax*rr) +\n               kph*(qp[type]+1.0f)/powf(l0,qp[type]+1.0f));\n      r32 lambda = mu/mu_targ[type];\n      kph = kph/lambda;\n      rr = ra/lmax;\n      r32 rlogarg = powf(ra,qp[type]+1.0f);\n      r32 vv = (delx*dvx + dely*dvy + delz*dvz)/ra;\n\n      if (rr >= 0.99) rr = 0.99f;\n      if (rlogarg < 0.01) rlogarg = 0.01f;\n\n      float4 wrr;\n      r32 ww[3][3];\n\n      for (int tes=0; tes<3; tes++) {\n        for (int see=0; see<3; see++) {\n          int v1 = __float_as_int(veloc1.w);\n          int v2 = __float_as_int(veloc2.w);\n          ww[tes][see] = gaussian_TEA_fast<4>(v1 > v2, v1+tes, v2+see);\n        }\n      }\n\n      wrr.w = (ww[0][0]+ww[1][1]+ww[2][2])/3.0f;\n      wrr.x = (ww[0][0]-wrr.w)*delx + 0.5f*(ww[0][1]+ww[1][0])*dely + 0.5f*(ww[0][2]+ww[2][0])*delz;\n      wrr.y = 0.5f*(ww[1][0]+ww[0][1])*delx + (ww[1][1]-wrr.w)*dely + 0.5f*(ww[1][2]+ww[2][1])*delz;\n      wrr.z = 0.5f*(ww[2][0]+ww[0][2])*delx + 0.5f*(ww[2][1]+ww[1][2])*dely + (ww[2][2]-wrr.w)*delz;\n\n      r32 fforce = - temp[type]*(0.25f/(1.0f-rr)/(1.0f-rr)-0.25f+rr)/lambda/ra + kph/rlogarg + (sigc[type]*wrr.w - gamc[type]*vv)/ra;\n      r32 fxij = delx*fforce - gamt[type]*dvx + sigt[type]*wrr.x/ra;\n      r32 fyij = dely*fforce - gamt[type]*dvy + sigt[type]*wrr.y/ra;\n      r32 fzij = delz*fforce - gamt[type]*dvz + sigt[type]*wrr.z/ra;\n\n      fxi += fxij;\n      fyi += fyij;\n      fzi += fzij;\n    }\n    force_x[i] += fxi;\n    force_y[i] += fyi;\n    force_z[i] += fzi;\n  }\n}"
        ]
    },
    "log2-cuda": {
        "/Users/gbolet/hecbench-roofline/src/log2-cuda/kernel.h": [
            "__host__ __device__\nfloat binary_log(float input, int precision)\n{\n  type_caster_union d1;\n  d1.f = input;\n  uint8_t exponent = ((d1.i & 0x7F800000) >> 23) - 127; // mask off the float's sign bit\n  int m = 0;\n  int sum_m = 0;\n  float result = 0;\n  int test = (1 << exponent);\n  float y = input / test;\n  bool max_condition_met = 0;\n  uint64_t one = 1;\n  uint64_t denom = 0;\n  uint64_t prev_denom = 0;\n  while((sum_m < precision + 1 && y != 1) || max_condition_met){\n    m = 0;\n    while((y < 2.f) && (sum_m + m < precision + 1)){\n      y *= y;\n      m++;\n    }\n\n    sum_m += m;\n    prev_denom = denom;\n    denom = one << sum_m;\n\n    if(sum_m >= precision){ //break when we deliver as much precision as requested\n      break;\n    }\n    if(prev_denom > denom){\n      max_condition_met = 1;\n      //std::cout << \"Warning : unable to provide precision of 2^-\" << precision << \n      //             \" requested. Providing maximum precision of 2^-64\" << std::endl;\n      break;\n    }\n\n    result += 1.f / (float)denom;\n    y /= 2.f;\n  }\n  return exponent + result;\n}\n\n__global__\nvoid compute_log( float* __restrict__ output,\n            const float* __restrict__  input,\n            int r, int num_inputs, int precision)\n{\n  int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < num_inputs) {\n    output[r*num_inputs+i] = binary_log(input[i], precision);\n  }\n}"
        ]
    },
    "floydwarshall-cuda": {
        "/Users/gbolet/hecbench-roofline/src/floydwarshall-cuda/main.cu": [
            "__global__ void floydWarshallPass(\n    unsigned int *__restrict__ pathDistanceBuffer,\n    unsigned int *__restrict__ pathBuffer,\n    const unsigned int numNodes,\n    const unsigned int pass)\n{\n  int xValue = threadIdx.x + blockIdx.x * blockDim.x;\n  int yValue = threadIdx.y + blockIdx.y * blockDim.y;\n\n  int k = pass;\n  int oldWeight = pathDistanceBuffer[yValue * numNodes + xValue];\n  int tempWeight = pathDistanceBuffer[yValue * numNodes + k] + \n                   pathDistanceBuffer[k * numNodes + xValue];\n\n  if (tempWeight < oldWeight)\n  {\n    pathDistanceBuffer[yValue * numNodes + xValue] = tempWeight;\n    pathBuffer[yValue * numNodes + xValue] = k;\n  }\n}"
        ]
    },
    "degrid-cuda": {
        "/Users/gbolet/hecbench-roofline/src/degrid-cuda/kernels.cu": [
            "inline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\ninline __host__ __device__ float4 floorf(float4 v)\n{\n    return make_float4(floorf(v.x), floorf(v.y), floorf(v.z), floorf(v.w));\n}\n\n__global__ void\ndegrid_kernel(CmplxType* __restrict out, \n              const CmplxType* __restrict in, \n              const size_t npts,\n              const CmplxType* __restrict img, \n              const size_t img_dim,\n              const CmplxType* __restrict gcf)\n{\n  const int blockIdx_x = blockIdx.x; \n  const int blockDim_x = blockDim.x; \n  const int threadIdx_x = threadIdx.x; \n  const int gridDim_x = gridDim.x; \n  const int blockDim_y = blockDim.y; \n  const int threadIdx_y = threadIdx.y;\n\n  for (int n = 32*blockIdx_x; n < npts; n += 32*gridDim_x) {\n    for (int q = threadIdx_y; q < 32; q += blockDim_y) {\n      CmplxType inn = in[n+q];\n      const int sub_x = floorf(GCF_GRID*(inn.x-floorf(inn.x)));\n      const int sub_y = floorf(GCF_GRID*(inn.y-floorf(inn.y)));\n      const int main_x = floorf(inn.x); \n      const int main_y = floorf(inn.y); \n      CmplxType sum = {0,0};\n      for(int a = threadIdx_x-GCF_DIM/2; a < GCF_DIM/2; a += blockDim_x)\n        for(int b = -GCF_DIM/2; b < GCF_DIM/2; b++)\n        {\n          auto r1 = img[main_x+a+img_dim*(main_y+b)].x; \n          auto i1 = img[main_x+a+img_dim*(main_y+b)].y; \n          if (main_x+a < 0 || main_y+b < 0 || \n              main_x+a >= img_dim  || main_y+b >= img_dim) {\n            r1 = i1 = 0;\n          }\n          auto r2 = gcf[GCF_DIM*GCF_DIM*(GCF_GRID*sub_y+sub_x) + GCF_DIM*b+a].x;\n          auto i2 = gcf[GCF_DIM*GCF_DIM*(GCF_GRID*sub_y+sub_x) + GCF_DIM*b+a].y;\n          sum.x += r1*r2 - i1*i2; \n          sum.y += r1*i2 + r2*i1;\n        }\n\n      for(int s = blockDim_x < 16 ? blockDim_x : 16; s>0;s/=2) {\n        sum.x += __shfl_down_sync(0xFFFFFFFF,sum.x,s);\n        sum.y += __shfl_down_sync(0xFFFFFFFF,sum.y,s);\n      }\n      if (threadIdx_x == 0) {\n        out[n+q] = sum;\n      }\n    }\n  }\n}"
        ]
    },
    "softmax-online-cuda": {
        "/Users/gbolet/hecbench-roofline/src/softmax-online-cuda/main.cu": [
            "inline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\ninline __host__ __device__ float4 expf(float4 v)\n{\n    return make_float4(expf(v.x), expf(v.y), expf(v.z), expf(v.w));\n}\n\n__device__ __forceinline__ SumMax reduce_sum_max_op(SumMax a, SumMax b) {\n    bool a_bigger = (a.maxval > b.maxval);\n    SumMax bigger_m = a_bigger ? a : b;\n    SumMax smaller_m = a_bigger ? b : a;\n    SumMax res;\n    res.maxval = bigger_m.maxval;\n    res.sum = bigger_m.sum + smaller_m.sum * expf(smaller_m.maxval - bigger_m.maxval);\n    return res;\n}\n\n__global__ void softmax_forward_online_kernel(float* out, const float* inp, int N, int C) {\n  namespace cg = cooperative_groups;\n  cg::thread_block block = cg::this_thread_block();\n  cg::thread_block_tile<32> warp = cg::tiled_partition<32>(block);\n\n  int row = blockIdx.x * warp.meta_group_size() + warp.meta_group_rank();\n  if (row >= N) {\n    return;\n  }\n\n  // one row of inp, i.e. inp[row, :] of shape (C,)\n  const float* x = inp + row * C;\n  float* const y = out + row * C;\n\n  // base case for the reduction\n  SumMax sm_partial;\n  sm_partial.maxval = -INFINITY;\n  sm_partial.sum = 0.0f;\n\n  // first, thread coarsening by directly accessing global memory in series\n  for (int i = warp.thread_rank(); i < C; i += warp.size()) {\n    sm_partial = reduce_sum_max_op(sm_partial, { x[i], 1.0f });\n  }\n\n  // second, the reduction\n  SumMax sm_total = cg::reduce(warp, sm_partial, reduce_sum_max_op);\n\n  // divide the whole row by the sum\n  for (int i = warp.thread_rank(); i < C; i += warp.size()) {\n    y[i] = expf(x[i] - sm_total.maxval) / sm_total.sum;\n  }\n}",
            "inline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\ninline __host__ __device__ float4 expf(float4 v)\n{\n    return make_float4(expf(v.x), expf(v.y), expf(v.z), expf(v.w));\n}\n\ninline __host__ __device__ float4 fmaxf(float4 a, float4 b)\n{\n    return make_float4(fmaxf(a.x,b.x), fmaxf(a.y,b.y), fmaxf(a.z,b.z), fmaxf(a.w,b.w));\n}\n\n__global__ void softmax_forward_online_kernel2(float* out, const float* inp, int N, int C) {\n  const int warpsPerBlock = blockDim.x / warpSize;\n  int tid = threadIdx.x;\n\n  if (tid >= C) {\n    return;\n  }\n\n  int warpId = tid / warpSize;\n  int laneId = tid % warpSize;\n  int row = blockIdx.x * warpsPerBlock + warpId;\n\n  if (row >= N) {\n    return;\n  }\n\n  const float* x = inp + row * C;\n  float* const y = out + row * C;\n\n  // merge calculating maxval and sumval in one loop\n  // which is an arithmetic improvment from online softmax over normal softmax\n  float maxval = -INFINITY, sumval = 0.0f, bigger;\n  for (int i = laneId; i < C; i += warpSize) {\n    // when updating the maxval, dynamically updates the previous sumval by\n    // multiplying e^{previous_maxval - current_maxval}\n    bigger = fmaxf(maxval, x[i]);\n    sumval = sumval * expf(maxval - bigger) + expf(x[i] - bigger);\n    maxval = bigger;\n  }\n\n  // use warp functions instead of cooperative groups for better readibility\n  // calculate the warp wised maxval and sumval\n  float offsetMaxval, offsetSumval;\n  for (int offset = warpSize / 2; offset > 0; offset >>= 1) {\n    __syncwarp();\n    offsetMaxval = __shfl_down_sync(0xFFFFFFFF, maxval, offset);\n    offsetSumval = __shfl_down_sync(0xFFFFFFFF, sumval, offset);\n    if (offsetMaxval > maxval) {\n      sumval *= expf(maxval - offsetMaxval);\n      maxval = offsetMaxval;\n    } else {\n      offsetSumval *= expf(offsetMaxval - maxval);\n    }\n    sumval += offsetSumval;\n  }\n\n  // sync the warp wised maxval and sumval\n  // which are also the maxval and sumval of one row in C\n  maxval = __shfl_sync(0xFFFFFFFF, maxval, 0);\n  sumval = __shfl_sync(0xFFFFFFFF, sumval, 0);\n\n  for (int i = laneId; i < C; i += warpSize) {\n    y[i] = expf(x[i] - maxval) / sumval;\n  }\n}"
        ]
    },
    "merge-cuda": {
        "/Users/gbolet/hecbench-roofline/src/merge-cuda/kernels.h": [
            "inline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\ninline  __host__ __device__ float4 fminf(float4 a, float4 b)\n{\n    return make_float4(fminf(a.x,b.x), fminf(a.y,b.y), fminf(a.z,b.z), fminf(a.w,b.w));\n}\n\n__device__ __forceinline__ float MIN<float>(float in, float in2)\n{\n    return fminf(in, in2);\n}\n\n__global__ void workloadDiagonals(\n  const vec_t *__restrict__ A, uint32_t A_length, \n  const vec_t *__restrict__ B, uint32_t B_length, \n  uint32_t *__restrict__ diagonal_path_intersections) {\n\n  // Calculate combined index around the MergePath \"matrix\"\n  int32_t combinedIndex = (uint64_t)blockIdx.x * ((uint64_t)A_length + (uint64_t)B_length) / (uint64_t)gridDim.x;\n  volatile __shared__ int32_t x_top, y_top, x_bottom, y_bottom,  found;\n  __shared__ int32_t oneorzero[32];\n\n  int threadOffset = threadIdx.x - 16;\n\n  // Figure out the coordinates of our diagonal\n  x_top = MIN(combinedIndex, A_length);\n  y_top = combinedIndex > (A_length) ? combinedIndex - (A_length) : 0;\n  x_bottom = y_top;\n  y_bottom = x_top;\n\n  found = 0;\n\n  // Search the diagonal\n  while(!found) {\n    // Update our coordinates within the 32-wide section of the diagonal \n    int32_t current_x = x_top - ((x_top - x_bottom) >> 1) - threadOffset;\n    int32_t current_y = y_top + ((y_bottom - y_top) >> 1) + threadOffset;\n\n    // Are we a '1' or '0' with respect to A[x] <= B[x]\n    if(current_x >= (int32_t)A_length || current_y < 0) {\n      oneorzero[threadIdx.x] = 0;\n    } else if(current_y >= (int32_t)B_length || current_x < 1) {\n      oneorzero[threadIdx.x] = 1;\n    } else {\n      oneorzero[threadIdx.x] = (A[current_x-1] <= B[current_y]) ? 1 : 0;\n    }\n\n    __syncthreads();\n\n    // If we find the meeting of the '1's and '0's, we found the \n    // intersection of the path and diagonal\n    if(threadIdx.x > 0 && (oneorzero[threadIdx.x] != oneorzero[threadIdx.x-1])) {\n      found = 1;\n      diagonal_path_intersections[blockIdx.x] = current_x;\n      diagonal_path_intersections[blockIdx.x + gridDim.x + 1] = current_y;\n    }\n\n    __syncthreads();\n\n    // Adjust the search window on the diagonal\n    if(threadIdx.x == 16) {\n      if(oneorzero[31] != 0) {\n        x_bottom = current_x;\n        y_bottom = current_y;\n      } else {\n        x_top = current_x;\n        y_top = current_y;\n      }\n    }\n    __syncthreads();\n  }\n\n  // Set the boundary diagonals (through 0,0 and A_length,B_length)\n  if(threadIdx.x == 0 && blockIdx.x == 0) {\n    diagonal_path_intersections[0] = 0;\n    diagonal_path_intersections[gridDim.x + 1] = 0;\n    diagonal_path_intersections[gridDim.x] = A_length;\n    diagonal_path_intersections[gridDim.x + gridDim.x + 1] = B_length;\n  }\n}",
            "__host__ __device__ uint64_t negativeInfinity(uint64_t tmp) {\n  return 0;\n}\n\n__host__ __device__ uint64_t positiveInfinity(uint64_t tmp) {\n  return 0xFFFFFFFFFFFFFFFFUL;\n}\n\n__host__ __device__ vec_t getNegativeInfinity() {\n  vec_t tmp = 0;\n  return negativeInfinity(tmp);\n}\n\n__host__ __device__ vec_t getPositiveInfinity() {\n  vec_t tmp = 0;\n  return positiveInfinity(tmp);\n}\n\n__global__ void mergeSinglePath(\n    const vec_t * __restrict__ A, uint32_t A_length,\n    const vec_t * __restrict__ B, uint32_t B_length, \n    const uint32_t * __restrict__ diagonal_path_intersections,\n    vec_t * __restrict__ C, uint32_t C_length)\n{\n\n  // Storage space for local merge window\n  __shared__ vec_t A_shared[(K+2) << 1];\n  vec_t* B_shared = A_shared + K+2;\n\n  volatile __shared__ uint32_t x_block_top, y_block_top, x_block_stop, y_block_stop;\n\n  // Pre-calculate reused indices\n  uint32_t threadIdX4 = threadIdx.x + threadIdx.x;\n  threadIdX4 = threadIdX4 + threadIdX4;\n  uint32_t threadIdX4p1 = threadIdX4 + 1;\n  uint32_t threadIdX4p2 = threadIdX4p1 + 1;\n  uint32_t threadIdX4p3 = threadIdX4p2 + 1;\n\n  // Define global window and create sentinels\n  switch(threadIdx.x) {\n    case 0:\n      x_block_top = diagonal_path_intersections[blockIdx.x];\n      A_shared[0] = getNegativeInfinity<vec_t>();\n      break;\n    case 64:\n      y_block_top = diagonal_path_intersections[blockIdx.x + gridDim.x + 1];\n      A_shared[K+1] = getPositiveInfinity<vec_t>();\n      break;\n    case 32:\n      x_block_stop = diagonal_path_intersections[blockIdx.x + 1];\n      B_shared[0] = getNegativeInfinity<vec_t>();\n      break;\n    case 96:\n      y_block_stop = diagonal_path_intersections[blockIdx.x + gridDim.x + 2];\n      B_shared[K+1] = getPositiveInfinity<vec_t>();\n      break;\n    default:\n      break;\n  }\n\n  A--;\n  B--;\n\n  __syncthreads();\n\n  // Construct and merge windows from diagonal_path_intersections[blockIdx.x] \n  // to diagonal_path_intersections[blockIdx.x+1]\n  while(((x_block_top < x_block_stop) || (y_block_top < y_block_stop))) {\n\n    // Load current local window\n    const vec_t * Atemp = A + x_block_top;\n    const vec_t * Btemp = B + y_block_top;\n    uint32_t sharedX = threadIdx.x+1;\n\n    A_shared[sharedX] = Atemp[sharedX];\n    B_shared[sharedX] = Btemp[sharedX];\n    sharedX += blockDim.x;\n    A_shared[sharedX] = Atemp[sharedX];\n    B_shared[sharedX] = Btemp[sharedX];\n    sharedX += blockDim.x;\n    A_shared[sharedX] = Atemp[sharedX];\n    B_shared[sharedX] = Btemp[sharedX];\n    sharedX += blockDim.x;\n    A_shared[sharedX] = Atemp[sharedX];\n    B_shared[sharedX] = Btemp[sharedX];\n    \n    // Make sure this is before the sync\n    vec_t *Ctemp = C + x_block_top + y_block_top;\n\n    __syncthreads();\n\n    // Binary search diagonal in the local window for path\n    int32_t offset = threadIdX4 >> 1;\n    uint32_t Ax = offset + 1;\n    vec_t * BSm1 = B_shared + threadIdX4p2;\n    vec_t * BS = BSm1 + 1;\n    while(true) {\n      offset = ((offset+1) >> 1);\n      if(A_shared[Ax] > BSm1[~Ax]) {\n        if(A_shared[Ax-1] <= BS[~Ax]) {\n          //Found it\n          break;\n        }\n        Ax -= offset;\n      } else {\n        Ax += offset;\n      }\n    }\n\n    uint32_t Bx = threadIdX4p2 - Ax;\n\n    // Merge four elements starting at the found path intersection\n    vec_t Ai, Bi, Ci;\n    Ai = A_shared[Ax];\n    Bi = B_shared[Bx];\n    if(Ai > Bi) {Ci = Bi; Bx++; Bi = B_shared[Bx];} else {Ci = Ai; Ax++; Ai = A_shared[Ax];}\n    Ctemp[threadIdX4] = Ci;\n    if(Ai > Bi) {Ci = Bi; Bx++; Bi = B_shared[Bx];} else {Ci = Ai; Ax++; Ai = A_shared[Ax];}\n    Ctemp[threadIdX4p1] = Ci;\n    if(Ai > Bi) {Ci = Bi; Bx++; Bi = B_shared[Bx];} else {Ci = Ai; Ax++; Ai = A_shared[Ax];}\n    Ctemp[threadIdX4p2] = Ci;\n    Ctemp[threadIdX4p3] = Ai > Bi ? Bi : Ai;\n\n    // Update for next window\n    if(threadIdx.x == 127) {\n      x_block_top += Ax - 1;\n      y_block_top += Bx - 1;\n    }\n    __syncthreads();\n  } // Go to next window\n}"
        ]
    },
    "mis-cuda": {
        "/Users/gbolet/hecbench-roofline/src/mis-cuda/main.cu": [
            "__global__ \nvoid findmins(const int nodes, \n    const int* const __restrict nidx,\n    const int* const __restrict nlist,\n    volatile stattype* const __restrict nstat)\n{\n  const int from = threadIdx.x + blockIdx.x * ThreadsPerBlock;\n  const int incr = gridDim.x * ThreadsPerBlock;\n\n  int missing;\n  do {\n    missing = 0;\n    for (int v = from; v < nodes; v += incr) {\n      const stattype nv = nstat[v];\n      if (nv & 1) {\n        int i = nidx[v];\n        while ((i < nidx[v + 1]) && ((nv > nstat[nlist[i]]) || ((nv == nstat[nlist[i]]) && (v > nlist[i])))) {\n          i++;\n        }\n        if (i < nidx[v + 1]) {\n          missing = 1;\n        } else {\n          for (int i = nidx[v]; i < nidx[v + 1]; i++) {\n            nstat[nlist[i]] = out;\n          }\n          nstat[v] = in;\n        }\n      }\n    }\n  } while (missing != 0);\n}"
        ]
    },
    "sss-cuda": {
        "/Users/gbolet/hecbench-roofline/src/sss-cuda/kernels.cu": [
            "#define myInt short // Using short interger\n\n\n__global__ void CanDeleteEdge(myInt *d_in_delete, myInt *isDecomposable) {\n  myInt tid = threadIdx.x;\n  int bid = blockIdx.x;\n  myInt bdim = blockDim.x;\n\n  int n = *d_in_delete;\n  myInt nCliques = *(d_in_delete + 1);\n  myInt *CliquesDimens = d_in_delete + 2;\n  myInt *Cliques = CliquesDimens + nCliques;\n  // myInt  nTasks = *(Cliques+nCliques*n);\n  myInt *d_a = Cliques + nCliques * n + 1;\n  myInt *d_b = d_a + *(Cliques + nCliques * n); // nTasks;\n\n  myInt i, j, k;\n  int ii; // myInt n = *d_n;\n  myInt a = d_a[bid];\n  myInt b = d_b[bid];\n  __shared__ myInt count;\n  if (tid == 0) {\n    count = 0;\n  }\n  __shared__ myInt contain_a, contain_b, which_ab;\n\n  for (i = 0; i < nCliques; i++) {\n    ii = i * n;\n    if (tid == 0) {\n      contain_a = 0;\n      contain_b = 0;\n    }\n    for (j = tid; j < CliquesDimens[i]; j += bdim) {\n      k = Cliques[ii + j];\n      if (k == a) {\n        contain_a = 1;\n      };\n      if (k == b) {\n        contain_b = 1;\n      }\n    }\n    if (tid == 0) {\n      if (contain_a && contain_b) {\n        count++;\n        which_ab = i;\n      }\n    }\n    if (count > 1) {\n      break;\n    }\n  }\n\n  if (tid == 0) {\n    if (count == 1) {\n      isDecomposable[bid] = which_ab;\n    } else {\n      isDecomposable[bid] = -1;\n    }\n  }\n}",
            "#define myBool bool\n\n\n#define myInt short // Using short interger\n\n\n__global__ void CanAddEdge(myInt *d_in_delete, myInt *d_in_add,\n                           myInt *isDecomposable) {\n  myInt tid = threadIdx.x;\n  int bid = blockIdx.x;\n  myInt bdim = blockDim.x;\n  extern __shared__ myInt shmem[];\n  myInt *bi = shmem;\n  int i, j, k, c, t;\n\n  int n = (int)*d_in_delete;\n  myInt nCliques = *(d_in_delete + 1);\n  myInt *CliquesDimens = d_in_delete + 2;\n  myInt *Cliques = CliquesDimens + nCliques;\n\n  myInt *d_Labels = d_in_add;\n  myInt nSeparators = *(d_in_add + n);\n  myInt *SeparatorsDimens = d_in_add + n + 1;\n  myInt *Separators = SeparatorsDimens + nSeparators;\n  myInt nTreeEdges = *(Separators + n * nSeparators);\n  myInt *TreeEdgeA = Separators + n * nSeparators + 1;\n  myInt *TreeEdgeB = TreeEdgeA + nTreeEdges;\n  myInt *d_Edge = TreeEdgeB + nTreeEdges;\n  // myInt  nTasks = *(d_Edge + n*n);\n  myInt *d_a = d_Edge + (n * n) + 1;\n  myInt *d_b = d_a + *(d_Edge + (n * n)); // nTasks;\n\n  __shared__ myInt nS, pR, pT, contain_a, contain_b, aSi, bSi, common_parent, a,\n      b;\n  __shared__ myBool flag;\n  myInt *R;\n  myInt *T;\n  myInt *S;\n  myInt *contain_S;\n  R = bi;\n  bi += nTreeEdges;\n  T = bi;\n  bi += nTreeEdges;\n  S = bi;\n  bi += n;\n  contain_S = bi; // bi += BLOCK_SIZE;\n\n  if (tid == 0) {\n    flag = 0;\n    aSi = -1;\n    bSi = -1;\n    common_parent = 0;\n    a = d_a[bid];\n    b = d_b[bid];\n    isDecomposable[bid] = 0;\n\n    if (d_Labels[a] != d_Labels[b]) {\n      flag = 1;\n      isDecomposable[bid] = 1;\n    }\n  };\n  SYNC;\n\n  if (flag) {\n    return;\n  } // else ..\n\n  if (tid == 0) {\n    nS = 0;\n    for (j = 0; j < n; j++) {\n      if (d_Edge[a * n + j] && d_Edge[b * n + j]) {\n        S[nS] = j;\n        nS++;\n      }\n    };\n    if (nS == 0) {\n      flag = 1;\n    }\n  };\n  SYNC;\n\n  if (flag) {\n    return;\n  } // else ..\n\n  // find higest-indexed cliques containing (a & S) and (b & S)\n  for (i = 0; i < nCliques; i++) {\n    if (tid == 0) {\n      contain_a = 0;\n      contain_b = 0;\n    };\n    contain_S[tid] = 0;\n    t = i * n;\n    for (j = tid; j < CliquesDimens[i]; j += bdim) {\n      c = Cliques[t + j];\n      if (c == a) {\n        contain_a = 1;\n      };\n      if (c == b) {\n        contain_b = 1;\n      }\n      for (k = 0; k < nS; k++) {\n        if (c == S[k]) {\n          contain_S[tid]++;\n        }\n      }\n    }\n    if (tid == 0) {\n      k = 0;\n      for (j = 0; j < BLOCK_SIZE; j++) {\n        k += contain_S[j];\n      }\n      if (contain_a && (k == nS)) {\n        aSi = i;\n      };\n      if (contain_b && (k == nS)) {\n        bSi = i;\n      }\n    }\n  }\n\n  if (tid == 0) { // find the path from aSi to root\n    R[0] = -1;\n    pR = -1;\n    for (i = 0; i < nTreeEdges; i++) {\n      if (TreeEdgeA[i] == aSi) {\n        R[0] = i;\n        pR = 0;\n        break;\n      }\n    }\n    for (i = R[0]; i >= 0; i--) {\n      if (TreeEdgeA[i] == TreeEdgeB[R[pR]]) {\n        pR++;\n        R[pR] = i;\n      }\n    }\n  } else if (tid == 1) {\n    // find the path from bSi to root\n    T[0] = -1;\n    pT = -1;\n    for (i = 0; i < nTreeEdges; i++) {\n      if (TreeEdgeA[i] == bSi) {\n        T[0] = i;\n        pT = 0;\n        break;\n      }\n    }\n    for (i = T[0]; i >= 0; i--) {\n      if (TreeEdgeA[i] == TreeEdgeB[T[pT]]) {\n        pT++;\n        T[pT] = i;\n      }\n    }\n  }\n\n  if (tid == 0) {\n    // find the branching point\n    t = ((pR <= pT) ? pR : pT);\n    for (i = 0; i <= t; i++) {\n      if (TreeEdgeB[R[pR - i]] == TreeEdgeB[T[pT - i]]) {\n        common_parent = i;\n      } else {\n        break;\n      }\n    }\n    if (t != -1) {\n      if (TreeEdgeA[R[pR - common_parent]] ==\n          TreeEdgeA[T[pT - common_parent]]) {\n        common_parent++;\n      }\n    }\n  }\n  SYNC;\n\n  // check if S is in the path from R[0] to common_parent\n  for (i = tid; i <= ((pR - common_parent) + (pT - common_parent) + 1);\n       i += bdim) {\n    if (i <= (pR - common_parent)) {\n      if (SeparatorsDimens[R[i]] == nS) {\n        contain_S[tid] = 0;\n        t = R[i] * n;\n        for (j = 0; j < nS; j++) {\n          for (k = 0; k < nS; k++) {\n            if (Separators[t + j] == S[k]) {\n              contain_S[tid]++;\n            }\n          }\n        }\n        if (contain_S[tid] == nS) {\n          flag = 1;\n          isDecomposable[bid] = 1;\n        }\n      }\n    } else {\n      c = i - (pR - common_parent) - 1;\n      if (SeparatorsDimens[T[c]] == nS) {\n        contain_S[tid] = 0;\n        t = T[c] * n;\n        for (j = 0; j < nS; j++) {\n          for (k = 0; k < nS; k++) {\n            if (Separators[t + j] == S[k]) {\n              contain_S[tid]++;\n            }\n          }\n        }\n        if (contain_S[tid] == nS) {\n          flag = 1;\n          isDecomposable[bid] = 1;\n        }\n      }\n    }\n  }\n  SYNC;\n\n  return;\n}"
        ]
    },
    "word2vec-cuda": {
        "/Users/gbolet/hecbench-roofline/src/word2vec-cuda/cbow.cu": [
            "__global__ \nvoid device_memset(real * array, int size){\n  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  if (idx < size)\n    array[idx] = 0;\n}",
            "__device__ void reduceInWarp(volatile float * f, int idInWarp){\n\n  for (unsigned int i=THREADS_PER_WORD /2; i>32; i>>=1) {\n    if (idInWarp < i) {\n      f[idInWarp] += f[idInWarp + i];\n    }\n    __syncthreads();\n  }\n  if (idInWarp < 32){\n    f[idInWarp] += f[idInWarp + 32];\n    f[idInWarp] += f[idInWarp + 16];\n    f[idInWarp] += f[idInWarp + 8];\n    f[idInWarp] += f[idInWarp + 4];\n    f[idInWarp] += f[idInWarp + 2];\n    f[idInWarp] += f[idInWarp + 1];\n  }\n}\n\n__global__ \nvoid device_cbow(\n    const int sentence_num,\n    const int layer1_size,\n    const int layer1_size_aligned,\n    const int window,\n    const int negative,\n    const int table_size,\n    const int vocab_size,\n    const int *__restrict__ d_sen,\n    const int *__restrict__ d_table,\n    float *__restrict__ d_syn0,\n    float *__restrict__ d_syn1neg,\n    unsigned int *__restrict__ d_random)\n{\n  int sentence_position = (threadIdx.x / THREADS_PER_WORD) + (blockDim.x / THREADS_PER_WORD) * blockIdx.x;\n  int idInWarp = threadIdx.x % THREADS_PER_WORD;\n\n  extern __shared__ float shared[];\n  float * f = shared + (threadIdx.x / THREADS_PER_WORD) * THREADS_PER_WORD;\n  float * neu1 = shared + BLOCK_SIZE + (threadIdx.x / THREADS_PER_WORD) * layer1_size_aligned;\n  float * neu1e= shared + BLOCK_SIZE + (blockDim.x / THREADS_PER_WORD) * layer1_size_aligned +\n                 (threadIdx.x / THREADS_PER_WORD) * layer1_size_aligned;\n\n  if (sentence_position < MAX_SENTENCE_LENGTH) {\n    unsigned int next_random = d_random[sentence_position];\n\n    for (int sentence_idx = 0; sentence_idx < sentence_num; sentence_idx++) {\n\n      for (int c = idInWarp; c < layer1_size; c+=THREADS_PER_WORD) {\n        neu1[c] = 0;\n        neu1e[c] = 0;\n      }\n\n      next_random = next_random * (unsigned int) 1664525 + 1013904223;\n      int b = next_random % window;\n      int word = d_sen[sentence_idx * MAX_SENTENCE_LENGTH + sentence_position];\n      // in -> hidden\n      int cw = 0;\n      for (int a = b; a < window * 2 + 1 - b; a++)\n        if (a != window) {\n          int w = sentence_position - window + a;\n          if (w < 0 || w>= MAX_SENTENCE_LENGTH) continue;\n          int last_word = d_sen[sentence_idx * MAX_SENTENCE_LENGTH + w];\n          for (int c = idInWarp; c < layer1_size; c+= THREADS_PER_WORD)\n            neu1[c] += d_syn0[c + last_word * layer1_size_aligned];\n\n          cw++;\n        }\n\n      if (cw) {\n        for (int c = idInWarp; c < layer1_size; c+= THREADS_PER_WORD)\n          neu1[c] /= cw;\n\n        // NEGATIVE SAMPLING\n        int target, label;\n        float alpha =*((float *) &d_sen[MAX_SENTENCE_NUM * MAX_SENTENCE_LENGTH + sentence_idx]);\n\n        if (negative > 0)\n\n          for (int d = 0; d < negative + 1; d++) {\n\n            if (d == 0) {\n              target = word;\n              label = 1;\n            } else {\n              next_random = next_random * (unsigned int) 1664525 + 1013904223;\n              target = d_table[(next_random) % table_size];\n              if (target == 0)\n                target = next_random % (vocab_size - 1) + 1;\n              if (target == word)\n                continue;\n              label = 0;\n            }\n            int l2 = target * layer1_size_aligned;\n            f[idInWarp] = 0;\n\n\n            for (int c = idInWarp; c < layer1_size; c+=THREADS_PER_WORD){\n              f[idInWarp] += neu1[c] * d_syn1neg[c + l2];   \n            }\n            __syncthreads();\n\n            // Do reduction here;\n            reduceInWarp(f, idInWarp);\n\n            __syncthreads();\n\n            float g;\n            if (f[0] > MAX_EXP)\n              g = (label - 1) * alpha;\n            else if (f[0] < -MAX_EXP)\n              g = (label - 0) * alpha;\n            else\n              g = (label - expTable[(int) ((f[0] + MAX_EXP)\n                    * (EXP_TABLE_SIZE / MAX_EXP / 2))]) * alpha;\n\n            for (int c = idInWarp; c < layer1_size; c+=THREADS_PER_WORD)\n              neu1e[c] += g * d_syn1neg[c + l2];\n            for (int c = idInWarp; c < layer1_size; c+=THREADS_PER_WORD)\n              d_syn1neg[c + l2] += g * neu1[c];\n          }\n\n        // hidden -> in\n        for (int a = b; a < window * 2 + 1 - b; a++)\n          if (a != window) {\n            int w = sentence_position - window + a;\n            if (w < 0)\n              continue;\n            if (w >= MAX_SENTENCE_LENGTH)\n              continue;\n            int last_word = d_sen[sentence_idx * MAX_SENTENCE_LENGTH + w];\n\n            for (int c = idInWarp; c < layer1_size; c+=THREADS_PER_WORD)\n              d_syn0[c + last_word * layer1_size_aligned] += neu1e[c];\n          }\n      }\n    }// End for sentence_idx\n\n    // Update d_random\n    if (idInWarp == 0 ) d_random[sentence_position] = next_random;\n  }\n}"
        ]
    },
    "secp256k1-cuda": {
        "/Users/gbolet/hecbench-roofline/src/secp256k1-cuda/main.cu": [
            "__device__\nvoid secp256k1_fe_mul_inner(uint *r, const uint *a, const uint * b) {\n  ulong c, d;\n  ulong u0, u1, u2, u3, u4, u5, u6, u7, u8;\n  uint t9, t1, t0, t2, t3, t4, t5, t6, t7;\n  const uint M = 0x3FFFFFFUL, R0 = 0x3D10UL, R1 = 0x400UL;\n  d  = (ulong)a[0] * b[9]\n    + (ulong)a[1] * b[8]\n    + (ulong)a[2] * b[7]\n    + (ulong)a[3] * b[6]\n    + (ulong)a[4] * b[5]\n    + (ulong)a[5] * b[4]\n    + (ulong)a[6] * b[3]\n    + (ulong)a[7] * b[2]\n    + (ulong)a[8] * b[1]\n    + (ulong)a[9] * b[0];\n  /* VERIFY_BITS(d, 64); */\n  /* [d 0 0 0 0 0 0 0 0 0] = [p9 0 0 0 0 0 0 0 0 0] */\n  t9 = d & M; d >>= 26;\n\n  /* [d t9 0 0 0 0 0 0 0 0 0] = [p9 0 0 0 0 0 0 0 0 0] */\n\n  c  = (ulong)a[0] * b[0];\n\n  /* [d t9 0 0 0 0 0 0 0 0 c] = [p9 0 0 0 0 0 0 0 0 p0] */\n  d += (ulong)a[1] * b[9]\n    + (ulong)a[2] * b[8]\n    + (ulong)a[3] * b[7]\n    + (ulong)a[4] * b[6]\n    + (ulong)a[5] * b[5]\n    + (ulong)a[6] * b[4]\n    + (ulong)a[7] * b[3]\n    + (ulong)a[8] * b[2]\n    + (ulong)a[9] * b[1];\n\n  /* [d t9 0 0 0 0 0 0 0 0 c] = [p10 p9 0 0 0 0 0 0 0 0 p0] */\n  u0 = d & M; d >>= 26; c += u0 * R0;\n\n  /* [d u0 t9 0 0 0 0 0 0 0 0 c-u0*R0] = [p10 p9 0 0 0 0 0 0 0 0 p0] */\n  t0 = c & M; c >>= 26; c += u0 * R1;\n\n  /* [d u0 t9 0 0 0 0 0 0 0 c-u0*R1 t0-u0*R0] = [p10 p9 0 0 0 0 0 0 0 0 p0] */\n  /* [d 0 t9 0 0 0 0 0 0 0 c t0] = [p10 p9 0 0 0 0 0 0 0 0 p0] */\n\n  c += (ulong)a[0] * b[1]\n    + (ulong)a[1] * b[0];\n\n  /* [d 0 t9 0 0 0 0 0 0 0 c t0] = [p10 p9 0 0 0 0 0 0 0 p1 p0] */\n  d += (ulong)a[2] * b[9]\n    + (ulong)a[3] * b[8]\n    + (ulong)a[4] * b[7]\n    + (ulong)a[5] * b[6]\n    + (ulong)a[6] * b[5]\n    + (ulong)a[7] * b[4]\n    + (ulong)a[8] * b[3]\n    + (ulong)a[9] * b[2];\n\n  /* [d 0 t9 0 0 0 0 0 0 0 c t0] = [p11 p10 p9 0 0 0 0 0 0 0 p1 p0] */\n  u1 = d & M; d >>= 26; c += u1 * R0;\n\n  /* [d u1 0 t9 0 0 0 0 0 0 0 c-u1*R0 t0] = [p11 p10 p9 0 0 0 0 0 0 0 p1 p0] */\n  t1 = c & M; c >>= 26; c += u1 * R1;\n\n  /* [d u1 0 t9 0 0 0 0 0 0 c-u1*R1 t1-u1*R0 t0] = [p11 p10 p9 0 0 0 0 0 0 0 p1 p0] */\n  /* [d 0 0 t9 0 0 0 0 0 0 c t1 t0] = [p11 p10 p9 0 0 0 0 0 0 0 p1 p0] */\n\n  c += (ulong)a[0] * b[2]\n    + (ulong)a[1] * b[1]\n    + (ulong)a[2] * b[0];\n\n  /* [d 0 0 t9 0 0 0 0 0 0 c t1 t0] = [p11 p10 p9 0 0 0 0 0 0 p2 p1 p0] */\n  d += (ulong)a[3] * b[9]\n    + (ulong)a[4] * b[8]\n    + (ulong)a[5] * b[7]\n    + (ulong)a[6] * b[6]\n    + (ulong)a[7] * b[5]\n    + (ulong)a[8] * b[4]\n    + (ulong)a[9] * b[3];\n\n  /* [d 0 0 t9 0 0 0 0 0 0 c t1 t0] = [p12 p11 p10 p9 0 0 0 0 0 0 p2 p1 p0] */\n  u2 = d & M; d >>= 26; c += u2 * R0;\n  /* [d u2 0 0 t9 0 0 0 0 0 0 c-u2*R0 t1 t0] = [p12 p11 p10 p9 0 0 0 0 0 0 p2 p1 p0] */\n  t2 = c & M; c >>= 26; c += u2 * R1;\n\n  /* [d u2 0 0 t9 0 0 0 0 0 c-u2*R1 t2-u2*R0 t1 t0] = [p12 p11 p10 p9 0 0 0 0 0 0 p2 p1 p0] */\n  /* [d 0 0 0 t9 0 0 0 0 0 c t2 t1 t0] = [p12 p11 p10 p9 0 0 0 0 0 0 p2 p1 p0] */\n  c += (ulong)a[0] * b[3]\n    + (ulong)a[1] * b[2]\n    + (ulong)a[2] * b[1]\n    + (ulong)a[3] * b[0];\n\n  d += (ulong)a[4] * b[9]\n    + (ulong)a[5] * b[8]\n    + (ulong)a[6] * b[7]\n    + (ulong)a[7] * b[6]\n    + (ulong)a[8] * b[5]\n    + (ulong)a[9] * b[4];\n  u3 = d & M; d >>= 26; c += u3 * R0;\n\n  /* VERIFY_BITS(c, 64); */\n  /* [d u3 0 0 0 t9 0 0 0 0 0 c-u3*R0 t2 t1 t0] = [p13 p12 p11 p10 p9 0 0 0 0 0 p3 p2 p1 p0] */\n  t3 = c & M; c >>= 26; c += u3 * R1;\n\n  c += (ulong)a[0] * b[4]\n    + (ulong)a[1] * b[3]\n    + (ulong)a[2] * b[2]\n    + (ulong)a[3] * b[1]\n    + (ulong)a[4] * b[0];\n\n  /* [d 0 0 0 0 t9 0 0 0 0 c t3 t2 t1 t0] = [p13 p12 p11 p10 p9 0 0 0 0 p4 p3 p2 p1 p0] */\n  d += (ulong)a[5] * b[9]\n    + (ulong)a[6] * b[8]\n    + (ulong)a[7] * b[7]\n    + (ulong)a[8] * b[6]\n    + (ulong)a[9] * b[5];\n\n  /* [d 0 0 0 0 t9 0 0 0 0 c t3 t2 t1 t0] = [p14 p13 p12 p11 p10 p9 0 0 0 0 p4 p3 p2 p1 p0] */\n  u4 = d & M; d >>= 26; c += u4 * R0;\n\n  /* VERIFY_BITS(c, 64); */\n  /* [d u4 0 0 0 0 t9 0 0 0 0 c-u4*R0 t3 t2 t1 t0] = [p14 p13 p12 p11 p10 p9 0 0 0 0 p4 p3 p2 p1 p0] */\n  t4 = c & M; c >>= 26; c += u4 * R1;\n\n  /* [d u4 0 0 0 0 t9 0 0 0 c-u4*R1 t4-u4*R0 t3 t2 t1 t0] = [p14 p13 p12 p11 p10 p9 0 0 0 0 p4 p3 p2 p1 p0] */\n  /* [d 0 0 0 0 0 t9 0 0 0 c t4 t3 t2 t1 t0] = [p14 p13 p12 p11 p10 p9 0 0 0 0 p4 p3 p2 p1 p0] */\n\n  c += (ulong)a[0] * b[5]\n    + (ulong)a[1] * b[4]\n    + (ulong)a[2] * b[3]\n    + (ulong)a[3] * b[2]\n    + (ulong)a[4] * b[1]\n    + (ulong)a[5] * b[0];\n\n  /* [d 0 0 0 0 0 t9 0 0 0 c t4 t3 t2 t1 t0] = [p14 p13 p12 p11 p10 p9 0 0 0 p5 p4 p3 p2 p1 p0] */\n  d += (ulong)a[6] * b[9]\n    + (ulong)a[7] * b[8]\n    + (ulong)a[8] * b[7]\n    + (ulong)a[9] * b[6];\n\n  /* [d 0 0 0 0 0 t9 0 0 0 c t4 t3 t2 t1 t0] = [p15 p14 p13 p12 p11 p10 p9 0 0 0 p5 p4 p3 p2 p1 p0] */\n  u5 = d & M; d >>= 26; c += u5 * R0;\n\n  /* VERIFY_BITS(c, 64); */\n  /* [d u5 0 0 0 0 0 t9 0 0 0 c-u5*R0 t4 t3 t2 t1 t0] = [p15 p14 p13 p12 p11 p10 p9 0 0 0 p5 p4 p3 p2 p1 p0] */\n  t5 = c & M; c >>= 26; c += u5 * R1;\n\n  /* [d u5 0 0 0 0 0 t9 0 0 c-u5*R1 t5-u5*R0 t4 t3 t2 t1 t0] = [p15 p14 p13 p12 p11 p10 p9 0 0 0 p5 p4 p3 p2 p1 p0] */\n  /* [d 0 0 0 0 0 0 t9 0 0 c t5 t4 t3 t2 t1 t0] = [p15 p14 p13 p12 p11 p10 p9 0 0 0 p5 p4 p3 p2 p1 p0] */\n\n  c += (ulong)a[0] * b[6]\n    + (ulong)a[1] * b[5]\n    + (ulong)a[2] * b[4]\n    + (ulong)a[3] * b[3]\n    + (ulong)a[4] * b[2]\n    + (ulong)a[5] * b[1]\n    + (ulong)a[6] * b[0];\n\n  /* [d 0 0 0 0 0 0 t9 0 0 c t5 t4 t3 t2 t1 t0] = [p15 p14 p13 p12 p11 p10 p9 0 0 p6 p5 p4 p3 p2 p1 p0] */\n  d += (ulong)a[7] * b[9]\n    + (ulong)a[8] * b[8]\n    + (ulong)a[9] * b[7];\n\n  /* [d 0 0 0 0 0 0 t9 0 0 c t5 t4 t3 t2 t1 t0] = [p16 p15 p14 p13 p12 p11 p10 p9 0 0 p6 p5 p4 p3 p2 p1 p0] */\n  u6 = d & M; d >>= 26; c += u6 * R0;\n\n  /* VERIFY_BITS(c, 64); */\n  /* [d u6 0 0 0 0 0 0 t9 0 0 c-u6*R0 t5 t4 t3 t2 t1 t0] = [p16 p15 p14 p13 p12 p11 p10 p9 0 0 p6 p5 p4 p3 p2 p1 p0] */\n  t6 = c & M; c >>= 26; c += u6 * R1;\n\n  /* [d u6 0 0 0 0 0 0 t9 0 c-u6*R1 t6-u6*R0 t5 t4 t3 t2 t1 t0] = [p16 p15 p14 p13 p12 p11 p10 p9 0 0 p6 p5 p4 p3 p2 p1 p0] */\n  /* [d 0 0 0 0 0 0 0 t9 0 c t6 t5 t4 t3 t2 t1 t0] = [p16 p15 p14 p13 p12 p11 p10 p9 0 0 p6 p5 p4 p3 p2 p1 p0] */\n\n  c += (ulong)a[0] * b[7]\n    + (ulong)a[1] * b[6]\n    + (ulong)a[2] * b[5]\n    + (ulong)a[3] * b[4]\n    + (ulong)a[4] * b[3]\n    + (ulong)a[5] * b[2]\n    + (ulong)a[6] * b[1]\n    + (ulong)a[7] * b[0];\n  /* VERIFY_BITS(c, 64); */\n\n  /* [d 0 0 0 0 0 0 0 t9 0 c t6 t5 t4 t3 t2 t1 t0] = [p16 p15 p14 p13 p12 p11 p10 p9 0 p7 p6 p5 p4 p3 p2 p1 p0] */\n  d += (ulong)a[8] * b[9]\n    + (ulong)a[9] * b[8];\n\n  /* [d 0 0 0 0 0 0 0 t9 0 c t6 t5 t4 t3 t2 t1 t0] = [p17 p16 p15 p14 p13 p12 p11 p10 p9 0 p7 p6 p5 p4 p3 p2 p1 p0] */\n  u7 = d & M; d >>= 26; c += u7 * R0;\n\n  t7 = c & M; c >>= 26; c += u7 * R1;\n\n\n  c += (ulong)a[0] * b[8]\n    + (ulong)a[1] * b[7]\n    + (ulong)a[2] * b[6]\n    + (ulong)a[3] * b[5]\n    + (ulong)a[4] * b[4]\n    + (ulong)a[5] * b[3]\n    + (ulong)a[6] * b[2]\n    + (ulong)a[7] * b[1]\n    + (ulong)a[8] * b[0];\n  /* VERIFY_BITS(c, 64); */\n\n  /* [d 0 0 0 0 0 0 0 0 t9 c t7 t6 t5 t4 t3 t2 t1 t0] = [p17 p16 p15 p14 p13 p12 p11 p10 p9 p8 p7 p6 p5 p4 p3 p2 p1 p0] */\n  d += (ulong)a[9] * b[9];\n\n  /* [d 0 0 0 0 0 0 0 0 t9 c t7 t6 t5 t4 t3 t2 t1 t0] = [p18 p17 p16 p15 p14 p13 p12 p11 p10 p9 p8 p7 p6 p5 p4 p3 p2 p1 p0] */\n  u8 = d & M; d >>= 26; c += u8 * R0;\n\n  /* [d u8 0 0 0 0 0 0 0 0 t9 c-u8*R0 t7 t6 t5 t4 t3 t2 t1 t0] = [p18 p17 p16 p15 p14 p13 p12 p11 p10 p9 p8 p7 p6 p5 p4 p3 p2 p1 p0] */\n\n  r[3] = t3;\n  r[4] = t4;\n  r[5] = t5;\n  r[6] = t6;\n  r[7] = t7;\n  r[8] = c & M; c >>= 26; c += u8 * R1;\n  c   += d * R0 + t9;\n  r[9] = c & (M >> 4); c >>= 22; c += d * (R1 << 4);\n  d    = c * (R0 >> 4) + t0;\n  r[0] = d & M; d >>= 26;\n  d   += c * (R1 >> 4) + t1;\n  r[1] = d & M; d >>= 26;\n  d   += t2;\n  r[2] = d;\n}\n\n__device__\nvoid secp256k1_fe_sqr_inner(uint *r, const uint *a) {\n  ulong c, d;\n  ulong u0, u1, u2, u3, u4, u5, u6, u7, u8;\n  uint t9, t0, t1, t2, t3, t4, t5, t6, t7;\n  const uint M = 0x3FFFFFFUL, R0 = 0x3D10UL, R1 = 0x400UL;\n\n  d  = (ulong)(a[0]*2) * a[9]\n    + (ulong)(a[1]*2) * a[8]\n    + (ulong)(a[2]*2) * a[7]\n    + (ulong)(a[3]*2) * a[6]\n    + (ulong)(a[4]*2) * a[5];\n  t9 = d & M; d >>= 26;\n  c  = (ulong)a[0] * a[0];\n  d += (ulong)(a[1]*2) * a[9]\n    + (ulong)(a[2]*2) * a[8]\n    + (ulong)(a[3]*2) * a[7]\n    + (ulong)(a[4]*2) * a[6]\n    + (ulong)a[5] * a[5];\n  u0 = d & M; d >>= 26; c += u0 * R0;\n  t0 = c & M; c >>= 26; c += u0 * R1;\n  c += (ulong)(a[0]*2) * a[1];\n  d += (ulong)(a[2]*2) * a[9]\n    + (ulong)(a[3]*2) * a[8]\n    + (ulong)(a[4]*2) * a[7]\n    + (ulong)(a[5]*2) * a[6];\n  u1 = d & M; d >>= 26; c += u1 * R0;\n  t1 = c & M; c >>= 26; c += u1 * R1;\n  c += (ulong)(a[0]*2) * a[2]\n    + (ulong)a[1] * a[1];\n  d += (ulong)(a[3]*2) * a[9]\n    + (ulong)(a[4]*2) * a[8]\n    + (ulong)(a[5]*2) * a[7]\n    + (ulong)a[6] * a[6];\n  u2 = d & M; d >>= 26; c += u2 * R0;\n  t2 = c & M; c >>= 26; c += u2 * R1;\n  c += (ulong)(a[0]*2) * a[3]\n    + (ulong)(a[1]*2) * a[2];\n  d += (ulong)(a[4]*2) * a[9]\n    + (ulong)(a[5]*2) * a[8]\n    + (ulong)(a[6]*2) * a[7];\n  u3 = d & M; d >>= 26; c += u3 * R0;\n  t3 = c & M; c >>= 26; c += u3 * R1;\n  c += (ulong)(a[0]*2) * a[4]\n    + (ulong)(a[1]*2) * a[3]\n    + (ulong)a[2] * a[2];\n  d += (ulong)(a[5]*2) * a[9]\n    + (ulong)(a[6]*2) * a[8]\n    + (ulong)a[7] * a[7];\n  u4 = d & M; d >>= 26; c += u4 * R0;\n  t4 = c & M; c >>= 26; c += u4 * R1;\n  c += (ulong)(a[0]*2) * a[5]\n    + (ulong)(a[1]*2) * a[4]\n    + (ulong)(a[2]*2) * a[3];\n  d += (ulong)(a[6]*2) * a[9]\n    + (ulong)(a[7]*2) * a[8];\n  u5 = d & M; d >>= 26; c += u5 * R0;\n  t5 = c & M; c >>= 26; c += u5 * R1;\n  c += (ulong)(a[0]*2) * a[6]\n    + (ulong)(a[1]*2) * a[5]\n    + (ulong)(a[2]*2) * a[4]\n    + (ulong)a[3] * a[3];\n  d += (ulong)(a[7]*2) * a[9]\n    + (ulong)a[8] * a[8];\n  u6 = d & M; d >>= 26; c += u6 * R0;\n  t6 = c & M; c >>= 26; c += u6 * R1;\n  c += (ulong)(a[0]*2) * a[7]\n    + (ulong)(a[1]*2) * a[6]\n    + (ulong)(a[2]*2) * a[5]\n    + (ulong)(a[3]*2) * a[4];\n  d += (ulong)(a[8]*2) * a[9];\n  u7 = d & M; d >>= 26; c += u7 * R0;\n  t7 = c & M; c >>= 26; c += u7 * R1;\n  c += (ulong)(a[0]*2) * a[8]\n    + (ulong)(a[1]*2) * a[7]\n    + (ulong)(a[2]*2) * a[6]\n    + (ulong)(a[3]*2) * a[5]\n    + (ulong)a[4] * a[4];\n  d += (ulong)a[9] * a[9];\n  u8 = d & M; d >>= 26; c += u8 * R0;\n  r[3] = t3;\n  r[4] = t4;\n  r[5] = t5;\n  r[6] = t6;\n  r[7] = t7;\n  r[8] = c & M; c >>= 26; c += u8 * R1;\n  c   += d * R0 + t9;\n  r[9] = c & (M >> 4); c >>= 22; c += d * (R1 << 4);\n  d    = c * (R0 >> 4) + t0;\n  r[0] = d & M; d >>= 26;\n  d   += c * (R1 >> 4) + t1;\n  r[1] = d & M; d >>= 26;\n  d   += t2;\n  r[2] = d;\n}\n\n__device__\nvoid secp256k1_fe_mul(secp256k1_fe *r, const secp256k1_fe *a, const secp256k1_fe * b) {\n  secp256k1_fe_mul_inner(r->n, a->n, b->n);\n}\n\n__device__\nvoid secp256k1_fe_sqr(secp256k1_fe *r, const secp256k1_fe *a) {\n  secp256k1_fe_sqr_inner(r->n, a->n);\n}\n\n__device__\nvoid secp256k1_fe_from_storage(secp256k1_fe *r, const secp256k1_fe_storage *a) {\n  r->n[0] = a->n[0] & 0x3FFFFFFUL;\n  r->n[1] = a->n[0] >> 26 | ((a->n[1] << 6) & 0x3FFFFFFUL);\n  r->n[2] = a->n[1] >> 20 | ((a->n[2] << 12) & 0x3FFFFFFUL);\n  r->n[3] = a->n[2] >> 14 | ((a->n[3] << 18) & 0x3FFFFFFUL);\n  r->n[4] = a->n[3] >> 8 | ((a->n[4] << 24) & 0x3FFFFFFUL);\n  r->n[5] = (a->n[4] >> 2) & 0x3FFFFFFUL;\n  r->n[6] = a->n[4] >> 28 | ((a->n[5] << 4) & 0x3FFFFFFUL);\n  r->n[7] = a->n[5] >> 22 | ((a->n[6] << 10) & 0x3FFFFFFUL);\n  r->n[8] = a->n[6] >> 16 | ((a->n[7] << 16) & 0x3FFFFFFUL);\n  r->n[9] = a->n[7] >> 10;\n}\n\n__device__\nvoid secp256k1_fe_add(secp256k1_fe *r, const secp256k1_fe *a) {\n  r->n[0] += a->n[0];\n  r->n[1] += a->n[1];\n  r->n[2] += a->n[2];\n  r->n[3] += a->n[3];\n  r->n[4] += a->n[4];\n  r->n[5] += a->n[5];\n  r->n[6] += a->n[6];\n  r->n[7] += a->n[7];\n  r->n[8] += a->n[8];\n  r->n[9] += a->n[9];\n}\n\n__device__\nvoid secp256k1_fe_mul_int(secp256k1_fe *r, int a) {\n  r->n[0] *= a;\n  r->n[1] *= a;\n  r->n[2] *= a;\n  r->n[3] *= a;\n  r->n[4] *= a;\n  r->n[5] *= a;\n  r->n[6] *= a;\n  r->n[7] *= a;\n  r->n[8] *= a;\n  r->n[9] *= a;\n}\n\n__device__\nvoid secp256k1_fe_negate(secp256k1_fe *r, const secp256k1_fe *a, int m) {\n  r->n[0] = 0x3FFFC2FUL * 2 * (m + 1) - a->n[0];\n  r->n[1] = 0x3FFFFBFUL * 2 * (m + 1) - a->n[1];\n  r->n[2] = 0x3FFFFFFUL * 2 * (m + 1) - a->n[2];\n  r->n[3] = 0x3FFFFFFUL * 2 * (m + 1) - a->n[3];\n  r->n[4] = 0x3FFFFFFUL * 2 * (m + 1) - a->n[4];\n  r->n[5] = 0x3FFFFFFUL * 2 * (m + 1) - a->n[5];\n  r->n[6] = 0x3FFFFFFUL * 2 * (m + 1) - a->n[6];\n  r->n[7] = 0x3FFFFFFUL * 2 * (m + 1) - a->n[7];\n  r->n[8] = 0x3FFFFFFUL * 2 * (m + 1) - a->n[8];\n  r->n[9] = 0x03FFFFFUL * 2 * (m + 1) - a->n[9];\n}\n\n__device__\nvoid secp256k1_fe_normalize_weak(secp256k1_fe *r) {\n  uint t0 = r->n[0], t1 = r->n[1], t2 = r->n[2], t3 = r->n[3], t4 = r->n[4],\n     t5 = r->n[5], t6 = r->n[6], t7 = r->n[7], t8 = r->n[8], t9 = r->n[9];\n\n  /* Reduce t9 at the start so there will be at most a single carry from the first pass */\n  uint x = t9 >> 22; t9 &= 0x03FFFFFUL;\n\n  /* The first pass ensures the magnitude is 1, ... */\n  t0 += x * 0x3D1UL; t1 += (x << 6);\n  t1 += (t0 >> 26); t0 &= 0x3FFFFFFUL;\n  t2 += (t1 >> 26); t1 &= 0x3FFFFFFUL;\n  t3 += (t2 >> 26); t2 &= 0x3FFFFFFUL;\n  t4 += (t3 >> 26); t3 &= 0x3FFFFFFUL;\n  t5 += (t4 >> 26); t4 &= 0x3FFFFFFUL;\n  t6 += (t5 >> 26); t5 &= 0x3FFFFFFUL;\n  t7 += (t6 >> 26); t6 &= 0x3FFFFFFUL;\n  t8 += (t7 >> 26); t7 &= 0x3FFFFFFUL;\n  t9 += (t8 >> 26); t8 &= 0x3FFFFFFUL;\n\n  r->n[0] = t0; r->n[1] = t1; r->n[2] = t2; r->n[3] = t3; r->n[4] = t4;\n  r->n[5] = t5; r->n[6] = t6; r->n[7] = t7; r->n[8] = t8; r->n[9] = t9;\n}\n\n__device__\nvoid secp256k1_fe_set_int(secp256k1_fe *r, int a) {\n  r->n[0] = a;\n  r->n[1] = r->n[2] = r->n[3] = r->n[4] = r->n[5] = r->n[6] = r->n[7] = r->n[8] = r->n[9] = 0;\n}\n\n__device__\nvoid secp256k1_fe_get_b32(unsigned char *r, const secp256k1_fe *a) {\n  r[0] = (a->n[9] >> 14) & 0xff;\n  r[1] = (a->n[9] >> 6) & 0xff;\n  r[2] = ((a->n[9] & 0x3F) << 2) | ((a->n[8] >> 24) & 0x3);\n  r[3] = (a->n[8] >> 16) & 0xff;\n  r[4] = (a->n[8] >> 8) & 0xff;\n  r[5] = a->n[8] & 0xff;\n  r[6] = (a->n[7] >> 18) & 0xff;\n  r[7] = (a->n[7] >> 10) & 0xff;\n  r[8] = (a->n[7] >> 2) & 0xff;\n  r[9] = ((a->n[7] & 0x3) << 6) | ((a->n[6] >> 20) & 0x3f);\n  r[10] = (a->n[6] >> 12) & 0xff;\n  r[11] = (a->n[6] >> 4) & 0xff;\n  r[12] = ((a->n[6] & 0xf) << 4) | ((a->n[5] >> 22) & 0xf);\n  r[13] = (a->n[5] >> 14) & 0xff;\n  r[14] = (a->n[5] >> 6) & 0xff;\n  r[15] = ((a->n[5] & 0x3f) << 2) | ((a->n[4] >> 24) & 0x3);\n  r[16] = (a->n[4] >> 16) & 0xff;\n  r[17] = (a->n[4] >> 8) & 0xff;\n  r[18] = a->n[4] & 0xff;\n  r[19] = (a->n[3] >> 18) & 0xff;\n  r[20] = (a->n[3] >> 10) & 0xff;\n  r[21] = (a->n[3] >> 2) & 0xff;\n  r[22] = ((a->n[3] & 0x3) << 6) | ((a->n[2] >> 20) & 0x3f);\n  r[23] = (a->n[2] >> 12) & 0xff;\n  r[24] = (a->n[2] >> 4) & 0xff;\n  r[25] = ((a->n[2] & 0xf) << 4) | ((a->n[1] >> 22) & 0xf);\n  r[26] = (a->n[1] >> 14) & 0xff;\n  r[27] = (a->n[1] >> 6) & 0xff;\n  r[28] = ((a->n[1] & 0x3f) << 2) | ((a->n[0] >> 24) & 0x3);\n  r[29] = (a->n[0] >> 16) & 0xff;\n  r[30] = (a->n[0] >> 8) & 0xff;\n  r[31] = a->n[0] & 0xff;\n}\n\n__device__\nvoid secp256k1_fe_inv(secp256k1_fe *r, const secp256k1_fe *a) {\n  secp256k1_fe x2, x3, x6, x9, x11, x22, x44, x88, x176, x220, x223, t1;\n  int j;\n\n  secp256k1_fe_sqr(&x2, a);\n  secp256k1_fe_mul(&x2, &x2, a);\n\n  secp256k1_fe_sqr(&x3, &x2);\n  secp256k1_fe_mul(&x3, &x3, a);\n\n  x6 = x3;\n  for (j=0; j<3; j++) {\n    secp256k1_fe_sqr(&x6, &x6);\n  }\n  secp256k1_fe_mul(&x6, &x6, &x3);\n\n  x9 = x6;\n  for (j=0; j<3; j++) {\n    secp256k1_fe_sqr(&x9, &x9);\n  }\n  secp256k1_fe_mul(&x9, &x9, &x3);\n\n  x11 = x9;\n  for (j=0; j<2; j++) {\n    secp256k1_fe_sqr(&x11, &x11);\n  }\n  secp256k1_fe_mul(&x11, &x11, &x2);\n\n  x22 = x11;\n  for (j=0; j<11; j++) {\n    secp256k1_fe_sqr(&x22, &x22);\n  }\n  secp256k1_fe_mul(&x22, &x22, &x11);\n\n  x44 = x22;\n  for (j=0; j<22; j++) {\n    secp256k1_fe_sqr(&x44, &x44);\n  }\n  secp256k1_fe_mul(&x44, &x44, &x22);\n\n  x88 = x44;\n  for (j=0; j<44; j++) {\n    secp256k1_fe_sqr(&x88, &x88);\n  }\n  secp256k1_fe_mul(&x88, &x88, &x44);\n\n  x176 = x88;\n  for (j=0; j<88; j++) {\n    secp256k1_fe_sqr(&x176, &x176);\n  }\n  secp256k1_fe_mul(&x176, &x176, &x88);\n\n  x220 = x176;\n  for (j=0; j<44; j++) {\n    secp256k1_fe_sqr(&x220, &x220);\n  }\n  secp256k1_fe_mul(&x220, &x220, &x44);\n\n  x223 = x220;\n  for (j=0; j<3; j++) {\n    secp256k1_fe_sqr(&x223, &x223);\n  }\n  secp256k1_fe_mul(&x223, &x223, &x3);\n\n  t1 = x223;\n  for (j=0; j<23; j++) {\n    secp256k1_fe_sqr(&t1, &t1);\n  }\n  secp256k1_fe_mul(&t1, &t1, &x22);\n  for (j=0; j<5; j++) {\n    secp256k1_fe_sqr(&t1, &t1);\n  }\n  secp256k1_fe_mul(&t1, &t1, a);\n  for (j=0; j<3; j++) {\n    secp256k1_fe_sqr(&t1, &t1);\n  }\n  secp256k1_fe_mul(&t1, &t1, &x2);\n  for (j=0; j<2; j++) {\n    secp256k1_fe_sqr(&t1, &t1);\n  }\n  secp256k1_fe_mul(r, a, &t1);\n}\n\n__device__\nvoid secp256k1_ge_from_storage(secp256k1_ge *r,  const secp256k1_ge_storage *a) {\n  secp256k1_fe_from_storage(&r->x, &a->x);\n  secp256k1_fe_from_storage(&r->y, &a->y);\n}\n\n__device__\nvoid secp256k1_gej_add_ge_var(secp256k1_gej *r, const secp256k1_gej *a, const secp256k1_ge *b, secp256k1_fe *rzr) {\n  /* 8 mul, 3 sqr, 4 normalize, 12 mul_int/add/negate */\n  secp256k1_fe z12, u1, u2, s1, s2, h, i, i2, h2, h3, t;\n\n  secp256k1_fe_sqr(&z12, &a->z);\n  u1 = a->x; secp256k1_fe_normalize_weak(&u1);\n  secp256k1_fe_mul(&u2, &b->x, &z12);\n  s1 = a->y; secp256k1_fe_normalize_weak(&s1);\n  secp256k1_fe_mul(&s2, &b->y, &z12); secp256k1_fe_mul(&s2, &s2, &a->z);\n  secp256k1_fe_negate(&h, &u1, 1); secp256k1_fe_add(&h, &u2);\n  secp256k1_fe_negate(&i, &s1, 1); secp256k1_fe_add(&i, &s2);\n  secp256k1_fe_sqr(&i2, &i);\n  secp256k1_fe_sqr(&h2, &h);\n  secp256k1_fe_mul(&h3, &h, &h2);\n  if (rzr) {\n    *rzr = h;\n  }\n  secp256k1_fe_mul(&r->z, &a->z, &h);\n  secp256k1_fe_mul(&t, &u1, &h2);\n  r->x = t; secp256k1_fe_mul_int(&r->x, 2); secp256k1_fe_add(&r->x, &h3); secp256k1_fe_negate(&r->x, &r->x, 3); secp256k1_fe_add(&r->x, &i2);\n  secp256k1_fe_negate(&r->y, &r->x, 5); secp256k1_fe_add(&r->y, &t); secp256k1_fe_mul(&r->y, &r->y, &i);\n  secp256k1_fe_mul(&h3, &h3, &s1); secp256k1_fe_negate(&h3, &h3, 1);\n  secp256k1_fe_add(&r->y, &h3);\n}\n\n__device__\nvoid secp256k1_gej_set_ge(secp256k1_gej *r, const secp256k1_ge *a) {\n  r->x = a->x;\n  r->y = a->y;\n  secp256k1_fe_set_int(&r->z, 1);\n}\n\n__global__ void secp256k1(const secp256k1_ge_storage *prec, unsigned char* result)\n{\n  secp256k1_ge ge[512];\n  secp256k1_gej sum;\n\n  secp256k1_ge_from_storage(&ge[0], &prec[0]);\n  secp256k1_gej_set_ge(&sum, &ge[0]);\n  secp256k1_fe z_all = sum.z;\n\n  for (uint i=1; i<512; ++i) {\n    secp256k1_ge_from_storage(&ge[i], &prec[i]);\n    secp256k1_gej_add_ge_var(&sum, &sum, &ge[i], 0);\n    secp256k1_fe_mul(&z_all, &z_all, &sum.z);\n  }\n  secp256k1_fe_inv(&z_all, &z_all);\n  secp256k1_fe_get_b32(result, &z_all);\n}"
        ]
    },
    "surfel-cuda": {
        "/Users/gbolet/hecbench-roofline/src/surfel-cuda/main.cu": [
            "#define T ((int)32)\n\n\n__global__ void surfel_render(\n  const T *__restrict__ s,\n  int N,\n  T f,\n  int w,\n  int h,\n  T *__restrict__ d)\n{\n  const int idx = threadIdx.x + blockIdx.x*blockDim.x;\n  const int idy = threadIdx.y + blockIdx.y*blockDim.y;\n\n  if(idx < w && idy < h)\n  {\n    T ray[3];\n    ray[0] = T(idx)-(w-1)*(T)0.5;\n    ray[1] = T(idy)-(h-1)*(T)0.5;\n    ray[2] = f;\n    T pt[3];\n    T n[3];\n    T p[3];\n    T dMin = 1e20;\n    \n    for (int i=0; i<N; ++i) {\n      p[0] = s[i*COL_DIM+COL_P_X];\n      p[1] = s[i*COL_DIM+COL_P_Y];\n      p[2] = s[i*COL_DIM+COL_P_Z];\n      n[0] = s[i*COL_DIM+COL_N_X];\n      n[1] = s[i*COL_DIM+COL_N_Y];\n      n[2] = s[i*COL_DIM+COL_N_Z];\n      T rSqMax = s[i*COL_DIM+COL_RSq];\n      T pDotn = p[0]*n[0]+p[1]*n[1]+p[2]*n[2];\n      T dsDotRay = ray[0]*n[0] + ray[1]*n[1] + ray[2]*n[2];\n      T alpha = pDotn / dsDotRay;\n      pt[0] = ray[0]*alpha - p[0];\n      pt[1] = ray[1]*alpha - p[1];\n      pt[2] = ray[2]*alpha - p[2];\n      T t = ray[2]*alpha;\n      T rSq = pt[0] * pt[0] + pt[1] * pt[1] + pt[2] * pt[2];\n      if (rSq < rSqMax && dMin > t) {\n        dMin = t; // ray hit the surfel \n      }\n    }\n    d[idy*w+idx] = dMin > (T)100 ? (T)0 : dMin;\n  }\n}"
        ]
    },
    "tridiagonal-cuda": {
        "/Users/gbolet/hecbench-roofline/src/tridiagonal-cuda/cyclic_kernels.cu": [
            "__global__ void cyclic_small_systems_kernel(\n    const float*__restrict__ a_d, \n    const float*__restrict__ b_d, \n    const float*__restrict__ c_d, \n    const float*__restrict__ d_d, \n          float*__restrict__ x_d, \n    const int system_size, \n    const int num_systems, \n    const int iterations)\n{\n  extern __shared__ float shared[];\n\n  int thid = threadIdx.x;\n  int blid = blockIdx.x;\n\n  int stride = 1;\n  int half_size = system_size >> 1;\n  int thid_num = half_size;\n\n  float* a = shared;\n  float* b = &a[system_size];\n  float* c = &b[system_size];\n  float* d = &c[system_size];\n  float* x = &d[system_size];\n\n  a[thid] = a_d[thid + blid * system_size];\n  a[thid + thid_num] = a_d[thid + thid_num + blid * system_size];\n\n  b[thid] = b_d[thid + blid * system_size];\n  b[thid + thid_num] = b_d[thid + thid_num + blid * system_size];\n\n  c[thid] = c_d[thid + blid * system_size];\n  c[thid + thid_num] = c_d[thid + thid_num + blid * system_size];\n\n  d[thid] = d_d[thid + blid * system_size];\n  d[thid + thid_num] = d_d[thid + thid_num + blid * system_size];\n\n  __syncthreads();\n\n  // forward elimination\n  for (int j = 0; j < iterations; j++)\n  {\n    __syncthreads();\n\n    stride <<= 1;\n    int delta = stride >> 1;\n    if (thid < thid_num)\n    { \n      int i = stride * thid + stride - 1;\n\n      if (i == system_size - 1)\n      {\n#ifndef NATIVE_DIVIDE\n        float tmp = a[i] / b[i-delta];\n#else\n        float tmp = __fdiv_rn(a[i], b[i-delta]);\n#endif\n        b[i] = b[i] - c[i-delta] * tmp;\n        d[i] = d[i] - d[i-delta] * tmp;\n        a[i] = -a[i-delta] * tmp;\n        c[i] = 0;      \n      }\n      else\n      {\n#ifndef NATIVE_DIVIDE\n        float tmp1 = a[i] / b[i-delta];\n        float tmp2 = c[i] / b[i+delta];\n#else\n        float tmp1 = __fdiv_rn(a[i], b[i-delta]);\n        float tmp2 = __fdiv_rn(c[i], b[i+delta]);\n#endif\n        b[i] = b[i] - c[i-delta] * tmp1 - a[i+delta] * tmp2;\n        d[i] = d[i] - d[i-delta] * tmp1 - d[i+delta] * tmp2;\n        a[i] = -a[i-delta] * tmp1;\n        c[i] = -c[i+delta] * tmp2;\n      }\n    }\n    thid_num >>= 1;\n  }\n\n  if (thid < 2)\n  {\n    int addr1 = stride - 1;\n    int addr2 = (stride << 1) - 1;\n    float tmp3 = b[addr2] * b[addr1] - c[addr1] * a[addr2];\n#ifndef NATIVE_DIVIDE\n    x[addr1] = (b[addr2] * d[addr1] - c[addr1] * d[addr2]) / tmp3;\n    x[addr2] = (d[addr2] * b[addr1] - d[addr1] * a[addr2]) / tmp3;\n#else\n    x[addr1] = __fdiv_rn((b[addr2] * d[addr1] - c[addr1] * d[addr2]), tmp3);\n    x[addr2] = __fdiv_rn((d[addr2] * b[addr1] - d[addr1] * a[addr2]), tmp3);\n#endif\n  }\n\n  // backward substitution\n  thid_num = 2;\n  for (int j = 0; j < iterations; j++)\n  {\n    int delta = stride >> 1;\n    __syncthreads();\n    if (thid < thid_num)\n    {\n      int i = stride * thid + (stride >> 1) - 1;\n#ifndef NATIVE_DIVIDE\n      if (i == delta - 1)\n        x[i] = (d[i] - c[i] * x[i+delta]) / b[i];\n      else\n        x[i] = (d[i] - a[i] * x[i-delta] - c[i] * x[i+delta]) / b[i];\n#else\n      if (i == delta - 1)\n        x[i] = __fdiv_rn((d[i] - c[i] * x[i+delta]), b[i]);\n      else\n        x[i] = __fdiv_rn((d[i] - a[i] * x[i-delta] - c[i] * x[i+delta]), b[i]);\n#endif\n    }\n    stride >>= 1;\n    thid_num <<= 1;\n  }\n\n  __syncthreads();   \n\n  x_d[thid + blid * system_size] = x[thid];\n  x_d[thid + half_size + blid * system_size] = x[thid + half_size];\n}",
            "__global__ void cyclic_branch_free_kernel(\n    const float*__restrict__ a_d, \n    const float*__restrict__ b_d, \n    const float*__restrict__ c_d, \n    const float*__restrict__ d_d, \n          float*__restrict__ x_d, \n    const int system_size, \n    const int num_systems, \n    const int iterations)\n{\n  extern __shared__ float shared[];\n\n  int thid = threadIdx.x;\n  int blid = blockIdx.x;\n\n  int stride = 1;\n  int half_size = system_size >> 1;\n  int thid_num = half_size;\n\n  float* a = shared;\n  float* b = &a[system_size];\n  float* c = &b[system_size];\n  float* d = &c[system_size];\n  float* x = &d[system_size];\n\n  a[thid] = a_d[thid + blid * system_size];\n  a[thid + thid_num] = a_d[thid + thid_num + blid * system_size];\n\n  b[thid] = b_d[thid + blid * system_size];\n  b[thid + thid_num] = b_d[thid + thid_num + blid * system_size];\n\n  c[thid] = c_d[thid + blid * system_size];\n  c[thid + thid_num] = c_d[thid + thid_num + blid * system_size];\n\n  d[thid] = d_d[thid + blid * system_size];\n  d[thid + thid_num] = d_d[thid + thid_num + blid * system_size];\n\n  __syncthreads();\n\n  // forward elimination\n  for (int j = 0; j < iterations; j++)\n  {\n    __syncthreads();\n\n    stride <<= 1;\n    int delta = stride >> 1;\n    if (thid < thid_num)\n    { \n      int i = stride * thid + stride - 1;\n      int iRight = i+delta;\n      iRight = iRight & (system_size-1);\n#ifndef NATIVE_DIVIDE\n      float tmp1 = a[i] / b[i-delta];\n      float tmp2 = c[i] / b[iRight];\n#else\n      float tmp1 = __fdiv_rn(a[i], b[i-delta]);\n      float tmp2 = __fdiv_rn(c[i], b[iRight]);\n#endif\n      b[i] = b[i] - c[i-delta] * tmp1 - a[iRight] * tmp2;\n      d[i] = d[i] - d[i-delta] * tmp1 - d[iRight] * tmp2;\n      a[i] = -a[i-delta] * tmp1;\n      c[i] = -c[iRight]  * tmp2;\n    }\n\n    thid_num >>= 1;\n  }\n\n  if (thid < 2)\n  {\n    int addr1 = stride - 1;\n    int addr2 = (stride << 1) - 1;\n    float tmp3 = b[addr2] * b[addr1] - c[addr1] * a[addr2];\n#ifndef NATIVE_DIVIDE\n    x[addr1] = (b[addr2] * d[addr1] - c[addr1] * d[addr2]) / tmp3;\n    x[addr2] = (d[addr2] * b[addr1] - d[addr1] * a[addr2]) / tmp3;\n#else\n    x[addr1] = __fdiv_rn((b[addr2] * d[addr1] - c[addr1] * d[addr2]), tmp3);\n    x[addr2] = __fdiv_rn((d[addr2] * b[addr1] - d[addr1] * a[addr2]), tmp3);\n#endif\n  }\n\n  // backward substitution\n  thid_num = 2;\n  for (int j = 0; j < iterations; j++)\n  {\n    int delta = stride >> 1;\n    __syncthreads();\n    if (thid < thid_num)\n    {\n      int i = stride * thid + (stride >> 1) - 1;\n#ifndef NATIVE_DIVIDE\n      if (i == delta - 1)\n        x[i] = (d[i] - c[i] * x[i+delta]) / b[i];\n      else\n        x[i] = (d[i] - a[i] * x[i-delta] - c[i] * x[i+delta]) / b[i];\n#else\n      if (i == delta - 1)\n        x[i] = __fdiv_rn((d[i] - c[i] * x[i+delta]), b[i]);\n      else\n        x[i] = __fdiv_rn((d[i] - a[i] * x[i-delta] - c[i] * x[i+delta]), b[i]);\n#endif\n    }\n    stride >>= 1;\n    thid_num <<= 1;\n  }\n\n  __syncthreads();   \n\n  x_d[thid + blid * system_size] = x[thid];\n  x_d[thid + half_size + blid * system_size] = x[thid + half_size];\n}"
        ],
        "/Users/gbolet/hecbench-roofline/src/tridiagonal-cuda/pcr_kernels.cu": [
            "__global__ void pcr_small_systems_kernel(\n    const float*__restrict__ a_d, \n    const float*__restrict__ b_d, \n    const float*__restrict__ c_d, \n    const float*__restrict__ d_d, \n          float*__restrict__ x_d, \n    const int system_size, \n    const int num_systems, \n    const int iterations)\n{\n  extern __shared__ float shared[];\n\n  int thid = threadIdx.x;\n  int blid = blockIdx.x;\n\n  int delta = 1;\n\n  float* a = shared;\n  float* b = &a[system_size+1];\n  float* c = &b[system_size+1];\n  float* d = &c[system_size+1];\n  float* x = &d[system_size+1];\n\n  a[thid] = a_d[thid + blid * system_size];\n  b[thid] = b_d[thid + blid * system_size];\n  c[thid] = c_d[thid + blid * system_size];\n  d[thid] = d_d[thid + blid * system_size];\n\n  float aNew, bNew, cNew, dNew;\n\n  __syncthreads();\n\n  // parallel cyclic reduction\n  for (int j = 0; j < iterations; j++)\n  {\n    int i = thid;\n\n    if(i < delta)\n    {\n#ifndef NATIVE_DIVIDE\n      float tmp2 = c[i] / b[i+delta];\n#else\n      float tmp2 = __fdiv_rn(c[i], b[i+delta]);\n#endif\n      bNew = b[i] - a[i+delta] * tmp2;\n      dNew = d[i] - d[i+delta] * tmp2;\n      aNew = 0;\n      cNew = -c[i+delta] * tmp2;  \n    }\n    else if((system_size-i-1) < delta)\n    {\n#ifndef NATIVE_DIVIDE\n      float tmp = a[i] / b[i-delta];\n#else\n      float tmp = __fdiv_rn(a[i], b[i-delta]);\n#endif\n      bNew = b[i] - c[i-delta] * tmp;\n      dNew = d[i] - d[i-delta] * tmp;\n      aNew = -a[i-delta] * tmp;\n      cNew = 0;      \n    }\n    else        \n    {\n#ifndef NATIVE_DIVIDE\n      float tmp1 = a[i] / b[i-delta];\n      float tmp2 = c[i] / b[i+delta];\n#else\n      float tmp1 = __fdiv_rn(a[i], b[i-delta]);\n      float tmp2 = __fdiv_rn(c[i], b[i+delta]);\n#endif\n      bNew = b[i] - c[i-delta] * tmp1 - a[i+delta] * tmp2;\n      dNew = d[i] - d[i-delta] * tmp1 - d[i+delta] * tmp2;\n      aNew = -a[i-delta] * tmp1;\n      cNew = -c[i+delta] * tmp2;\n    }\n\n    __syncthreads();\n\n    b[i] = bNew;\n    d[i] = dNew;\n    a[i] = aNew;\n    c[i] = cNew;  \n\n    delta *= 2;\n    __syncthreads();\n  }\n\n  if (thid < delta)\n  {\n    int addr1 = thid;\n    int addr2 = thid + delta;\n    float tmp3 = b[addr2] * b[addr1] - c[addr1] * a[addr2];\n#ifndef NATIVE_DIVIDE\n    x[addr1] = (b[addr2] * d[addr1] - c[addr1] * d[addr2]) / tmp3;\n    x[addr2] = (d[addr2] * b[addr1] - d[addr1] * a[addr2]) / tmp3;\n#else\n    x[addr1] = __fdiv_rn((b[addr2] * d[addr1] - c[addr1] * d[addr2]), tmp3);\n    x[addr2] = __fdiv_rn((d[addr2] * b[addr1] - d[addr1] * a[addr2]), tmp3);\n#endif\n  }\n\n  __syncthreads();\n\n  x_d[thid + blid * system_size] = x[thid];\n}",
            "__global__ void pcr_branch_free_kernel(\n    const float*__restrict__ a_d, \n    const float*__restrict__ b_d, \n    const float*__restrict__ c_d, \n    const float*__restrict__ d_d, \n          float*__restrict__ x_d, \n    const int system_size, \n    const int num_systems, \n    const int iterations)\n{\n  extern __shared__ float shared[];\n\n  int thid = threadIdx.x;\n  int blid = blockIdx.x;\n\n  int delta = 1;\n\n  float* a = shared;\n  float* b = &a[system_size+1];\n  float* c = &b[system_size+1];\n  float* d = &c[system_size+1];\n  float* x = &d[system_size+1];\n\n  a[thid] = a_d[thid + blid * system_size];\n  b[thid] = b_d[thid + blid * system_size];\n  c[thid] = c_d[thid + blid * system_size];\n  d[thid] = d_d[thid + blid * system_size];\n\n  float aNew, bNew, cNew, dNew;\n\n  __syncthreads();\n\n  // parallel cyclic reduction\n  for (int j = 0; j < iterations; j++)\n  {\n    int i = thid;\n\n    int iRight = i+delta;\n    iRight = iRight & (system_size-1);\n\n    int iLeft = i-delta;\n    iLeft = iLeft & (system_size-1);\n\n#ifndef NATIVE_DIVIDE\n    float tmp1 = a[i] / b[iLeft];\n    float tmp2 = c[i] / b[iRight];\n#else\n    float tmp1 = __fdiv_rn(a[i], b[iLeft]);\n    float tmp2 = __fdiv_rn(c[i], b[iRight]);\n#endif\n\n    bNew = b[i] - c[iLeft] * tmp1 - a[iRight] * tmp2;\n    dNew = d[i] - d[iLeft] * tmp1 - d[iRight] * tmp2;\n    aNew = -a[iLeft] * tmp1;\n    cNew = -c[iRight] * tmp2;\n\n    __syncthreads();\n\n    b[i] = bNew;\n    d[i] = dNew;\n    a[i] = aNew;\n    c[i] = cNew;  \n\n    delta *= 2;\n    __syncthreads();\n  }\n\n  if (thid < delta)\n  {\n    int addr1 = thid;\n    int addr2 = thid + delta;\n    float tmp3 = b[addr2] * b[addr1] - c[addr1] * a[addr2];\n#ifndef NATIVE_DIVIDE\n    x[addr1] = (b[addr2] * d[addr1] - c[addr1] * d[addr2]) / tmp3;\n    x[addr2] = (d[addr2] * b[addr1] - d[addr1] * a[addr2]) / tmp3;\n#else\n    x[addr1] = __fdiv_rn((b[addr2] * d[addr1] - c[addr1] * d[addr2]), tmp3);\n    x[addr2] = __fdiv_rn((d[addr2] * b[addr1] - d[addr1] * a[addr2]), tmp3);\n#endif\n  }\n\n  __syncthreads();\n\n  x_d[thid + blid * system_size] = x[thid];\n}"
        ],
        "/Users/gbolet/hecbench-roofline/src/tridiagonal-cuda/sweep_kernels.cu": [
            "__global__ void sweep_small_systems_local_kernel(\n    const float*__restrict__ a_d, \n    const float*__restrict__ b_d, \n    const float*__restrict__ c_d, \n    const float*__restrict__ d_d, \n          float*__restrict__ x_d, \n    const int system_size, \n    const int num_systems,\n    const bool reorder)\n{\n  int i = blockIdx.x * blockDim.x + threadIdx.x;\n\n  // need to check for in-bounds because of the thread block size\n  if (i >= num_systems) return;\n\n  int stride = reorder ? num_systems: 1;\n  int base_idx = reorder ? i : i * system_size;\n\n  // local memory\n  float a[128];\n\n  float c1, c2, c3;\n  float f_i, x_prev, x_next;\n\n  // solving next system:  \n  // c1 * u_i+1 + c2 * u_i + c3 * u_i-1 = f_i\n\n  c1 = c_d[base_idx];\n  c2 = b_d[base_idx];\n  f_i = d_d[base_idx];\n\n#ifndef NATIVE_DIVIDE\n  a[1] = - c1 / c2;\n  x_prev = f_i / c2;\n#else\n  a[1] = - __fdiv_rn(c1, c2);\n  x_prev = __fdiv_rn(f_i, c2);\n#endif\n\n  // forward trace\n  int idx = base_idx;\n  x_d[base_idx] = x_prev;\n  for (int k = 1; k < system_size-1; k++)\n  {\n    idx += stride;\n\n    c1 = c_d[idx];\n    c2 = b_d[idx];\n    c3 = a_d[idx];\n    f_i = d_d[idx];\n\n    float q = (c3 * a[k] + c2);\n#ifndef NATIVE_DIVIDE\n    float t = 1 / q; \n#else\n    float t = __frcp_rn(q);\n#endif\n    x_next = (f_i - c3 * x_prev) * t;\n    x_d[idx] = x_prev = x_next;\n\n    a[k+1] = - c1 * t;\n  }\n\n  idx += stride;\n\n  c2 = b_d[idx];\n  c3 = a_d[idx];\n  f_i = d_d[idx];\n\n  float q = (c3 * a[system_size-1] + c2);\n#ifndef NATIVE_DIVIDE\n  float t = 1 / q; \n#else\n  float t = __frcp_rn(q);\n#endif \n  x_next = (f_i - c3 * x_prev) * t;\n  x_d[idx] = x_prev = x_next;\n\n  // backward trace\n  for (int k = system_size-2; k >= 0; k--)\n  {\n    idx -= stride;\n    x_next = x_d[idx];\n    x_next += x_prev * a[k+1];\n    x_d[idx] = x_prev = x_next;\n  }\n}",
            "__device__\ninline int getLocalIdx(int i, int k, int num_systems)\n{\n  return i + num_systems * k;\n\n  // uncomment for uncoalesced mem access\n  // return k + system_size * i;\n}\n\n__global__ void sweep_small_systems_global_kernel(\n    const float*__restrict__ a_d, \n    const float*__restrict__ b_d, \n    const float*__restrict__ c_d, \n    const float*__restrict__ d_d, \n          float*__restrict__ x_d, \n          float*__restrict__ w_d, \n    const int system_size, \n    const int num_systems,\n    const bool reorder)\n{\n  int i = blockIdx.x * blockDim.x + threadIdx.x;\n\n  // need to check for in-bounds because of the thread block size\n  if (i >= num_systems) return;\n\n  int stride = reorder ? num_systems: 1;\n  int base_idx = reorder ? i : i * system_size;\n\n  float c1, c2, c3;\n  float f_i, x_prev, x_next;\n\n  // solving next system:  \n  // c1 * u_i+1 + c2 * u_i + c3 * u_i-1 = f_i\n\n  c1 = c_d[base_idx];\n  c2 = b_d[base_idx];\n  f_i = d_d[base_idx];\n\n#ifndef NATIVE_DIVIDE\n  w_d[getLocalIdx(i, 1, num_systems)] = - c1 / c2;\n  x_prev = f_i / c2;\n#else\n  w_d[getLocalIdx(i, 1, num_systems)] = __fdiv_rn(-c1, c2);\n  x_prev = __fdiv_rn(f_i, c2);\n#endif\n\n  // forward trace\n  int idx = base_idx;\n  x_d[base_idx] = x_prev;\n  for (int k = 1; k < system_size-1; k++)\n  {\n    idx += stride;\n\n    c1 = c_d[idx];\n    c2 = b_d[idx];\n    c3 = a_d[idx];\n    f_i = d_d[idx];\n\n    float q = (c3 * w_d[getLocalIdx(i, k, num_systems)] + c2);\n#ifndef NATIVE_DIVIDE\n    float t = 1 / q; \n#else\n    float t = __frcp_rn(q);\n#endif\n    x_next = (f_i - c3 * x_prev) * t;\n    x_d[idx] = x_prev = x_next;\n\n    w_d[getLocalIdx(i, k+1, num_systems)] = - c1 * t;\n  }\n\n  idx += stride;\n\n  c2 = b_d[idx];\n  c3 = a_d[idx];\n  f_i = d_d[idx];\n\n  float q = (c3 * w_d[getLocalIdx(i, system_size-1, num_systems)] + c2);\n#ifndef NATIVE_DIVIDE\n  float t = 1 / q; \n#else\n  float t = __frcp_rn(q);\n#endif \n  x_next = (f_i - c3 * x_prev) * t;\n  x_d[idx] = x_prev = x_next;\n\n  // backward trace\n  for (int k = system_size-2; k >= 0; k--)\n  {\n    idx -= stride;\n    x_next = x_d[idx];\n    x_next += x_prev * w_d[getLocalIdx(i, k+1, num_systems)];\n    x_d[idx] = x_prev = x_next;\n  }\n}",
            "inline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\n__device__\ninline int getLocalIdx(int i, int k, int num_systems)\n{\n  return i + num_systems * k;\n\n  // uncomment for uncoalesced mem access\n  // return k + system_size * i;\n}\n\n__device__ inline void load(args_t *args, int idx) {\n    constexpr int arity = std::tuple_size<args_t>::value;\n    int thread_idx = threadIdx.x;\n    #pragma unroll\n    for (int i = 0; i < thread_work_size(); i++) {\n      if (thread_idx >= remaining) {\n        return;\n      }\n      int linear_idx = thread_idx + block_work_size() * idx;\n      auto offset = input_offset_calculator.get(linear_idx);\n      static_unroll<unroll_load_helper, arity>::with_args(*this, args, offset, loader, i, num_outputs);\n      thread_idx += num_threads();\n    }\n  }\n\ninline __device__ float4 native_divide(float4 a, float4 b)\n{\n  return make_float4(\n  __fdiv_rn(a.x , b.x),\n  __fdiv_rn(a.y , b.y),\n  __fdiv_rn(a.z , b.z),\n  __fdiv_rn(a.w , b.w));\n}\n\ninline __device__ float4 native_recip(float4 a)\n{\n  return make_float4(\n  __frcp_rn(a.x),\n  __frcp_rn(a.y),\n  __frcp_rn(a.z),\n  __frcp_rn(a.w));\n}\n\n__device__ inline void store(return_t *from, int idx) {\n    int thread_idx = threadIdx.x;\n    #pragma unroll\n    for (int i = 0; i < thread_work_size(); i++) {\n      if (thread_idx >= this->remaining) {\n        return;\n      }\n      int linear_idx = thread_idx + block_work_size() * idx;\n      auto offsets = this->output_offset_calculator.get(linear_idx);\n      static_unroll<multi_outputs_store_helper, num_outputs>::with_args(this->data, offsets, from[i]);\n      thread_idx += num_threads();\n    }\n  }\n\n__global__ void sweep_small_systems_global_vec4_kernel(\n    const float*__restrict__ a_d, \n    const float*__restrict__ b_d, \n    const float*__restrict__ c_d, \n    const float*__restrict__ d_d, \n          float*__restrict__ x_d, \n          float*__restrict__ w_d, \n    const int system_size, \n    const int num_systems,\n    const bool reorder)\n{\n  int j = blockIdx.x * blockDim.x + threadIdx.x;\n  int i = j << 2;\n\n  // need to check for in-bounds because of the thread block size\n  if (i >= num_systems) return;\n\n  int stride = reorder ? num_systems: 4;\n  int base_idx = reorder ? i : i * system_size;\n\n  float4 c1, c2, c3;\n  float4 f_i, x_prev, x_next;\n\n  // solving next system:  \n  // c1 * u_i+1 + c2 * u_i + c3 * u_i-1 = f_i\n\n  c1 = load(c_d, base_idx);\n  c2 = load(b_d, base_idx);\n  f_i = load(d_d, base_idx);\n\n#ifndef NATIVE_DIVIDE\n  store(w_d, getLocalIdx(i, 1, num_systems), - c1 / c2);\n  x_prev = f_i / c2;\n#else\n  store(w_d, getLocalIdx(i, 1, num_systems), native_divide(-c1, c2));\n  x_prev = native_divide(f_i, c2);\n#endif\n\n  // forward trace\n  int idx = base_idx;\n  store(x_d, base_idx, x_prev);\n  for (int k = 1; k < system_size-1; k++)\n  {\n    idx += stride;\n\n    c1 = load(c_d, idx);\n    c2 = load(b_d, idx);\n    c3 = load(a_d, idx);\n    f_i = load(d_d, idx);\n\n    float4 q = (c3 * load(w_d, getLocalIdx(i, k, num_systems)) + c2);\n#ifndef NATIVE_DIVIDE\n    float4 t = make_float4(1,1,1,1) / q; \n#else\n    float4 t = native_recip(q);\n#endif\n    x_next = (f_i - c3 * x_prev) * t;\n    x_prev = x_next;\n    store(x_d, idx, x_prev);\n\n    store(w_d, getLocalIdx(i, k+1, num_systems), - c1 * t);\n  }\n\n  idx += stride;\n\n  c2 = load(b_d, idx);\n  c3 = load(a_d, idx);\n  f_i = load(d_d, idx);\n\n  float4 q = (c3 * load(w_d, getLocalIdx(i, system_size-1, num_systems)) + c2);\n#ifndef NATIVE_DIVIDE\n  float4 t = make_float4(1,1,1,1) / q; \n#else\n  float4 t = native_recip(q);\n#endif \n  x_next = (f_i - c3 * x_prev) * t;\n  x_prev = x_next;\n  store(x_d, idx, x_prev);\n\n  // backward trace\n  for (int k = system_size-2; k >= 0; k--)\n  {\n    idx -= stride;\n    x_next = load(x_d, idx);\n    x_next += x_prev * load(w_d, getLocalIdx(i, k+1, num_systems));\n    x_prev = x_next;\n    store(x_d, idx, x_prev); \n  }\n}"
        ]
    },
    "metropolis-cuda": {
        "/Users/gbolet/hecbench-roofline/src/metropolis-cuda/kernel_reduction.h": [
            "#define T ((int)32)\n\n\n__device__ __forceinline__ T\nWARP_SHFL_XOR_NATIVE(T value, int laneMask, int width = warpSize,\n                     unsigned int mask = 0xffffffff) {\n  return __shfl_xor_sync(mask, value, laneMask, width);\n}\n\n__device__ __forceinline__ void warp_reduce(acc_t *sum) {\n  ReduceOp<acc_t> r;\n#pragma unroll\n  for (int offset = WARP_SIZE / 2; offset > 0; offset /= 2) {\n#pragma unroll\n    for (int i = 0; i < WARP_BATCH; ++i) {\n      acc_t b = WARP_SHFL_XOR_NATIVE(sum[i], offset, WARP_SIZE);\n      sum[i] = r(sum[i], b);\n    }\n  }\n}\n\n__device__ double C(int l)\n{\n  return (2.0*l-1)*2.0*l*(2.0*l+2)/(4.0*l-3.0)/(4.0*l-1.0);\n}\n\n__inline__ __device__ float block_reduce(T val)\n{\n  static __shared__ T shared[WARPSIZE];\n  int tid = threadIdx.z * BY * BX + threadIdx.y * BX + threadIdx.x;\n  int lane = tid & (WARPSIZE-1);\n  int wid = tid/WARPSIZE;\n  val = warp_reduce<T>(val);\n\n  if(lane == 0)\n    shared[wid] = val;\n\n  __syncthreads();\n\n  val = (tid < (blockDim.x * blockDim.y * blockDim.z)/WARPSIZE) ? shared[lane] : 0;\n  if(wid == 0){\n    val = warp_reduce<T>(val);\n  }\n  return val;\n}\n\n__global__ void kernel_redenergy(const int *s, int L, T *out, const int *H, float h)\n{\n  // offsets\n  int x = blockIdx.x *blockDim.x + threadIdx.x;\n  int y = blockIdx.y *blockDim.y + threadIdx.y;\n  int z = blockIdx.z *blockDim.z + threadIdx.z;\n  int tid = threadIdx.z * BY * BX + threadIdx.y * BX + threadIdx.x;\n  int id = C(x,y,z,L);\n  // this optimization only works for L being a power of 2\n  //float sum = -(float)(s[id] * ((float)(s[C((x+1) & (L-1), y, z, L)] + \n  // s[C(x, (y+1) & (L-1), z, L)] + s[C(x, y, (z+1) & (L-1), L)]) + h*H[id]));\n\n  // this line works always\n  float sum = -(float)(s[id] * ((float)(s[C((x+1) >=  L? 0: x+1, y, z, L)] + \n              s[C(x, (y+1) >= L? 0 : y+1, z, L)] + s[C(x, y, (z+1) >= L? 0 : z+1, L)]) + h*H[id]));\n  sum = block_reduce<T>(sum); \n\n  if(tid == 0) atomicAdd(out, sum);\n}"
        ],
        "/Users/gbolet/hecbench-roofline/src/metropolis-cuda/kernel_metropolis.h": [
            "inline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\ninline __host__ __device__ float4 expf(float4 v)\n{\n    return make_float4(expf(v.x), expf(v.y), expf(v.z), expf(v.w));\n}\n\n__host__ __device__\ninline uint32_t gpu_pcg32_random_r(uint64_t *state, uint64_t *inc)\n{\n  uint64_t oldstate = *state;\n  *state = oldstate * 6364136223846793005ULL + *inc;\n  uint32_t xorshifted = ((oldstate >> 18u) ^ oldstate) >> 27u;\n  uint32_t rot = oldstate >> 59u;\n  return (xorshifted >> rot) | (xorshifted << ((-rot) & 31));\n}\n\n__device__ double C(int l)\n{\n  return (2.0*l-1)*2.0*l*(2.0*l+2)/(4.0*l-3.0)/(4.0*l-1.0);\n}\n\n__host__ __device__\ninline float gpu_rand01(uint64_t *state, uint64_t *inc)\n{\n  return (float) gpu_pcg32_random_r(state, inc) * INV_UINT_MAX;\n}\n\n__global__ void\nkernel_metropolis(const int N, const int L, site_t *s, const int *H, \n                  const float h, const float B, uint64_t *state, uint64_t *inc, int alt)\n{\n\n  // offsets\n  int offx = blockIdx.x * BX;\n  int offy = (2*blockIdx.y + ((blockIdx.x + blockIdx.z + alt) & 1)) * BY;\n  int offz = blockIdx.z * BZ;\n\n  // halo shared memory coords\n  int sx = threadIdx.x;\n  int sy1 = 2*threadIdx.y;\n  int sy2 = 2*threadIdx.y + 1;\n  int sz = threadIdx.z;\n\n  // global coords\n  int x = offx + sx;\n  int y1 = offy + sy1;\n  int y2 = offy + sy2;\n  int z = offz + sz;\n\n  //if(x >= N || y1 >= N || y2 >= N || z >= N)\n  //return;\n\n  // global and local and block id in soc\n  int tid = z*L*L/4 + (blockIdx.y * BY/2 + threadIdx.y)*L + x;\n  // shared memory\n  __shared__ site_t ss[SVOLUME];\n\n  // load the spins into shared memory\n  ss[sC(sx, sy1, sz, sLx, sLy)] = s[C(x, y1, z, L)];\n  ss[sC(sx, sy2, sz, sLx, sLy)] = s[C(x, y2, z, L)];\n  // get the h1,h2 values for y1 y2.\n  int h1 = H[C(x, y1, z, L)];\n  int h2 = H[C(x, y2, z, L)];\n  //printf(\"thread %i   h1=%i   h2=%i\\n\", tid, h1, h2);\n\n  // ------------------------------------------------\n  // halo\n  // ------------------------------------------------\n  // Y boundary\n  if(threadIdx.y == 0){\n    // we also check if we are on the limit of the lattice\n    ss[sC(sx, -1, sz, sLx, sLy)] = (offy == 0) ? s[C(x, L-1, z, L)] : s[C(x, offy-1, z, L)];\n  }\n  if(threadIdx.y == BY/2-1){\n    ss[sC(sx, BY, sz, sLx, sLy)] = (offy == L-BY) ? s[C(x, 0, z, L)] : s[C(x, offy+BY, z, L)];\n  }\n\n  // X boundary\n  if(threadIdx.x == 0){\n    if(blockIdx.x == 0){\n      ss[sC(-1, sy1, sz, sLx, sLy)] = s[C(L-1, y1, z, L)];\n      ss[sC(-1, sy2, sz, sLx, sLy)] = s[C(L-1, y2, z, L)];\n    }\n    else{\n      ss[sC(-1, sy1, sz, sLx, sLy)] = s[C(offx-1, y1, z, L)];\n      ss[sC(-1, sy2, sz, sLx, sLy)] = s[C(offx-1, y2, z, L)];\n    }\n  }\n  if(threadIdx.x == BX-1){\n    if(blockIdx.x == gridDim.x-1){\n      ss[sC(BX, sy1, sz, sLx, sLy)] = s[C(0, y1, z, L)];\n      ss[sC(BX, sy2, sz, sLx, sLy)] = s[C(0, y2, z, L)];\n    }\n    else{\n      ss[sC(BX, sy1, sz, sLx, sLy)] = s[C(offx+BX, y1, z, L)];\n      ss[sC(BX, sy2, sz, sLx, sLy)] = s[C(offx+BX, y2, z, L)];\n    }\n  }\n\n  // Z boundary\n  if(threadIdx.z == 0){\n    if(blockIdx.z == 0){\n      ss[sC(sx, sy1, -1, sLx, sLy)] = s[C(x, y1, L-1, L)];\n      ss[sC(sx, sy2, -1, sLx, sLy)] = s[C(x, y2, L-1, L)];\n    }\n    else{\n      ss[sC(sx, sy1, -1, sLx, sLy)] = s[C(x, y1, offz-1, L)];\n      ss[sC(sx, sy2, -1, sLx, sLy)] = s[C(x, y2, offz-1, L)];\n    }\n  }\n  if(threadIdx.z == BZ-1){\n    if(blockIdx.z == gridDim.z-1){\n      ss[sC(sx, sy1, BZ, sLx, sLy)] = s[C(x, y1, 0, L)];\n      ss[sC(sx, sy2, BZ, sLx, sLy)] = s[C(x, y2, 0, L)];\n    }\n    else{\n      ss[sC(sx, sy1, BZ, sLx, sLy)] = s[C(x, y1, offz+BZ, L)];\n      ss[sC(sx, sy2, BZ, sLx, sLy)] = s[C(x, y2, offz+BZ, L)];\n    }\n  }\n\n  // get random number state\n  uint64_t lstate = state[tid];\n  uint64_t linc = inc[tid];\n  // the white and black y\n  int wy = ((sx + sz) & 1)     + 2*threadIdx.y;\n  int by = ((sx + sz + 1) & 1)  + 2*threadIdx.y;\n  float dh;\n  int c;\n\n  __syncthreads();\n  //#pragma unroll\n  for(int i = 0; i < BLOCK_STEPS; ++i){\n\n    /* -------- white update -------- */\n    dh = (float)(ss[sC(sx, wy, sz, sLx, sLy)] * (\n         (float)(ss[sC(sx-1,wy,sz, sLx, sLy)] + ss[sC(sx+1, wy, sz, sLx, sLy)] + \n                 ss[sC(sx,wy-1,sz, sLx, sLy)] + ss[sC(sx, wy+1, sz, sLx, sLy)] +\n                 ss[sC(sx,wy,sz-1, sLx, sLy)] + ss[sC(sx, wy, sz+1, sLx, sLy)]) + h*h1));\n    c = signbit(dh-EPSILON) | signbit(gpu_rand01(&lstate, &linc) - expf(dh * B));\n    ss[sC(sx, wy, sz, sLx, sLy)] *= (1 - 2*c);\n    __syncthreads();\n\n    /* -------- black update -------- */\n    dh = (float)(ss[sC(sx, by, sz, sLx, sLy)] * (\n         (float)(ss[sC(sx-1,by,sz, sLx, sLy)] + ss[sC(sx+1, by, sz, sLx, sLy)] + \n                 ss[sC(sx,by-1,sz, sLx, sLy)] + ss[sC(sx, by+1, sz, sLx, sLy)] +\n                 ss[sC(sx,by,sz-1, sLx, sLy)] + ss[sC(sx, by, sz+1, sLx, sLy)]) + h*h2));\n\n    c = signbit(dh-EPSILON) | signbit(gpu_rand01(&lstate, &linc) - expf(dh * B));\n    ss[sC(sx, by, sz, sLx, sLy)] *= (1 - 2*c);\n    __syncthreads();\n  }\n\n  /* copy data back to gmem */\n  s[C(x, y1, z, L)] = ss[sC(sx, sy1, sz, sLx, sLy)];\n  s[C(x, y2, z, L)] = ss[sC(sx, sy2, sz, sLx, sLy)]; \n  /* update random number state */\n  state[tid] = lstate;\n  inc[tid] = linc;\n}",
            "__host__ __device__\ninline uint32_t gpu_pcg32_random_r(uint64_t *state, uint64_t *inc)\n{\n  uint64_t oldstate = *state;\n  *state = oldstate * 6364136223846793005ULL + *inc;\n  uint32_t xorshifted = ((oldstate >> 18u) ^ oldstate) >> 27u;\n  uint32_t rot = oldstate >> 59u;\n  return (xorshifted >> rot) | (xorshifted << ((-rot) & 31));\n}\n\n__host__ __device__\ninline float gpu_rand01(uint64_t *state, uint64_t *inc)\n{\n  return (float) gpu_pcg32_random_r(state, inc) * INV_UINT_MAX;\n}\n\n__global__ void \nkernel_reset_random_gpupcg(int *s, int N, uint64_t *state, uint64_t *inc)\n{\n  int x = blockIdx.x * blockDim.x + threadIdx.x;\n  float v;\n  /* Each thread gets same seed, a different sequence number, no offset */\n  if( x >= N/4 ) return;\n\n  /* get the prng state in register memory */\n  uint64_t lstate = state[x];\n  uint64_t linc = inc[x];\n\n  // first random\n  v = (int) (gpu_rand01(&lstate, &linc) + 0.5f);\n  s[x] = 1-2*v;\n  // second random\n  v = (int) (gpu_rand01(&lstate, &linc) + 0.5f);\n  s[x + N/4] = 1-2*v;\n  // third random\n  v = (int) (gpu_rand01(&lstate, &linc) + 0.5f);\n  s[x + N/2 ]  = 1-2*v;\n  // fourth random\n  v = (int) (gpu_rand01(&lstate, &linc) + 0.5f);\n  s[x + 3*N/4] = 1-2*v;\n\n  /* save the state back to global memory */\n  state[x] = lstate;\n  inc[x] = linc;\n}",
            "#define T ((int)32)\n\n\n__global__ void kernel_reset(T *a, int N, T val){\n  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  if(idx < N) a[idx] = val;\n}"
        ],
        "/Users/gbolet/hecbench-roofline/src/metropolis-cuda/kernel_prng.h": [
            "__host__ __device__\ninline uint32_t gpu_pcg32_random_r(uint64_t *state, uint64_t *inc)\n{\n  uint64_t oldstate = *state;\n  *state = oldstate * 6364136223846793005ULL + *inc;\n  uint32_t xorshifted = ((oldstate >> 18u) ^ oldstate) >> 27u;\n  uint32_t rot = oldstate >> 59u;\n  return (xorshifted >> rot) | (xorshifted << ((-rot) & 31));\n}\n\n__host__ __device__\ninline void gpu_pcg32_srandom_r(uint64_t *state, uint64_t *inc, uint64_t initstate, uint64_t initseq)\n{\n  *state = 0U;\n  *inc = (initseq << 1u) | 1u;\n  gpu_pcg32_random_r(state, inc);\n  *state += initstate;\n  gpu_pcg32_random_r(state, inc);\n}\n\n__device__\nuint64_t mmhash64( const void * key, int len, unsigned int seed )\n{\n  const uint64_t m = 0xc6a4a7935bd1e995;\n  const int r = 47;\n\n  uint64_t h = seed ^ (len * m);\n\n  const uint64_t * data = (const uint64_t *)key;\n  const uint64_t * end = data + (len/8);\n\n  while(data != end){\n    uint64_t k = *data++;\n\n    k *= m; \n    k ^= k >> r; \n    k *= m; \n\n    h ^= k;\n    h *= m; \n  }\n  const unsigned char * data2 = (const unsigned char*)data;\n  switch(len & 7)\n  {\n    case 7: h ^= uint64_t(data2[6]) << 48;\n    case 6: h ^= uint64_t(data2[5]) << 40;\n    case 5: h ^= uint64_t(data2[4]) << 32;\n    case 4: h ^= uint64_t(data2[3]) << 24;\n    case 3: h ^= uint64_t(data2[2]) << 16;\n    case 2: h ^= uint64_t(data2[1]) << 8;\n    case 1: h ^= uint64_t(data2[0]);\n      h *= m;\n  };\n\n  h ^= h >> r;\n  h *= m;\n  h ^= h >> r;\n  return h;\n}\n\n__global__\nvoid kernel_gpupcg_setup(uint64_t *state, uint64_t *inc, int N, \n                         uint64_t seed, uint64_t seq)\n{\n  int x = blockIdx.x * blockDim.x + threadIdx.x;\n  if( x < N ){\n    // exclusive seeds, per replica sequences \n    uint64_t tseed = x + seed;\n    uint64_t hseed = mmhash64(&tseed, sizeof(uint64_t), 17);\n    uint64_t hseq = mmhash64(&seq, sizeof(uint64_t), 47);\n    gpu_pcg32_srandom_r(&state[x], &inc[x], hseed, hseq);\n  }\n}"
        ]
    },
    "gibbs-cuda": {
        "/Users/gbolet/hecbench-roofline/src/gibbs-cuda/main.cu": [
            "__global__ void setup_kernel(curandState *state, int num_sample, unsigned int seed)\n{\n  int id = threadIdx.x + blockIdx.x * blockDim.x;\n  if (id < num_sample)\n    /* Each thread gets same seed, a different sequence number, no offset */\n    curand_init(seed, id, 0, &state[id]);\n}",
            "inline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\ninline __host__ __device__ float4 sqrtf(float4 v)\n{\n    return make_float4(sqrtf(v.x), sqrtf(v.y), sqrtf(v.z), sqrtf(v.w));\n}\n\n__device__ float rgamma(curandState *state, float a, float b)\n{\n  float d = a - 1.f / 3.f;\n  float c = 1.f / sqrtf(9.f * d);\n  bool flag = true;\n  float V;\n\n  while (flag) {\n    // Generate a standard normal random variable\n    float Z = curand_normal(state);\n    if (Z > -1.f / c) {\n      V = powf(1.f + c * Z, 3.f);\n      float U = curand_uniform(state);\n      flag = logf(U) > (0.5f * Z * Z + d - d * V + d * logf(V));\n    }\n  }\n  return d * V / b;\n}\n\n__global__ void sample_theta(\n    curandState *__restrict__ state, \n    float *__restrict__ theta,\n    const int *__restrict__ y,\n    const float *__restrict__ n, \n    float a, float b, int num_sample)\n{\n  int id = threadIdx.x + blockIdx.x * blockDim.x;\n  if(id < num_sample) {\n    curandState *s = state + id;\n    const float hyperA = a + y[id];\n    const float hyperB = b + n[id];\n    theta[id] = (hyperA < 1.f) ?\n      rgamma(s, hyperA + 1.f, hyperB) * powf(curand_uniform(s), 1.f / hyperA) : \n      rgamma(s, hyperA, hyperB);\n  }\n}",
            "__global__ void sum_blocks(const float *__restrict__ theta,\n                                 float *__restrict__ flat_sums, \n                                 float *__restrict__ log_sums,\n                                   int num_sample)\n{\n  __shared__ float flats[THREADS_PER_BLOCK_ADD];\n  __shared__ float logs[THREADS_PER_BLOCK_ADD];\n\n  int id = threadIdx.x + blockIdx.x * blockDim.x;\n\n  flats[threadIdx.x] = (id < num_sample) ? theta[id] : 0.f;\n  logs[threadIdx.x] = (id < num_sample) ? logf( theta[id] ) : 0.f;\n\n  __syncthreads();\n\n  int i = blockDim.x / 2;\n  while(i != 0){\n    if(threadIdx.x < i){\n      flats[threadIdx.x] += flats[threadIdx.x + i];\n      logs[threadIdx.x] += logs[threadIdx.x + i];\n    }\n    __syncthreads();\n    i /= 2;\n  }\n\n  if(threadIdx.x == 0){\n    flat_sums[blockIdx.x] = flats[0];\n    log_sums[blockIdx.x] = logs[0];\n  }\n}"
        ]
    },
    "adv-cuda": {
        "/Users/gbolet/hecbench-roofline/src/adv-cuda/adv.cu": [
            "__global__ void advCubatureHex3D (\n    const int Nelements,\n    const dfloat * __restrict vgeo,\n    const dfloat * __restrict cubvgeo,\n    const dfloat * __restrict cubD,\n    const dfloat * __restrict cubInterpT,\n    const int offset,\n    const dfloat * __restrict U,\n    dfloat * __restrict NU)\n{\n  __shared__ dfloat s_cubD[16][16];\n  __shared__ dfloat s_cubInterpT[8][16];\n  __shared__ dfloat s_U[8][8];\n  __shared__ dfloat s_V[8][8];\n  __shared__ dfloat s_W[8][8];\n  __shared__ dfloat s_U1[16][16];\n  __shared__ dfloat s_V1[16][16];\n  __shared__ dfloat s_W1[16][16];\n\n  dfloat r_U[16], r_V[16], r_W[16];\n  dfloat r_Ud[16], r_Vd[16], r_Wd[16];\n\n  const int e = blockIdx.x;\n  const int i = threadIdx.x;\n  const int j = threadIdx.y;\n  const int id = j * 16 + i;\n\n  if (j < 8 && i < 16) s_cubInterpT[j][i] = cubInterpT[id];\n  s_cubD[j][i] = cubD[id];\n\n  for (int k = 0; k < 16; ++k) {\n    r_U[k] = 0;\n    r_V[k] = 0;\n    r_W[k] = 0;\n    r_Ud[k] = 0;\n    r_Vd[k] = 0;\n    r_Wd[k] = 0;\n  }\n\n  for (int c = 0; c < 8; ++c) {\n    if (j < 8 && i < 8) {\n      const int id = e * p_Np + c * 8 * 8 + j * 8 + i;\n      s_U[j][i] = U[id + 0 * offset];\n      s_V[j][i] = U[id + 1 * offset];\n      s_W[j][i] = U[id + 2 * offset];\n    }\n    __syncthreads();\n\n    if (j < 8) {\n      dfloat U1 = 0, V1 = 0, W1 = 0;\n      for (int a = 0; a < 8; ++a) {\n        dfloat Iia = s_cubInterpT[a][i];\n        U1 += Iia * s_U[j][a];\n        V1 += Iia * s_V[j][a];\n        W1 += Iia * s_W[j][a];\n      }\n      s_U1[j][i] = U1;\n      s_V1[j][i] = V1;\n      s_W1[j][i] = W1;\n    } else {\n      s_U1[j][i] = 0;\n      s_V1[j][i] = 0;\n      s_W1[j][i] = 0;\n    }\n\n    __syncthreads();\n\n    dfloat U2 = 0, V2 = 0, W2 = 0;\n    for (int b = 0; b < 8; ++b) {\n      dfloat Ijb = s_cubInterpT[b][j];\n      U2 += Ijb * s_U1[b][i];\n      V2 += Ijb * s_V1[b][i];\n      W2 += Ijb * s_W1[b][i];\n    }\n    for (int k = 0; k < 16; ++k) {\n      dfloat Ikc = s_cubInterpT[c][k];\n      r_U[k] += Ikc * U2;\n      r_V[k] += Ikc * V2;\n      r_W[k] += Ikc * W2;\n    }\n    for (int k = 0; k < 16; ++k) {\n      r_Ud[k] = r_U[k];\n      r_Vd[k] = r_V[k];\n      r_Wd[k] = r_W[k];\n    }\n  }\n\n  for (int k = 0; k < 16; ++k) {\n    s_U1[j][i] = r_Ud[k];\n    s_V1[j][i] = r_Vd[k];\n    s_W1[j][i] = r_Wd[k];\n\n    __syncthreads();\n\n    dfloat Udr = 0, Uds = 0, Udt = 0;\n    dfloat Vdr = 0, Vds = 0, Vdt = 0;\n    dfloat Wdr = 0, Wds = 0, Wdt = 0;\n    for (int n = 0; n < 16; ++n) {\n      dfloat Din = s_cubD[i][n];\n      Udr += Din * s_U1[j][n];\n      Vdr += Din * s_V1[j][n];\n      Wdr += Din * s_W1[j][n];\n    }\n    for (int n = 0; n < 16; ++n) {\n      dfloat Djn = s_cubD[j][n];\n      Uds += Djn * s_U1[n][i];\n      Vds += Djn * s_V1[n][i];\n      Wds += Djn * s_W1[n][i];\n    }\n    for (int n = 0; n < 16; ++n) {\n      dfloat Dkn = s_cubD[k][n];\n      Udt += Dkn * r_Ud[n];\n      Vdt += Dkn * r_Vd[n];\n      Wdt += Dkn * r_Wd[n];\n    }\n\n    const int gid = e * p_cubNp * p_Nvgeo + k * 16 * 16 + j * 16 + i;\n    const dfloat drdx = cubvgeo[gid + p_RXID * p_cubNp];\n    const dfloat drdy = cubvgeo[gid + p_RYID * p_cubNp];\n    const dfloat drdz = cubvgeo[gid + p_RZID * p_cubNp];\n    const dfloat dsdx = cubvgeo[gid + p_SXID * p_cubNp];\n    const dfloat dsdy = cubvgeo[gid + p_SYID * p_cubNp];\n    const dfloat dsdz = cubvgeo[gid + p_SZID * p_cubNp];\n    const dfloat dtdx = cubvgeo[gid + p_TXID * p_cubNp];\n    const dfloat dtdy = cubvgeo[gid + p_TYID * p_cubNp];\n    const dfloat dtdz = cubvgeo[gid + p_TZID * p_cubNp];\n    const dfloat JW = cubvgeo[gid + p_JWID * p_cubNp];\n    const dfloat Un = r_U[k];\n    const dfloat Vn = r_V[k];\n    const dfloat Wn = r_W[k];\n    const dfloat Uhat = JW * (Un * drdx + Vn * drdy + Wn * drdz);\n    const dfloat Vhat = JW * (Un * dsdx + Vn * dsdy + Wn * dsdz);\n    const dfloat What = JW * (Un * dtdx + Vn * dtdy + Wn * dtdz);\n    r_U[k] = Uhat * Udr + Vhat * Uds + What * Udt;\n    r_V[k] = Uhat * Vdr + Vhat * Vds + What * Vdt;\n    r_W[k] = Uhat * Wdr + Vhat * Wds + What * Wdt;\n  }\n\n  for (int c = 0; c < 8; ++c) {\n    dfloat rhsU = 0, rhsV = 0, rhsW = 0;\n    for (int k = 0; k < 16; ++k) {\n      dfloat Ikc = s_cubInterpT[c][k];\n      rhsU += Ikc * r_U[k];\n      rhsV += Ikc * r_V[k];\n      rhsW += Ikc * r_W[k];\n    }\n\n    if (i < 8 && j < 8) {\n      s_U[j][i] = rhsU;\n      s_V[j][i] = rhsV;\n      s_W[j][i] = rhsW;\n    }\n\n    __syncthreads();\n\n    if (j < 8) {\n      dfloat rhsU = 0, rhsV = 0, rhsW = 0;\n      for (int k = 0; k < 16; ++k) {\n        dfloat Ijb = s_cubInterpT[j][k];\n        if (k < 8 && i < 8) {\n          rhsU += Ijb * s_U[k][i];\n          rhsV += Ijb * s_V[k][i];\n          rhsW += Ijb * s_W[k][i];\n        }\n      }\n      s_U1[j][i] = rhsU;\n      s_V1[j][i] = rhsV;\n      s_W1[j][i] = rhsW;\n    }\n\n    __syncthreads();\n\n    if (i < 8 && j < 8) {\n      dfloat rhsU = 0, rhsV = 0, rhsW = 0;\n      for (int k = 0; k < 16; ++k) {\n        dfloat Iia = s_cubInterpT[i][k];\n        rhsU += Iia * s_U1[j][k];\n        rhsV += Iia * s_V1[j][k];\n        rhsW += Iia * s_W1[j][k];\n      }\n      const int gid = e * p_Np * p_Nvgeo + c * 8 * 8 + j * 8 + i;\n      const dfloat IJW = vgeo[gid + p_IJWID * p_Np];\n      const int id = e * p_Np + c * 8 * 8 + j * 8 + i;\n      NU[id + 0 * offset] = IJW * rhsU;\n      NU[id + 1 * offset] = IJW * rhsV;\n      NU[id + 2 * offset] = IJW * rhsW;\n    }\n  }\n}"
        ]
    },
    "cobahh-cuda": {
        "/Users/gbolet/hecbench-roofline/src/cobahh-cuda/neuron_update.h": [
            "inline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\ninline __host__ __device__ float4 expf(float4 v)\n{\n    return make_float4(expf(v.x), expf(v.y), expf(v.z), expf(v.w));\n}\n\n__device__  __host__\ninline int _timestep(float t, float dt)\n{\n  return (int)((t + 1e-3f*dt)/dt); \n}\n\n__global__ void cobahh (\n    float* __restrict__ d_h, \n    float* __restrict__ d_m,\n    float* __restrict__ d_n,\n    float* __restrict__ d_ge,\n    float* __restrict__ d_v,\n    float* __restrict__ d_gi,\n    const float* __restrict__ d_lastspike, \n    char* __restrict__ d_not_refractory, \n    const int _N ,\n    const float dt,\n    const float t,\n    const int    _lio_1,\n    const float  _lio_2,\n    const float  _lio_3,\n    const float  _lio_4,\n    const float  _lio_5,\n    const float  _lio_6,\n    const float  _lio_7,\n    const float  _lio_8,\n    const float  _lio_9,\n    const float _lio_10,\n    const float _lio_11,\n    const float _lio_12,\n    const float _lio_13,\n    const float _lio_14,\n    const float _lio_15,\n    const float _lio_16,\n    const float _lio_17,\n    const float _lio_18,\n    const float _lio_19,\n    const float _lio_20,\n    const float _lio_21,\n    const float _lio_22,\n    const float _lio_23,\n    const float _lio_24,\n    const float _lio_25,\n    const float _lio_26,\n    const float _lio_27,\n    const float _lio_28,\n    const float _lio_29,\n    const float _lio_30,\n    const float _lio_31,\n    const float _lio_32,\n    const float _lio_33\n    )\n\n{\n  int _idx = blockIdx.x * blockDim.x + threadIdx.x;\n  if (_idx >= _N) return;\n  float h = d_h[_idx];\n  float m = d_m[_idx];\n  float n = d_n[_idx];\n  float ge = d_ge[_idx];\n  float v = d_v[_idx];\n  const float lastspike = d_lastspike[_idx];\n  float gi = d_gi[_idx];\n  char not_refractory;\n  not_refractory = _timestep(t - lastspike, dt) >= _lio_1;\n  const float _BA_h = (_lio_2 * expf(_lio_3 * v))/(((-4.0f)/(0.001f + (_lio_4 * expf(_lio_5 * v)))) - (_lio_2 * expf(_lio_3 * v)));\n  const float _h = (- _BA_h) + ((_BA_h + h) * expf(dt * (((-4.0f)/(0.001f + (_lio_4 * expf(_lio_5 * v)))) - (_lio_2 * expf(_lio_3 * v)))));\n  const float _BA_m = (((_lio_6/(_lio_7 + (_lio_8 * expf(_lio_9 * v)))) + (_lio_10/(_lio_7 + (_lio_8 * expf(_lio_9 * v))))) - ((0.32f * v)/(_lio_7 + (_lio_8 * expf(_lio_9 * v)))))/(((((_lio_11/(_lio_7 + (_lio_8 * expf(_lio_9 * v)))) + (_lio_12/(_lio_13 + (_lio_14 * expf(_lio_15 * v))))) + (_lio_16/(_lio_13 + (_lio_14 * expf(_lio_15 * v))))) + ((0.32f * v)/(_lio_7 + (_lio_8 * expf(_lio_9 * v))))) - ((_lio_10/(_lio_7 + (_lio_8 * expf(_lio_9 * v)))) + ((0.28f * v)/(_lio_13 + (_lio_14 * expf(_lio_15 * v))))));\n  const float _m = (- _BA_m) + ((_BA_m + m) * expf(dt * (((((_lio_11/(_lio_7 + (_lio_8 * expf(_lio_9 * v)))) + (_lio_12/(_lio_13 + (_lio_14 * expf(_lio_15 * v))))) + (_lio_16/(_lio_13 + (_lio_14 * expf(_lio_15 * v))))) + ((0.32f * v)/(_lio_7 + (_lio_8 * expf(_lio_9 * v))))) - ((_lio_10/(_lio_7 + (_lio_8 * expf(_lio_9 * v)))) + ((0.28f * v)/(_lio_13 + (_lio_14 * expf(_lio_15 * v))))))));\n  const float _BA_n = (((_lio_17/(_lio_7 + (_lio_18 * expf(_lio_5 * v)))) + (_lio_19/(_lio_7 + (_lio_18 * expf(_lio_5 * v))))) - ((0.032f * v)/(_lio_7 + (_lio_18 * expf(_lio_5 * v)))))/(((_lio_20/(_lio_7 + (_lio_18 * expf(_lio_5 * v)))) + ((0.032f * v)/(_lio_7 + (_lio_18 * expf(_lio_5 * v))))) - ((_lio_19/(_lio_7 + (_lio_18 * expf(_lio_5 * v)))) + (_lio_21 * expf(_lio_22 * v))));\n  const float _n = (- _BA_n) + ((_BA_n + n) * expf(dt * (((_lio_20/(_lio_7 + (_lio_18 * expf(_lio_5 * v)))) + ((0.032f * v)/(_lio_7 + (_lio_18 * expf(_lio_5 * v))))) - ((_lio_19/(_lio_7 + (_lio_18 * expf(_lio_5 * v)))) + (_lio_21 * expf(_lio_22 * v))))));\n  const float _ge = _lio_23 * ge;\n  const float _BA_v = (_lio_24 + ((((_lio_25 * (n*n*n*n)) + (_lio_26 * (h * (m*m*m)))) + (_lio_27 * ge)) + (_lio_28 * gi)))/((_lio_29 + (_lio_30 * (n*n*n*n))) - (((_lio_31 * (h * (m*m*m))) + (_lio_32 * ge)) + (_lio_32 * gi)));\n  const float _v = (- _BA_v) + ((_BA_v + v) * expf(dt * ((_lio_29 + (_lio_30 * (n*n*n*n))) - (((_lio_31 * (h * (m*m*m))) + (_lio_32 * ge)) + (_lio_32 * gi)))));\n  const float _gi = _lio_33 * gi;\n\n  d_h[_idx] = _h;\n  d_m[_idx] = _m;\n  d_n[_idx] = _n;\n  d_ge[_idx] = _ge;\n  d_v[_idx] = _v;\n  d_gi[_idx] = _gi;\n  d_not_refractory[_idx] = not_refractory;\n}"
        ]
    },
    "ced-cuda": {
        "/Users/gbolet/hecbench-roofline/src/ced-cuda/main.cu": [
            "inline __host__ __device__ uint4 make_uint4(int4 a)\n{\n    return make_uint4(uint(a.x), uint(a.y), uint(a.z), uint(a.w));\n}\n\ninline __host__ __device__ uint4 max(uint4 a, uint4 b)\n{\n    return make_uint4(max(a.x,b.x), max(a.y,b.y), max(a.z,b.z), max(a.w,b.w));\n}\n\ninline __host__ __device__ uint4 min(uint4 a, uint4 b)\n{\n    return make_uint4(min(a.x,b.x), min(a.y,b.y), min(a.z,b.z), min(a.w,b.w));\n}\n\n__global__ void \ngaussian_kernel(const unsigned char *__restrict__ data,\n                unsigned char *__restrict__ out,\n                const int rows, const int cols) \n{\n  extern __shared__ int l_mem[];\n  int* l_data = l_mem;\n\n  const int L_SIZE = blockDim.x;\n  int sum         = 0;\n  const int l_row = threadIdx.y + 1;\n  const int l_col = threadIdx.x + 1;\n  const int g_row = blockIdx.y * blockDim.y + l_row;\n  const int g_col = blockIdx.x * blockDim.x + l_col;\n\n  const int pos = g_row * cols + g_col;\n\n  // copy to local\n  l_data[l_row * (L_SIZE + 2) + l_col] = data[pos];\n\n  // top most row\n  if(l_row == 1) {\n      l_data[0 * (L_SIZE + 2) + l_col] = data[pos - cols];\n      // top left\n      if(l_col == 1)\n          l_data[0 * (L_SIZE + 2) + 0] = data[pos - cols - 1];\n\n      // top right\n      else if(l_col == L_SIZE)\n          l_data[0 * (L_SIZE + 2) + L_SIZE + 1] = data[pos - cols + 1];\n  }\n  // bottom most row\n  else if(l_row == L_SIZE) {\n      l_data[(L_SIZE + 1) * (L_SIZE + 2) + l_col] = data[pos + cols];\n      // bottom left\n      if(l_col == 1)\n          l_data[(L_SIZE + 1) * (L_SIZE + 2) + 0] = data[pos + cols - 1];\n\n      // bottom right\n      else if(l_col == L_SIZE)\n          l_data[(L_SIZE + 1) * (L_SIZE + 2) + L_SIZE + 1] = data[pos + cols + 1];\n  }\n\n  if(l_col == 1)\n      l_data[l_row * (L_SIZE + 2) + 0] = data[pos - 1];\n  else if(l_col == L_SIZE)\n      l_data[l_row * (L_SIZE + 2) + L_SIZE + 1] = data[pos + 1];\n\n  __syncthreads();\n\n  for(int i = 0; i < 3; i++) {\n      for(int j = 0; j < 3; j++) {\n          sum += c_gaus[i*3+j] * l_data[(i + l_row - 1) * (L_SIZE + 2) + j + l_col - 1];\n      }\n  }\n\n  out[pos] = min(255, max(0, sum));\n}",
            "inline __host__ __device__ uint4 make_uint4(int4 a)\n{\n    return make_uint4(uint(a.x), uint(a.y), uint(a.z), uint(a.w));\n}\n\ninline __host__ __device__ uint4 max(uint4 a, uint4 b)\n{\n    return make_uint4(max(a.x,b.x), max(a.y,b.y), max(a.z,b.z), max(a.w,b.w));\n}\n\ninline __host__ __device__ uint4 min(uint4 a, uint4 b)\n{\n    return make_uint4(min(a.x,b.x), min(a.y,b.y), min(a.z,b.z), min(a.w,b.w));\n}\n\n__global__ void \nsobel_kernel(const unsigned char *__restrict__ data,\n             unsigned char *__restrict__ out,\n             unsigned char *__restrict__ theta,\n             const int rows, const int cols)\n{\n  extern __shared__ int l_mem[];\n  int* l_data = l_mem;\n\n  // collect sums separately. we're storing them into floats because that\n  // is what hypot and atan2 will expect.\n  const int L_SIZE = blockDim.x;\n  const float PI    = 3.14159265f;\n  const int   l_row = threadIdx.y + 1;\n  const int   l_col = threadIdx.x + 1;\n  const int   g_row = blockIdx.y * blockDim.y + l_row;\n  const int   g_col = blockIdx.x * blockDim.x + l_col;\n\n  const int pos = g_row * cols + g_col;\n\n  // copy to local\n  l_data[l_row * (L_SIZE + 2) + l_col] = data[pos];\n\n  // top most row\n  if(l_row == 1) {\n      l_data[0 * (L_SIZE + 2) + l_col] = data[pos - cols];\n      // top left\n      if(l_col == 1)\n          l_data[0 * (L_SIZE + 2) + 0] = data[pos - cols - 1];\n\n      // top right\n      else if(l_col == L_SIZE)\n          l_data[0 * (L_SIZE + 2) + (L_SIZE + 1)] = data[pos - cols + 1];\n  }\n  // bottom most row\n  else if(l_row == L_SIZE) {\n      l_data[(L_SIZE + 1) * (L_SIZE + 2) + l_col] = data[pos + cols];\n      // bottom left\n      if(l_col == 1)\n          l_data[(L_SIZE + 1) * (L_SIZE + 2) + 0] = data[pos + cols - 1];\n\n      // bottom right\n      else if(l_col == L_SIZE)\n          l_data[(L_SIZE + 1) * (L_SIZE + 2) + (L_SIZE + 1)] = data[pos + cols + 1];\n  }\n\n  // left\n  if(l_col == 1)\n      l_data[l_row * (L_SIZE + 2) + 0] = data[pos - 1];\n  // right\n  else if(l_col == L_SIZE)\n      l_data[l_row * (L_SIZE + 2) + (L_SIZE + 1)] = data[pos + 1];\n\n  __syncthreads();\n\n  float sumx = 0, sumy = 0, angle = 0;\n  // find x and y derivatives\n  for(int i = 0; i < 3; i++) {\n      for(int j = 0; j < 3; j++) {\n          sumx += c_sobx[i*3+j] * l_data[(i + l_row - 1) * (L_SIZE + 2) + j + l_col - 1];\n          sumy += c_soby[i*3+j] * l_data[(i + l_row - 1) * (L_SIZE + 2) + j + l_col - 1];\n      }\n  }\n\n  // The output is now the square root of their squares, but they are\n  // constrained to 0 <= value <= 255. Note that hypot is a built in function\n  // defined as: hypot(x,y) = sqrt(x*x, y*y).\n  out[pos] = min(255, max(0, (int)hypot(sumx, sumy)));\n\n  // Compute the direction angle theta in radians\n  // atan2 has a range of (-PI, PI) degrees\n  angle = atan2(sumy, sumx);\n\n  // If the angle is negative,\n  // shift the range to (0, 2PI) by adding 2PI to the angle,\n  // then perform modulo operation of 2PI\n  if(angle < 0) {\n      angle = fmod((angle + 2 * PI), (2 * PI));\n  }\n\n  // Round the angle to one of four possibilities: 0, 45, 90, 135 degrees\n  // then store it in the theta buffer at the proper position\n  //theta[pos] = ((int)(degrees(angle * (PI/8) + PI/8-0.0001) / 45) * 45) % 180;\n  if(angle <= PI / 8)\n      theta[pos] = 0;\n  else if(angle <= 3 * PI / 8)\n      theta[pos] = 45;\n  else if(angle <= 5 * PI / 8)\n      theta[pos] = 90;\n  else if(angle <= 7 * PI / 8)\n      theta[pos] = 135;\n  else if(angle <= 9 * PI / 8)\n      theta[pos] = 0;\n  else if(angle <= 11 * PI / 8)\n      theta[pos] = 45;\n  else if(angle <= 13 * PI / 8)\n      theta[pos] = 90;\n  else if(angle <= 15 * PI / 8)\n      theta[pos] = 135;\n  else\n      theta[pos] = 0; // (angle <= 16*PI/8)\n}",
            "__global__ void \nnon_max_supp_kernel(const unsigned char *__restrict__ data,\n                          unsigned char *__restrict__ out, \n                    const unsigned char *__restrict__ theta,\n                    const int rows, const int cols)\n{\n  extern __shared__ int l_mem[];\n  int* l_data = l_mem;\n\n  // These variables are offset by one to avoid seg. fault errors\n  // As such, this kernel ignores the outside ring of pixels\n  const int L_SIZE = blockDim.x;\n  const int l_row = threadIdx.y + 1;\n  const int l_col = threadIdx.x + 1;\n  const int g_row = blockIdx.y * blockDim.y + l_row;\n  const int g_col = blockIdx.x * blockDim.x + l_col;\n\n  const int pos = g_row * cols + g_col;\n\n  // copy to l_data\n  l_data[l_row * (L_SIZE + 2) + l_col] = data[pos];\n\n  // top most row\n  if(l_row == 1) {\n      l_data[0 * (L_SIZE + 2) + l_col] = data[pos - cols];\n      // top left\n      if(l_col == 1)\n          l_data[0 * (L_SIZE + 2) + 0] = data[pos - cols - 1];\n\n      // top right\n      else if(l_col == L_SIZE)\n          l_data[0 * (L_SIZE + 2) + (L_SIZE + 1)] = data[pos - cols + 1];\n  }\n  // bottom most row\n  else if(l_row == L_SIZE) {\n      l_data[(L_SIZE + 1) * (L_SIZE + 2) + l_col] = data[pos + cols];\n      // bottom left\n      if(l_col == 1)\n          l_data[(L_SIZE + 1) * (L_SIZE + 2) + 0] = data[pos + cols - 1];\n\n      // bottom right\n      else if(l_col == L_SIZE)\n          l_data[(L_SIZE + 1) * (L_SIZE + 2) + (L_SIZE + 1)] = data[pos + cols + 1];\n  }\n\n  if(l_col == 1)\n      l_data[l_row * (L_SIZE + 2) + 0] = data[pos - 1];\n  else if(l_col == L_SIZE)\n      l_data[l_row * (L_SIZE + 2) + (L_SIZE + 1)] = data[pos + 1];\n\n  __syncthreads();\n\n  unsigned char my_magnitude = l_data[l_row * (L_SIZE + 2) + l_col];\n\n  // The following variables are used to address the matrices more easily\n  switch(theta[pos]) {\n    // A gradient angle of 0 degrees = an edge that is North/South\n    // Check neighbors to the East and West\n    case 0:\n        // supress me if my neighbor has larger magnitude\n        if(my_magnitude <= l_data[l_row * (L_SIZE + 2) + l_col + 1] || // east\n            my_magnitude <= l_data[l_row * (L_SIZE + 2) + l_col - 1]) // west\n        {\n            out[pos] = 0;\n        }\n        // otherwise, copy my value to the output buffer\n        else {\n            out[pos] = my_magnitude;\n        }\n        break;\n\n    // A gradient angle of 45 degrees = an edge that is NW/SE\n    // Check neighbors to the NE and SW\n    case 45:\n        // supress me if my neighbor has larger magnitude\n        if(my_magnitude <= l_data[(l_row - 1) * (L_SIZE + 2) + l_col + 1] || // north east\n            my_magnitude <= l_data[(l_row + 1) * (L_SIZE + 2) + l_col - 1]) // south west\n        {\n            out[pos] = 0;\n        }\n        // otherwise, copy my value to the output buffer\n        else {\n            out[pos] = my_magnitude;\n        }\n        break;\n\n    // A gradient angle of 90 degrees = an edge that is E/W\n    // Check neighbors to the North and South\n    case 90:\n        // supress me if my neighbor has larger magnitude\n        if(my_magnitude <= l_data[(l_row - 1) * (L_SIZE + 2) + l_col] || // north\n            my_magnitude <= l_data[(l_row + 1) * (L_SIZE + 2) + l_col]) // south\n        {\n            out[pos] = 0;\n        }\n        // otherwise, copy my value to the output buffer\n        else {\n            out[pos] = my_magnitude;\n        }\n        break;\n\n    // A gradient angle of 135 degrees = an edge that is NE/SW\n    // Check neighbors to the NW and SE\n    case 135:\n        // supress me if my neighbor has larger magnitude\n        if(my_magnitude <= l_data[(l_row - 1) * (L_SIZE + 2) + l_col - 1] || // north west\n            my_magnitude <= l_data[(l_row + 1) * (L_SIZE + 2) + l_col + 1]) // south east\n        {\n            out[pos] = 0;\n        }\n        // otherwise, copy my value to the output buffer\n        else {\n            out[pos] = my_magnitude;\n        }\n        break;\n\n    default: out[pos] = my_magnitude; break;\n  }\n}",
            "__global__ void \nhyst_kernel(const unsigned char *__restrict__ data,\n                  unsigned char *__restrict__ out,\n            const int rows, const int cols)\n{\n  // Establish our high and low thresholds as floats\n  float lowThresh  = 10;\n  float highThresh = 70;\n\n  // These variables are offset by one to avoid seg. fault errors\n  // As such, this kernel ignores the outside ring of pixels\n  const int row = blockIdx.y * blockDim.y + threadIdx.y + 1;\n  const int col = blockIdx.x * blockDim.x + threadIdx.x + 1;\n  const int pos = row * cols + col;\n\n  const unsigned char EDGE = 255;\n\n  unsigned char magnitude = data[pos];\n\n  if(magnitude >= highThresh)\n      out[pos] = EDGE;\n  else if(magnitude <= lowThresh)\n      out[pos] = 0;\n  else {\n      float med = (highThresh + lowThresh) / 2;\n\n      if(magnitude >= med)\n          out[pos] = EDGE;\n      else\n          out[pos] = 0;\n  }\n}"
        ]
    },
    "rodrigues-cuda": {
        "/Users/gbolet/hecbench-roofline/src/rodrigues-cuda/main.cu": [
            "__global__ \nvoid rotate2 (const int n, const float angle, const float3 w, float4 *d)\n{\n  int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i >= n) return;\n\n  float s, c;\n  sincosf(angle, &s,&c);\n  \n  const float4 p = d[i];\n  const float mc = 1.f - c;\n\n  // Rodrigues' formula:\n  float m1 = c+(w.x)*(w.x)*(mc);\n  float m2 = (w.z)*s+(w.x)*(w.y)*(mc);\n  float m3 =-(w.y)*s+(w.x)*(w.z)*(mc);\n  \n  float m4 =-(w.z)*s+(w.x)*(w.y)*(mc);\n  float m5 = c+(w.y)*(w.y)*(mc);\n  float m6 = (w.x)*s+(w.y)*(w.z)*(mc);\n  \n  float m7 = (w.y)*s+(w.x)*(w.z)*(mc);\n  float m8 =-(w.x)*s+(w.y)*(w.z)*(mc);\n  float m9 = c+(w.z)*(w.z)*(mc);\n\n  float ox = p.x*m1 + p.y*m2 + p.z*m3;\n  float oy = p.x*m4 + p.y*m5 + p.z*m6;\n  float oz = p.x*m7 + p.y*m8 + p.z*m9;\n  d[i] = {ox, oy, oz, 0.f};\n}"
        ]
    },
    "gru-cuda": {
        "/Users/gbolet/hecbench-roofline/src/gru-cuda/main.cu": [
            "inline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\ninline __host__ __device__ float4 expf(float4 v)\n{\n    return make_float4(expf(v.x), expf(v.y), expf(v.z), expf(v.w));\n}\n\n__device__\nfloat sigmoid(const float x) { return 1.0f / (1.0f + expf(-x)); }\n\n__global__ void gru_cell_forward(\n            scalar_t *__restrict__ Input,\n            scalar_t *__restrict__ Hidden,\n            scalar_t *__restrict__ Bias1,\n            scalar_t *__restrict__ Bias2,\n            scalar_t *__restrict__ _hx,   // h(t-1) \n            scalar_t *__restrict__ _hy,   // h(t)\n            scalar_t *__restrict__ storage,\n            index_type hsz,\n            index_type totalElements)\n{\n  index_type linearIndex = blockIdx.x * blockDim.x + threadIdx.x;\n  if (linearIndex < totalElements) {\n    index_type offset = (linearIndex/hsz)*3*hsz+linearIndex%hsz;\n\n    scalar_t ir = DEVICE_LINEAR_GET(Input,  offset+0*hsz);\n    scalar_t ii = DEVICE_LINEAR_GET(Input,  offset+1*hsz);\n    scalar_t in = DEVICE_LINEAR_GET(Input,  offset+2*hsz);\n    scalar_t hr = DEVICE_LINEAR_GET(Hidden, offset+0*hsz);\n    scalar_t hi = DEVICE_LINEAR_GET(Hidden, offset+1*hsz);\n    scalar_t hn = DEVICE_LINEAR_GET(Hidden, offset+2*hsz);\n\n    scalar_t hx = DEVICE_LINEAR_GET(_hx, linearIndex);\n    scalar_t* hy = &DEVICE_LINEAR_GET(_hy, linearIndex);\n\n    scalar_t b1r, b1i, b1n, b2r, b2i, b2n;\n\n    b1r = DEVICE_BIAS_GET(Bias1, linearIndex%hsz+0*hsz);\n    b1i = DEVICE_BIAS_GET(Bias1, linearIndex%hsz+1*hsz);\n    b1n = DEVICE_BIAS_GET(Bias1, linearIndex%hsz+2*hsz);\n\n    b2r = DEVICE_BIAS_GET(Bias2, linearIndex%hsz+0*hsz);\n    b2i = DEVICE_BIAS_GET(Bias2, linearIndex%hsz+1*hsz);\n    b2n = DEVICE_BIAS_GET(Bias2, linearIndex%hsz+2*hsz);\n\n    offset = (linearIndex/hsz)*5*hsz+linearIndex%hsz;\n\n    accscalar_t rg, ig, ng;\n\n    // reset: ir = Wr * xt , hr = Ur * h(t-1)\n    rg = sigmoid(H2F(ir) + H2F(hr) + H2F(b1r) + H2F(b2r));\n\n    // update: ii = Wz * xt , hi = Uz * h(t-1)\n    ig = sigmoid(H2F(ii) + H2F(hi) + H2F(b1i) + H2F(b2i));\n\n    // in = Wh * xt, hn = Uh * h(t-1)\n    ng = H2F(in) + H2F(b1n) + rg*( H2F(hn) + H2F(b2n) );\n    ng = tanh(ng); // h'\n\n    // z * h(t-1) + (1-z)*h', hx = h(t-1)\n    *hy = F2H( ng + ig * ( H2F(hx)-ng ) );\n\n    //save for backwards\n    DEVICE_LINEAR_GET(storage, offset+0*hsz) = F2H(rg);\n    DEVICE_LINEAR_GET(storage, offset+1*hsz) = F2H(ig);\n    DEVICE_LINEAR_GET(storage, offset+2*hsz) = F2H(ng);\n    DEVICE_LINEAR_GET(storage, offset+3*hsz) = hx;\n    DEVICE_LINEAR_GET(storage, offset+4*hsz) = F2H(H2F(hn) + H2F(b2n));\n  }\n}"
        ]
    },
    "scan-cuda": {
        "/Users/gbolet/hecbench-roofline/src/scan-cuda/main.cu": [
            "#define T ((int)32)\n\n\n__global__ void scan_bcao (\n  const int64_t nblocks,\n        T *__restrict__ g_odata,\n  const T *__restrict__ g_idata)\n{\n  __shared__ T temp[2*N];\n\n  for (int64_t bid = blockIdx.x; bid < nblocks; bid += gridDim.x)\n  {\n    auto gi = g_idata + bid * N;\n    auto go = g_odata + bid * N;\n\n    int thid = threadIdx.x;\n    int a = thid;\n    int b = a + (N/2);\n    int oa = OFFSET(a);\n    int ob = OFFSET(b);\n\n    temp[a + oa] = gi[a];\n    temp[b + ob] = gi[b];\n\n    int offset = 1;\n    for (int d = N >> 1; d > 0; d >>= 1)\n    {\n      __syncthreads();\n      if (thid < d)\n      {\n        int ai = offset*(2*thid+1)-1;\n        int bi = offset*(2*thid+2)-1;\n        ai += OFFSET(ai);\n        bi += OFFSET(bi);\n        temp[bi] += temp[ai];\n      }\n      offset *= 2;\n    }\n\n    if (thid == 0) temp[N-1+OFFSET(N-1)] = 0; // clear the last elem\n    for (int d = 1; d < N; d *= 2) // traverse down\n    {\n      offset >>= 1;\n      __syncthreads();\n      if (thid < d)\n      {\n        int ai = offset*(2*thid+1)-1;\n        int bi = offset*(2*thid+2)-1;\n        ai += OFFSET(ai);\n        bi += OFFSET(bi);\n        T t = temp[ai];\n        temp[ai] = temp[bi];\n        temp[bi] += t;\n      }\n    }\n    __syncthreads(); // required\n\n    go[a] = temp[a + oa];\n    go[b] = temp[b + ob];\n  }\n}"
        ]
    },
    "f16max-cuda": {
        "/Users/gbolet/hecbench-roofline/src/f16max-cuda/main.cu": [
            "#define T ((int)32)\n\n\n__device__ half half_max(const half a, const half b) {\n  const half sub = __hsub(a, b);\n  const unsigned sign = (*reinterpret_cast<const short*>(&sub)) & 0x8000u;\n  const unsigned sw = 0x00000010 | ((sign >> 13) * 0x11);\n  const unsigned short res = __byte_perm(*reinterpret_cast<const short*>(&a), \n                                         *reinterpret_cast<const short*>(&b), sw);\n  return *reinterpret_cast<const half*>(&res);\n}\n\n__global__\nvoid hmax(T const *__restrict__ const a,\n          T const *__restrict__ const b,\n          T *__restrict__ const r,\n          const size_t size)\n{\n  for (size_t i = threadIdx.x + blockDim.x * blockIdx.x; \n              i < size; i += blockDim.x * gridDim.x)\n    r[i] = half_max(a[i], b[i]);\n}"
        ]
    },
    "mriQ-cuda": {
        "/Users/gbolet/hecbench-roofline/src/mriQ-cuda/computeQ.cu": [
            "__global__ void\nComputePhiMag_GPU(\n  const float* __restrict__ phiR,\n  const float* __restrict__ phiI,\n        float* __restrict__ phiMag,\n  const int numK)\n{\n  int indexK = blockIdx.x*KERNEL_PHI_MAG_THREADS_PER_BLOCK + threadIdx.x;\n  if (indexK < numK) {\n    float real = phiR[indexK];\n    float imag = phiI[indexK];\n    phiMag[indexK] = real*real + imag*imag;\n  }\n}",
            "__constant__ __device__ kValues ck[KERNEL_Q_K_ELEMS_PER_GRID];\n\n__global__ void\nComputeQ_GPU(\n  const int numK,\n        int kGlobalIndex,\n  const float* __restrict__ x,\n  const float* __restrict__ y,\n  const float* __restrict__ z,\n        float* __restrict__ Qr,\n        float* __restrict__ Qi)\n{\n  // Determine the element of the X arrays computed by this thread\n  int xIndex = blockIdx.x*KERNEL_Q_THREADS_PER_BLOCK + threadIdx.x;\n\n  // Read block's X values from global mem to shared mem\n  float sX = x[xIndex];\n  float sY = y[xIndex];\n  float sZ = z[xIndex];\n  float sQr = Qr[xIndex];\n  float sQi = Qi[xIndex];\n\n  // Loop over all elements of K in constant mem to compute a partial value\n  // for X.\n  int kIndex = 0;\n  if (numK % 2) {\n    float expArg = PIx2 * (ck[0].Kx * sX + ck[0].Ky * sY + ck[0].Kz * sZ);\n    sQr += ck[0].PhiMag * cosf(expArg);\n    sQi += ck[0].PhiMag * sinf(expArg);\n    kIndex++;\n    kGlobalIndex++;\n  }\n\n  for (; (kIndex < KERNEL_Q_K_ELEMS_PER_GRID) && (kGlobalIndex < numK);\n       kIndex += 2, kGlobalIndex += 2) {\n    float expArg = PIx2 * (ck[kIndex].Kx * sX +\n\t\t\t   ck[kIndex].Ky * sY +\n\t\t\t   ck[kIndex].Kz * sZ);\n    sQr += ck[kIndex].PhiMag * cosf(expArg);\n    sQi += ck[kIndex].PhiMag * sinf(expArg);\n\n    int kIndex1 = kIndex + 1;\n    float expArg1 = PIx2 * (ck[kIndex1].Kx * sX +\n\t\t\t    ck[kIndex1].Ky * sY +\n\t\t\t    ck[kIndex1].Kz * sZ);\n    sQr += ck[kIndex1].PhiMag * cosf(expArg1);\n    sQi += ck[kIndex1].PhiMag * sinf(expArg1);\n  }\n\n  Qr[xIndex] = sQr;\n  Qi[xIndex] = sQi;\n}"
        ]
    },
    "zoom-cuda": {
        "/Users/gbolet/hecbench-roofline/src/zoom-cuda/main.cu": [
            "inline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\ninline __host__ __device__ float4 floorf(float4 v)\n{\n    return make_float4(floorf(v.x), floorf(v.y), floorf(v.z), floorf(v.w));\n}\n\n__global__\nvoid zoom_in_kernel(\n    const float *input_tensor,float *output_tensor,\n    int input_h, int input_w, int output_h, int output_w,\n    int pitch, int out_h_start, int out_h_end,\n    int out_w_start, int out_w_end)\n{\n  extern __shared__ float staging_tile[];\n\n  // H -> block Y, row\n  // W -> block X, col\n  int out_start_h = blockIdx.y * blockDim.y;\n  int out_end_h   = (blockIdx.y + 1) * blockDim.y - 1;\n  int out_start_w = blockIdx.x * blockDim.x;\n  int out_end_w   = (blockIdx.x + 1) * blockDim.x - 1;\n\n  int img_start_offset = blockIdx.z * pitch;\n\n  // ideally should go in unified register\n  int smem_load_h_start = floorf((out_start_h * input_h) / (float)output_h);\n  int smem_load_h_end = ceilf(((out_end_h+1) * input_h) / (float)output_h);\n  int smem_h_load_stretch = smem_load_h_end - smem_load_h_start;\n\n  int smem_load_w_start = floorf((out_start_w * input_w) / (float)output_w);\n  int smem_load_w_end = ceilf(((out_end_w+1) * input_w) / (float)output_w);\n  int smem_w_load_stretch = smem_load_w_end - smem_load_w_start;\n\n  for (int i = threadIdx.y; i < smem_h_load_stretch; i+=blockDim.y) {\n    for (int j = threadIdx.x; j < smem_w_load_stretch; j+=blockDim.x) {\n\n      if (((i+smem_load_h_start) < input_h) &&\n          ((j+smem_load_w_start) < input_w)) {\n        staging_tile[i * smem_w_load_stretch + j] = \\\n                      input_tensor[img_start_offset +\n                      (smem_load_h_start + i) * input_w +\n                      smem_load_w_start + j];\n      } else {\n        staging_tile[i * smem_w_load_stretch + j] = 0.0f;\n      }\n    }\n  }\n  __syncthreads();\n\n  int out_pixel_h = blockIdx.y * blockDim.y + threadIdx.y;\n  int out_pixel_w = blockIdx.x * blockDim.x + threadIdx.x;\n\n  if (out_pixel_h < output_h && out_pixel_w < output_w\n      && out_pixel_h >= out_h_start && out_pixel_h < out_h_end\n      && out_pixel_w >= out_w_start && out_pixel_w < out_w_end) {\n\n    // compute pixels oh, ow span\n    int start_h = floorf((out_pixel_h * input_h) / (float)output_h);\n    int end_h = ceilf(((out_pixel_h+1) * input_h) / (float)output_h);\n\n    int start_w = floorf((out_pixel_w * input_w) / (float)output_w);\n    int end_w = ceilf(((out_pixel_w+1) * input_w) / (float)output_w);\n\n    int del_h = end_h - start_h;\n    int del_w = end_w - start_w;\n\n    float sum_ = 0.0f;\n\n    for (int i = 0; i < del_h; i++) {\n      for (int j = 0; j < del_w; j++) {\n        int smem_row = (start_h + i) - smem_load_h_start;\n        int smem_col = (start_w + j) - smem_load_w_start;\n        sum_ += staging_tile[smem_row * smem_w_load_stretch + smem_col];\n      }\n    }\n    sum_ /= (float)del_h;\n    sum_ /= (float)del_w;\n\n    output_tensor[(blockIdx.z * pitch) +\n      ((out_pixel_h - out_h_start) * input_w) +\n      (out_pixel_w - out_w_start)] = sum_;\n  }\n}",
            "inline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\ninline __host__ __device__ float4 floorf(float4 v)\n{\n    return make_float4(floorf(v.x), floorf(v.y), floorf(v.z), floorf(v.w));\n}\n\n__global__\nvoid zoom_out_kernel(\n    const float *input_tensor, float *output_tensor,\n    int input_h, int input_w, int output_h, int output_w,\n    int pitch, int out_h_start, int out_h_end, int out_w_start,\n    int out_w_end)\n{\n  extern __shared__ float staging_tile[];\n\n  // H -> block Y, row\n  // W -> block X, col\n  int out_start_h = blockIdx.y * blockDim.y;\n  int out_end_h   = (blockIdx.y + 1) * blockDim.y - 1;\n  int out_start_w = blockIdx.x * blockDim.x;\n  int out_end_w   = (blockIdx.x + 1) * blockDim.x - 1;\n\n  int img_start_offset = blockIdx.z * pitch;\n\n  // ideally should go in unified register\n  int smem_load_h_start = floorf((out_start_h * input_h) / (float)output_h);\n  int smem_load_h_end = ceilf(((out_end_h+1) * input_h) / (float)output_h);\n  int smem_h_load_stretch = smem_load_h_end - smem_load_h_start;\n\n  int smem_load_w_start = floorf((out_start_w * input_w) / (float)output_w);\n  int smem_load_w_end = ceilf(((out_end_w+1) * input_w) / (float)output_w);\n  int smem_w_load_stretch = smem_load_w_end - smem_load_w_start;\n\n  for (int i = threadIdx.y; i < smem_h_load_stretch; i+=blockDim.y) {\n    for (int j = threadIdx.x; j < smem_w_load_stretch; j+=blockDim.x) {\n\n      if (((i+smem_load_h_start) < input_h) &&\n          ((j+smem_load_w_start) < input_w)) {\n        staging_tile[i * smem_w_load_stretch + j] = \\\n                      input_tensor[img_start_offset +\n                      (smem_load_h_start + i)*input_w +\n                      smem_load_w_start + j];\n      } else {\n        staging_tile[i * smem_w_load_stretch + j] = 0.0f;\n      }\n    }\n  }\n  __syncthreads();\n\n  int out_pixel_h = blockIdx.y * blockDim.y + threadIdx.y;\n  int out_pixel_w = blockIdx.x * blockDim.x + threadIdx.x;\n\n  if (out_pixel_h < output_h && out_pixel_w < output_w) {\n\n    // compute pixels oh, ow span\n    int start_h = floorf((out_pixel_h * input_h) / (float)output_h);\n    int end_h = ceilf(((out_pixel_h+1) * input_h) / (float)output_h);\n\n    int start_w = floorf((out_pixel_w * input_w) / (float)output_w);\n    int end_w = ceilf(((out_pixel_w+1) * input_w) / (float)output_w);\n\n    int del_h = end_h - start_h;\n    int del_w = end_w - start_w;\n\n    float sum_ = 0.0f;\n\n    for (int i = 0; i < del_h; i++) {\n      for (int j = 0; j < del_w; j++) {\n        int smem_row = (start_h + i) - smem_load_h_start;\n        int smem_col = (start_w + j) - smem_load_w_start;\n        sum_ += staging_tile[smem_row * smem_w_load_stretch + smem_col];\n      }\n    }\n    sum_ /= (float)del_h;\n    sum_ /= (float)del_w;\n\n    output_tensor[(blockIdx.z * pitch) +\n      ((out_pixel_h + out_h_start) * input_w) +\n      (out_pixel_w + out_w_start)] = sum_;\n  }\n}",
            "__global__\nvoid zoom_out_edge_pad(\n    float *output_tensor,\n    int height, int width,\n    int pitch, int no_padding_h_start,\n    int no_padding_w_start,\n    int no_padding_h_end, int no_padding_w_end) {\n  // H -> block Y, row\n  // W -> block X, col\n\n  int out_pixel_h = blockIdx.y * blockDim.y + threadIdx.y;\n  int out_pixel_w = blockIdx.x * blockDim.x + threadIdx.x;\n\n  // no_padding_h_end, no_padding_w_end --> w_cropped+wstart, same for height\n  int out_location = (blockIdx.z * pitch) + (out_pixel_h * width) + out_pixel_w;\n\n  if (out_pixel_h < height && out_pixel_w < width) {\n    if (out_pixel_h < no_padding_h_start && out_pixel_w >= no_padding_w_start\n        && out_pixel_w < no_padding_w_end) {\n      // top pad\n      output_tensor[out_location] = output_tensor[(blockIdx.z * pitch) +\n        (no_padding_h_start * width) + out_pixel_w];\n    } else if (out_pixel_h >= no_padding_h_end\n        && out_pixel_w >= no_padding_w_start\n        && out_pixel_w < no_padding_w_end) {\n      // bottom pad\n      output_tensor[out_location] = output_tensor[(blockIdx.z * pitch) +\n        ((no_padding_h_end-1) * width) + out_pixel_w];\n    } else if (out_pixel_w < no_padding_w_start\n        && out_pixel_h >= no_padding_h_start\n        && out_pixel_h < no_padding_h_end) {\n      // left pad\n      output_tensor[out_location] = output_tensor[(blockIdx.z * pitch) +\n        (out_pixel_h * width) + no_padding_w_start];\n    } else if (out_pixel_w >= no_padding_w_end\n        && out_pixel_h >= no_padding_h_start\n        && out_pixel_h < no_padding_h_end) {\n      // right pad\n      output_tensor[out_location] = output_tensor[(blockIdx.z * pitch) +\n        (out_pixel_h * width) + (no_padding_w_end-1)];\n    } else if (out_pixel_h < no_padding_h_start\n        && out_pixel_w < no_padding_w_start) {\n      // top-left corner\n      output_tensor[out_location] = output_tensor[(blockIdx.z * pitch) +\n        (no_padding_h_start * width) +\n        no_padding_w_start];\n    } else if (out_pixel_h < no_padding_h_start\n        && out_pixel_w >= no_padding_w_end) {\n      // top-right corner\n      output_tensor[out_location] = output_tensor[(blockIdx.z * pitch) +\n        (no_padding_h_start * width) +\n        (no_padding_w_end-1)];\n    } else if (out_pixel_h >= no_padding_h_end\n        && out_pixel_w < no_padding_w_start) {\n      // bottom-left corner\n      output_tensor[out_location] = output_tensor[(blockIdx.z * pitch) +\n        ((no_padding_h_end-1) * width) +\n        no_padding_w_start];\n    } else if (out_pixel_h >= no_padding_h_end\n        && out_pixel_w >= no_padding_w_end) {\n      // bottom-right corner\n      output_tensor[out_location] = output_tensor[(blockIdx.z * pitch) +\n        ((no_padding_h_end-1) * width) +\n        (no_padding_w_end-1)];\n    }\n  }\n}"
        ]
    },
    "adamw-cuda": {
        "/Users/gbolet/hecbench-roofline/src/adamw-cuda/kernels.h": [
            "#define T ((int)32)\n\n\ninline __host__ __device__ uint4 make_uint4(int4 a)\n{\n    return make_uint4(uint(a.x), uint(a.y), uint(a.z), uint(a.w));\n}\n\ninline __host__ __device__ uint4 max(uint4 a, uint4 b)\n{\n    return make_uint4(max(a.x,b.x), max(a.y,b.y), max(a.z,b.z), max(a.w,b.w));\n}\n\ninline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\ninline __host__ __device__ float4 sqrtf(float4 v)\n{\n    return make_float4(sqrtf(v.x), sqrtf(v.y), sqrtf(v.z), sqrtf(v.w));\n}\n\n__device__ __forceinline__ void seq_threads_max_reducer(int tid, float* local_val) {\n\n        //unsigned mask = 0xFFFFFFFFU;\n        int lane = tid % 32;\n        int warpId = tid / 32;\n        float val = *local_val;\n        int offset = 16;\n\n        for (offset = 16; offset > 0; offset >>=1) {\n            val = max(val, __shfl_down_sync(__activemask(), val, offset));\n        }\n\n        if (lane==0) {\n            _exp_reducer[warpId] = val;\n        }\n        __syncthreads();\n\n        // final warp reduction with warp 0 only\n\n        // careful - this assumes q block size of 128...expand to loop if larger\n        if (warpId ==0) {\n            if (tid < 2) val = _exp_reducer[lane];\n\n            offset = 1;\n            val = max(val, __shfl_down_sync(__activemask(), val, offset));\n\n            if (tid==0) {\n                *local_val = val;\n            }\n        }\n\n}\n\n__global__ void fused_4bit_kernel(\n    T* __restrict__ p,\n    const T* __restrict__ g,\n    T* __restrict__ exp_qscale,// m\n    T* __restrict__ sq_qscale, // v\n    int8_t* __restrict__ exp,\n    int8_t* __restrict__ sq,\n    const float beta1,\n    const float beta2,\n    const float lr,\n    const float weight_decay,\n    const float eps,\n    const float step,\n    const int64_t total_size,\n    const float correction1,\n    const float correction2_sqrt,\n    const float step_size,\n    const float weight_decay_update,\n    const float resid_beta1,\n    const float resid_beta2)\n{\n    __shared__ float absmax_exp;\n    __shared__ float absmax_sq;\n\n    int64_t global_id = blockIdx.x * blockDim.x + threadIdx.x;\n    if (global_id < total_size) {\n\n      if (threadIdx.x == 0) {\n          absmax_exp = 0;\n          absmax_sq = 0;\n      }\n\n      const int8_t exp_full = exp[global_id];\n      const int8_t sq_full = sq[global_id];\n\n      float2 p2 = reinterpret_cast<float2*>(p)[global_id];\n      const float2 g2 = reinterpret_cast<const float2*>(g)[global_id];\n\n      // left side processing -------------------------------------\n      const int8_t exp_left_index = exp_full & _bitmask;\n      const int8_t sq_left_index = sq_full & _bitmask;\n\n      //decoupled weight decay\n      p2.x = p2.x * weight_decay_update;\n\n      // left exp and sq updates\n      float exp_avg_qscale = exp_qscale[blockIdx.x];\n\n      float exp_left = _exp_qmap[exp_left_index] * exp_avg_qscale;\n      exp_left = beta1 * exp_left + resid_beta1 * g2.x;\n\n      float sq_left = _sq_qmap[sq_left_index] * sq_qscale[blockIdx.x];\n      sq_left = beta2 * sq_left + resid_beta2 * (g2.x * g2.x);\n\n      // param update\n      p[global_id*2] = p2.x - (step_size * (exp_left/(sqrtf(sq_left) / correction2_sqrt + eps)));\n\n      // right side processing -------------------------------\n\n      const int8_t exp_right_index = (exp_full >> 4) & _bitmask;\n      const int8_t sq_right_index = (sq_full >> 4) & _bitmask;\n\n      //decoupled weight decay, right side\n      p2.y = p2.y * weight_decay_update;\n\n      float exp_right = _exp_qmap[exp_right_index] * exp_avg_qscale;\n      exp_right = beta1 * exp_right + resid_beta1 * g2.y;\n\n      float sq_right = _sq_qmap[sq_right_index] * sq_qscale[blockIdx.x];\n      sq_right = beta2 * sq_right + resid_beta2 * (g2.y * g2.y);\n\n      // param update\n      p[global_id*2+1] = p2.y - (step_size * (exp_right/(sqrtf(sq_right) / correction2_sqrt + eps)));\n\n      // prepare quantization info - update absmax scales\n      float local_absmax_exp = max((float)exp_left, (float)exp_right);\n      float local_absmax_sq = max((float)sq_left, (float)sq_right);\n\n      // --- sequential threads parallel reduction to\n      // determine global absmax for exp\n      seq_threads_max_reducer(threadIdx.x, &local_absmax_exp);\n      if (threadIdx.x ==0) {\n          exp_qscale[blockIdx.x] = local_absmax_exp;\n          absmax_exp = local_absmax_exp;\n      }\n\n      // same for sq\n      seq_threads_max_reducer(threadIdx.x, &local_absmax_sq);\n      if (threadIdx.x ==0) {\n          sq_qscale[blockIdx.x] = local_absmax_sq;\n          absmax_sq = local_absmax_sq;\n      }\n      __syncthreads();\n\n      int8_t local_packed_exp = 0;\n      int8_t local_packed_sq = 0;\n\n      // quantize and pack\n      const int8_t q_exp_left = (int8_t)q_mapping(_exp_qmap, _exp_qmidpt, (float)exp_left / absmax_exp);\n      const int8_t q_sq_left = (int8_t)q_mapping(_sq_qmap, _sq_qmidpt, (float)sq_left / absmax_sq);\n      local_packed_exp |= (q_exp_left & _bitmask);\n      local_packed_sq |= (q_sq_left & _bitmask);\n\n      const int8_t q_exp_right = (int8_t)q_mapping(_exp_qmap, _exp_qmidpt, (float)exp_right / absmax_exp);\n      const int8_t q_sq_right = (int8_t)q_mapping(_sq_qmap, _sq_qmidpt, (float)sq_right / absmax_sq);\n      local_packed_exp |= (q_exp_right & _right_pack_bitmask);\n      local_packed_sq |= (q_sq_right & _right_pack_bitmask);\n\n      // store updated exp and sq\n      exp[global_id] = local_packed_exp;\n      sq[global_id] = local_packed_sq;\n\n      __syncthreads();\n   }\n}"
        ]
    },
    "langevin-cuda": {
        "/Users/gbolet/hecbench-roofline/src/langevin-cuda/main.cu": [
            "__global__\nvoid k0 (const float *__restrict__ a, float *__restrict__ o) {\n  int t = blockIdx.x * blockDim.x + threadIdx.x;\n  float x = a[t];\n  o[t] = coshf(x)/sinhf(x) - 1.f/x;\n}"
        ]
    },
    "all-pairs-distance-cuda": {
        "/Users/gbolet/hecbench-roofline/src/all-pairs-distance-cuda/main.cu": [
            "__global__ void k1 (const char *data, int *distance) {\n  int idx = threadIdx.x;\n  int gx = blockIdx.x;\n  int gy = blockIdx.y;\n\n  for(int i = 4*idx; i < ATTRIBUTES; i+=THREADS*4) {\n    char4 j = *(char4 *)(data + i + ATTRIBUTES*gx);\n    char4 k = *(char4 *)(data + i + ATTRIBUTES*gy);\n\n    /* use a local variable (stored in register) to hold intermediate\n       values. This reduces writes to global memory */\n    char count = 0;\n\n    if(j.x ^ k.x) \n      count++; \n    if(j.y ^ k.y)\n      count++;\n    if(j.z ^ k.z)\n      count++;\n    if(j.w ^ k.w)\n      count++;\n\n    /* atomic write to global memory */\n    atomicAdd(distance + INSTANCES*gx + gy, count);\n  }\n}",
            "__global__ void k2 (const char *data, int *distance) {\n  int idx = threadIdx.x;\n  int gx = blockIdx.x;\n  int gy = blockIdx.y;\n\n  /* Shared memory is the other major memory (other than registers and\n     global). It is used to store values between multiple threads. In\n     particular, the shared memory access is defined by the __shared__\n     attribute and it is a special area of memory on the GPU\n     itself. Because the memory is on the chip, it is a lot faster\n     than global memory. Multiple threads can still access it, though,\n     provided they are in the same block.\n   */\n  __shared__ int dist[THREADS];\n\n  /* each thread initializes its own location of the shared array */ \n  dist[idx] = 0;\n\n  /* At this point, the threads must be synchronized to ensure that\n     the shared array is fully initialized. */\n  __syncthreads();\n\n  for(int i = idx*4; i < ATTRIBUTES; i+=THREADS*4) {\n    char4 j = *(char4 *)(data + i + ATTRIBUTES*gx);\n    char4 k = *(char4 *)(data + i + ATTRIBUTES*gy);\n    char count = 0;\n\n    if(j.x ^ k.x) \n      count++;\n    if(j.y ^ k.y)\n      count++;\n    if(j.z ^ k.z)\n      count++;\n    if(j.w ^ k.w)\n      count++;\n\n    /* Increment shared array */\n    dist[idx] += count;\n  }\n\n  /* Synchronize threads to make sure all have completed their updates\n     of the shared array. Since the distances for each thread are read\n     by thread 0 below, this must be ensured. Above, it was not\n     necessary because each thread was accessing its own memory\n   */\n  __syncthreads();\n\n  /* Reduction: Thread 0 will add the value of all other threads to\n     its own */ \n  if(idx == 0) {\n    for(int i = 1; i < THREADS; i++) {\n      dist[0] += dist[i];\n    }\n\n    /* Thread 0 will then write the output to global memory. Note that\n       this does not need to be performed atomically, because only one\n       thread per block is writing to global memory, and each block\n       corresponds to a unique memory address. \n     */\n    distance[INSTANCES*gy + gx] = dist[0];\n  }\n}",
            "#define THREADS ((int)B*T)\n\n\n__global__ void k3 (const char *data, int *distance) {\n  int idx = threadIdx.x;\n  int gx = blockIdx.x;\n  int gy = blockIdx.y;\n\n  typedef cub::BlockReduce<int, THREADS> BlockReduce;\n  __shared__ typename BlockReduce::TempStorage temp_storage;\n\n  int dist = 0;\n\n  for(int i = idx*4; i < ATTRIBUTES; i+=THREADS*4) {\n    char4 j = *(char4 *)(data + i + ATTRIBUTES*gx);\n    char4 k = *(char4 *)(data + i + ATTRIBUTES*gy);\n    char count = 0;\n\n    if(j.x ^ k.x) \n      count++;\n    if(j.y ^ k.y)\n      count++;\n    if(j.z ^ k.z)\n      count++;\n    if(j.w ^ k.w)\n      count++;\n\n    dist += count;\n  }\n\n  int sum = BlockReduce(temp_storage).Sum(dist);\n\n  if(idx == 0) {\n    distance[INSTANCES*gy + gx] = sum;\n  }\n}"
        ]
    },
    "rsc-cuda": {
        "/Users/gbolet/hecbench-roofline/src/rsc-cuda/model_eval.cu": [
            "inline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\ninline __host__ __device__ float4 fabs(float4 v)\n{\n    return make_float4(fabs(v.x), fabs(v.y), fabs(v.z), fabs(v.w));\n}\n\n__global__\nvoid RANSAC_kernel_block(const float *__restrict__ model_param_local,\n                         const flowvector *__restrict__ flowvectors,\n                         int flowvector_count,\n                         int max_iter,\n                         int error_threshold,\n                         float convergence_threshold,\n                         int *__restrict__ g_out_id,\n                         int *__restrict__ model_candidate,\n                         int *__restrict__ outliers_candidate) {\n\n  extern __shared__ int l_mem[];\n  int* outlier_block_count = l_mem;\n\n  const int tx         = threadIdx.x;\n  const int bx         = blockIdx.x;\n  const int num_blocks = gridDim.x;\n\n  float vx_error, vy_error;\n  int   outlier_local_count = 0;\n\n  // Each block performs one iteration\n  for(int loop_count = bx; loop_count < max_iter; loop_count += num_blocks) {\n\n    // xc=model_param_sh[0], yc=model_param_sh[1], D=model_param_sh[2], R=model_param_sh[3]\n    const float *model_param = &model_param_local [4 * loop_count];\n\n    // Wait until CPU computes F-o-F model\n    if(tx == 0) {\n      outlier_block_count[0] = 0;\n    }\n    __syncthreads();\n\n    if(model_param[0] == -2011)\n      continue;\n\n    // Reset local outlier counter\n    outlier_local_count = 0;\n\n    // Compute number of outliers\n    for(int i = tx; i < flowvector_count; i += blockDim.x) {\n      flowvector fvreg = flowvectors[i]; // x, y, vx, vy\n      vx_error         = fvreg.x + ((int)((fvreg.x - model_param[0]) * model_param[2]) -\n                         (int)((fvreg.y - model_param[1]) * model_param[3])) - fvreg.vx;\n      vy_error = fvreg.y + ((int)((fvreg.y - model_param[1]) * model_param[2]) +\n                 (int)((fvreg.x - model_param[0]) * model_param[3])) - fvreg.vy;\n      if((fabs(vx_error) >= error_threshold) || (fabs(vy_error) >= error_threshold)) {\n        outlier_local_count++;\n      }\n    }\n\n    atomicAdd(outlier_block_count, outlier_local_count);\n\n    __syncthreads();\n\n    if(tx == 0) {\n      // Compare to threshold\n      if(outlier_block_count[0] < flowvector_count * convergence_threshold) {\n        int index                 = atomicAdd(g_out_id, 1);\n        model_candidate[index]    = loop_count;\n        outliers_candidate[index] = outlier_block_count[0];\n      }\n    }\n  }\n}"
        ]
    },
    "vmc-cuda": {
        "/Users/gbolet/hecbench-roofline/src/vmc-cuda/vmc.cu": [
            "#define FLOAT \t\t1\n\n\n__global__ void SumWithinBlocks(const int n, const FLOAT* data, FLOAT* blocksums) {\n  int nthread = blockDim.x*gridDim.x;\n  int i = blockDim.x * blockIdx.x + threadIdx.x;\n  __shared__ FLOAT sdata[512];  // max threads\n\n  // Every thread in every block computes partial sum over rest of vector\n  FLOAT st=ZERO;\n  while (i < n) {\n    st += data[i];\n    i+=nthread;\n  }\n  sdata[threadIdx.x] = st;\n  __syncthreads();\n\n  // Now do binary tree sum within a block\n\n  int tid = threadIdx.x;\n  for (unsigned int s=128; s>0; s>>=1) {\n    if (tid<s && (tid+s)<blockDim.x) {\n      sdata[tid] += sdata[tid + s];\n    }\n    __syncthreads();\n  }\n  if (tid==0) blocksums[blockIdx.x] = sdata[0];\n}",
            "__global__ void initran(unsigned int seed, curandState_t* states) {\n  int i = blockDim.x * blockIdx.x + threadIdx.x;\n  curand_init(seed, i, 0, &states[i]);\n}",
            "#define FLOAT \t\t1\n\n\n__global__ void zero_stats(int Npoint, FLOAT* stats) {\n  int i = blockDim.x * blockIdx.x + threadIdx.x;\n  stats[0*Npoint+i] = ZERO; // r1\n  stats[1*Npoint+i] = ZERO; // r2\n  stats[2*Npoint+i] = ZERO; // r12\n  stats[3*Npoint+i] = ZERO; // accept count\n}",
            "#define FLOAT \t\t1\n\n\ninline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\ninline __host__ __device__ float4 expf(float4 v)\n{\n    return make_float4(expf(v.x), expf(v.y), expf(v.z), expf(v.w));\n}\n\n__device__ __forceinline__ double SQRT(double x) {return sqrt(x);}\n\n__device__ __forceinline__ float EXP<float>(float in)\n{\n    return expf(in);\n}\n\n__device__ __forceinline__ void compute_distances(FLOAT x1, FLOAT y1, FLOAT z1, FLOAT x2, FLOAT y2, FLOAT z2,\n    FLOAT& r1, FLOAT& r2, FLOAT& r12) {\n  r1 = SQRT(x1*x1 + y1*y1 + z1*z1);\n  r2 = SQRT(x2*x2 + y2*y2 + z2*z2);\n  FLOAT xx = x1-x2;\n  FLOAT yy = y1-y2;\n  FLOAT zz = z1-z2;\n  r12 = SQRT(xx*xx + yy*yy + zz*zz);\n}\n\n__device__ __forceinline__ double rand<double>(curandState_t* state) {\n  return curand_uniform_double(state);\n}\n\n__device__  __forceinline__ FLOAT wave_function(FLOAT x1, FLOAT y1, FLOAT z1, FLOAT x2, FLOAT y2, FLOAT z2) {\n  FLOAT r1, r2, r12;\n  compute_distances(x1, y1, z1, x2, y2, z2, r1, r2, r12);\n  return (ONE + HALF*r12)*EXP(-TWO*(r1 + r2));\n}\n\n__global__ void initialize(FLOAT* x1, FLOAT* y1, FLOAT* z1, FLOAT* x2, FLOAT* y2, FLOAT* z2, FLOAT* psi, curandState_t* states) {\n  int i = blockDim.x * blockIdx.x + threadIdx.x;\n  x1[i] = (rand<FLOAT>(states+i) - HALF)*FOUR;\n  y1[i] = (rand<FLOAT>(states+i) - HALF)*FOUR;\n  z1[i] = (rand<FLOAT>(states+i) - HALF)*FOUR;\n  x2[i] = (rand<FLOAT>(states+i) - HALF)*FOUR;\n  y2[i] = (rand<FLOAT>(states+i) - HALF)*FOUR;\n  z2[i] = (rand<FLOAT>(states+i) - HALF)*FOUR;\n  psi[i] = wave_function(x1[i], y1[i], z1[i], x2[i], y2[i], z2[i]);\n}",
            "#define FLOAT \t\t1\n\n\n__device__ __forceinline__ double SQRT(double x) {return sqrt(x);}\n\ninline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\ninline __host__ __device__ float4 expf(float4 v)\n{\n    return make_float4(expf(v.x), expf(v.y), expf(v.z), expf(v.w));\n}\n\n__device__ __forceinline__ float EXP<float>(float in)\n{\n    return expf(in);\n}\n\n__device__ __forceinline__ void compute_distances(FLOAT x1, FLOAT y1, FLOAT z1, FLOAT x2, FLOAT y2, FLOAT z2,\n    FLOAT& r1, FLOAT& r2, FLOAT& r12) {\n  r1 = SQRT(x1*x1 + y1*y1 + z1*z1);\n  r2 = SQRT(x2*x2 + y2*y2 + z2*z2);\n  FLOAT xx = x1-x2;\n  FLOAT yy = y1-y2;\n  FLOAT zz = z1-z2;\n  r12 = SQRT(xx*xx + yy*yy + zz*zz);\n}\n\n__device__ __forceinline__ double rand<double>(curandState_t* state) {\n  return curand_uniform_double(state);\n}\n\n__device__  __forceinline__ FLOAT wave_function(FLOAT x1, FLOAT y1, FLOAT z1, FLOAT x2, FLOAT y2, FLOAT z2) {\n  FLOAT r1, r2, r12;\n  compute_distances(x1, y1, z1, x2, y2, z2, r1, r2, r12);\n  return (ONE + HALF*r12)*EXP(-TWO*(r1 + r2));\n}\n\n__global__ void propagate(const int Npoint, const int nstep, FLOAT* X1, FLOAT* Y1, FLOAT* Z1, \n                          FLOAT* X2, FLOAT* Y2, FLOAT* Z2, FLOAT* P, FLOAT* stats, curandState_t* states) {\n  int i = blockDim.x * blockIdx.x + threadIdx.x;\n  FLOAT x1 = X1[i];\n  FLOAT y1 = Y1[i];\n  FLOAT z1 = Z1[i];\n  FLOAT x2 = X2[i];\n  FLOAT y2 = Y2[i];\n  FLOAT z2 = Z2[i];\n  FLOAT p = P[i];\n\n  for (int step=0; step<nstep; step++) {\n    FLOAT x1new = x1 + (rand<FLOAT>(states+i)-HALF)*DELTA;\n    FLOAT y1new = y1 + (rand<FLOAT>(states+i)-HALF)*DELTA;\n    FLOAT z1new = z1 + (rand<FLOAT>(states+i)-HALF)*DELTA;\n    FLOAT x2new = x2 + (rand<FLOAT>(states+i)-HALF)*DELTA;\n    FLOAT y2new = y2 + (rand<FLOAT>(states+i)-HALF)*DELTA;\n    FLOAT z2new = z2 + (rand<FLOAT>(states+i)-HALF)*DELTA;\n    FLOAT pnew = wave_function(x1new, y1new, z1new, x2new, y2new, z2new);\n\n    if (pnew*pnew > p*p*rand<FLOAT>(states+i)) {\n      stats[3*Npoint+i]++; //naccept ++;\n      p = pnew;\n      x1 = x1new;\n      y1 = y1new;\n      z1 = z1new;\n      x2 = x2new;\n      y2 = y2new;\n      z2 = z2new;\n    }\n\n    FLOAT r1, r2, r12;\n    compute_distances(x1, y1, z1, x2, y2, z2, r1, r2, r12);\n\n    stats[0*Npoint+i] += r1;\n    stats[1*Npoint+i] += r2;\n    stats[2*Npoint+i] += r12;\n  }\n  X1[i] = x1;  \n  Y1[i] = y1;  \n  Z1[i] = z1;  \n  X2[i] = x2;  \n  Y2[i] = y2;  \n  Z2[i] = z2;  \n  P[i] = p;\n}"
        ]
    },
    "p2p-cuda": {
        "/Users/gbolet/hecbench-roofline/src/p2p-cuda/main.cu": [
            "__global__ void SimpleKernel(const float *src, float *dst)\n{\n  // Just a dummy kernel, doing enough for us to verify that everything\n  // worked\n  const int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  dst[idx] = src[idx] * 2.0f;\n}"
        ]
    },
    "opticalFlow-cuda": {
        "/Users/gbolet/hecbench-roofline/src/opticalFlow-cuda/warpingKernel.cuh": [
            "INLINE __device__\nuint2 tex2D(const int rows, const int cols, const int _c, const int _r,\n            const uint sample_method) {\n  int c = _c;\n  int r = _r;\n  if (sample_method == ADDRESS_REFLECT_BORDER_EXCLUSIVE) {\n    c = c < 0 ? -c : c;\n    c = c >= cols ? cols - (c - cols) - 2 : c;\n    r = r < 0 ? -r : r;\n    r = r >= rows ? rows - (r - rows) - 2 : r;\n  } else if (sample_method == ADDRESS_CLAMP) {\n    c = c < 0 ? 0 : c;\n    c = c > cols - 1 ? cols - 1 : c;\n    r = r < 0 ? 0 : r;\n    r = r > rows - 1 ? rows - 1 : r;\n  } else if (sample_method == ADDRESS_REFLECT_BORDER_INCLUSIVE) {\n    c = c < 0 ? -c - 1 : c;\n    c = c >= cols ? cols - (c - cols) - 1 : c;\n    r = r < 0 ? -r - 1 : r;\n    r = r >= rows ? rows - (r - rows) - 1 : r;\n  } else if (sample_method == ADDRESS_ZERO) {\n  } else if (sample_method == ADDRESS_NOOP) {\n  } else {\n    assert(false);\n  }\n  assert_val(r >= 0 && r < rows, r);\n  assert_val(c >= 0 && c < cols, c);\n  uint2 result; \n  result.x = r;\n  result.y = c;\n  return result;\n}\n\n__global__ void WarpingKernel(int width, int height, int stride, const float *u,\n                              const float *v, float *out,\n                              cudaTextureObject_t texToWarp) {\n  const int ix = threadIdx.x + blockIdx.x * blockDim.x;\n  const int iy = threadIdx.y + blockIdx.y * blockDim.y;\n\n  const int pos = ix + iy * stride;\n\n  if (ix >= width || iy >= height) return;\n\n  float x = ((float)ix + u[pos] + 0.5f) / (float)width;\n  float y = ((float)iy + v[pos] + 0.5f) / (float)height;\n\n  out[pos] = tex2D<float>(texToWarp, x, y);\n}"
        ],
        "/Users/gbolet/hecbench-roofline/src/opticalFlow-cuda/addKernel.cuh": [
            "__global__ void AddKernel(const float *op1, const float *op2, int count,\n                          float *sum) {\n  const int pos = threadIdx.x + blockIdx.x * blockDim.x;\n\n  if (pos >= count) return;\n\n  sum[pos] = op1[pos] + op2[pos];\n}"
        ],
        "/Users/gbolet/hecbench-roofline/src/opticalFlow-cuda/derivativesKernel.cuh": [
            "INLINE __device__\nuint2 tex2D(const int rows, const int cols, const int _c, const int _r,\n            const uint sample_method) {\n  int c = _c;\n  int r = _r;\n  if (sample_method == ADDRESS_REFLECT_BORDER_EXCLUSIVE) {\n    c = c < 0 ? -c : c;\n    c = c >= cols ? cols - (c - cols) - 2 : c;\n    r = r < 0 ? -r : r;\n    r = r >= rows ? rows - (r - rows) - 2 : r;\n  } else if (sample_method == ADDRESS_CLAMP) {\n    c = c < 0 ? 0 : c;\n    c = c > cols - 1 ? cols - 1 : c;\n    r = r < 0 ? 0 : r;\n    r = r > rows - 1 ? rows - 1 : r;\n  } else if (sample_method == ADDRESS_REFLECT_BORDER_INCLUSIVE) {\n    c = c < 0 ? -c - 1 : c;\n    c = c >= cols ? cols - (c - cols) - 1 : c;\n    r = r < 0 ? -r - 1 : r;\n    r = r >= rows ? rows - (r - rows) - 1 : r;\n  } else if (sample_method == ADDRESS_ZERO) {\n  } else if (sample_method == ADDRESS_NOOP) {\n  } else {\n    assert(false);\n  }\n  assert_val(r >= 0 && r < rows, r);\n  assert_val(c >= 0 && c < cols, c);\n  uint2 result; \n  result.x = r;\n  result.y = c;\n  return result;\n}\n\n__global__ void ComputeDerivativesKernel(int width, int height, int stride,\n                                         float *Ix, float *Iy, float *Iz,\n                                         cudaTextureObject_t texSource,\n                                         cudaTextureObject_t texTarget) {\n  const int ix = threadIdx.x + blockIdx.x * blockDim.x;\n  const int iy = threadIdx.y + blockIdx.y * blockDim.y;\n\n  const int pos = ix + iy * stride;\n\n  if (ix >= width || iy >= height) return;\n\n  float dx = 1.0f / (float)width;\n  float dy = 1.0f / (float)height;\n\n  float x = ((float)ix + 0.5f) * dx;\n  float y = ((float)iy + 0.5f) * dy;\n\n  float t0, t1;\n  // x derivative\n  t0 = tex2D<float>(texSource, x - 2.0f * dx, y);\n  t0 -= tex2D<float>(texSource, x - 1.0f * dx, y) * 8.0f;\n  t0 += tex2D<float>(texSource, x + 1.0f * dx, y) * 8.0f;\n  t0 -= tex2D<float>(texSource, x + 2.0f * dx, y);\n  t0 /= 12.0f;\n\n  t1 = tex2D<float>(texTarget, x - 2.0f * dx, y);\n  t1 -= tex2D<float>(texTarget, x - 1.0f * dx, y) * 8.0f;\n  t1 += tex2D<float>(texTarget, x + 1.0f * dx, y) * 8.0f;\n  t1 -= tex2D<float>(texTarget, x + 2.0f * dx, y);\n  t1 /= 12.0f;\n\n  Ix[pos] = (t0 + t1) * 0.5f;\n\n  // t derivative\n  Iz[pos] = tex2D<float>(texTarget, x, y) - tex2D<float>(texSource, x, y);\n\n  // y derivative\n  t0 = tex2D<float>(texSource, x, y - 2.0f * dy);\n  t0 -= tex2D<float>(texSource, x, y - 1.0f * dy) * 8.0f;\n  t0 += tex2D<float>(texSource, x, y + 1.0f * dy) * 8.0f;\n  t0 -= tex2D<float>(texSource, x, y + 2.0f * dy);\n  t0 /= 12.0f;\n\n  t1 = tex2D<float>(texTarget, x, y - 2.0f * dy);\n  t1 -= tex2D<float>(texTarget, x, y - 1.0f * dy) * 8.0f;\n  t1 += tex2D<float>(texTarget, x, y + 1.0f * dy) * 8.0f;\n  t1 -= tex2D<float>(texTarget, x, y + 2.0f * dy);\n  t1 /= 12.0f;\n\n  Iy[pos] = (t0 + t1) * 0.5f;\n}"
        ],
        "/Users/gbolet/hecbench-roofline/src/opticalFlow-cuda/upscaleKernel.cuh": [
            "INLINE __device__\nuint2 tex2D(const int rows, const int cols, const int _c, const int _r,\n            const uint sample_method) {\n  int c = _c;\n  int r = _r;\n  if (sample_method == ADDRESS_REFLECT_BORDER_EXCLUSIVE) {\n    c = c < 0 ? -c : c;\n    c = c >= cols ? cols - (c - cols) - 2 : c;\n    r = r < 0 ? -r : r;\n    r = r >= rows ? rows - (r - rows) - 2 : r;\n  } else if (sample_method == ADDRESS_CLAMP) {\n    c = c < 0 ? 0 : c;\n    c = c > cols - 1 ? cols - 1 : c;\n    r = r < 0 ? 0 : r;\n    r = r > rows - 1 ? rows - 1 : r;\n  } else if (sample_method == ADDRESS_REFLECT_BORDER_INCLUSIVE) {\n    c = c < 0 ? -c - 1 : c;\n    c = c >= cols ? cols - (c - cols) - 1 : c;\n    r = r < 0 ? -r - 1 : r;\n    r = r >= rows ? rows - (r - rows) - 1 : r;\n  } else if (sample_method == ADDRESS_ZERO) {\n  } else if (sample_method == ADDRESS_NOOP) {\n  } else {\n    assert(false);\n  }\n  assert_val(r >= 0 && r < rows, r);\n  assert_val(c >= 0 && c < cols, c);\n  uint2 result; \n  result.x = r;\n  result.y = c;\n  return result;\n}\n\n__global__ void UpscaleKernel(int width, int height, int stride, float scale,\n                              float *out, cudaTextureObject_t texCoarse) {\n  const int ix = threadIdx.x + blockIdx.x * blockDim.x;\n  const int iy = threadIdx.y + blockIdx.y * blockDim.y;\n\n  if (ix >= width || iy >= height) return;\n\n  float x = ((float)ix + 0.5f) / (float)width;\n  float y = ((float)iy + 0.5f) / (float)height;\n\n  // exploit hardware interpolation\n  // and scale interpolated vector to match next pyramid level resolution\n  out[ix + iy * stride] = tex2D<float>(texCoarse, x, y) * scale;\n}"
        ],
        "/Users/gbolet/hecbench-roofline/src/opticalFlow-cuda/solverKernel.cuh": [
            "inline __host__ __device__ uint4 make_uint4(int4 a)\n{\n    return make_uint4(uint(a.x), uint(a.y), uint(a.z), uint(a.w));\n}\n\ninline __host__ __device__ uint4 max(uint4 a, uint4 b)\n{\n    return make_uint4(max(a.x,b.x), max(a.y,b.y), max(a.z,b.z), max(a.w,b.w));\n}\n\ninline __host__ __device__ uint4 min(uint4 a, uint4 b)\n{\n    return make_uint4(min(a.x,b.x), min(a.y,b.y), min(a.z,b.z), min(a.w,b.w));\n}\n\n__global__ void JacobiIteration(const float *du0, const float *dv0,\n                                const float *Ix, const float *Iy,\n                                const float *Iz, int w, int h, int s,\n                                float alpha, float *du1, float *dv1) {\n  // Handle to thread block group\n  cg::thread_block cta = cg::this_thread_block();\n\n  volatile __shared__ float du[(bx + 2) * (by + 2)];\n  volatile __shared__ float dv[(bx + 2) * (by + 2)];\n\n  const int ix = threadIdx.x + blockIdx.x * blockDim.x;\n  const int iy = threadIdx.y + blockIdx.y * blockDim.y;\n\n  // position within global memory array\n  const int pos = min(ix, w - 1) + min(iy, h - 1) * s;\n\n  // position within shared memory array\n  const int shMemPos = threadIdx.x + 1 + (threadIdx.y + 1) * (bx + 2);\n\n  // Load data to shared memory.\n  // load tile being processed\n  du[shMemPos] = du0[pos];\n  dv[shMemPos] = dv0[pos];\n\n  // load necessary neighbouring elements\n  // We clamp out-of-range coordinates.\n  // It is equivalent to mirroring\n  // because we access data only one step away from borders.\n  if (threadIdx.y == 0) {\n    // beginning of the tile\n    const int bsx = blockIdx.x * blockDim.x;\n    const int bsy = blockIdx.y * blockDim.y;\n    // element position within matrix\n    int x, y;\n    // element position within linear array\n    // gm - global memory\n    // sm - shared memory\n    int gmPos, smPos;\n\n    x = min(bsx + threadIdx.x, w - 1);\n    // row just below the tile\n    y = max(bsy - 1, 0);\n    gmPos = y * s + x;\n    smPos = threadIdx.x + 1;\n    du[smPos] = du0[gmPos];\n    dv[smPos] = dv0[gmPos];\n\n    // row above the tile\n    y = min(bsy + by, h - 1);\n    smPos += (by + 1) * (bx + 2);\n    gmPos = y * s + x;\n    du[smPos] = du0[gmPos];\n    dv[smPos] = dv0[gmPos];\n  } else if (threadIdx.y == 1) {\n    // beginning of the tile\n    const int bsx = blockIdx.x * blockDim.x;\n    const int bsy = blockIdx.y * blockDim.y;\n    // element position within matrix\n    int x, y;\n    // element position within linear array\n    // gm - global memory\n    // sm - shared memory\n    int gmPos, smPos;\n\n    y = min(bsy + threadIdx.x, h - 1);\n    // column to the left\n    x = max(bsx - 1, 0);\n    smPos = bx + 2 + threadIdx.x * (bx + 2);\n    gmPos = x + y * s;\n\n    // check if we are within tile\n    if (threadIdx.x < by) {\n      du[smPos] = du0[gmPos];\n      dv[smPos] = dv0[gmPos];\n      // column to the right\n      x = min(bsx + bx, w - 1);\n      gmPos = y * s + x;\n      smPos += bx + 1;\n      du[smPos] = du0[gmPos];\n      dv[smPos] = dv0[gmPos];\n    }\n  }\n\n  cg::sync(cta);\n\n  if (ix >= w || iy >= h) return;\n\n  // now all necessary data are loaded to shared memory\n  int left, right, up, down;\n  left = shMemPos - 1;\n  right = shMemPos + 1;\n  up = shMemPos + bx + 2;\n  down = shMemPos - bx - 2;\n\n  float sumU = (du[left] + du[right] + du[up] + du[down]) * 0.25f;\n  float sumV = (dv[left] + dv[right] + dv[up] + dv[down]) * 0.25f;\n\n  float frac = (Ix[pos] * sumU + Iy[pos] * sumV + Iz[pos]) /\n               (Ix[pos] * Ix[pos] + Iy[pos] * Iy[pos] + alpha);\n\n  du1[pos] = sumU - Ix[pos] * frac;\n  dv1[pos] = sumV - Iy[pos] * frac;\n}"
        ],
        "/Users/gbolet/hecbench-roofline/src/opticalFlow-cuda/downscaleKernel.cuh": [
            "INLINE __device__\nuint2 tex2D(const int rows, const int cols, const int _c, const int _r,\n            const uint sample_method) {\n  int c = _c;\n  int r = _r;\n  if (sample_method == ADDRESS_REFLECT_BORDER_EXCLUSIVE) {\n    c = c < 0 ? -c : c;\n    c = c >= cols ? cols - (c - cols) - 2 : c;\n    r = r < 0 ? -r : r;\n    r = r >= rows ? rows - (r - rows) - 2 : r;\n  } else if (sample_method == ADDRESS_CLAMP) {\n    c = c < 0 ? 0 : c;\n    c = c > cols - 1 ? cols - 1 : c;\n    r = r < 0 ? 0 : r;\n    r = r > rows - 1 ? rows - 1 : r;\n  } else if (sample_method == ADDRESS_REFLECT_BORDER_INCLUSIVE) {\n    c = c < 0 ? -c - 1 : c;\n    c = c >= cols ? cols - (c - cols) - 1 : c;\n    r = r < 0 ? -r - 1 : r;\n    r = r >= rows ? rows - (r - rows) - 1 : r;\n  } else if (sample_method == ADDRESS_ZERO) {\n  } else if (sample_method == ADDRESS_NOOP) {\n  } else {\n    assert(false);\n  }\n  assert_val(r >= 0 && r < rows, r);\n  assert_val(c >= 0 && c < cols, c);\n  uint2 result; \n  result.x = r;\n  result.y = c;\n  return result;\n}\n\n__global__ void DownscaleKernel(int width, int height, int stride, float *out,\n                                cudaTextureObject_t texFine) {\n  const int ix = threadIdx.x + blockIdx.x * blockDim.x;\n  const int iy = threadIdx.y + blockIdx.y * blockDim.y;\n\n  if (ix >= width || iy >= height) {\n    return;\n  }\n\n  float dx = 1.0f / (float)width;\n  float dy = 1.0f / (float)height;\n\n  float x = ((float)ix + 0.5f) * dx;\n  float y = ((float)iy + 0.5f) * dy;\n\n  out[ix + iy * stride] = 0.25f * (tex2D<float>(texFine, x - dx * 0.25f, y) +\n                                   tex2D<float>(texFine, x + dx * 0.25f, y) +\n                                   tex2D<float>(texFine, x, y - dy * 0.25f) +\n                                   tex2D<float>(texFine, x, y + dy * 0.25f));\n}"
        ]
    },
    "sc-cuda": {
        "/Users/gbolet/hecbench-roofline/src/sc-cuda/device_sc.cu": [
            "#define T ((int)32)\n\n\n#define fp float\n\n\n__device__ __inline__ unsigned int lanemask_lt()\n{\n#ifdef ASM\n  unsigned int mask;\n  asm(\"mov.u32 %0, %lanemask_lt;\" : \"=r\"(mask));\n  return mask;\n#else\n  const unsigned int lane = threadIdx.x & (warpSize-1);\n  return (1 << (lane)) - 1;\n#endif\n}\n\n__device__ __inline__ unsigned int binary_warp_scan(bool p)\n{\n  const unsigned int mask = lanemask_lt();\n#if (CUDART_VERSION < 9000)\n  unsigned int b = __ballot(p);\n  return __popc(b & mask);\n#else\n  unsigned int b = __ballot_sync(mask, p);\n  return __popc(b);\n#endif\n}\n\n__host__ __device__ __inline__\nbool valid(int x) {\n  return x > 0;\n}\n\n__device__ __inline__ int warp_scan(int val, volatile int *s_data)\n{\n  // initialize shared memory accessed by each warp with zeros\n  int idx = 2 * threadIdx.x - (threadIdx.x & (warpSize-1));\n  s_data[idx] = 0;\n  idx += warpSize;\n  int t = s_data[idx] = val;\n  s_data[idx] = t += s_data[idx - 1];\n  s_data[idx] = t += s_data[idx - 2];\n  s_data[idx] = t += s_data[idx - 4];\n  s_data[idx] = t += s_data[idx - 8];\n  s_data[idx] = t += s_data[idx -16];\n  return s_data[idx-1];\n}\n\n__device__ __inline__ int block_binary_prefix_sums(int x)\n{\n  // 2 x warpIdx's upper bound (1024/32)\n  __shared__ int sdata[64];\n\n  bool predicate = valid(x);\n\n  // A. Compute exclusive prefix sums within each warp\n  int warpPrefix = binary_warp_scan(predicate);\n  int idx = threadIdx.x;\n  int warpIdx = idx / warpSize;\n  int laneIdx = idx & (warpSize - 1);\n#ifdef DEBUG\n  printf(\"A %d %d %d\\n\", warpIdx, laneIdx, warpPrefix);\n#endif\n\n  // B. The last thread of each warp stores inclusive\n  // prefix sum to the warp\u2019s index in shared memory\n  if (laneIdx == warpSize - 1) {\n    sdata[warpIdx] = warpPrefix + predicate;\n#ifdef DEBUG\n    printf(\"B %d %d\\n\", warpIdx, sdata[warpIdx]);\n#endif\n  }\n  __syncthreads();\n\n  // C. One warp scans the warp partial sums\n  if (idx < warpSize) {\n    sdata[idx] = warp_scan(sdata[idx], sdata);\n#ifdef DEBUG\n    printf(\"C: %d %d\\n\", idx, sdata[idx]);\n#endif\n  }\n  __syncthreads();\n\n  // D. Each thread adds prefix sums of warp partial\n  // sums to its own intra\u2212warp prefix sums\n  return warpPrefix + sdata[warpIdx];\n}\n\n__device__ inline int gpu_first(Partitioner *p) {\n#ifdef DYNAMIC_PARTITION\n    if(p->strategy == DYNAMIC_PARTITIONING) {\n        if(threadIdx.x == 0) {\n            p->tmp[0] = atomicAdd_system(p->worklist, 1);\n        }\n        __syncthreads();\n        p->current = p->tmp[0];\n    } else\n#endif\n    {\n        p->current = p->cut + blockIdx.x;\n    }\n    return p->current;\n}\n\n__device__ inline bool gpu_more(const Partitioner *p) {\n    return (p->current < p->n_tasks);\n}\n\n__device__ inline int gpu_next(Partitioner *p) {\n#ifdef DYNAMIC_PARTITION\n    if(p->strategy == DYNAMIC_PARTITIONING) {\n        if(threadIdx.x == 0) {\n            p->tmp[0] = atomicAdd_system(p->worklist, 1);\n        }\n        __syncthreads();\n        p->current = p->tmp[0];\n    } else\n#endif\n    {\n        p->current = p->current + gridDim.x;\n    }\n    return p->current;\n}\n\n__device__\n#endif\ninline Partitioner partitioner_create(int n_tasks, float alpha\n#ifndef _GPU_COMPILER_\n    , int thread_id, int n_threads\n#endif\n#ifdef DYNAMIC_PARTITION\n#ifdef _GPU_COMPILER_\n    , int *worklist\n    , int *tmp\n#else\n    , std::atomic_int *worklist\n#endif\n#endif\n    ) {\n    Partitioner p;\n    p.n_tasks = n_tasks;\n#ifndef _GPU_COMPILER_\n    p.thread_id = thread_id;\n    p.n_threads = n_threads;\n#endif\n    if(alpha >= 0.0 && alpha <= 1.0) {\n        p.cut = p.n_tasks * alpha;\n#ifdef DYNAMIC_PARTITION\n        p.strategy = STATIC_PARTITIONING;\n#endif\n    } else {\n#ifdef DYNAMIC_PARTITION\n        p.strategy = DYNAMIC_PARTITIONING;\n        p.worklist = worklist;\n#ifdef _GPU_COMPILER_\n        p.tmp = tmp;\n#endif\n#endif\n    }\n    return p;\n}\n\n__global__ void reduce(const  long d_Ne,  // number of elements in array\n                    const int d_no,       // number of sums to reduce\n                    const int d_mul,      // increment\n                    fp *d_sums,           // pointer to partial sums variable (DEVICE GLOBAL MEMORY)\n                    fp *d_sums2){\n\n  // indexes\n    int bx = blockIdx.x;                  // get current horizontal block index\n  int tx = threadIdx.x;                   // get current horizontal thread index\n  int ei = (bx*NUMBER_THREADS)+tx;        // unique thread id, more threads than actual elements !!!\n  int nf = NUMBER_THREADS-(gridDim.x*NUMBER_THREADS-d_no);        // number of elements assigned to last block\n  int df = 0;                             // divisibility factor for the last block\n\n  // statistical\n  __shared__ fp d_psum[NUMBER_THREADS];   // data for block calculations allocated by every block in its shared memory\n  __shared__ fp d_psum2[NUMBER_THREADS];\n\n  // counters\n  int i;\n\n  // copy data to shared memory\n  if(ei<d_no){                            // do only for the number of elements, omit extra threads\n\n    d_psum[tx] = d_sums[ei*d_mul];\n    d_psum2[tx] = d_sums2[ei*d_mul];\n\n  }\n\n  // Lingjie Zhang modifited at Nov 1 / 2015\n    __syncthreads();\n    // end Lingjie Zhang's modification\n\n  // reduction of sums if all blocks are full (rare case)  \n  if(nf == NUMBER_THREADS){\n    // sum of every 2, 4, ..., NUMBER_THREADS elements\n    for(i=2; i<=NUMBER_THREADS; i=2*i){\n      // sum of elements\n      if((tx+1) % i == 0){                      // every ith\n        d_psum[tx] = d_psum[tx] + d_psum[tx-i/2];\n        d_psum2[tx] = d_psum2[tx] + d_psum2[tx-i/2];\n      }\n      // synchronization\n      __syncthreads();\n    }\n    // final sumation by last thread in every block\n    if(tx==(NUMBER_THREADS-1)){                      // block result stored in global memory\n      d_sums[bx*d_mul*NUMBER_THREADS] = d_psum[tx];\n      d_sums2[bx*d_mul*NUMBER_THREADS] = d_psum2[tx];\n    }\n  }\n  // reduction of sums if last block is not full (common case)\n  else{ \n    // for full blocks (all except for last block)\n    if(bx != (gridDim.x - 1)){                      //\n      // sum of every 2, 4, ..., NUMBER_THREADS elements\n      for(i=2; i<=NUMBER_THREADS; i=2*i){                //\n        // sum of elements\n        if((tx+1) % i == 0){                    // every ith\n          d_psum[tx] = d_psum[tx] + d_psum[tx-i/2];\n          d_psum2[tx] = d_psum2[tx] + d_psum2[tx-i/2];\n        }\n        // synchronization\n        __syncthreads();                      //\n      }\n      // final sumation by last thread in every block\n      if(tx==(NUMBER_THREADS-1)){                    // block result stored in global memory\n        d_sums[bx*d_mul*NUMBER_THREADS] = d_psum[tx];\n        d_sums2[bx*d_mul*NUMBER_THREADS] = d_psum2[tx];\n      }\n    }\n    // for not full block (last block)\n    else{                                //\n      // figure out divisibility\n      for(i=2; i<=NUMBER_THREADS; i=2*i){                //\n        if(nf >= i){\n          df = i;\n        }\n      }\n      // sum of every 2, 4, ..., NUMBER_THREADS elements\n      for(i=2; i<=df; i=2*i){                      //\n        // sum of elements (only busy threads)\n        if((tx+1) % i == 0 && tx<df){                // every ith\n          d_psum[tx] = d_psum[tx] + d_psum[tx-i/2];\n          d_psum2[tx] = d_psum2[tx] + d_psum2[tx-i/2];\n        }\n        // synchronization (all threads)\n        __syncthreads();                      //\n      }\n      // remainder / final summation by last thread\n      if(tx==(df-1)){                    //\n        // compute the remainder and final summation by last busy thread\n        for(i=(bx*NUMBER_THREADS)+df; i<(bx*NUMBER_THREADS)+nf; i++){            //\n          d_psum[tx] = d_psum[tx] + d_sums[i];\n          d_psum2[tx] = d_psum2[tx] + d_sums2[i];\n        }\n        // final sumation by last thread in every block\n        d_sums[bx*d_mul*NUMBER_THREADS] = d_psum[tx];\n        d_sums2[bx*d_mul*NUMBER_THREADS] = d_psum2[tx];\n      }\n    }\n  }\n\n}\n\n__global__\nvoid StreamCompaction (int size, T value, int n_tasks, float alpha,\n                       T *__restrict__ output,\n                       const T *__restrict__ input,\n                       int *__restrict__ flags\n#ifdef DYNAMIC_PARTITION\n                       , int *__restrict__ worklist\n#endif\n    ) {\n\n  extern __shared__ int l_mem[];\n  int* l_data = l_mem;\n  int* l_count = &l_data[blockDim.x];\n#ifdef DYNAMIC_PARTITION\n  int* l_tmp = &l_count[1];\n#endif\n\n#ifdef DYNAMIC_PARTITION\n  Partitioner p = partitioner_create(n_tasks, alpha, worklist, l_tmp);\n#else\n  Partitioner p = partitioner_create(n_tasks, alpha);\n#endif\n\n  for(int my_s = gpu_first(&p); gpu_more(&p); my_s = gpu_next(&p)) {\n\n    if(threadIdx.x == 0) {\n      l_count[0] = 0;\n    }\n    __syncthreads();\n\n    int local_cnt = 0;\n    // Declare on-chip memory\n    T reg[REGS];\n#ifdef DYNAMIC_PARTITION\n    int pos = my_s * REGS * blockDim.x + threadIdx.x;\n#else\n    int pos = (my_s - p.cut) * REGS * blockDim.x + threadIdx.x;\n#endif\n    // Load in on-chip memory\n#pragma unroll\n    for(int j = 0; j < REGS; j++) {\n      if(pos < size) {\n        reg[j] = input[pos];\n        if(reg[j] != value)\n          local_cnt++;\n      } else\n        reg[j] = value;\n      pos += blockDim.x;\n    }\n    reduce(&l_count[0], local_cnt, &l_data[0]);\n\n    // Set global synch\n    if(threadIdx.x == 0) {\n      int p_count;\n#ifdef DYNAMIC_PARTITION\n      while((p_count = atomicAdd_system(&flags[my_s], 0)) == 0) {\n      }\n      atomicAdd_system(&flags[my_s + 1], p_count + l_count[0]);\n#else\n      while((p_count = atomicAdd(&flags[my_s], 0)) == 0) {\n      }\n      atomicAdd(&flags[my_s + 1], p_count + l_count[0]);\n#endif\n      l_count[0] = p_count - 1;\n    }\n    __syncthreads();\n\n    // Store to global memory\n#pragma unroll\n    for(int j = 0; j < REGS; j++) {\n      pos = block_binary_prefix_sums(&l_count[0], (int)((reg[j] != value) ? 1 : 0), &l_data[0]);\n      if(reg[j] != value) {\n        output[pos] = reg[j];\n      }\n    }\n  }\n}"
        ]
    },
    "rotary-cuda": {
        "/Users/gbolet/hecbench-roofline/src/rotary-cuda/main.cu": [
            "__device__ inline void elementwise_kernel_helper(func_t f, policy_t policy) {\n  using traits = function_traits<func_t>;\n  using return_t = typename traits::result_type;\n  using args_t = typename traits::ArgsTuple;\n\n  int idx = blockIdx.x;\n\n  return_t results[thread_work_size()];\n  args_t args[thread_work_size()];\n\n  // load\n  policy.load(args, idx);\n\n  // compute\n  #pragma unroll\n  for (int i = 0; i < thread_work_size(); i++) {\n    if (policy.check_inbounds(i)) {\n      results[i] = guts_apply(f, args[i]);\n    }\n  }\n\n  // store\n  policy.store(results, idx);\n}\n\n__device__ multi_outputs_unroll(data_t data, int remaining, inp_calc_t ic, out_calc_t oc):\n    data(data), remaining(remaining), input_offset_calculator(ic), output_offset_calculator(oc) {}\n\n__global__ void unrolled_elementwise_kernel_for_multi_outputs(int N, func_t f, array_t data, inp_calc_t ic, out_calc_t oc) {\n  int remaining = N - block_work_size() * blockIdx.x;\n  elementwise_kernel_helper(f, multi_outputs_unroll<array_t, inp_calc_t, out_calc_t, num_outputs>(data, remaining, ic, oc));\n}"
        ]
    },
    "sptrsv-cuda": {
        "/Users/gbolet/hecbench-roofline/src/sptrsv-cuda/sptrsv_syncfree.cu": [
            "__device__ int atomic_load(const int *addr)\n{\n  const volatile int *vaddr = addr; // volatile to bypass cache\n  //__threadfence(); // for seq_cst loads. Remove for acquire semantics.\n  const int value = *vaddr;\n  // fence to ensure that dependent reads are correctly ordered\n  __threadfence(); \n  return value; \n}\n\n__device__ void atomic_store(int *addr, int value)\n{\n  volatile int *vaddr = addr; // volatile to bypass cache\n  // fence to ensure that previous non-atomic stores are visible to other threads\n  __threadfence(); \n  *vaddr = value;\n}\n\n__global__\nvoid sptrsv_mix(\n    const int        *__restrict__ csrRowPtr,\n    const int        *__restrict__ csrColIdx,\n    const VALUE_TYPE *__restrict__ csrVal,\n    int              *__restrict__ get_value,\n    const int        m,\n    const VALUE_TYPE *__restrict__ b,\n    VALUE_TYPE       *__restrict__ x,\n    const int        *__restrict__ warp_num,\n    const int        Len)\n{\n  const int local_id = threadIdx.x;\n  const int global_id = blockIdx.x * blockDim.x + local_id;\n  const int warp_id = global_id/WARP_SIZE;\n\n  int row;\n  __shared__ VALUE_TYPE s_left_sum[WARP_PER_BLOCK*WARP_SIZE];\n\n  if(warp_id>=(Len-1)) return;\n\n  const int lane_id = (WARP_SIZE - 1) & local_id;\n\n  if(warp_num[warp_id+1]>(warp_num[warp_id]+1))\n  {\n    //thread\n    row =warp_num[warp_id]+lane_id;\n    if(row>=m) return;\n\n    int col,j,i;\n    VALUE_TYPE xi;\n    VALUE_TYPE left_sum=0;\n    i=row;\n    j=csrRowPtr[i];\n\n    while(j<csrRowPtr[i+1])\n    {\n      col=csrColIdx[j];\n      if(atomic_load(&get_value[col])==1)\n      {\n        left_sum+=csrVal[j]*x[col];\n        j++;\n        col=csrColIdx[j];\n      }\n      if(i==col)\n      {\n        xi = (b[i] - left_sum) / csrVal[csrRowPtr[i+1]-1];\n        x[i] = xi;\n        atomic_store(&get_value[i], 1);\n        j++;\n      }\n    }\n  }\n  else\n  {\n    row = warp_num[warp_id];\n    if(row>=m)\n      return;\n\n    int col,j=csrRowPtr[row]  + lane_id;\n    VALUE_TYPE xi,sum=0;\n    while(j < (csrRowPtr[row+1]-1))\n    {\n      col=csrColIdx[j];\n      if(atomic_load(&get_value[col])==1)\n      {\n        sum += x[col] * csrVal[j];\n        j += WARP_SIZE;\n      }\n    }\n\n    s_left_sum[local_id]=sum;\n\n    for (int offset = WARP_SIZE/2; offset > 0; offset /= 2)\n    {\n      if(lane_id < offset)\n      {\n        s_left_sum[local_id] += s_left_sum[local_id+offset];\n      }\n    }\n\n    if (!lane_id)\n    {\n      xi = (b[row] - s_left_sum[local_id]) / csrVal[csrRowPtr[row+1]-1];\n      x[row]=xi;\n      atomic_store(&get_value[row], 1);\n    }\n  }\n}"
        ]
    },
    "md5hash-cuda": {
        "/Users/gbolet/hecbench-roofline/src/md5hash-cuda/MD5Hash.cu": [
            "__host__ __device__\nvoid IndexToKey(unsigned int index, int byteLength, int valsPerByte,\n                unsigned char vals[8])\n{\n  // loop pointlessly unrolled to avoid CUDA compiler complaints\n  // about unaligned accesses (!?) on older compute capabilities\n  vals[0] = index % valsPerByte;\n  index /= valsPerByte;\n\n  vals[1] = index % valsPerByte;\n  index /= valsPerByte;\n\n  vals[2] = index % valsPerByte;\n  index /= valsPerByte;\n\n  vals[3] = index % valsPerByte;\n  index /= valsPerByte;\n\n  vals[4] = index % valsPerByte;\n  index /= valsPerByte;\n\n  vals[5] = index % valsPerByte;\n  index /= valsPerByte;\n\n  vals[6] = index % valsPerByte;\n  index /= valsPerByte;\n\n  vals[7] = index % valsPerByte;\n  index /= valsPerByte;\n}\n\n__host__ __device__\ninline void md5_2words(unsigned int *words, unsigned int len,\n    unsigned int *digest)\n{\n  // For any block but the first one, these should be passed in, not\n  // initialized, but we are assuming we only operate on a single block.\n  unsigned int h0 = 0x67452301;\n  unsigned int h1 = 0xefcdab89;\n  unsigned int h2 = 0x98badcfe;\n  unsigned int h3 = 0x10325476;\n\n  unsigned int a = h0;\n  unsigned int b = h1;\n  unsigned int c = h2;\n  unsigned int d = h3;\n\n  unsigned int WL = len * 8;\n  unsigned int W0 = words[0];\n  unsigned int W1 = words[1];\n\n  switch (len)\n  {\n    case 0: W0 |= 0x00000080; break;\n    case 1: W0 |= 0x00008000; break;\n    case 2: W0 |= 0x00800000; break;\n    case 3: W0 |= 0x80000000; break;\n    case 4: W1 |= 0x00000080; break;\n    case 5: W1 |= 0x00008000; break;\n    case 6: W1 |= 0x00800000; break;\n    case 7: W1 |= 0x80000000; break;\n      //default: printf(\"ERROR, ONLY SUPPORT UP TO 7 BYTES IN THIS FUNC\\n\"); break;\n  }\n\n  // args: word data, per-round shift amt, constant, 4 vars, function macro\n  ROUND(W0,   7, 0xd76aa478, a, b, c, d, F);\n  ROUND(W1,  12, 0xe8c7b756, d, a, b, c, F);\n  ROUND(0,   17, 0x242070db, c, d, a, b, F);\n  ROUND(0,   22, 0xc1bdceee, b, c, d, a, F);\n  ROUND(0,    7, 0xf57c0faf, a, b, c, d, F);\n  ROUND(0,   12, 0x4787c62a, d, a, b, c, F);\n  ROUND(0,   17, 0xa8304613, c, d, a, b, F);\n  ROUND(0,   22, 0xfd469501, b, c, d, a, F);\n  ROUND(0,    7, 0x698098d8, a, b, c, d, F);\n  ROUND(0,   12, 0x8b44f7af, d, a, b, c, F);\n  ROUND(0,   17, 0xffff5bb1, c, d, a, b, F);\n  ROUND(0,   22, 0x895cd7be, b, c, d, a, F);\n  ROUND(0,    7, 0x6b901122, a, b, c, d, F);\n  ROUND(0,   12, 0xfd987193, d, a, b, c, F);\n  ROUND(WL,  17, 0xa679438e, c, d, a, b, F);\n  ROUND(0,   22, 0x49b40821, b, c, d, a, F);\n\n  ROUND(W1,   5, 0xf61e2562, a, b, c, d, G);\n  ROUND(0,    9, 0xc040b340, d, a, b, c, G);\n  ROUND(0,   14, 0x265e5a51, c, d, a, b, G);\n  ROUND(W0,  20, 0xe9b6c7aa, b, c, d, a, G);\n  ROUND(0,    5, 0xd62f105d, a, b, c, d, G);\n  ROUND(0,    9, 0x02441453, d, a, b, c, G);\n  ROUND(0,   14, 0xd8a1e681, c, d, a, b, G);\n  ROUND(0,   20, 0xe7d3fbc8, b, c, d, a, G);\n  ROUND(0,    5, 0x21e1cde6, a, b, c, d, G);\n  ROUND(WL,   9, 0xc33707d6, d, a, b, c, G);\n  ROUND(0,   14, 0xf4d50d87, c, d, a, b, G);\n  ROUND(0,   20, 0x455a14ed, b, c, d, a, G);\n  ROUND(0,    5, 0xa9e3e905, a, b, c, d, G);\n  ROUND(0,    9, 0xfcefa3f8, d, a, b, c, G);\n  ROUND(0,   14, 0x676f02d9, c, d, a, b, G);\n  ROUND(0,   20, 0x8d2a4c8a, b, c, d, a, G);\n\n  ROUND(0,    4, 0xfffa3942, a, b, c, d, H);\n  ROUND(0,   11, 0x8771f681, d, a, b, c, H);\n  ROUND(0,   16, 0x6d9d6122, c, d, a, b, H);\n  ROUND(WL,  23, 0xfde5380c, b, c, d, a, H);\n  ROUND(W1,   4, 0xa4beea44, a, b, c, d, H);\n  ROUND(0,   11, 0x4bdecfa9, d, a, b, c, H);\n  ROUND(0,   16, 0xf6bb4b60, c, d, a, b, H);\n  ROUND(0,   23, 0xbebfbc70, b, c, d, a, H);\n  ROUND(0,    4, 0x289b7ec6, a, b, c, d, H);\n  ROUND(W0,  11, 0xeaa127fa, d, a, b, c, H);\n  ROUND(0,   16, 0xd4ef3085, c, d, a, b, H);\n  ROUND(0,   23, 0x04881d05, b, c, d, a, H);\n  ROUND(0,    4, 0xd9d4d039, a, b, c, d, H);\n  ROUND(0,   11, 0xe6db99e5, d, a, b, c, H);\n  ROUND(0,   16, 0x1fa27cf8, c, d, a, b, H);\n  ROUND(0,   23, 0xc4ac5665, b, c, d, a, H);\n\n  ROUND(W0,   6, 0xf4292244, a, b, c, d, I);\n  ROUND(0,   10, 0x432aff97, d, a, b, c, I);\n  ROUND(WL,  15, 0xab9423a7, c, d, a, b, I);\n  ROUND(0,   21, 0xfc93a039, b, c, d, a, I);\n  ROUND(0,    6, 0x655b59c3, a, b, c, d, I);\n  ROUND(0,   10, 0x8f0ccc92, d, a, b, c, I);\n  ROUND(0,   15, 0xffeff47d, c, d, a, b, I);\n  ROUND(W1,  21, 0x85845dd1, b, c, d, a, I);\n  ROUND(0,    6, 0x6fa87e4f, a, b, c, d, I);\n  ROUND(0,   10, 0xfe2ce6e0, d, a, b, c, I);\n  ROUND(0,   15, 0xa3014314, c, d, a, b, I);\n  ROUND(0,   21, 0x4e0811a1, b, c, d, a, I);\n  ROUND(0,    6, 0xf7537e82, a, b, c, d, I);\n  ROUND(0,   10, 0xbd3af235, d, a, b, c, I);\n  ROUND(0,   15, 0x2ad7d2bb, c, d, a, b, I);\n  ROUND(0,   21, 0xeb86d391, b, c, d, a, I);\n\n  h0 += a;\n  h1 += b;\n  h2 += c;\n  h3 += d;\n\n  // write the final result out\n  digest[0] = h0;\n  digest[1] = h1;\n  digest[2] = h2;\n  digest[3] = h3;\n}\n\n__global__\nvoid md5hash_kernel(\n  int *__restrict__ foundIndex,\n  unsigned char *__restrict__ foundKey,\n  unsigned int *__restrict__ foundDigest,\n  int keyspace,\n  const int byteLength,\n  const int valsPerByte,\n  const unsigned int searchDigest0, \n  const unsigned int searchDigest1, \n  const unsigned int searchDigest2, \n  const unsigned int searchDigest3 )\n{\n  int threadid = blockDim.x * blockIdx.x + threadIdx.x;\n  int startindex = threadid * valsPerByte;\n  unsigned char key[8] = {0,0,0,0, 0,0,0,0};\n  IndexToKey(startindex, byteLength, valsPerByte, key);\n\n  for (int j=0; j < valsPerByte && startindex+j < keyspace; ++j)\n  {\n    unsigned int digest[4];\n    md5_2words((unsigned int*)key, byteLength, digest);\n    if (digest[0] == searchDigest0 &&\n        digest[1] == searchDigest1 &&\n        digest[2] == searchDigest2 &&\n        digest[3] == searchDigest3)\n    {\n      foundIndex[0] = startindex + j;\n      foundKey[0] = key[0];\n      foundKey[1] = key[1];\n      foundKey[2] = key[2];\n      foundKey[3] = key[3];\n      foundKey[4] = key[4];\n      foundKey[5] = key[5];\n      foundKey[6] = key[6];\n      foundKey[7] = key[7];\n      foundDigest[0] = digest[0];\n      foundDigest[1] = digest[1];\n      foundDigest[2] = digest[2];\n      foundDigest[3] = digest[3];\n    }\n    ++key[0];\n  }   \n}"
        ]
    },
    "page-rank-cuda": {
        "/Users/gbolet/hecbench-roofline/src/page-rank-cuda/main.cu": [
            "__global__\nvoid map(const int *__restrict__ pages,\n         const float *__restrict__ page_ranks,\n               float *__restrict__ maps,\n         const unsigned int *__restrict__ noutlinks,\n         const int n)\n{\n  int i = threadIdx.x + blockIdx.x * blockDim.x;\n  int j;\n  if(i < n){\n    float outbound_rank = page_ranks[i]/(float)noutlinks[i];\n    for(j=0; j<n; ++j){\n      maps[i*n+j] = pages[i*n+j]*outbound_rank;\n    }\n  }\n}"
        ]
    },
    "pingpong-cuda": {
        "/Users/gbolet/hecbench-roofline/src/pingpong-cuda/main-nccl.cu": [
            "__global__\nvoid test(double *d, const long int n) {\n  for (long i = blockDim.x * blockIdx.x + threadIdx.x;\n       i < n; i += blockDim.x * gridDim.x) {\n    d[i] = d[i] + 1;\n  }\n}"
        ]
    },
    "tsp-cuda": {
        "/Users/gbolet/hecbench-roofline/src/tsp-cuda/main.cu": [
            "#define T ((int)32)\n\n\ninline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\ninline __host__ __device__ float4 sqrtf(float4 v)\n{\n    return make_float4(sqrtf(v.x), sqrtf(v.y), sqrtf(v.z), sqrtf(v.w));\n}\n\n__device__\nunsigned int LCG_random(unsigned int * seed) {\n  const unsigned int m = 2147483648;\n  const unsigned int a = 26757677;\n  const unsigned int c = 1;\n  *seed = (a * (*seed) + c) % m;\n  return *seed;\n}\n\n__device__ inline void swap(T &a, T &b) {\n  T tmp = a;\n  a = b;\n  b = tmp;\n}\n\n__global__\nvoid TwoOpt(int cities, \n    const float *__restrict posx_d,\n    const float *__restrict posy_d,\n    int *__restrict glob_d,\n    int *__restrict climbs_d,\n    int *__restrict best_d)\n{\n  extern __shared__ int buf_s[];\n  __shared__ float px_s[tilesize];\n  __shared__ float py_s[tilesize];\n  __shared__ int bf_s[tilesize];\n\n  int *buf = &glob_d[blockIdx.x * ((3 * cities + 2 + 31) / 32 * 32)];\n  float *px = (float *)(&buf[cities]);\n  float *py = &px[cities + 1];\n\n  for (int i = threadIdx.x; i < cities; i += blockDim.x) px[i] = posx_d[i];\n  for (int i = threadIdx.x; i < cities; i += blockDim.x) py[i] = posy_d[i];\n  __syncthreads();\n\n  if (threadIdx.x == 0) {  // serial permutation\n    unsigned int seed = blockIdx.x;\n    for (unsigned int i = 1; i < cities; i++) {\n      int j = (int)(LCG_random(&seed) * (cities - 1)) + 1;\n      swap(px[i], px[j]);\n      swap(py[i], py[j]);\n    }\n    px[cities] = px[0];\n    py[cities] = py[0];\n  }\n  __syncthreads();\n\n  int minchange;\n  do {\n    for (int i = threadIdx.x; i < cities; i += blockDim.x) buf[i] = -dist(i, i + 1);\n    __syncthreads();\n\n    minchange = 0;\n    int mini = 1;\n    int minj = 0;\n    for (int ii = 0; ii < cities - 2; ii += blockDim.x) {\n      int i = ii + threadIdx.x;\n      float pxi0, pyi0, pxi1, pyi1, pxj1, pyj1;\n      if (i < cities - 2) {\n        minchange -= buf[i];\n        pxi0 = px[i];\n        pyi0 = py[i];\n        pxi1 = px[i + 1];\n        pyi1 = py[i + 1];\n        pxj1 = px[cities];\n        pyj1 = py[cities];\n      }\n      for (int jj = cities - 1; jj >= ii + 2; jj -= tilesize) {\n        int bound = jj - tilesize + 1;\n        for (int k = threadIdx.x; k < tilesize; k += blockDim.x) {\n          if (k + bound >= ii + 2) {\n            px_s[k] = px[k + bound];\n            py_s[k] = py[k + bound];\n            bf_s[k] = buf[k + bound];\n          }\n        }\n        __syncthreads();\n\n        int lower = bound;\n        if (lower < i + 2) lower = i + 2;\n        for (int j = jj; j >= lower; j--) {\n          int jm = j - bound;\n          float pxj0 = px_s[jm];\n          float pyj0 = py_s[jm];\n          int change = bf_s[jm]\n            + int(sqrtf((pxi0 - pxj0) * (pxi0 - pxj0) + (pyi0 - pyj0) * (pyi0 - pyj0)))\n            + int(sqrtf((pxi1 - pxj1) * (pxi1 - pxj1) + (pyi1 - pyj1) * (pyi1 - pyj1)));\n          pxj1 = pxj0;\n          pyj1 = pyj0;\n          if (minchange > change) {\n            minchange = change;\n            mini = i;\n            minj = j;\n          }\n        }\n        __syncthreads();\n      }\n\n      if (i < cities - 2) {\n        minchange += buf[i];\n      }\n    }\n    __syncthreads();\n\n    int change = buf_s[threadIdx.x] = minchange;\n    if (threadIdx.x == 0) atomicAdd(climbs_d, 1);  // stats only\n    __syncthreads();\n\n    int j = blockDim.x;\n    do {\n      int k = (j + 1) / 2;\n      if ((threadIdx.x + k) < j) {\n        int tmp = buf_s[threadIdx.x + k];\n        if (change > tmp) change = tmp;\n        buf_s[threadIdx.x] = change;\n      }\n      j = k;\n      __syncthreads();\n    } while (j > 1);\n\n    if (minchange == buf_s[0]) {\n      buf_s[1] = threadIdx.x;  // non-deterministic winner\n    }\n    __syncthreads();\n\n    if (threadIdx.x == buf_s[1]) {\n      buf_s[2] = mini + 1;\n      buf_s[3] = minj;\n    }\n    __syncthreads();\n\n    minchange = buf_s[0];\n    mini = buf_s[2];\n    int sum = buf_s[3] + mini;\n    for (int i = threadIdx.x; (i + i) < sum; i += blockDim.x) {\n      if (mini <= i) {\n        int j = sum - i;\n        swap(px[i], px[j]);\n        swap(py[i], py[j]);\n      }\n    }\n    __syncthreads();\n  } while (minchange < 0);\n\n  int term = 0;\n  for (int i = threadIdx.x; i < cities; i += blockDim.x) {\n    term += dist(i, i + 1);\n  }\n  buf_s[threadIdx.x] = term;\n  __syncthreads();\n\n  int j = blockDim.x;\n  do {\n    int k = (j + 1) / 2;\n    if ((threadIdx.x + k) < j) {\n      term += buf_s[threadIdx.x + k];\n    }\n    __syncthreads();\n    if ((threadIdx.x + k) < j) {\n      buf_s[threadIdx.x] = term;\n    }\n    j = k;\n    __syncthreads();\n  } while (j > 1);\n\n  if (threadIdx.x == 0) {\n    atomicMin(best_d, term);\n  }\n}"
        ]
    },
    "rayleighBenardConvection-cuda": {
        "/Users/gbolet/hecbench-roofline/src/rayleighBenardConvection-cuda/main.cu": [
            "__global__ void ElemMultOmega(float* A, const float* B) {\n  if(THREADID < (M-2)*(N-2) ) {\n    A[THREADID] = A[THREADID]*B[THREADID];\n  }\n}",
            "__global__ void ElemMultT(float* A, const float* B) {\n  if(THREADID < (M-2)*(N)) {\n    A[THREADID] = A[THREADID]*B[THREADID];\n  }\n}",
            "__global__ void ElemMultNu(float* A, const float* B) {\n  if(THREADID < N) {\n    A[THREADID] = A[THREADID]*B[THREADID];\n  }\n}",
            "__global__ void SubOne(float* A) {\n  if(THREADID < N) {\n    A[THREADID] = A[THREADID] - 1.f;\n  }\n}",
            "__global__ void AddOne(float* A) {\n  if(THREADID < N) {\n    A[THREADID] = A[THREADID] + 1.f;\n  }\n}",
            "__global__ void AddX(float* A, const float x) {\n  if(THREADID < (M-2)*N) {\n    A[THREADID] = A[THREADID] + x;\n  }\n}",
            "inline __host__ __device__ int4 make_int4(float4 a)\n{\n    return make_int4(int(a.x), int(a.y), int(a.z), int(a.w));\n}\n\ninline __host__ __device__ int4 abs(int4 v)\n{\n    return make_int4(abs(v.x), abs(v.y), abs(v.z), abs(v.w));\n}\n\ninline __host__ __device__ uint4 make_uint4(int4 a)\n{\n    return make_uint4(uint(a.x), uint(a.y), uint(a.z), uint(a.w));\n}\n\ninline __host__ __device__ uint4 max(uint4 a, uint4 b)\n{\n    return make_uint4(max(a.x,b.x), max(a.y,b.y), max(a.z,b.z), max(a.w,b.w));\n}\n\ninline __host__ __device__ uint4 min(uint4 a, uint4 b)\n{\n    return make_uint4(min(a.x,b.x), min(a.y,b.y), min(a.z,b.z), min(a.w,b.w));\n}\n\n__global__ void Updatedt(int ptru, const float*__restrict__ u, int ptrv,\n                         const float*__restrict__ v, float* dt) {\n  dt[0] = max(abs(u[ptru - 1]),abs(v[ptrv - 1]));\n  dt[0] = min(DX/dt[0],DX2/4.f);\n}"
        ]
    },
    "crs-cuda": {
        "/Users/gbolet/hecbench-roofline/src/crs-cuda/kernels.cu": [
            "__global__ void gcrs_m_1_w_4_coding_dotprod(\n  int k, int index, \n  const long *__restrict in, \n  long *__restrict out, \n  const unsigned int *__restrict bm, \n  int size)\n{\n  extern __shared__ long shared_data[];\n\n  int w = 4;\n  int i,j;\n  long result = 0;\n\n  const unsigned long fullOneBit = 0xFFFFFFFFFFFFFFFF;\n\n  int worksize_perblock = blockDim.x / w * w;\n  const unsigned int idx = worksize_perblock * blockIdx.x + threadIdx.x;\n\n  if (threadIdx.x >= worksize_perblock) {\n    return;\n  }\n\n  if (idx >= size) {\n    return;\n  }\n\n  int group_offset = (threadIdx.x / w) * w;\n  int group_inner_offset = threadIdx.x % w;\n  // row for each thread in the bitmatrix * row size which is k * w\n\n  unsigned int bitInt = 0x01;\n  unsigned int matrixInt;\n\n  for ( i = 0; i < k; i++ ) {\n\n    shared_data[threadIdx.x] = *(in + i*size + idx);\n\n    __syncthreads();\n\n#pragma unroll\n    for ( j = 0; j < w; j++ ) {\n      matrixInt = bm[index];\n      result = result ^ ( (((matrixInt & (bitInt<< group_inner_offset)) >> group_inner_offset) * fullOneBit) & shared_data[group_offset + j]);\n      ++index;\n    }\n    __syncthreads();\n\n  }\n\n  out[idx] = result;\n}",
            "__global__ void gcrs_m_1_w_5_coding_dotprod(\n  int k, int index, \n  const long *__restrict in, \n  long *__restrict out, \n  const unsigned int *__restrict bm, \n  int size)\n{\n  extern __shared__ long shared_data[];\n\n  int w = 5;\n  int i,j;\n  long result = 0;\n\n  const unsigned long fullOneBit = 0xFFFFFFFFFFFFFFFF;\n\n  int worksize_perblock = blockDim.x / w * w;\n  const unsigned int idx = worksize_perblock * blockIdx.x + threadIdx.x;\n\n  if (threadIdx.x >= worksize_perblock) {\n    return;\n  }\n\n  if (idx >= size) {\n    return;\n  }\n\n  int group_offset = (threadIdx.x / w) * w;\n  int group_inner_offset = threadIdx.x % w;\n  // row for each thread in the bitmatrix * row size which is k * w\n\n  unsigned int bitInt = 0x01;\n  unsigned int matrixInt;\n\n  for ( i = 0; i < k; i++ ) {\n\n    shared_data[threadIdx.x] = *(in + i*size + idx);\n\n    __syncthreads();\n\n#pragma unroll\n    for ( j = 0; j < w; j++ ) {\n      matrixInt = bm[index];\n      result = result ^ ( (((matrixInt & (bitInt<< group_inner_offset)) >> group_inner_offset) * fullOneBit) & shared_data[group_offset + j]);\n      ++index;\n    }\n    __syncthreads();\n\n  }\n\n  out[idx] = result;\n}",
            "__global__ void gcrs_m_1_w_6_coding_dotprod(\n  int k, int index, \n  const long *__restrict in, \n  long *__restrict out, \n  const unsigned int *__restrict bm, \n  int size)\n{\n  extern __shared__ long shared_data[];\n\n  int w = 6;\n  int i,j;\n  long result = 0;\n\n  const unsigned long fullOneBit = 0xFFFFFFFFFFFFFFFF;\n\n  int worksize_perblock = blockDim.x / w * w;\n  const unsigned int idx = worksize_perblock * blockIdx.x + threadIdx.x;\n\n  if (threadIdx.x >= worksize_perblock) {\n    return;\n  }\n\n  if (idx >= size) {\n    return;\n  }\n\n  int group_offset = (threadIdx.x / w) * w;\n  int group_inner_offset = threadIdx.x % w;\n  // row for each thread in the bitmatrix * row size which is k * w\n\n  unsigned int bitInt = 0x01;\n  unsigned int matrixInt;\n\n  for ( i = 0; i < k; i++ ) {\n\n    shared_data[threadIdx.x] = *(in + i*size + idx);\n\n    __syncthreads();\n\n#pragma unroll\n    for ( j = 0; j < w; j++ ) {\n      matrixInt = bm[index];\n      result = result ^ ( (((matrixInt & (bitInt<< group_inner_offset)) >> group_inner_offset) * fullOneBit) & shared_data[group_offset + j]);\n      ++index;\n    }\n    __syncthreads();\n\n  }\n\n  out[idx] = result;\n}",
            "__global__ void gcrs_m_1_w_7_coding_dotprod(\n  int k, int index, \n  const long *__restrict in, \n  long *__restrict out, \n  const unsigned int *__restrict bm, \n  int size)\n{\n  extern __shared__ long shared_data[];\n\n  int w = 7;\n  int i,j;\n  long result = 0;\n\n  const unsigned long fullOneBit = 0xFFFFFFFFFFFFFFFF;\n\n  int worksize_perblock = blockDim.x / w * w;\n  const unsigned int idx = worksize_perblock * blockIdx.x + threadIdx.x;\n\n  if (threadIdx.x >= worksize_perblock) {\n    return;\n  }\n\n  if (idx >= size) {\n    return;\n  }\n\n  int group_offset = (threadIdx.x / w) * w;\n  int group_inner_offset = threadIdx.x % w;\n  // row for each thread in the bitmatrix * row size which is k * w\n\n  unsigned int bitInt = 0x01;\n  unsigned int matrixInt;\n\n  for ( i = 0; i < k; i++ ) {\n\n    shared_data[threadIdx.x] = *(in + i*size + idx);\n\n    __syncthreads();\n\n#pragma unroll\n    for ( j = 0; j < w; j++ ) {\n      matrixInt = bm[index];\n      result = result ^ ( (((matrixInt & (bitInt<< group_inner_offset)) >> group_inner_offset) * fullOneBit) & shared_data[group_offset + j]);\n      ++index;\n    }\n    __syncthreads();\n\n  }\n\n  out[idx] = result;\n}",
            "__global__ void gcrs_m_1_w_8_coding_dotprod(\n  int k, int index, \n  const long *__restrict in, \n  long *__restrict out, \n  const unsigned int *__restrict bm, \n  int size)\n{\n  extern __shared__ long shared_data[];\n\n  int w = 8;\n  int i,j;\n  long result = 0;\n\n  const unsigned long fullOneBit = 0xFFFFFFFFFFFFFFFF;\n\n  int worksize_perblock = blockDim.x / w * w;\n  const unsigned int idx = worksize_perblock * blockIdx.x + threadIdx.x;\n\n  if (threadIdx.x >= worksize_perblock) {\n    return;\n  }\n\n  if (idx >= size) {\n    return;\n  }\n\n  int group_offset = (threadIdx.x / w) * w;\n  int group_inner_offset = threadIdx.x % w;\n  // row for each thread in the bitmatrix * row size which is k * w\n\n  unsigned int bitInt = 0x01;\n  unsigned int matrixInt;\n\n  for ( i = 0; i < k; i++ ) {\n\n    shared_data[threadIdx.x] = *(in + i*size + idx);\n\n    __syncthreads();\n\n#pragma unroll\n    for ( j = 0; j < w; j++ ) {\n      matrixInt = bm[index];\n      result = result ^ ( (((matrixInt & (bitInt<< group_inner_offset)) >> group_inner_offset) * fullOneBit) & shared_data[group_offset + j]);\n      ++index;\n    }\n    __syncthreads();\n\n  }\n\n  out[idx] = result;\n}",
            "__global__ void gcrs_m_2_w_4_coding_dotprod(\n  int k, int index, \n  const long *__restrict in, \n  long *__restrict out, \n  const unsigned int *__restrict bm, \n  int size)\n{\n  extern __shared__ long shared_data[];\n\n  int w = 4;\n  int i,j;\n  long result[2];\n\n  result[0] = 0;\n  result[1] = 0;\n\n  const unsigned long fullOneBit = 0xFFFFFFFFFFFFFFFF;\n\n  int worksize_perblock = blockDim.x / w * w;\n  const unsigned int idx = worksize_perblock * blockIdx.x + threadIdx.x;\n\n  if (threadIdx.x >= worksize_perblock) {\n    return;\n  }\n\n  if (idx >= size) {\n    return;\n  }\n\n  int group_offset = (threadIdx.x / w) * w;\n  int group_inner_offset = threadIdx.x % w;\n  // row for each thread in the bitmatrix * row size which is k * w\n\n  unsigned int bitInt = 0x01;\n  unsigned int matrixInt;\n\n  for ( i = 0; i < k; i++ ) {\n\n    shared_data[threadIdx.x] = *(in + i*size + idx);\n\n    __syncthreads();\n\n#pragma unroll\n    for ( j = 0; j < w; j++ ) {\n      matrixInt = bm[index];\n      result[0] = result[0] ^ ((((matrixInt & (bitInt<< group_inner_offset)) >> group_inner_offset) * fullOneBit) & shared_data[group_offset + j]);\n      result[1] = result[1] ^ ((((matrixInt & (bitInt<< (group_inner_offset+w))) >> (group_inner_offset+w)) * fullOneBit) & shared_data[group_offset + j]);\n\n      ++index;\n    }\n    __syncthreads();\n\n  }\n\n  out[idx] = result[0];\n  out[idx + size] = result[1];\n}",
            "__global__ void gcrs_m_2_w_5_coding_dotprod(\n  int k, int index, \n  const long *__restrict in, \n  long *__restrict out, \n  const unsigned int *__restrict bm, \n  int size)\n{\n  extern __shared__ long shared_data[];\n\n  int w = 5;\n  int i,j;\n  long result[2];\n\n  result[0] = 0;\n  result[1] = 0;\n\n  const unsigned long fullOneBit = 0xFFFFFFFFFFFFFFFF;\n\n  int worksize_perblock = blockDim.x / w * w;\n  const unsigned int idx = worksize_perblock * blockIdx.x + threadIdx.x;\n\n  if (threadIdx.x >= worksize_perblock) {\n    return;\n  }\n\n  if (idx >= size) {\n    return;\n  }\n\n  int group_offset = (threadIdx.x / w) * w;\n  int group_inner_offset = threadIdx.x % w;\n  // row for each thread in the bitmatrix * row size which is k * w\n\n  unsigned int bitInt = 0x01;\n  unsigned int matrixInt;\n\n  for ( i = 0; i < k; i++ ) {\n\n    shared_data[threadIdx.x] = *(in + i*size + idx);\n\n    __syncthreads();\n\n#pragma unroll\n    for ( j = 0; j < w; j++ ) {\n      matrixInt = bm[index];\n      result[0] = result[0] ^ ((((matrixInt & (bitInt<< group_inner_offset)) >> group_inner_offset) * fullOneBit) & shared_data[group_offset + j]);\n      result[1] = result[1] ^ ((((matrixInt & (bitInt<< (group_inner_offset+w))) >> (group_inner_offset+w)) * fullOneBit) & shared_data[group_offset + j]);\n\n      ++index;\n    }\n    __syncthreads();\n\n  }\n\n  out[idx] = result[0];\n  out[idx + size] = result[1];\n}",
            "__global__ void gcrs_m_2_w_6_coding_dotprod(\n  int k, int index, \n  const long *__restrict in, \n  long *__restrict out, \n  const unsigned int *__restrict bm, \n  int size)\n{\n  extern __shared__ long shared_data[];\n\n  int w = 6;\n  int i,j;\n  long result[2];\n\n  result[0] = 0;\n  result[1] = 0;\n\n  const unsigned long fullOneBit = 0xFFFFFFFFFFFFFFFF;\n\n  int worksize_perblock = blockDim.x / w * w;\n  const unsigned int idx = worksize_perblock * blockIdx.x + threadIdx.x;\n\n  if (threadIdx.x >= worksize_perblock) {\n    return;\n  }\n\n  if (idx >= size) {\n    return;\n  }\n\n  int group_offset = (threadIdx.x / w) * w;\n  int group_inner_offset = threadIdx.x % w;\n  // row for each thread in the bitmatrix * row size which is k * w\n\n  unsigned int bitInt = 0x01;\n  unsigned int matrixInt;\n\n  for ( i = 0; i < k; i++ ) {\n\n    shared_data[threadIdx.x] = *(in + i*size + idx);\n\n    __syncthreads();\n\n#pragma unroll\n    for ( j = 0; j < w; j++ ) {\n      matrixInt = bm[index];\n      result[0] = result[0] ^ ((((matrixInt & (bitInt<< group_inner_offset)) >> group_inner_offset) * fullOneBit) & shared_data[group_offset + j]);\n      result[1] = result[1] ^ ((((matrixInt & (bitInt<< (group_inner_offset+w))) >> (group_inner_offset+w)) * fullOneBit) & shared_data[group_offset + j]);\n\n      ++index;\n    }\n    __syncthreads();\n\n  }\n\n  out[idx] = result[0];\n  out[idx + size] = result[1];\n}",
            "__global__ void gcrs_m_2_w_7_coding_dotprod(\n  int k, int index, \n  const long *__restrict in, \n  long *__restrict out, \n  const unsigned int *__restrict bm, \n  int size)\n{\n  extern __shared__ long shared_data[];\n\n  int w = 7;\n  int i,j;\n  long result[2];\n\n  result[0] = 0;\n  result[1] = 0;\n\n  const unsigned long fullOneBit = 0xFFFFFFFFFFFFFFFF;\n\n  int worksize_perblock = blockDim.x / w * w;\n  const unsigned int idx = worksize_perblock * blockIdx.x + threadIdx.x;\n\n  if (threadIdx.x >= worksize_perblock) {\n    return;\n  }\n\n  if (idx >= size) {\n    return;\n  }\n\n  int group_offset = (threadIdx.x / w) * w;\n  int group_inner_offset = threadIdx.x % w;\n  // row for each thread in the bitmatrix * row size which is k * w\n\n  unsigned int bitInt = 0x01;\n  unsigned int matrixInt;\n\n  for ( i = 0; i < k; i++ ) {\n\n    shared_data[threadIdx.x] = *(in + i*size + idx);\n\n    __syncthreads();\n\n#pragma unroll\n    for ( j = 0; j < w; j++ ) {\n      matrixInt = bm[index];\n      result[0] = result[0] ^ ((((matrixInt & (bitInt<< group_inner_offset)) >> group_inner_offset) * fullOneBit) & shared_data[group_offset + j]);\n      result[1] = result[1] ^ ((((matrixInt & (bitInt<< (group_inner_offset+w))) >> (group_inner_offset+w)) * fullOneBit) & shared_data[group_offset + j]);\n\n      ++index;\n    }\n    __syncthreads();\n\n  }\n\n  out[idx] = result[0];\n  out[idx + size] = result[1];\n}",
            "__global__ void gcrs_m_2_w_8_coding_dotprod(\n  int k, int index, \n  const long *__restrict in, \n  long *__restrict out, \n  const unsigned int *__restrict bm, \n  int size)\n{\n  extern __shared__ long shared_data[];\n\n  int w = 8;\n  int i,j;\n  long result[2];\n\n  result[0] = 0;\n  result[1] = 0;\n\n  const unsigned long fullOneBit = 0xFFFFFFFFFFFFFFFF;\n\n  int worksize_perblock = blockDim.x / w * w;\n  const unsigned int idx = worksize_perblock * blockIdx.x + threadIdx.x;\n\n  if (threadIdx.x >= worksize_perblock) {\n    return;\n  }\n\n  if (idx >= size) {\n    return;\n  }\n\n  int group_offset = (threadIdx.x / w) * w;\n  int group_inner_offset = threadIdx.x % w;\n  // row for each thread in the bitmatrix * row size which is k * w\n\n  unsigned int bitInt = 0x01;\n  unsigned int matrixInt;\n\n  for ( i = 0; i < k; i++ ) {\n\n    shared_data[threadIdx.x] = *(in + i*size + idx);\n\n    __syncthreads();\n\n#pragma unroll\n    for ( j = 0; j < w; j++ ) {\n      matrixInt = bm[index];\n      result[0] = result[0] ^ ((((matrixInt & (bitInt<< group_inner_offset)) >> group_inner_offset) * fullOneBit) & shared_data[group_offset + j]);\n      result[1] = result[1] ^ ((((matrixInt & (bitInt<< (group_inner_offset+w))) >> (group_inner_offset+w)) * fullOneBit) & shared_data[group_offset + j]);\n\n      ++index;\n    }\n    __syncthreads();\n\n  }\n\n  out[idx] = result[0];\n  out[idx + size] = result[1];\n}",
            "__global__ void gcrs_m_3_w_4_coding_dotprod(\n  int k, int index, \n  const long *__restrict in, \n  long *__restrict out, \n  const unsigned int *__restrict bm, \n  int size)\n{\n  extern __shared__ long shared_data[];\n\n  int w = 4;\n\n  int i,j;\n  long result[3];\n\n  result[0] = 0;\n  result[1] = 0;\n  result[2] = 0;\n\n  const unsigned long fullOneBit = 0xFFFFFFFFFFFFFFFF;\n\n  int worksize_perblock = blockDim.x / w * w;\n  const unsigned int idx = worksize_perblock * blockIdx.x + threadIdx.x;\n\n  if (threadIdx.x >= worksize_perblock) {\n    return;\n  }\n\n  if (idx >= size) {\n    return;\n  }\n\n  int group_offset = (threadIdx.x / w) * w;\n  int group_inner_offset = threadIdx.x % w;\n  // row for each thread in the bitmatrix * row size which is k * w\n\n  unsigned int bitInt = 0x01;\n  unsigned int matrixInt;\n\n  for ( i = 0; i < k; i++ ) {\n\n    shared_data[threadIdx.x] = *(in + i*size + idx);\n\n    __syncthreads();\n\n#pragma unroll\n    for ( j = 0; j < w; j++ ) {\n      matrixInt = bm[index];\n      result[0] = result[0] ^ ((((matrixInt & (bitInt<< group_inner_offset)) >> group_inner_offset) * fullOneBit) & shared_data[group_offset + j]);\n      result[1] = result[1] ^ ((((matrixInt & (bitInt<< (group_inner_offset+w))) >> (group_inner_offset+w)) * fullOneBit) & shared_data[group_offset + j]);\n      result[2] = result[2] ^ ((((matrixInt & (bitInt<< (group_inner_offset + 2*w))) >> (group_inner_offset + 2*w)) * fullOneBit) & shared_data[group_offset + j]);\n\n      ++index;\n    }\n    __syncthreads();\n\n  }\n\n  out[idx] = result[0];\n  out[idx + size] = result[1];\n  out[idx + 2 * size] = result[2];\n}",
            "__global__ void gcrs_m_3_w_5_coding_dotprod(\n  int k, int index, \n  const long *__restrict in, \n  long *__restrict out, \n  const unsigned int *__restrict bm, \n  int size)\n{\n  extern __shared__ long shared_data[];\n\n  int w = 5;\n\n  int i,j;\n  long result[3];\n\n  result[0] = 0;\n  result[1] = 0;\n  result[2] = 0;\n\n  const unsigned long fullOneBit = 0xFFFFFFFFFFFFFFFF;\n\n  int worksize_perblock = blockDim.x / w * w;\n  const unsigned int idx = worksize_perblock * blockIdx.x + threadIdx.x;\n\n  if (threadIdx.x >= worksize_perblock) {\n    return;\n  }\n\n  if (idx >= size) {\n    return;\n  }\n\n  int group_offset = (threadIdx.x / w) * w;\n  int group_inner_offset = threadIdx.x % w;\n  // row for each thread in the bitmatrix * row size which is k * w\n\n  unsigned int bitInt = 0x01;\n  unsigned int matrixInt;\n\n  for ( i = 0; i < k; i++ ) {\n\n    shared_data[threadIdx.x] = *(in + i*size + idx);\n\n    __syncthreads();\n\n#pragma unroll\n    for ( j = 0; j < w; j++ ) {\n      matrixInt = bm[index];\n      result[0] = result[0] ^ ((((matrixInt & (bitInt<< group_inner_offset)) >> group_inner_offset) * fullOneBit) & shared_data[group_offset + j]);\n      result[1] = result[1] ^ ((((matrixInt & (bitInt<< (group_inner_offset+w))) >> (group_inner_offset+w)) * fullOneBit) & shared_data[group_offset + j]);\n      result[2] = result[2] ^ ((((matrixInt & (bitInt<< (group_inner_offset + 2*w))) >> (group_inner_offset + 2*w)) * fullOneBit) & shared_data[group_offset + j]);\n\n      ++index;\n    }\n    __syncthreads();\n\n  }\n\n  out[idx] = result[0];\n  out[idx + size] = result[1];\n  out[idx + 2 * size] = result[2];\n}",
            "__global__ void gcrs_m_3_w_6_coding_dotprod(\n  int k, int index, \n  const long *__restrict in, \n  long *__restrict out, \n  const unsigned int *__restrict bm, \n  int size)\n{\n  extern __shared__ long shared_data[];\n\n  int w = 6;\n\n  int i,j;\n  long result[3];\n\n  result[0] = 0;\n  result[1] = 0;\n  result[2] = 0;\n\n  const unsigned long fullOneBit = 0xFFFFFFFFFFFFFFFF;\n\n  int worksize_perblock = blockDim.x / w * w;\n  const unsigned int idx = worksize_perblock * blockIdx.x + threadIdx.x;\n\n  if (threadIdx.x >= worksize_perblock) {\n    return;\n  }\n\n  if (idx >= size) {\n    return;\n  }\n\n  int group_offset = (threadIdx.x / w) * w;\n  int group_inner_offset = threadIdx.x % w;\n  // row for each thread in the bitmatrix * row size which is k * w\n\n  unsigned int bitInt = 0x01;\n  unsigned int matrixInt;\n\n  for ( i = 0; i < k; i++ ) {\n\n    shared_data[threadIdx.x] = *(in + i*size + idx);\n\n    __syncthreads();\n\n#pragma unroll\n    for ( j = 0; j < w; j++ ) {\n      matrixInt = bm[index];\n      result[0] = result[0] ^ ((((matrixInt & (bitInt<< group_inner_offset)) >> group_inner_offset) * fullOneBit) & shared_data[group_offset + j]);\n      result[1] = result[1] ^ ((((matrixInt & (bitInt<< (group_inner_offset+w))) >> (group_inner_offset+w)) * fullOneBit) & shared_data[group_offset + j]);\n      result[2] = result[2] ^ ((((matrixInt & (bitInt<< (group_inner_offset + 2*w))) >> (group_inner_offset + 2*w)) * fullOneBit) & shared_data[group_offset + j]);\n\n      ++index;\n    }\n    __syncthreads();\n\n  }\n\n  out[idx] = result[0];\n  out[idx + size] = result[1];\n  out[idx + 2 * size] = result[2];\n}",
            "__global__ void gcrs_m_3_w_7_coding_dotprod(\n  int k, int index, \n  const long *__restrict in, \n  long *__restrict out, \n  const unsigned int *__restrict bm, \n  int size)\n{\n  extern __shared__ long shared_data[];\n\n  int w = 7;\n\n  int i,j;\n  long result[3];\n\n  result[0] = 0;\n  result[1] = 0;\n  result[2] = 0;\n\n  const unsigned long fullOneBit = 0xFFFFFFFFFFFFFFFF;\n\n  int worksize_perblock = blockDim.x / w * w;\n  const unsigned int idx = worksize_perblock * blockIdx.x + threadIdx.x;\n\n  if (threadIdx.x >= worksize_perblock) {\n    return;\n  }\n\n  if (idx >= size) {\n    return;\n  }\n\n  int group_offset = (threadIdx.x / w) * w;\n  int group_inner_offset = threadIdx.x % w;\n  // row for each thread in the bitmatrix * row size which is k * w\n\n  unsigned int bitInt = 0x01;\n  unsigned int matrixInt;\n\n  for ( i = 0; i < k; i++ ) {\n\n    shared_data[threadIdx.x] = *(in + i*size + idx);\n\n    __syncthreads();\n\n#pragma unroll\n    for ( j = 0; j < w; j++ ) {\n      matrixInt = bm[index];\n      result[0] = result[0] ^ ((((matrixInt & (bitInt<< group_inner_offset)) >> group_inner_offset) * fullOneBit) & shared_data[group_offset + j]);\n      result[1] = result[1] ^ ((((matrixInt & (bitInt<< (group_inner_offset+w))) >> (group_inner_offset+w)) * fullOneBit) & shared_data[group_offset + j]);\n      result[2] = result[2] ^ ((((matrixInt & (bitInt<< (group_inner_offset + 2*w))) >> (group_inner_offset + 2*w)) * fullOneBit) & shared_data[group_offset + j]);\n\n      ++index;\n    }\n    __syncthreads();\n\n  }\n\n  out[idx] = result[0];\n  out[idx + size] = result[1];\n  out[idx + 2 * size] = result[2];\n}",
            "__global__ void gcrs_m_3_w_8_coding_dotprod(\n  int k, int index, \n  const long *__restrict in, \n  long *__restrict out, \n  const unsigned int *__restrict bm, \n  int size)\n{\n  extern __shared__ long shared_data[];\n\n  int w = 8;\n\n  int i,j;\n  long result[3];\n\n  result[0] = 0;\n  result[1] = 0;\n  result[2] = 0;\n\n  const unsigned long fullOneBit = 0xFFFFFFFFFFFFFFFF;\n\n  int worksize_perblock = blockDim.x / w * w;\n  const unsigned int idx = worksize_perblock * blockIdx.x + threadIdx.x;\n\n  if (threadIdx.x >= worksize_perblock) {\n    return;\n  }\n\n  if (idx >= size) {\n    return;\n  }\n\n  int group_offset = (threadIdx.x / w) * w;\n  int group_inner_offset = threadIdx.x % w;\n  // row for each thread in the bitmatrix * row size which is k * w\n\n  unsigned int bitInt = 0x01;\n  unsigned int matrixInt;\n\n  for ( i = 0; i < k; i++ ) {\n\n    shared_data[threadIdx.x] = *(in + i*size + idx);\n\n    __syncthreads();\n\n#pragma unroll\n    for ( j = 0; j < w; j++ ) {\n      matrixInt = bm[index];\n      result[0] = result[0] ^ ((((matrixInt & (bitInt<< group_inner_offset)) >> group_inner_offset) * fullOneBit) & shared_data[group_offset + j]);\n      result[1] = result[1] ^ ((((matrixInt & (bitInt<< (group_inner_offset+w))) >> (group_inner_offset+w)) * fullOneBit) & shared_data[group_offset + j]);\n      result[2] = result[2] ^ ((((matrixInt & (bitInt<< (group_inner_offset + 2*w))) >> (group_inner_offset + 2*w)) * fullOneBit) & shared_data[group_offset + j]);\n\n      ++index;\n    }\n    __syncthreads();\n\n  }\n\n  out[idx] = result[0];\n  out[idx + size] = result[1];\n  out[idx + 2 * size] = result[2];\n}",
            "__global__ void gcrs_m_4_w_4_coding_dotprod(\n  int k, int index, \n  const long *__restrict in, \n  long *__restrict out, \n  const unsigned int *__restrict bm, \n  int size)\n{\n  extern __shared__ long shared_data[];\n\n  int w = 4;\n  int i,j;\n  long result[4];\n\n  result[0] = 0;\n  result[1] = 0;\n  result[2] = 0;\n  result[3] = 0;\n\n  const unsigned long fullOneBit = 0xFFFFFFFFFFFFFFFF;\n\n  int worksize_perblock = blockDim.x / w * w;\n  const unsigned int idx = worksize_perblock * blockIdx.x + threadIdx.x;\n\n  if (threadIdx.x >= worksize_perblock) {\n    return;\n  }\n\n  if (idx >= size) {\n    return;\n  }\n\n  int group_offset = (threadIdx.x / w) * w;\n  int group_inner_offset = threadIdx.x % w;\n  // row for each thread in the bitmatrix * row size which is k * w\n\n  unsigned int bitInt = 0x01;\n  unsigned int matrixInt;\n\n  for ( i = 0; i < k; i++ ) {\n\n    shared_data[threadIdx.x] = *(in + i*size + idx);\n\n    __syncthreads();\n\n#pragma unroll\n    for ( j = 0; j < w; j++ ) {\n      matrixInt = bm[index];\n      result[0] = result[0] ^ ((((matrixInt & (bitInt<< group_inner_offset)) >> group_inner_offset) * fullOneBit) & shared_data[group_offset + j]);\n      result[1] = result[1] ^ ((((matrixInt & (bitInt<< (group_inner_offset+w))) >> (group_inner_offset+w)) * fullOneBit) & shared_data[group_offset + j]);\n      result[2] = result[2] ^ ((((matrixInt & (bitInt<< (group_inner_offset + 2*w))) >> (group_inner_offset + 2*w)) * fullOneBit) & shared_data[group_offset + j]);\n      result[3] = result[3] ^ ((((matrixInt & (bitInt<< (group_inner_offset + 3*w))) >> (group_inner_offset + 3*w)) * fullOneBit) & shared_data[group_offset + j]);\n\n      ++index;\n    }\n    __syncthreads();\n\n  }\n\n  out[idx] = result[0];\n  out[idx + size] = result[1];\n  out[idx + 2 * size] = result[2];\n  out[idx + 3 * size] = result[3];\n}",
            "__global__ void gcrs_m_4_w_5_coding_dotprod(\n  int k, int index, \n  const long *__restrict in, \n  long *__restrict out, \n  const unsigned int *__restrict bm, \n  int size)\n{\n  extern __shared__ long shared_data[];\n\n  int w = 5;\n  int i,j;\n  long result[4];\n\n  result[0] = 0;\n  result[1] = 0;\n  result[2] = 0;\n  result[3] = 0;\n\n  const unsigned long fullOneBit = 0xFFFFFFFFFFFFFFFF;\n\n  int worksize_perblock = blockDim.x / w * w;\n  const unsigned int idx = worksize_perblock * blockIdx.x + threadIdx.x;\n\n  if (threadIdx.x >= worksize_perblock) {\n    return;\n  }\n\n  if (idx >= size) {\n    return;\n  }\n\n  int group_offset = (threadIdx.x / w) * w;\n  int group_inner_offset = threadIdx.x % w;\n  // row for each thread in the bitmatrix * row size which is k * w\n\n  unsigned int bitInt = 0x01;\n  unsigned int matrixInt;\n\n  for ( i = 0; i < k; i++ ) {\n\n    shared_data[threadIdx.x] = *(in + i*size + idx);\n\n    __syncthreads();\n\n#pragma unroll\n    for ( j = 0; j < w; j++ ) {\n      matrixInt = bm[index];\n      result[0] = result[0] ^ ((((matrixInt & (bitInt<< group_inner_offset)) >> group_inner_offset) * fullOneBit) & shared_data[group_offset + j]);\n      result[1] = result[1] ^ ((((matrixInt & (bitInt<< (group_inner_offset+w))) >> (group_inner_offset+w)) * fullOneBit) & shared_data[group_offset + j]);\n      result[2] = result[2] ^ ((((matrixInt & (bitInt<< (group_inner_offset + 2*w))) >> (group_inner_offset + 2*w)) * fullOneBit) & shared_data[group_offset + j]);\n      result[3] = result[3] ^ ((((matrixInt & (bitInt<< (group_inner_offset + 3*w))) >> (group_inner_offset + 3*w)) * fullOneBit) & shared_data[group_offset + j]);\n\n      ++index;\n    }\n    __syncthreads();\n\n  }\n\n  out[idx] = result[0];\n  out[idx + size] = result[1];\n  out[idx + 2 * size] = result[2];\n  out[idx + 3 * size] = result[3];\n}",
            "__global__ void gcrs_m_4_w_6_coding_dotprod(\n  int k, int index, \n  const long *__restrict in, \n  long *__restrict out, \n  const unsigned int *__restrict bm, \n  int size)\n{\n  extern __shared__ long shared_data[];\n\n  int w = 6;\n  int i,j;\n  long result[4];\n\n  result[0] = 0;\n  result[1] = 0;\n  result[2] = 0;\n  result[3] = 0;\n\n  const unsigned long fullOneBit = 0xFFFFFFFFFFFFFFFF;\n\n  int worksize_perblock = blockDim.x / w * w;\n  const unsigned int idx = worksize_perblock * blockIdx.x + threadIdx.x;\n\n  if (threadIdx.x >= worksize_perblock) {\n    return;\n  }\n\n  if (idx >= size) {\n    return;\n  }\n\n  int group_offset = (threadIdx.x / w) * w;\n  int group_inner_offset = threadIdx.x % w;\n  // row for each thread in the bitmatrix * row size which is k * w\n\n  unsigned int bitInt = 0x01;\n  unsigned int matrixInt;\n\n  for ( i = 0; i < k; i++ ) {\n\n    shared_data[threadIdx.x] = *(in + i*size + idx);\n\n    __syncthreads();\n\n#pragma unroll\n    for ( j = 0; j < w; j++ ) {\n      matrixInt = bm[index];\n      result[0] = result[0] ^ ((((matrixInt & (bitInt<< group_inner_offset)) >> group_inner_offset) * fullOneBit) & shared_data[group_offset + j]);\n      result[1] = result[1] ^ ((((matrixInt & (bitInt<< (group_inner_offset+w))) >> (group_inner_offset+w)) * fullOneBit) & shared_data[group_offset + j]);\n      result[2] = result[2] ^ ((((matrixInt & (bitInt<< (group_inner_offset + 2*w))) >> (group_inner_offset + 2*w)) * fullOneBit) & shared_data[group_offset + j]);\n      result[3] = result[3] ^ ((((matrixInt & (bitInt<< (group_inner_offset + 3*w))) >> (group_inner_offset + 3*w)) * fullOneBit) & shared_data[group_offset + j]);\n\n      ++index;\n    }\n    __syncthreads();\n\n  }\n\n  out[idx] = result[0];\n  out[idx + size] = result[1];\n  out[idx + 2 * size] = result[2];\n  out[idx + 3 * size] = result[3];\n}",
            "__global__ void gcrs_m_4_w_7_coding_dotprod(\n  int k, int index, \n  const long *__restrict in, \n  long *__restrict out, \n  const unsigned int *__restrict bm, \n  int size)\n{\n  extern __shared__ long shared_data[];\n\n  int w = 7;\n  int i,j;\n  long result[4];\n\n  result[0] = 0;\n  result[1] = 0;\n  result[2] = 0;\n  result[3] = 0;\n\n  const unsigned long fullOneBit = 0xFFFFFFFFFFFFFFFF;\n\n  int worksize_perblock = blockDim.x / w * w;\n  const unsigned int idx = worksize_perblock * blockIdx.x + threadIdx.x;\n\n  if (threadIdx.x >= worksize_perblock) {\n    return;\n  }\n\n  if (idx >= size) {\n    return;\n  }\n\n  int group_offset = (threadIdx.x / w) * w;\n  int group_inner_offset = threadIdx.x % w;\n  // row for each thread in the bitmatrix * row size which is k * w\n\n  unsigned int bitInt = 0x01;\n  unsigned int matrixInt;\n\n  for ( i = 0; i < k; i++ ) {\n\n    shared_data[threadIdx.x] = *(in + i*size + idx);\n\n    __syncthreads();\n\n#pragma unroll\n    for ( j = 0; j < w; j++ ) {\n      matrixInt = bm[index];\n      result[0] = result[0] ^ ((((matrixInt & (bitInt<< group_inner_offset)) >> group_inner_offset) * fullOneBit) & shared_data[group_offset + j]);\n      result[1] = result[1] ^ ((((matrixInt & (bitInt<< (group_inner_offset+w))) >> (group_inner_offset+w)) * fullOneBit) & shared_data[group_offset + j]);\n      result[2] = result[2] ^ ((((matrixInt & (bitInt<< (group_inner_offset + 2*w))) >> (group_inner_offset + 2*w)) * fullOneBit) & shared_data[group_offset + j]);\n      result[3] = result[3] ^ ((((matrixInt & (bitInt<< (group_inner_offset + 3*w))) >> (group_inner_offset + 3*w)) * fullOneBit) & shared_data[group_offset + j]);\n\n      ++index;\n    }\n    __syncthreads();\n\n  }\n\n  out[idx] = result[0];\n  out[idx + size] = result[1];\n  out[idx + 2 * size] = result[2];\n  out[idx + 3 * size] = result[3];\n}",
            "__global__ void gcrs_m_4_w_8_coding_dotprod(\n  int k, int index, \n  const long *__restrict in, \n  long *__restrict out, \n  const unsigned int *__restrict bm, \n  int size)\n{\n  extern __shared__ long shared_data[];\n\n  int w = 8;\n  int i,j;\n  long result[4];\n\n  result[0] = 0;\n  result[1] = 0;\n  result[2] = 0;\n  result[3] = 0;\n\n  const unsigned long fullOneBit = 0xFFFFFFFFFFFFFFFF;\n\n  int worksize_perblock = blockDim.x / w * w;\n  const unsigned int idx = worksize_perblock * blockIdx.x + threadIdx.x;\n\n  if (threadIdx.x >= worksize_perblock) {\n    return;\n  }\n\n  if (idx >= size) {\n    return;\n  }\n\n  int group_offset = (threadIdx.x / w) * w;\n  int group_inner_offset = threadIdx.x % w;\n  // row for each thread in the bitmatrix * row size which is k * w\n\n  unsigned int bitInt = 0x01;\n  unsigned int matrixInt;\n\n  for ( i = 0; i < k; i++ ) {\n\n    shared_data[threadIdx.x] = *(in + i*size + idx);\n\n    __syncthreads();\n\n#pragma unroll\n    for ( j = 0; j < w; j++ ) {\n      matrixInt = bm[index];\n      result[0] = result[0] ^ ((((matrixInt & (bitInt<< group_inner_offset)) >> group_inner_offset) * fullOneBit) & shared_data[group_offset + j]);\n      result[1] = result[1] ^ ((((matrixInt & (bitInt<< (group_inner_offset+w))) >> (group_inner_offset+w)) * fullOneBit) & shared_data[group_offset + j]);\n      result[2] = result[2] ^ ((((matrixInt & (bitInt<< (group_inner_offset + 2*w))) >> (group_inner_offset + 2*w)) * fullOneBit) & shared_data[group_offset + j]);\n      result[3] = result[3] ^ ((((matrixInt & (bitInt<< (group_inner_offset + 3*w))) >> (group_inner_offset + 3*w)) * fullOneBit) & shared_data[group_offset + j]);\n\n      ++index;\n    }\n    __syncthreads();\n\n  }\n\n  out[idx] = result[0];\n  out[idx + size] = result[1];\n  out[idx + 2 * size] = result[2];\n  out[idx + 3 * size] = result[3];\n}"
        ]
    },
    "s8n-cuda": {
        "/Users/gbolet/hecbench-roofline/src/s8n-cuda/main.cu": [
            "__global__\nvoid k_cube_select(int b, int n, int radius, const int* xyz, int* idx_out) {\n  int batch_idx = blockIdx.x;\n  xyz += batch_idx * n * 3;\n  idx_out += batch_idx * n * 8;\n  int temp_dist[8];\n  for(int i = threadIdx.x; i < n; i += blockDim.x) {\n    int x = xyz[i * 3];\n    int y = xyz[i * 3 + 1];\n    int z = xyz[i * 3 + 2];\n    for(int j = 0; j < 8;j ++) {\n      temp_dist[j] = radius;\n      idx_out[i * 8 + j] = i; // if not found, just return itself..\n    }\n    for(int j = 0; j < n; j ++) {\n      if(i == j) continue;\n      int tx = xyz[j * 3];\n      int ty = xyz[j * 3 + 1];\n      int tz = xyz[j * 3 + 2];\n      int dist = (x - tx) * (x - tx) + (y - ty) * (y - ty) + (z - tz) * (z - tz);\n      if(dist > radius) continue;\n      int _x = (tx > x);\n      int _y = (ty > y);\n      int _z = (tz > z);\n      int temp_idx = _x * 4 + _y * 2 + _z;\n      if(dist < temp_dist[temp_idx]) {\n        idx_out[i * 8 + temp_idx] = j;\n        temp_dist[temp_idx] = dist;\n      }\n    }\n  }\n}",
            "__global__\nvoid k_cube_select_two(int b, int n, int radius, const int* xyz, int* idx_out) {\n  int batch_idx = blockIdx.x;\n  xyz += batch_idx * n * 3;\n  idx_out += batch_idx * n * 16;\n  int temp_dist[16];\n  for(int i = threadIdx.x; i < n; i += blockDim.x) {\n    int x = xyz[i * 3];\n    int y = xyz[i * 3 + 1];\n    int z = xyz[i * 3 + 2];\n    for(int j = 0; j < 16;j ++) {\n      temp_dist[j] = radius;\n      idx_out[i * 16 + j] = i; // if not found, just return itself..\n    }\n    for(int j = 0; j < n; j ++) {\n      if(i == j) continue;\n      int tx = xyz[j * 3];\n      int ty = xyz[j * 3 + 1];\n      int tz = xyz[j * 3 + 2];\n      int dist = (x - tx) * (x - tx) + (y - ty) * (y - ty) + (z - tz) * (z - tz);\n      if(dist > radius) continue;\n      int _x = (tx > x);\n      int _y = (ty > y);\n      int _z = (tz > z);\n      int temp_idx = _x * 8 + _y * 4 + _z * 2;\n      bool flag = false;\n      for(int k = 0; k < 2; k ++) {\n        if (dist < temp_dist[temp_idx + k]) {\n          flag = true;\n        }\n        if (flag) {\n          for (int kk = 1; kk >= k + 1; kk --) {\n            idx_out[i * 16 + temp_idx + kk] = idx_out[i * 16 + temp_idx + kk - 1];\n            temp_dist[temp_idx + kk] = temp_dist[temp_idx + kk - 1];\n          }\n          idx_out[i * 16 + temp_idx + k] = j;\n          temp_dist[temp_idx + k] = dist;\n          break;\n        }\n      }\n    }\n  }\n}",
            "__global__\nvoid k_cube_select_four(int b, int n, int radius, const int* xyz, int* idx_out) {\n  int batch_idx = blockIdx.x;\n  xyz += batch_idx * n * 3;\n  idx_out += batch_idx * n * 32;\n  int temp_dist[32];\n  for(int i = threadIdx.x; i < n; i += blockDim.x) {\n    int x = xyz[i * 3];\n    int y = xyz[i * 3 + 1];\n    int z = xyz[i * 3 + 2];\n    for(int j = 0; j < 32;j ++) {\n      temp_dist[j] = radius;\n      idx_out[i * 32 + j] = i; // if not found, just return itself..\n    }\n    for(int j = 0; j < n; j ++) {\n      if(i == j) continue;\n      int tx = xyz[j * 3];\n      int ty = xyz[j * 3 + 1];\n      int tz = xyz[j * 3 + 2];\n      int dist = (x - tx) * (x - tx) + (y - ty) * (y - ty) + (z - tz) * (z - tz);\n      if(dist > radius) continue;\n      int _x = (tx > x);\n      int _y = (ty > y);\n      int _z = (tz > z);\n      int temp_idx = _x * 16 + _y * 8 + _z * 4;\n      bool flag = false;\n      for(int k = 0; k < 4; k ++) {\n        if (dist < temp_dist[temp_idx + k]) {\n          flag = true;\n        }\n        if (flag) {\n          for (int kk = 3; kk >= k + 1; kk --) {\n            idx_out[i * 32 + temp_idx + kk] = idx_out[i * 32 + temp_idx + kk - 1];\n            temp_dist[temp_idx + kk] = temp_dist[temp_idx + kk - 1];\n          }\n          idx_out[i * 32 + temp_idx + k] = j;\n          temp_dist[temp_idx + k] = dist;\n          break;\n        }\n      }\n    }\n  }\n}"
        ]
    },
    "bscan-cuda": {
        "/Users/gbolet/hecbench-roofline/src/bscan-cuda/main.cu": [
            "__device__ __inline__ unsigned int lanemask_lt()\n{\n#ifdef ASM\n  unsigned int mask;\n  asm(\"mov.u32 %0, %lanemask_lt;\" : \"=r\"(mask));\n  return mask;\n#else\n  const unsigned int lane = threadIdx.x & (warpSize-1);\n  return (1 << (lane)) - 1;\n#endif\n}\n\n__device__ __inline__ unsigned int binary_warp_scan(bool p)\n{\n  const unsigned int mask = lanemask_lt();\n#if (CUDART_VERSION < 9000)\n  unsigned int b = __ballot(p);\n  return __popc(b & mask);\n#else\n  unsigned int b = __ballot_sync(mask, p);\n  return __popc(b);\n#endif\n}\n\n__host__ __device__ __inline__\nbool valid(int x) {\n  return x > 0;\n}\n\n__device__ __inline__ int warp_scan(int val, volatile int *s_data)\n{\n  // initialize shared memory accessed by each warp with zeros\n  int idx = 2 * threadIdx.x - (threadIdx.x & (warpSize-1));\n  s_data[idx] = 0;\n  idx += warpSize;\n  int t = s_data[idx] = val;\n  s_data[idx] = t += s_data[idx - 1];\n  s_data[idx] = t += s_data[idx - 2];\n  s_data[idx] = t += s_data[idx - 4];\n  s_data[idx] = t += s_data[idx - 8];\n  s_data[idx] = t += s_data[idx -16];\n  return s_data[idx-1];\n}\n\n__device__ __inline__ int block_binary_prefix_sums(int x)\n{\n  // 2 x warpIdx's upper bound (1024/32)\n  __shared__ int sdata[64];\n\n  bool predicate = valid(x);\n\n  // A. Compute exclusive prefix sums within each warp\n  int warpPrefix = binary_warp_scan(predicate);\n  int idx = threadIdx.x;\n  int warpIdx = idx / warpSize;\n  int laneIdx = idx & (warpSize - 1);\n#ifdef DEBUG\n  printf(\"A %d %d %d\\n\", warpIdx, laneIdx, warpPrefix);\n#endif\n\n  // B. The last thread of each warp stores inclusive\n  // prefix sum to the warp\u2019s index in shared memory\n  if (laneIdx == warpSize - 1) {\n    sdata[warpIdx] = warpPrefix + predicate;\n#ifdef DEBUG\n    printf(\"B %d %d\\n\", warpIdx, sdata[warpIdx]);\n#endif\n  }\n  __syncthreads();\n\n  // C. One warp scans the warp partial sums\n  if (idx < warpSize) {\n    sdata[idx] = warp_scan(sdata[idx], sdata);\n#ifdef DEBUG\n    printf(\"C: %d %d\\n\", idx, sdata[idx]);\n#endif\n  }\n  __syncthreads();\n\n  // D. Each thread adds prefix sums of warp partial\n  // sums to its own intra\u2212warp prefix sums\n  return warpPrefix + sdata[warpIdx];\n}\n\n__global__ void binary_scan(\n        int *__restrict__ g_odata,\n  const int *__restrict__ g_idata)\n{\n  int i = threadIdx.x;\n  g_odata[i] = block_binary_prefix_sums(g_idata[i]);\n}"
        ]
    },
    "heat-cuda": {
        "/Users/gbolet/hecbench-roofline/src/heat-cuda/heat.cu": [
            "__global__ void initial_value(const unsigned int n, const double dx, const double length, double * u) {\n\n  int idx = blockDim.x * blockIdx.x + threadIdx.x;\n  if (idx < n*n) {\n    int i = idx % n;\n    int j = idx / n;\n    double y = dx * (j+1); // Physical y position\n    double x = dx * (i+1); // Physical x position\n    u[i+j*n] = sin(PI * x / length) * sin(PI * y / length);\n  }\n}",
            "__global__ void zero(const unsigned int n, double * u) {\n\n  int idx = blockDim.x * blockIdx.x + threadIdx.x;\n  if (idx < n*n) u[idx] = 0.0;\n}",
            "__global__ void solve(const unsigned int n, const double alpha, const double dx, const double dt, \n\t\tconst double r, const double r2,\n\t\tdouble * __restrict__ u, double * __restrict__ u_tmp) {\n\n  int idx = blockDim.x * blockIdx.x + threadIdx.x;\n  if (idx < n * n) {\n    int i = idx % n;\n    int j = idx / n;\n    // Boundaries are zero because the MMS solution is zero there.\n    u_tmp[i+j*n] =  r2 * u[i+j*n] +\n    r * ((i < n-1) ? u[i+1+j*n] : 0.0) +\n    r * ((i > 0)   ? u[i-1+j*n] : 0.0) +\n    r * ((j < n-1) ? u[i+(j+1)*n] : 0.0) +\n    r * ((j > 0)   ? u[i+(j-1)*n] : 0.0);\n  }\n}"
        ]
    },
    "rsbench-cuda": {
        "/Users/gbolet/hecbench-roofline/src/rsbench-cuda/simulation.cu": [
            "__device__\nRSComplex c_div( RSComplex A, RSComplex B)\n{\n  double a = A.r;\n  double b = A.i;\n  double c = B.r;\n  double d = B.i;\n  RSComplex C;\n  double denom = c*c + d*d;\n  C.r = ( (a*c) + (b*d) ) / denom;\n  C.i = ( (b*c) - (a*d) ) / denom;\n  return C;\n}\n\n__host__ __device__\nRSComplex c_mul( RSComplex A, RSComplex B)\n{\n  double a = A.r;\n  double b = A.i;\n  double c = B.r;\n  double d = B.i;\n  RSComplex C;\n  C.r = (a*c) - (b*d);\n  C.i = (a*d) + (b*c);\n  return C;\n}\n\n__device__\nRSComplex c_sub( RSComplex A, RSComplex B)\n{\n  RSComplex C;\n  C.r = A.r - B.r;\n  C.i = A.i - B.i;\n  return C;\n}\n\n__device__\nvoid calculate_sig_T( int nuc, double E, int input_numL, DOUBLE_T pseudo_K0RS, RSComplex * sigTfactors )\n{\n  double phi;\n\n  for( int i = 0; i < 4; i++ )\n  {\n    phi = pseudo_K0RS[nuc * input_numL + i] * sqrt(E);\n\n    if( i == 1 )\n      phi -= - atan( phi );\n    else if( i == 2 )\n      phi -= atan( 3.0 * phi / (3.0 - phi*phi));\n    else if( i == 3 )\n      phi -= atan(phi*(15.0-phi*phi)/(15.0-6.0*phi*phi));\n\n    phi *= 2.0;\n\n    sigTfactors[i].r = cos(phi);\n    sigTfactors[i].i = -sin(phi);\n  }\n}\n\n__device__\ndouble fast_exp(double x)\n{\n  x = 1.0 + x * 0.000244140625;\n  x *= x; x *= x; x *= x; x *= x;\n  x *= x; x *= x; x *= x; x *= x;\n  x *= x; x *= x; x *= x; x *= x;\n  return x;\n}\n\n__device__\ndouble c_abs( RSComplex A)\n{\n  return sqrt(A.r*A.r + A.i * A.i);\n}\n\n__device__\nRSComplex c_add( RSComplex A, RSComplex B)\n{\n  RSComplex C;\n  C.r = A.r + B.r;\n  C.i = A.i + B.i;\n  return C;\n}\n\n__device__\nRSComplex fast_cexp( RSComplex z )\n{\n  double x = z.r;\n  double y = z.i;\n\n  // For consistency across architectures, we\n  // will use our own exponetial implementation\n  //double t1 = exp(x);\n  double t1 = fast_exp(x);\n  double t2 = cos(y);\n  double t3 = sin(y);\n  RSComplex t4 = {t2, t3};\n  RSComplex t5 = {t1, 0};\n  RSComplex result = c_mul(t5, (t4));\n  return result;\n}\n\n__device__\nRSComplex fast_nuclear_W( RSComplex Z )\n{\n  // Abrarov \n  if( c_abs(Z) < 6.0 )\n  {\n    // Precomputed parts for speeding things up\n    // (N = 10, Tm = 12.0)\n    RSComplex prefactor = {0, 8.124330e+01};\n    double an[10] = {\n      2.758402e-01,\n      2.245740e-01,\n      1.594149e-01,\n      9.866577e-02,\n      5.324414e-02,\n      2.505215e-02,\n      1.027747e-02,\n      3.676164e-03,\n      1.146494e-03,\n      3.117570e-04\n    };\n    double neg_1n[10] = {\n      -1.0,\n      1.0,\n      -1.0,\n      1.0,\n      -1.0,\n      1.0,\n      -1.0,\n      1.0,\n      -1.0,\n      1.0\n    };\n\n    double denominator_left[10] = {\n      9.869604e+00,\n      3.947842e+01,\n      8.882644e+01,\n      1.579137e+02,\n      2.467401e+02,\n      3.553058e+02,\n      4.836106e+02,\n      6.316547e+02,\n      7.994380e+02,\n      9.869604e+02\n    };\n\n    RSComplex t1 = {0, 12};\n    RSComplex t2 = {12, 0};\n    RSComplex i = {0,1};\n    RSComplex one = {1, 0};\n    RSComplex W = c_div(c_mul(i, ( c_sub(one, fast_cexp(c_mul(t1, Z))) )) , c_mul(t2, Z));\n    RSComplex sum = {0,0};\n    for( int n = 0; n < 10; n++ )\n    {\n      RSComplex t3 = {neg_1n[n], 0};\n      RSComplex top = c_sub(c_mul(t3, fast_cexp(c_mul(t1, Z))), one);\n      RSComplex t4 = {denominator_left[n], 0};\n      RSComplex t5 = {144, 0};\n      RSComplex bot = c_sub(t4, c_mul(t5,c_mul(Z,Z)));\n      RSComplex t6 = {an[n], 0};\n      sum = c_add(sum, c_mul(t6, c_div(top,bot)));\n    }\n    W = c_add(W, c_mul(prefactor, c_mul(Z, sum)));\n    return W;\n  }\n  else\n  {\n    // QUICK_2 3 Term Asymptotic Expansion (Accurate to O(1e-6)).\n    // Pre-computed parameters\n    RSComplex a = {0.512424224754768462984202823134979415014943561548661637413182,0};\n    RSComplex b = {0.275255128608410950901357962647054304017026259671664935783653, 0};\n    RSComplex c = {0.051765358792987823963876628425793170829107067780337219430904, 0};\n    RSComplex d = {2.724744871391589049098642037352945695982973740328335064216346, 0};\n\n    RSComplex i = {0,1};\n    RSComplex Z2 = c_mul(Z, Z);\n    // Three Term Asymptotic Expansion\n    RSComplex W = c_mul(c_mul(Z,i), (c_add(c_div(a,(c_sub(Z2, b))) , c_div(c,(c_sub(Z2, d))))));\n\n    return W;\n  }\n}\n\n__device__\nvoid calculate_micro_xs(double * micro_xs, int nuc, double E, int input_numL,\n                        INT_T n_windows, DOUBLE_T pseudo_K0RS, WINDOW_T windows,\n                        POLE_T poles, int max_num_windows, int max_num_poles)\n{\n  // MicroScopic XS's to Calculate\n  double sigT;\n  double sigA;\n  double sigF;\n  double sigE;\n\n  // Calculate Window Index\n  double spacing = 1.0 / n_windows[nuc];\n  int window = (int) ( E / spacing );\n  if( window == n_windows[nuc] )\n    window--;\n\n  // Calculate sigTfactors\n  RSComplex sigTfactors[4]; // Of length input.numL, which is always 4\n  calculate_sig_T(nuc, E, input_numL, pseudo_K0RS, sigTfactors );\n\n  // Calculate contributions from window \"background\" (i.e., poles outside window (pre-calculated)\n  Window w = windows[nuc * max_num_windows + window];\n  sigT = E * w.T;\n  sigA = E * w.A;\n  sigF = E * w.F;\n\n  // Loop over Poles within window, add contributions\n  for( int i = w.start; i < w.end; i++ )\n  {\n    RSComplex PSIIKI;\n    RSComplex CDUM;\n    Pole pole = poles[nuc * max_num_poles + i];\n    RSComplex t1 = {0, 1};\n    RSComplex t2 = {sqrt(E), 0 };\n    PSIIKI = c_div( t1 , c_sub(pole.MP_EA,t2) );\n    RSComplex E_c = {E, 0};\n    CDUM = c_div(PSIIKI, E_c);\n    sigT += (c_mul(pole.MP_RT, c_mul(CDUM, sigTfactors[pole.l_value])) ).r;\n    sigA += (c_mul( pole.MP_RA, CDUM)).r;\n    sigF += (c_mul(pole.MP_RF, CDUM)).r;\n  }\n\n  sigE = sigT - sigA;\n\n  micro_xs[0] = sigT;\n  micro_xs[1] = sigA;\n  micro_xs[2] = sigF;\n  micro_xs[3] = sigE;\n}\n\n__device__\nvoid calculate_micro_xs_doppler(double * micro_xs, int nuc, double E,\n                                int input_numL, INT_T n_windows,\n                                DOUBLE_T pseudo_K0RS, WINDOW_T windows,\n                                POLE_T poles, int max_num_windows, int max_num_poles )\n{\n  // MicroScopic XS's to Calculate\n  double sigT;\n  double sigA;\n  double sigF;\n  double sigE;\n\n  // Calculate Window Index\n  double spacing = 1.0 / n_windows[nuc];\n  int window = (int) ( E / spacing );\n  if( window == n_windows[nuc] )\n    window--;\n\n  // Calculate sigTfactors\n  RSComplex sigTfactors[4]; // Of length input.numL, which is always 4\n  calculate_sig_T(nuc, E, input_numL, pseudo_K0RS, sigTfactors );\n\n  // Calculate contributions from window \"background\" (i.e., poles outside window (pre-calculated)\n  Window w = windows[nuc * max_num_windows + window];\n  sigT = E * w.T;\n  sigA = E * w.A;\n  sigF = E * w.F;\n\n  double dopp = 0.5;\n\n  // Loop over Poles within window, add contributions\n  for( int i = w.start; i < w.end; i++ )\n  {\n    Pole pole = poles[nuc * max_num_poles + i];\n\n    // Prep Z\n    RSComplex E_c = {E, 0};\n    RSComplex dopp_c = {dopp, 0};\n    RSComplex Z = c_mul(c_sub(E_c, pole.MP_EA), dopp_c);\n\n    // Evaluate Fadeeva Function\n    RSComplex faddeeva = fast_nuclear_W( Z );\n\n    // Update W\n    sigT += (c_mul( pole.MP_RT, c_mul(faddeeva, sigTfactors[pole.l_value]) )).r;\n    sigA += (c_mul( pole.MP_RA , faddeeva)).r;\n    sigF += (c_mul( pole.MP_RF , faddeeva)).r;\n  }\n\n  sigE = sigT - sigA;\n\n  micro_xs[0] = sigT;\n  micro_xs[1] = sigA;\n  micro_xs[2] = sigF;\n  micro_xs[3] = sigE;\n}\n\n__device__\ndouble LCG_random_double(uint64_t * seed)\n{\n  const uint64_t m = 9223372036854775808ULL; // 2^63\n  const uint64_t a = 2806196910506780709ULL;\n  const uint64_t c = 1ULL;\n  *seed = (a * (*seed) + c) % m;\n  return (double) (*seed) / (double) m;\n}\n\n__device__\nvoid calculate_macro_xs(double * macro_xs, int mat, double E,\n                        int input_doppler, int input_numL,\n                        INT_T num_nucs, INT_T mats,\n                        int max_num_nucs,\n                        DOUBLE_T concs,\n                        INT_T n_windows,\n                        DOUBLE_T pseudo_K0Rs,\n                        WINDOW_T windows,\n                        POLE_T poles,\n                        int max_num_windows,\n                        int max_num_poles ) \n{\n  // zero out macro vector\n  for( int i = 0; i < 4; i++ )\n    macro_xs[i] = 0;\n\n  // for nuclide in mat\n  for( int i = 0; i < num_nucs[mat]; i++ )\n  {\n    double micro_xs[4];\n    int nuc = mats[mat * max_num_nucs + i];\n\n    if( input_doppler == 1 )\n      calculate_micro_xs_doppler( micro_xs, nuc, E, input_numL, n_windows, pseudo_K0Rs, windows, poles, max_num_windows, max_num_poles);\n    else\n      calculate_micro_xs( micro_xs, nuc, E, input_numL, n_windows, pseudo_K0Rs, windows, poles, max_num_windows, max_num_poles);\n\n    for( int j = 0; j < 4; j++ )\n    {\n      macro_xs[j] += micro_xs[j] * concs[mat * max_num_nucs + i];\n    }\n    // Debug\n    /*\n       printf(\"E = %.2lf, mat = %d, macro_xs[0] = %.2lf, macro_xs[1] = %.2lf, macro_xs[2] = %.2lf, macro_xs[3] = %.2lf\\n\",\n       E, mat, macro_xs[0], macro_xs[1], macro_xs[2], macro_xs[3] );\n     */\n  }\n\n  // Debug\n  /*\n     printf(\"E = %.2lf, mat = %d, macro_xs[0] = %.2lf, macro_xs[1] = %.2lf, macro_xs[2] = %.2lf, macro_xs[3] = %.2lf\\n\",\n     E, mat, macro_xs[0], macro_xs[1], macro_xs[2], macro_xs[3] );\n   */\n}\n\n__device__ uint64_t fast_forward_LCG(uint64_t seed, uint64_t n)\n{\n  const uint64_t m = 9223372036854775808ULL; // 2^63\n  uint64_t a = 2806196910506780709ULL;\n  uint64_t c = 1ULL;\n\n  n = n % m;\n\n  uint64_t a_new = 1;\n  uint64_t c_new = 0;\n\n  while(n > 0) \n  {\n    if(n & 1)\n    {\n      a_new *= a;\n      c_new = c_new * a + c;\n    }\n    c *= (a + 1);\n    a *= a;\n\n    n >>= 1;\n  }\n\n  return (a_new * seed + c_new) % m;\n}\n\n__device__\nint pick_mat( uint64_t * seed )\n{\n  // I have a nice spreadsheet supporting these numbers. They are\n  // the fractions (by volume) of material in the core. Not a \n  // *perfect* approximation of where XS lookups are going to occur,\n  // but this will do a good job of biasing the system nonetheless.\n\n  double dist[12];\n  dist[0]  = 0.140;  // fuel\n  dist[1]  = 0.052;  // cladding\n  dist[2]  = 0.275;  // cold, borated water\n  dist[3]  = 0.134;  // hot, borated water\n  dist[4]  = 0.154;  // RPV\n  dist[5]  = 0.064;  // Lower, radial reflector\n  dist[6]  = 0.066;  // Upper reflector / top plate\n  dist[7]  = 0.055;  // bottom plate\n  dist[8]  = 0.008;  // bottom nozzle\n  dist[9]  = 0.015;  // top nozzle\n  dist[10] = 0.025;  // top of fuel assemblies\n  dist[11] = 0.013;  // bottom of fuel assemblies\n\n  double roll = LCG_random_double(seed);\n\n  // makes a pick based on the distro\n  for( int i = 0; i < 12; i++ )\n  {\n    double running = 0;\n    for( int j = i; j > 0; j-- )\n      running += dist[j];\n    if( roll < running )\n      return i;\n  }\n\n  return 0;\n}\n\n__global__ void lookup ( \n    const int*__restrict__ num_nucs,\n    const double*__restrict__ concs,\n    const int*__restrict__ mats,\n          int*__restrict__ verification,\n    const int*__restrict__ n_windows,\n    const double*__restrict__ pseudo_K0RS,\n    const Window*__restrict__ windows,\n    const Pole*__restrict__ poles,\n    int n_lookups,\n    int input_doppler,\n    int input_numL,\n    int max_num_windows,\n    int max_num_poles,\n    int max_num_nucs ) {\n\n  // get the index to operate on, first dimemsion\n  size_t i = threadIdx.x + blockIdx.x * blockDim.x;\n\n  if (i < n_lookups) {\n\n    // Set the initial seed value\n    uint64_t seed = STARTING_SEED;  \n\n    // Forward seed to lookup index (we need 2 samples per lookup)\n    seed = fast_forward_LCG(seed, 2*i);\n\n    // Randomly pick an energy and material for the particle\n    double p_energy = LCG_random_double(&seed);\n    int mat         = pick_mat(&seed); \n\n    // debugging\n    //printf(\"E = %lf mat = %d\\n\", p_energy, mat);\n\n    double macro_xs_vector[4] = {0};\n\n    // Perform macroscopic Cross Section Lookup\n    calculate_macro_xs(\n        macro_xs_vector, mat, p_energy, \n        input_doppler, //in, \n        input_numL,\n        num_nucs, mats, \n        max_num_nucs, concs, n_windows, pseudo_K0RS, windows, poles, \n        max_num_windows, max_num_poles );\n\n    // For verification, and to prevent the compiler from optimizing\n    // all work out, we interrogate the returned macro_xs_vector array\n    // to find its maximum value index, then increment the verification\n    // value by that index. In this implementation, we store to a global\n    // array that will get tranferred back and reduced on the host.\n    double max = -DBL_MAX;\n    int max_idx = 0;\n    for(int j = 0; j < 4; j++ )\n    {\n      if( macro_xs_vector[j] > max )\n      {\n        max = macro_xs_vector[j];\n        max_idx = j;\n      }\n    }\n    verification[i] = max_idx+1;\n  }\n}"
        ]
    },
    "atomicCost-cuda": {
        "/Users/gbolet/hecbench-roofline/src/atomicCost-cuda/main.cu": [
            "#define T ((int)32)\n\n\n__global__ void woAtomicOnGlobalMem(T* result, int size)\n{\n  unsigned int tid = blockIdx.x * blockDim.x + threadIdx.x;\n  for ( unsigned int i = tid * size; i < (tid + 1) * size; i++){\n    result[tid] += i % 2;\n  }\n}",
            "#define T ((int)32)\n\n\n__global__ void wiAtomicOnGlobalMem(T* result, int size)\n{\n  unsigned int tid = blockIdx.x * blockDim.x + threadIdx.x;\n  for ( unsigned int i = tid * size; i < (tid + 1) * size; i++){\n    atomicAdd(&result[tid], i % 2);\n  }\n}"
        ]
    },
    "streamcluster-cuda": {
        "/Users/gbolet/hecbench-roofline/src/streamcluster-cuda/kernel.h": [
            "__global__\nvoid compute_cost(\n    const Point_Struct *__restrict__ p_d_acc,       \n    const float *__restrict__ coord_d_acc,\n          float *__restrict__ work_mem_d_acc,      \n    const int *__restrict__ center_table_d_acc,\n    char *__restrict__ switch_membership_d_acc,      \n    const int num,\n    const int dim,\n    const long x,\n    const int K)\n{  \n  extern __shared__ float coord_s_acc[]; \n  /* block ID and global thread ID */\n  const int local_id = threadIdx.x; \n  const int thread_id = blockDim.x*blockIdx.x+local_id;\n\n  if(thread_id<num){\n    // coordinate mapping of point[x] to shared mem\n    if(local_id == 0)\n      for(int i=0; i<dim; i++){ \n        coord_s_acc[i] = coord_d_acc[i*num + x];\n      }\n    __syncthreads();\n\n    // cost between this point and point[x]: euclidean distance multiplied by weight\n    float x_cost = 0.0;\n    for(int i=0; i<dim; i++)\n      x_cost += (coord_d_acc[(i*num)+thread_id]-coord_s_acc[i]) * (coord_d_acc[(i*num)+thread_id]-coord_s_acc[i]);\n    x_cost = x_cost * p_d_acc[thread_id].weight;\n\n    float current_cost = p_d_acc[thread_id].cost;\n\n    int base = thread_id*(K+1);   \n    // if computed cost is less then original (it saves), mark it as to reassign    \n    if ( x_cost < current_cost ){\n      switch_membership_d_acc[thread_id] = '1';\n      int addr_1 = base + K;\n      work_mem_d_acc[addr_1] = x_cost - current_cost;\n    }\n    // if computed cost is larger, save the difference\n    else {\n      int assign = p_d_acc[thread_id].assign;\n      int addr_2 = base + center_table_d_acc[assign];\n      work_mem_d_acc[addr_2] += current_cost - x_cost;\n    }\n  }\n}"
        ]
    },
    "clink-cuda": {
        "/Users/gbolet/hecbench-roofline/src/clink-cuda/kernel.h": [
            "inline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\ninline __host__ __device__ float4 expf(float4 v)\n{\n    return make_float4(expf(v.x), expf(v.y), expf(v.z), expf(v.w));\n}\n\n__device__\nfloat sigmoid(const float x) { return 1.0f / (1.0f + expf(-x)); }\n\n__global__ void\nlstm_inference(\n  const float*__restrict__ d_x, \n  const float*__restrict__ d_inW, \n  const float*__restrict__ d_intW, \n  const float*__restrict__ d_intB, \n  const float*__restrict__ d_outW, \n  const float*__restrict__ d_outB, \n        float*__restrict__ d_y)\n{\n\n  int t,i,j;\n  int gid = blockDim.x * blockIdx.x + threadIdx.x;\n\n  float h_state[5] = {0,0,0,0,0};\n  float c_state[5] = {0,0,0,0,0};\n  float i_state[5] = {0,0,0,0,0};\n  float f_state[5] = {0,0,0,0,0};\n  float o_state[5] = {0,0,0,0,0};\n  float g_state[5] = {0,0,0,0,0};\n\n  for (t = 0; t < SAMPLE_TEST_LEN; ++t) {\n\n    float x = d_x[gid * SAMPLE_TEST_LEN + t];\n\n    for (j = 0; j < 5; ++j) {\n      i_state[j] = d_inW[j] * x;\n      for (i = 0; i < 5; ++i)\n        i_state[j] += h_state[i] * d_intW[j*5+i];\n      i_state[j] += d_intB[j];\n      i_state[j] = sigmoid(i_state[j]);\n    }\n\n    for (j = 0; j < 5; ++j) {\n      f_state[j] = d_inW[5+j] * x;\n      for (i = 0; i < 5; ++i)\n        f_state[j] += h_state[i] * d_intW[25+j*5+i];\n      f_state[j] += d_intB[5+j];\n      f_state[j] = sigmoid(f_state[j]);\n    }\n\n    for (j = 0; j < 5; ++j) {\n      o_state[j] = d_inW[10+j] * x;\n      for (i = 0; i < 5; ++i)\n        o_state[j] += h_state[i] * d_intW[50+j*5+i];\n      o_state[j] += d_intB[10+j];\n      o_state[j] = sigmoid(o_state[j]);\n    }\n\n    for (j = 0; j < 5; ++j) {\n      g_state[j] = d_inW[15+j] * x;\n      for (i = 0; i < 5; ++i)\n        g_state[j] += h_state[i] * d_intW[75+j*5+i];\n      g_state[j] += d_intB[15+j];\n      g_state[j] = tanh(g_state[j]);\n    }\n\n    for (j = 0; j < 5; ++j) {\n      c_state[j] = c_state[j] * f_state[j] + g_state[j] * i_state[j];\n      h_state[j] = tanh(c_state[j]) * o_state[j];\n    }\n\n    float y = d_outB[0];\n    for (j = 0; j < 5; ++j)\n      y += h_state[j] * d_outW[j];\n    d_y[gid * SAMPLE_TEST_LEN + t] = y;\n  }\n}"
        ]
    },
    "mixbench-cuda": {
        "/Users/gbolet/hecbench-roofline/src/mixbench-cuda/main.cu": [
            "__global__ void benchmark_func(float *g_data,\n                               const int compute_iterations)\n{\n  const unsigned int blockSize = blockDim.x;\n  const int stride = blockSize;\n  int idx = blockIdx.x*blockSize*granularity + threadIdx.x;\n  const int big_stride = gridDim.x*blockSize*granularity;\n\n  float tmps[granularity];\n  for(int k=0; k<fusion_degree; k++) {\n    #pragma unroll\n    for(int j=0; j<granularity; j++) {\n      // Load elements (memory intensive part)\n      tmps[j] = g_data[idx+j*stride+k*big_stride];\n\n      // Perform computations (compute intensive part)\n      for(int i=0; i<compute_iterations; i++)\n        tmps[j] = tmps[j]*tmps[j]+seed;\n    }\n\n    // Multiply add reduction\n    float sum = 0.f;\n    #pragma unroll\n    for(int j=0; j<granularity; j+=2)\n      sum += tmps[j]*tmps[j+1];\n\n    #pragma unroll\n    for(int j=0; j<granularity; j++)\n      g_data[idx+k*big_stride] = sum;\n  }\n}"
        ]
    },
    "miniWeather-cuda": {
        "/Users/gbolet/hecbench-roofline/src/miniWeather-cuda/kernels.h": [
            "__global__\nvoid compute_flux_x (const double *__restrict__ state,\n                           double *__restrict__ flux,\n                     const double *__restrict__ hy_dens_cell,\n                     const double *__restrict__ hy_dens_theta_cell,\n                     const double hv_coef,\n                     const int nx,\n                     const int nz,\n                     const int hs)\n{\n  int k = blockIdx.y * blockDim.y + threadIdx.y;\n  int i = blockIdx.x * blockDim.x + threadIdx.x;\n  double stencil[4], d3_vals[NUM_VARS], vals[NUM_VARS];\n\n  if (i < nx+1 && k < nz) { \n    //Use fourth-order interpolation from four cell averages to compute the value at the interface in question\n    for (int ll=0; ll<NUM_VARS; ll++) {\n      for (int s=0; s < sten_size; s++) {\n        int inds = ll*(nz+2*hs)*(nx+2*hs) + (k+hs)*(nx+2*hs) + i+s;\n        stencil[s] = state[inds];\n      }\n      //Fourth-order-accurate interpolation of the state\n      vals[ll] = -stencil[0]/12 + 7*stencil[1]/12 + 7*stencil[2]/12 - stencil[3]/12;\n      //First-order-accurate interpolation of the third spatial derivative of the state (for artificial viscosity)\n      d3_vals[ll] = -stencil[0] + 3*stencil[1] - 3*stencil[2] + stencil[3];\n    }\n\n    //Compute density, u-wind, w-wind, potential temperature, and pressure (r,u,w,t,p respectively)\n    double r = vals[ID_DENS] + hy_dens_cell[k+hs];\n    double u = vals[ID_UMOM] / r;\n    double w = vals[ID_WMOM] / r;\n    double t = ( vals[ID_RHOT] + hy_dens_theta_cell[k+hs] ) / r;\n    double p = C0*pow((r*t),gamm);\n\n    //Compute the flux vector\n    flux[ID_DENS*(nz+1)*(nx+1) + k*(nx+1) + i] = r*u     - hv_coef*d3_vals[ID_DENS];\n    flux[ID_UMOM*(nz+1)*(nx+1) + k*(nx+1) + i] = r*u*u+p - hv_coef*d3_vals[ID_UMOM];\n    flux[ID_WMOM*(nz+1)*(nx+1) + k*(nx+1) + i] = r*u*w   - hv_coef*d3_vals[ID_WMOM];\n    flux[ID_RHOT*(nz+1)*(nx+1) + k*(nx+1) + i] = r*u*t   - hv_coef*d3_vals[ID_RHOT];\n  }\n}",
            "__global__\nvoid compute_tend_x (const double *__restrict__ flux,\n                           double *__restrict__ tend,\n                     const int nx,\n                     const int nz,\n                     const int dx )\n{\n  int ll = blockIdx.z * blockDim.z + threadIdx.z;\n  int k = blockIdx.y * blockDim.y + threadIdx.y;\n  int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < nx && k < nz) { \n    int indt  = ll* nz   * nx    + k* nx    + i  ;\n    int indf1 = ll*(nz+1)*(nx+1) + k*(nx+1) + i  ;\n    int indf2 = ll*(nz+1)*(nx+1) + k*(nx+1) + i+1;\n    tend[indt] = -( flux[indf2] - flux[indf1] ) / dx;\n  }\n}",
            "__global__\nvoid compute_flux_z (const double *__restrict__ state,\n                           double *__restrict__ flux,\n                           double *__restrict__ hy_dens_int,\n                           double *__restrict__ hy_pressure_int,\n                           double *__restrict__ hy_dens_theta_int,\n                     const double hv_coef,\n                     const int nx,\n                     const int nz,\n                     const int hs)\n{\n  int k = blockIdx.y * blockDim.y + threadIdx.y;\n  int i = blockIdx.x * blockDim.x + threadIdx.x;\n  double stencil[4], d3_vals[NUM_VARS], vals[NUM_VARS];\n\n  if (i < nx && k < nz+1) { \n    //Use fourth-order interpolation from four cell averages to compute the value at the interface in question\n    for (int ll=0; ll<NUM_VARS; ll++) {\n      for (int s=0; s<sten_size; s++) {\n        int inds = ll*(nz+2*hs)*(nx+2*hs) + (k+s)*(nx+2*hs) + i+hs;\n        stencil[s] = state[inds];\n      }\n      //Fourth-order-accurate interpolation of the state\n      vals[ll] = -stencil[0]/12 + 7*stencil[1]/12 + 7*stencil[2]/12 - stencil[3]/12;\n      //First-order-accurate interpolation of the third spatial derivative of the state\n      d3_vals[ll] = -stencil[0] + 3*stencil[1] - 3*stencil[2] + stencil[3];\n    }\n\n    //Compute density, u-wind, w-wind, potential temperature, and pressure (r,u,w,t,p respectively)\n    double r = vals[ID_DENS] + hy_dens_int[k];\n    double u = vals[ID_UMOM] / r;\n    double w = vals[ID_WMOM] / r;\n    double t = ( vals[ID_RHOT] + hy_dens_theta_int[k] ) / r;\n    double p = C0*pow((r*t),gamm) - hy_pressure_int[k];\n    //Enforce vertical boundary condition and exact mass conservation\n    if (k == 0 || k == nz) {\n      w                = 0;\n      d3_vals[ID_DENS] = 0;\n    }\n\n    //Compute the flux vector with hyperviscosity\n    flux[ID_DENS*(nz+1)*(nx+1) + k*(nx+1) + i] = r*w     - hv_coef*d3_vals[ID_DENS];\n    flux[ID_UMOM*(nz+1)*(nx+1) + k*(nx+1) + i] = r*w*u   - hv_coef*d3_vals[ID_UMOM];\n    flux[ID_WMOM*(nz+1)*(nx+1) + k*(nx+1) + i] = r*w*w+p - hv_coef*d3_vals[ID_WMOM];\n    flux[ID_RHOT*(nz+1)*(nx+1) + k*(nx+1) + i] = r*w*t   - hv_coef*d3_vals[ID_RHOT];\n  }\n}",
            "__global__\nvoid compute_tend_z (const double *__restrict__ state,\n                     const double *__restrict__ flux,\n                           double *__restrict__ tend,\n                     const int nx,\n                     const int nz,\n                     const int dz )\n{\n  int ll = blockIdx.z * blockDim.z + threadIdx.z;\n  int k = blockIdx.y * blockDim.y + threadIdx.y;\n  int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < nx && k < nz) { \n    int indt  = ll* nz   * nx    + k* nx    + i  ;\n    int indf1 = ll*(nz+1)*(nx+1) + (k  )*(nx+1) + i;\n    int indf2 = ll*(nz+1)*(nx+1) + (k+1)*(nx+1) + i;\n    tend[indt] = -( flux[indf2] - flux[indf1] ) / dz;\n    if (ll == ID_WMOM) {\n      int inds = ID_DENS*(nz+2*hs)*(nx+2*hs) + (k+hs)*(nx+2*hs) + i+hs;\n      tend[indt] = tend[indt] - state[inds]*grav;\n    }\n  }\n}",
            "__global__\nvoid pack_send_buf (const double *__restrict__ state,\n                          double *__restrict__ sendbuf_l,\n                          double *__restrict__ sendbuf_r,\n                    const int nx,\n                    const int nz,\n                    const int hs)\n{\n  int ll = blockIdx.z * blockDim.z + threadIdx.z;\n  int k = blockIdx.y * blockDim.y + threadIdx.y;\n  int s = blockIdx.x * blockDim.x + threadIdx.x;\n  if (s < hs && k < nz) { \n    sendbuf_l[ll*nz*hs + k*hs + s] = state[ll*(nz+2*hs)*(nx+2*hs) + (k+hs)*(nx+2*hs) + hs+s];\n    sendbuf_r[ll*nz*hs + k*hs + s] = state[ll*(nz+2*hs)*(nx+2*hs) + (k+hs)*(nx+2*hs) + nx+s];\n  }\n}",
            "__global__\nvoid unpack_recv_buf (double *__restrict__ state,\n                      const double *__restrict__ recvbuf_l,\n                      const double *__restrict__ recvbuf_r,\n                      const int nx,\n                      const int nz,\n                      const int hs)\n{\n  int ll = blockIdx.z * blockDim.z + threadIdx.z;\n  int k = blockIdx.y * blockDim.y + threadIdx.y;\n  int s = blockIdx.x * blockDim.x + threadIdx.x;\n  if (s < hs && k < nz) { \n    state[ll*(nz+2*hs)*(nx+2*hs) + (k+hs)*(nx+2*hs) + s      ] = recvbuf_l[ll*nz*hs + k*hs + s];\n    state[ll*(nz+2*hs)*(nx+2*hs) + (k+hs)*(nx+2*hs) + nx+hs+s] = recvbuf_r[ll*nz*hs + k*hs + s];\n  }\n}",
            "inline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\ninline __host__ __device__ float4 fabs(float4 v)\n{\n    return make_float4(fabs(v.x), fabs(v.y), fabs(v.z), fabs(v.w));\n}\n\n__global__\nvoid update_state_x (double *__restrict__ state,\n                     const double *__restrict__ hy_dens_cell,\n                     const double *__restrict__ hy_dens_theta_cell,\n                     const int nx,\n                     const int nz,\n                     const int hs,\n                     const int k_beg,\n                     const double dz)\n{\n  int k = blockIdx.y * blockDim.y + threadIdx.y;\n  int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < hs && k < nz) { \n    double z = (k_beg + k+0.5)*dz;\n    if (fabs(z-3*zlen/4) <= zlen/16) {\n      int ind_r = ID_DENS*(nz+2*hs)*(nx+2*hs) + (k+hs)*(nx+2*hs) + i;\n      int ind_u = ID_UMOM*(nz+2*hs)*(nx+2*hs) + (k+hs)*(nx+2*hs) + i;\n      int ind_t = ID_RHOT*(nz+2*hs)*(nx+2*hs) + (k+hs)*(nx+2*hs) + i;\n      state[ind_u] = (state[ind_r]+hy_dens_cell[k+hs]) * 50.;\n      state[ind_t] = (state[ind_r]+hy_dens_cell[k+hs]) * 298. - hy_dens_theta_cell[k+hs];\n    }\n  }\n}",
            "inline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\ninline __host__ __device__ float4 fabs(float4 v)\n{\n    return make_float4(fabs(v.x), fabs(v.y), fabs(v.z), fabs(v.w));\n}\n\n__global__\nvoid update_state_z (double *__restrict__ state,\n                     const int data_spec_int,\n                     const int i_beg,\n                     const int nx,\n                     const int nz,\n                     const int hs,\n                     const double dx,\n                     const double mnt_width)\n{\n  int ll = blockIdx.y * blockDim.y + threadIdx.y;\n  int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < nx+2*hs && ll < NUM_VARS) { \n    if (ll == ID_WMOM) {\n      state[ll*(nz+2*hs)*(nx+2*hs) + (0      )*(nx+2*hs) + i] = 0.;\n      state[ll*(nz+2*hs)*(nx+2*hs) + (1      )*(nx+2*hs) + i] = 0.;\n      state[ll*(nz+2*hs)*(nx+2*hs) + (nz+hs  )*(nx+2*hs) + i] = 0.;\n      state[ll*(nz+2*hs)*(nx+2*hs) + (nz+hs+1)*(nx+2*hs) + i] = 0.;\n      //Impose the vertical momentum effects of an artificial cos^2 mountain at the lower boundary\n      if (data_spec_int == DATA_SPEC_MOUNTAIN) {\n        double x = (i_beg+i-hs+0.5)*dx;\n        if ( fabs(x-xlen/4) < mnt_width ) {\n          double xloc = (x-(xlen/4)) / mnt_width;\n          //Compute the derivative of the fake mountain\n          double mnt_deriv = -pi*cos(pi*xloc/2)*sin(pi*xloc/2)*10/dx;\n          //w = (dz/dx)*u\n          state[ID_WMOM*(nz+2*hs)*(nx+2*hs) + (0)*(nx+2*hs) + i] = mnt_deriv*state[ID_UMOM*(nz+2*hs)*(nx+2*hs) + hs*(nx+2*hs) + i];\n          state[ID_WMOM*(nz+2*hs)*(nx+2*hs) + (1)*(nx+2*hs) + i] = mnt_deriv*state[ID_UMOM*(nz+2*hs)*(nx+2*hs) + hs*(nx+2*hs) + i];\n        }\n      }\n    } else {\n      state[ll*(nz+2*hs)*(nx+2*hs) + (0      )*(nx+2*hs) + i] = state[ll*(nz+2*hs)*(nx+2*hs) + (hs     )*(nx+2*hs) + i];\n      state[ll*(nz+2*hs)*(nx+2*hs) + (1      )*(nx+2*hs) + i] = state[ll*(nz+2*hs)*(nx+2*hs) + (hs     )*(nx+2*hs) + i];\n      state[ll*(nz+2*hs)*(nx+2*hs) + (nz+hs  )*(nx+2*hs) + i] = state[ll*(nz+2*hs)*(nx+2*hs) + (nz+hs-1)*(nx+2*hs) + i];\n      state[ll*(nz+2*hs)*(nx+2*hs) + (nz+hs+1)*(nx+2*hs) + i] = state[ll*(nz+2*hs)*(nx+2*hs) + (nz+hs-1)*(nx+2*hs) + i];\n    }\n  }\n}",
            "__device__ __forceinline__\ndouble atomic_add(double *address, double val)\n{\n  // Doing it all as longlongs cuts one __longlong_as_double from the inner loop\n  unsigned long long *ptr = (unsigned long long *)address;\n  unsigned long long old, newdbl, ret = *ptr;\n  do {\n    old = ret;\n    newdbl = __double_as_longlong(__longlong_as_double(old)+val);\n  } while((ret = atomicCAS(ptr, old, newdbl)) != old);\n  return __longlong_as_double(ret);\n}\n\n__global__\nvoid acc_mass_te (double *__restrict__ mass,\n                  double *__restrict__ te,\n                  const double *__restrict__ state,\n                  const double *__restrict__ hy_dens_cell,\n                  const double *__restrict__ hy_dens_theta_cell,\n                  const int nx,\n                  const int nz,\n                  const double dx,\n                  const double dz)\n{\n  int k = blockIdx.y * blockDim.y + threadIdx.y;\n  int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (k < nz && i < nx) {\n    int ind_r = ID_DENS*(nz+2*hs)*(nx+2*hs) + (k+hs)*(nx+2*hs) + i+hs;\n    int ind_u = ID_UMOM*(nz+2*hs)*(nx+2*hs) + (k+hs)*(nx+2*hs) + i+hs;\n    int ind_w = ID_WMOM*(nz+2*hs)*(nx+2*hs) + (k+hs)*(nx+2*hs) + i+hs;\n    int ind_t = ID_RHOT*(nz+2*hs)*(nx+2*hs) + (k+hs)*(nx+2*hs) + i+hs;\n    double r  = state[ind_r] + hy_dens_cell[hs+k];               // Density\n    double u  = state[ind_u] / r;                                // U-wind\n    double w  = state[ind_w] / r;                                // W-wind\n    double th = ( state[ind_t] + hy_dens_theta_cell[hs+k] ) / r; // Potential Temperature (theta)\n    double p  = C0*pow(r*th,gamm);                         // Pressure\n    double t  = th / pow(p0/p,rd/cp);                      // Temperature\n    double ke = r*(u*u+w*w);                                     // Kinetic Energy\n    double ie = r*cv*t;                                          // Internal Energy\n\n    // mass += r        *dx*dz; // Accumulate domain mass\n    // te   += (ke + ie)*dx*dz; // Accumulate domain total energy\n    atomic_add(mass, r*dx*dz);\n    atomic_add(te, (ke+ie)*dx*dz);\n  }\n}",
            "__global__\nvoid update_fluid_state (const double *__restrict__ state_init,\n                               double *__restrict__ state_out,\n                         const double *__restrict__ tend,\n                         const int nx,\n                         const int nz,\n                         const int hs,\n                         const double dt)\n{\n  int ll = blockIdx.z * blockDim.z + threadIdx.z;\n  int k = blockIdx.y * blockDim.y + threadIdx.y;\n  int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < nx && k < nz) { \n    int inds = ll*(nz+2*hs)*(nx+2*hs) + (k+hs)*(nx+2*hs) + i+hs;\n    int indt = ll*nz*nx + k*nx + i;\n    state_out[inds] = state_init[inds] + dt * tend[indt];\n  }\n}"
        ]
    },
    "extend2-cuda": {
        "/Users/gbolet/hecbench-roofline/src/extend2-cuda/main.cu": [
            "inline __host__ __device__ int4 make_int4(float4 a)\n{\n    return make_int4(int(a.x), int(a.y), int(a.z), int(a.w));\n}\n\ninline __host__ __device__ int4 abs(int4 v)\n{\n    return make_int4(abs(v.x), abs(v.y), abs(v.z), abs(v.w));\n}\n\n__global__ void\nkernel_extend2(\n    const unsigned char* query,\n    const unsigned char* target,\n    const char* mat,\n    eh_t* eh,\n    char* qp,\n    int* qle_acc,\n    int* tle_acc,\n    int* gtle_acc,\n    int* gscore_acc,\n    int* max_off_acc,\n    int* score_acc,\n    const int qlen, \n    const int tlen, \n    const int m, \n    const int o_del, \n    const int e_del, \n    const int o_ins, \n    const int e_ins, \n    int w, \n    const int end_bonus, \n    const int zdrop, \n    const int h0)\n{\n  int oe_del = o_del + e_del;\n  int oe_ins = o_ins + e_ins; \n  int i, j, k;\n  int beg, end;\n  int max, max_i, max_j, max_ins, max_del, max_ie;\n  int gscore;\n  int max_off;\n\n  // generate the query profile\n  for (k = i = 0; k < m; ++k) {\n    const char *p = mat + k * m;\n    for (j = 0; j < qlen; ++j)\n      qp[i++] = p[query[j]];\n  }\n\n  // fill the first row\n  eh[0].h = h0; \n  eh[1].h = h0 > oe_ins? h0 - oe_ins : 0;\n\n  for (j = 2; j <= qlen && eh[j-1].h > e_ins; ++j)\n    eh[j].h = eh[j-1].h - e_ins;\n\n  // adjust $w if it is too large\n  k = m * m;\n  for (i = 0, max = 0; i < k; ++i) // get the max score\n    max = max > mat[i]? max : mat[i];\n  max_ins = (int)((float)(qlen * max + end_bonus - o_ins) / e_ins + 1.f);\n  max_ins = max_ins > 1? max_ins : 1;\n  w = w < max_ins? w : max_ins;\n  max_del = (int)((float)(qlen * max + end_bonus - o_del) / e_del + 1.f);\n  max_del = max_del > 1? max_del : 1;\n  w = w < max_del? w : max_del; // TODO: is this necessary?\n  // DP loop\n  max = h0, max_i = max_j = -1; max_ie = -1, gscore = -1;\n  max_off = 0;\n  beg = 0, end = qlen;\n  for (i = 0; i < tlen; ++i) {\n    int t, f = 0, h1, m = 0, mj = -1;\n    char *q = qp + target[i] * qlen;\n\n    // apply the band and the constraint (if provided)\n    if (beg < i - w) beg = i - w;\n    if (end > i + w + 1) end = i + w + 1;\n    if (end > qlen) end = qlen;\n\n    // compute the first column\n    if (beg == 0) {\n      h1 = h0 - (o_del + e_del * (i + 1));\n      if (h1 < 0) h1 = 0;\n    } \n    else \n      h1 = 0;\n\n    for (j = beg; j < end; ++j) {\n      // At the beginning of the loop: eh[j] = { H(i-1,j-1), E(i,j) }, f = F(i,j) and h1 = H(i,j-1)\n      // Similar to SSE2-SW, cells are computed in the following order:\n      //   H(i,j)   = max{H(i-1,j-1)+S(i,j), E(i,j), F(i,j)}\n      //   E(i+1,j) = max{H(i,j)-gapo, E(i,j)} - gape\n      //   F(i,j+1) = max{H(i,j)-gapo, F(i,j)} - gape\n      eh_t *p = eh+j;\n      int h, M = p->h, e = p->e; // get H(i-1,j-1) and E(i-1,j)\n      p->h = h1;          // set H(i,j-1) for the next row\n      M = M? M + q[j] : 0;// separating H and M to disallow a cigar like \"100M3I3D20M\"\n      h = M > e? M : e;   // e and f are guaranteed to be non-negative, so h>=0 even if M<0\n      h = h > f? h : f;\n      h1 = h;             // save H(i,j) to h1 for the next column\n      mj = m > h? mj : j; // record the position where max score is achieved\n      m = m > h? m : h;   // m is stored at eh[mj+1]\n      t = M - oe_del;\n      t = t > 0? t : 0;\n      e -= e_del;\n      e = e > t? e : t;   // computed E(i+1,j)\n      p->e = e;           // save E(i+1,j) for the next row\n      t = M - oe_ins;\n      t = t > 0? t : 0;\n      f -= e_ins;\n      f = f > t? f : t;   // computed F(i,j+1)\n    }\n    eh[end].h = h1; eh[end].e = 0;\n    if (j == qlen) {\n      max_ie = gscore > h1? max_ie : i;\n      gscore = gscore > h1? gscore : h1;\n    }\n    if (m == 0) break;\n    if (m > max) {\n      max = m, max_i = i, max_j = mj;\n      max_off = max_off > abs(mj - i)? max_off : abs(mj - i);\n    } else if (zdrop > 0) {\n      if (i - max_i > mj - max_j) {\n        if (max - m - ((i - max_i) - (mj - max_j)) * e_del > zdrop) break;\n      } else {\n        if (max - m - ((mj - max_j) - (i - max_i)) * e_ins > zdrop) break;\n      }\n    }\n    // update beg and end for the next round\n    for (j = beg; j < end && eh[j].h == 0 && eh[j].e == 0; ++j);\n    beg = j;\n    for (j = end; j >= beg && eh[j].h == 0 && eh[j].e == 0; --j);\n    end = j + 2 < qlen? j + 2 : qlen;\n    //beg = 0; end = qlen; // uncomment this line for debugging\n  }\n  *qle_acc = max_j + 1;\n  *tle_acc = max_i + 1;\n  *gtle_acc = max_ie + 1;\n  *gscore_acc = gscore;\n  *max_off_acc = max_off;\n  *score_acc = max;\n}"
        ]
    },
    "graphExecution-cuda": {
        "/Users/gbolet/hecbench-roofline/src/graphExecution-cuda/main.cu": [
            "__global__ void reduceFinal(double *inputVec, double *result, size_t inputSize)\n{\n  __shared__ double tmp[THREADS_PER_BLOCK];\n\n  cg::thread_block cta = cg::this_thread_block();\n  size_t globaltid = blockIdx.x*blockDim.x + threadIdx.x;\n\n  double temp_sum = 0.0;\n  for (int i=globaltid; i < inputSize; i+=gridDim.x*blockDim.x)\n  {\n    temp_sum += (double) inputVec[i];\n  }\n  tmp[cta.thread_rank()] = temp_sum;\n\n  cg::sync(cta);\n\n  cg::thread_block_tile<32> tile32 = cg::tiled_partition<32>(cta);\n\n  // do reduction in shared mem\n  if ((blockDim.x >= 512) && (cta.thread_rank() < 256))\n  {\n    tmp[cta.thread_rank()] = temp_sum = temp_sum + tmp[cta.thread_rank() + 256];\n  }\n\n  cg::sync(cta);\n\n  if ((blockDim.x >= 256) &&(cta.thread_rank() < 128))\n  {\n    tmp[cta.thread_rank()] = temp_sum = temp_sum + tmp[cta.thread_rank() + 128];\n  }\n\n  cg::sync(cta);\n\n  if ((blockDim.x >= 128) && (cta.thread_rank() <  64))\n  {\n    tmp[cta.thread_rank()] = temp_sum = temp_sum + tmp[cta.thread_rank() +  64];\n  }\n\n  cg::sync(cta);\n\n  if (cta.thread_rank() < 32)\n  {\n    // Fetch final intermediate sum from 2nd warp\n    if (blockDim.x >=  64) temp_sum += tmp[cta.thread_rank() + 32];\n    // Reduce final warp using shuffle\n    for (int offset = tile32.size()/2; offset > 0; offset /= 2) \n    {\n      temp_sum += tile32.shfl_down(temp_sum, offset);\n    }\n  }\n  // write result for this block to global mem\n  if (cta.thread_rank() == 0) result[0] = temp_sum;\n}"
        ]
    },
    "chemv-cuda": {
        "/Users/gbolet/hecbench-roofline/src/chemv-cuda/kernel.cu": [
            "__global__ void kernel0(struct ComplexFloat *AT, struct ComplexFloat *X, struct ComplexFloat *Y, float alpha_im, float alpha_re, float beta_im, float beta_re)\n{\n    int b0 = blockIdx.x;\n    int t0 = threadIdx.x;\n    float private_var5_Re;\n    float private_var5_Im;\n    float private_var2_Re;\n    float private_var3_Im;\n    float private_var2_Im;\n    float private_var4_Im;\n    float private_var4_Re;\n    float private_var3_Re;\n    float private_var99_Re;\n    float private_var98_Im;\n    float private_var97_Im;\n    float private_var99_Im;\n    float private_var97_Re;\n    float private_var98_Re;\n\n    #define ppcg_min(x,y)    ({ __typeof__(x) _x = (x); __typeof__(y) _y = (y); _x < _y ? _x : _y; })\n    for (int c1 = 0; c1 <= ppcg_min(368, 32 * b0 + 30); c1 += 32) {\n      if (32 * b0 + t0 <= 369 && c1 == 0) {\n        private_var5_Re = ((Y[32 * b0 + t0].Re * beta_re) - (Y[32 * b0 + t0].Im * beta_im));\n        private_var5_Im = ((Y[32 * b0 + t0].Im * beta_re) + (Y[32 * b0 + t0].Re * beta_im));\n        Y[32 * b0 + t0].Re = private_var5_Re;\n        Y[32 * b0 + t0].Im = private_var5_Im;\n        private_var2_Re = (alpha_re * AT[11872 * b0 + 371 * t0].Re);\n        private_var2_Im = (alpha_im * AT[11872 * b0 + 371 * t0].Re);\n        private_var3_Re = ((private_var2_Re * X[32 * b0 + t0].Re) - (private_var2_Im * X[32 * b0 + t0].Im));\n        private_var3_Im = ((private_var2_Im * X[32 * b0 + t0].Re) + (private_var2_Re * X[32 * b0 + t0].Im));\n        private_var4_Re = (Y[32 * b0 + t0].Re + private_var3_Re);\n        private_var4_Im = (Y[32 * b0 + t0].Im + private_var3_Im);\n        Y[32 * b0 + t0].Re = private_var4_Re;\n        Y[32 * b0 + t0].Im = private_var4_Im;\n      }\n      if (32 * b0 + t0 <= 369)\n        for (int c3 = 0; c3 <= ppcg_min(31, 32 * b0 + t0 - c1 - 1); c3 += 1) {\n          private_var97_Re = ((alpha_re * AT[32 * b0 + t0 + 370 * c1 + 370 * c3].Re) - (alpha_im * AT[32 * b0 + t0 + 370 * c1 + 370 * c3].Im));\n          private_var97_Im = ((alpha_im * AT[32 * b0 + t0 + 370 * c1 + 370 * c3].Re) + (alpha_re * AT[32 * b0 + t0 + 370 * c1 + 370 * c3].Im));\n          private_var98_Re = ((private_var97_Re * X[c1 + c3].Re) - (private_var97_Im * X[c1 + c3].Im));\n          private_var98_Im = ((private_var97_Im * X[c1 + c3].Re) + (private_var97_Re * X[c1 + c3].Im));\n          private_var99_Re = (Y[32 * b0 + t0].Re + private_var98_Re);\n          private_var99_Im = (Y[32 * b0 + t0].Im + private_var98_Im);\n          Y[32 * b0 + t0].Re = private_var99_Re;\n          Y[32 * b0 + t0].Im = private_var99_Im;\n        }\n      __syncthreads();\n    }\n}"
        ]
    },
    "attention-cuda": {
        "/Users/gbolet/hecbench-roofline/src/attention-cuda/kernels.h": [
            "inline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\ninline __host__ __device__ float4 expf(float4 v)\n{\n    return make_float4(expf(v.x), expf(v.y), expf(v.z), expf(v.w));\n}\n\n__global__\nvoid kernel1 (\n    const float*__restrict__ key,\n    const float*__restrict__ query,\n    float*__restrict__ dot_product,\n    float*__restrict__ exp_sum,\n    const int n,\n    const int d)\n{\n  int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < n) {\n    float sum = 0;\n    for (int j = 0; j < d; j++)\n      sum += key[i * d + j] * query[j];\n    dot_product[i] = sum;\n    atomicAdd(exp_sum, expf(sum));\n  }\n}",
            "inline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\ninline __host__ __device__ float4 expf(float4 v)\n{\n    return make_float4(expf(v.x), expf(v.y), expf(v.z), expf(v.w));\n}\n\n__global__\nvoid kernel1_blockReduce (\n    const float*__restrict__ key,\n    const float*__restrict__ query,\n    float*__restrict__ dot_product,\n    float*__restrict__ exp_sum,\n    const int n,\n    const int d)\n{\n  // each i iteration is assigned to a block\n  int i = blockIdx.x;\n  float sum = 0;\n  for (int j = threadIdx.x; j < d; j += blockDim.x) {\n    sum += key[i * d + j] * query[j];\n  }\n  using BlockReduce = cub::BlockReduce<float, 256>;\n  __shared__ typename BlockReduce::TempStorage temp_storage;\n  sum = BlockReduce(temp_storage).Sum(sum);\n  if (threadIdx.x == 0) {\n    dot_product[i] = sum;\n    atomicAdd(exp_sum, expf(sum));\n  }\n}",
            "inline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\ninline __host__ __device__ float4 expf(float4 v)\n{\n    return make_float4(expf(v.x), expf(v.y), expf(v.z), expf(v.w));\n}\n\n__global__\nvoid kernel1_warpReduce (\n    const float*__restrict__ key,\n    const float*__restrict__ query,\n    float*__restrict__ dot_product,\n    float*__restrict__ exp_sum,\n    const int n,\n    const int d)\n{\n  namespace cg = cooperative_groups;\n  cg::thread_block block = cg::this_thread_block();\n  cg::thread_block_tile<32> warp = cg::tiled_partition<32>(block);\n  // each i iteration is assigned to a warp\n  // meta_group_size is the number of warps in a block, and meta_group_rank is the warp index\n  int i = blockIdx.x * warp.meta_group_size() + warp.meta_group_rank();\n  if (i < n) {\n    float sum = 0;\n    for (int j = warp.thread_rank(); j < d; j += warp.size()) {\n      sum += key[i * d + j] * query[j];\n    }\n    sum = cg::reduce(warp, sum, cg::plus<float>{});\n    if (warp.thread_rank() == 0) {\n      dot_product[i] = sum;\n      atomicAdd(exp_sum, expf(sum));\n    }\n  }\n}",
            "inline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\ninline __host__ __device__ float4 expf(float4 v)\n{\n    return make_float4(expf(v.x), expf(v.y), expf(v.z), expf(v.w));\n}\n\n__global__\nvoid kernel2_blockReduce (\n    const float*__restrict__ exp_sum,\n    const float*__restrict__ dot_product,\n    const float*__restrict__ value,\n    float*__restrict__ output,\n    const int n,\n    const int d)\n{\n  int j = blockIdx.x;\n  float sum = 0;\n  for (int i = threadIdx.x; i < n; i += blockDim.x) {\n    float score = expf(dot_product[i]) / exp_sum[0];\n    sum += score * value[i * d + j];\n  }\n  using BlockReduce = cub::BlockReduce<float, 256>;\n  __shared__ typename BlockReduce::TempStorage temp_storage;\n  sum = BlockReduce(temp_storage).Sum(sum);\n  if (threadIdx.x == 0)\n    output[j] = sum;\n}"
        ]
    },
    "spm-cuda": {
        "/Users/gbolet/hecbench-roofline/src/spm-cuda/main.cu": [
            "inline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\ninline __host__ __device__ float4 floorf(float4 v)\n{\n    return make_float4(floorf(v.x), floorf(v.y), floorf(v.z), floorf(v.w));\n}\n\n__host__ __device__ \nfloat interp(const int3 d, const unsigned char f[], float x, float y, float z)\n{\n  int ix, iy, iz;\n  float dx1, dy1, dz1, dx2, dy2, dz2;\n  int k111,k112,k121,k122,k211,k212,k221,k222;\n  float vf;\n  const unsigned char *ff;\n\n  ix = floorf(x); dx1=x-ix; dx2=1.f-dx1;\n  iy = floorf(y); dy1=y-iy; dy2=1.f-dy1;\n  iz = floorf(z); dz1=z-iz; dz2=1.f-dz1;\n\n  ff   = f + ix-1+d.x*(iy-1+d.y*(iz-1));\n  k222 = ff[   0]; k122 = ff[     1];\n  k212 = ff[d.x]; k112 = ff[d.x+1];\n  ff  += d.x*d.y;\n  k221 = ff[   0]; k121 = ff[     1];\n  k211 = ff[d.x]; k111 = ff[d.x+1];\n\n  vf = (((k222*dx2+k122*dx1)*dy2 + (k212*dx2+k112*dx1)*dy1))*dz2 +\n       (((k221*dx2+k121*dx1)*dy2 + (k211*dx2+k111*dx1)*dy1))*dz1;\n\n  return(vf);\n}\n\n__global__ void spm (\n  const float *__restrict__ M, \n  const int data_size,\n  const unsigned char *__restrict__ g_d,\n  const unsigned char *__restrict__ f_d,\n  const int3 dg,\n  const int3 df,\n  unsigned char *__restrict__ ivf_d,\n  unsigned char *__restrict__ ivg_d,\n  bool *__restrict__ data_threshold_d)\n{\n  // 97 random values\n  const float ran[] = {\n    0.656619,0.891183,0.488144,0.992646,0.373326,0.531378,0.181316,0.501944,0.422195,\n    0.660427,0.673653,0.95733,0.191866,0.111216,0.565054,0.969166,0.0237439,0.870216,\n    0.0268766,0.519529,0.192291,0.715689,0.250673,0.933865,0.137189,0.521622,0.895202,\n    0.942387,0.335083,0.437364,0.471156,0.14931,0.135864,0.532498,0.725789,0.398703,\n    0.358419,0.285279,0.868635,0.626413,0.241172,0.978082,0.640501,0.229849,0.681335,\n    0.665823,0.134718,0.0224933,0.262199,0.116515,0.0693182,0.85293,0.180331,0.0324186,\n    0.733926,0.536517,0.27603,0.368458,0.0128863,0.889206,0.866021,0.254247,0.569481,\n    0.159265,0.594364,0.3311,0.658613,0.863634,0.567623,0.980481,0.791832,0.152594,\n    0.833027,0.191863,0.638987,0.669,0.772088,0.379818,0.441585,0.48306,0.608106,\n    0.175996,0.00202556,0.790224,0.513609,0.213229,0.10345,0.157337,0.407515,0.407757,\n    0.0526927,0.941815,0.149972,0.384374,0.311059,0.168534,0.896648};\n  \n  const int idx = blockIdx.x * NUM_THREADS + threadIdx.x;\n\n  int x_datasize=(dg.x-2);\n  int y_datasize=(dg.y-2);\n\n  for(int i = idx; i < data_size; i += NUM_THREADS*NUM_BLOCKS)\n  {\n    float xx_temp = (i%x_datasize)+1.f;\n    float yy_temp = ((int)floorf((float)i/x_datasize)%y_datasize)+1.f;\n    float zz_temp = (floorf((float)i/x_datasize))/y_datasize+1.f;\n\n    // generate rx,ry,rz coordinates\n    float rx = xx_temp + ran[i%97];\n    float ry = yy_temp + ran[i%97];\n    float rz = zz_temp + ran[i%97];\n\n    // rigid transformation over rx,ry,rz coordinates\n    float xp = M[0]*rx + M[4]*ry + M[ 8]*rz + M[12];\n    float yp = M[1]*rx + M[5]*ry + M[ 9]*rz+ M[13];\n    float zp = M[2]*rx + M[6]*ry + M[10]*rz+ M[14];\n\n    if (zp>=1.f && zp<df.z && yp>=1.f && yp<df.y && xp>=1.f && xp<df.x)\n    {\n      // interpolation\n      ivf_d[i] = floorf(interp(df, f_d, xp,yp,zp)+0.5f);\n      ivg_d[i] = floorf(interp(dg, g_d, rx,ry,rz)+0.5f);\n      data_threshold_d[i] = true;\n    }\n    else\n    {\n      ivf_d[i] = 0;\n      ivg_d[i] = 0;\n      data_threshold_d[i] = false;\n    }\n  }\n}"
        ]
    },
    "nn-cuda": {
        "/Users/gbolet/hecbench-roofline/src/nn-cuda/nearestNeighbor.cu": [
            "inline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\ninline __host__ __device__ float4 sqrtf(float4 v)\n{\n    return make_float4(sqrtf(v.x), sqrtf(v.y), sqrtf(v.z), sqrtf(v.w));\n}\n\n__global__ void \nnn (const int numRecords, const float lat, const float lng,\n    const LatLong *__restrict__ locations,\n    float*__restrict__ distances) \n{\n  int gid = blockDim.x * blockIdx.x + threadIdx.x;\n  if (gid < numRecords) {\n    LatLong latLong = locations[gid];\n    distances[gid] = sqrtf((lat-latLong.lat)*(lat-latLong.lat)+\n        (lng-latLong.lng)*(lng-latLong.lng));\n  }\n}"
        ]
    },
    "tsne-cuda": {
        "/Users/gbolet/hecbench-roofline/src/tsne-cuda/perplexity_search.cu": [
            "__global__\nvoid ComputePijKernel(\n    volatile float* __restrict__ pij,\n    const    float* __restrict__ squared_dist,\n    const    float* __restrict__ betas,\n    const unsigned int num_points,\n    const unsigned int num_neighbors)\n{\n    int tid, i, j;\n    float dist, beta;\n\n    tid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (tid >= num_points * num_neighbors)\n        return;\n\n    i = tid / num_neighbors;\n    j = tid % num_neighbors;\n\n    beta = betas[i];\n    dist = squared_dist[tid];\n\n    // condition deals with evaluation of pii\n    // FAISS neighbor zero is i so ignore it\n    pij[tid] = (j == 0 & dist == 0.0f) ? 0.0f : __expf(-beta * dist); //TODO: This probably never evaluates to true\n}",
            "__global__\nvoid RowSumKernel(\n    volatile float* __restrict__ row_sum,\n    const    float* __restrict__ pij,\n    const unsigned int num_points,\n    const unsigned int num_neighbors)\n{\n    int tid;\n    tid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (tid >= num_points) {\n        return;\n    }\n\n    float temp_sum = 0.0f;\n    for (int j = 0; j < num_neighbors; ++j) {\n        temp_sum += pij[tid * num_neighbors + j];\n    }\n    row_sum[tid] = temp_sum;\n}",
            "__global__\nvoid NegEntropyKernel(\n    volatile float* __restrict__ neg_entropy,\n    const    float* __restrict__ pij,\n    const unsigned int num_points,\n    const unsigned int num_neighbors)\n{\n    int tid;\n    tid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (tid >= num_points) {\n        return;\n    }\n\n    float temp_sum = 0.0f;\n    for (int j = 0; j < num_neighbors; ++j) {\n        float x = pij[tid * num_neighbors + j];\n        temp_sum += (x == 0.0f ? 0.0f : x * __logf(x));\n    }\n    neg_entropy[tid] = -1.0f * temp_sum;\n}",
            "__global__\nvoid PerplexitySearchKernel(\n    volatile float* __restrict__ betas,\n    volatile float* __restrict__ lower_bound,\n    volatile float* __restrict__ upper_bound,\n    volatile int*   __restrict__ found,\n    const    float* __restrict__ neg_entropy,\n    const    float* __restrict__ row_sum,\n    const float perplexity_target,  // 50.0f\n    const float epsilon,            // 1e-4\n    const int num_points)\n{\n    int tid, is_found;\n    float perplexity, neg_ent, sum_P, perplexity_diff, beta, min_beta, max_beta;\n\n    tid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (tid >= num_points)\n        return;\n\n    neg_ent  = neg_entropy[tid];\n    sum_P    = row_sum[tid];\n    beta     = betas[tid];\n    min_beta = lower_bound[tid];\n    max_beta = upper_bound[tid];\n\n    perplexity      = (neg_ent / sum_P) + __logf(sum_P);\n    perplexity_diff = perplexity - __logf(perplexity_target);\n    is_found        = (perplexity_diff < epsilon && -perplexity_diff < epsilon);\n    if (!is_found)\n    {\n        if (perplexity_diff > 0)\n        {\n            min_beta = beta;\n            beta = (max_beta == FLT_MAX || max_beta == -FLT_MAX) ? beta * 2.0f : (beta + max_beta) / 2.0f;\n        }\n        else\n        {\n            max_beta = beta;\n            beta = (min_beta == -FLT_MAX || min_beta == FLT_MAX) ? beta / 2.0f : (beta + min_beta) / 2.0f;\n        }\n        betas[tid] = beta;\n        lower_bound[tid] = min_beta;\n        upper_bound[tid] = max_beta;\n    }\n    found[tid] = is_found;\n}"
        ],
        "/Users/gbolet/hecbench-roofline/src/tsne-cuda/rep_forces.cu": [
            "__global__ void compute_repulsive_forces_kernel(\n    volatile float* __restrict__ repulsive_forces,\n    volatile float* __restrict__ normalization_vec,\n    const float* const xs,\n    const float* const ys,\n    const float* const potentialsQij,\n    const int num_points,\n    const int n_terms)\n{\n    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (tid >= num_points)\n        return;\n\n    float phi1, phi2, phi3, phi4, x_pt, y_pt;\n\n    phi1 = potentialsQij[tid * n_terms + 0];\n    phi2 = potentialsQij[tid * n_terms + 1];\n    phi3 = potentialsQij[tid * n_terms + 2];\n    phi4 = potentialsQij[tid * n_terms + 3];\n\n    x_pt = xs[tid];\n    y_pt = ys[tid];\n\n    normalization_vec[tid] = (1 + x_pt * x_pt + y_pt * y_pt) * phi1 - 2 * (x_pt * phi2 + y_pt * phi3) + phi4;\n\n    repulsive_forces[tid] = x_pt * phi1 - phi2;\n    repulsive_forces[tid + num_points] = y_pt * phi1 - phi3;\n}",
            "__global__ void compute_chargesQij_kernel(\n    volatile float* __restrict__ chargesQij,\n    const float* const xs,\n    const float* const ys,\n    const int num_points,\n    const int n_terms)\n{\n    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (tid >= num_points)\n        return;\n\n    float x_pt, y_pt;\n    x_pt = xs[tid];\n    y_pt = ys[tid];\n\n    chargesQij[tid * n_terms + 0] = 1;\n    chargesQij[tid * n_terms + 1] = x_pt;\n    chargesQij[tid * n_terms + 2] = y_pt;\n    chargesQij[tid * n_terms + 3] = x_pt * x_pt + y_pt * y_pt;\n}"
        ],
        "/Users/gbolet/hecbench-roofline/src/tsne-cuda/matrix_broadcast_utils.cu": [
            "#define T ((int)32)\n\n\n__global__ void tsne::utils::BroadcastRowVector(\n          T* __restrict__ d_matrix,\n    const T* __restrict__ d_vector,\n    const int N,\n    const int M,\n    BinaryFunction binary_operation,\n    const T alpha)\n{\n    const int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    const int i = tid % N;\n    const int j = tid / N;\n    if (j < M) {\n        d_matrix[j * N + i] = binary_operation(d_matrix[j * N + i], alpha * d_vector[j]);\n    }\n}",
            "#define T ((int)32)\n\n\n__global__ void tsne::utils::BroadcastColumnVector(\n          T* __restrict__ d_matrix,     // 4 x 780 x (780 / 2 + 1) = 4 x 304980 = 1219920\n    const T* __restrict__ d_vector,     // 780 x 780 = 608400\n    const int N,                        // 780 x (780 / 2 + 1) = 304980\n    const int M,                        // 4\n    BinaryFunction binary_operation,\n    const T alpha)\n{\n    const int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    const int i = tid % N;\n    const int j = tid / N;\n\n    if (j < M) {    // condition makes sure tid < size of d_matrix\n        d_matrix[j * N + i] = binary_operation(d_matrix[j * N + i], alpha * d_vector[i]);\n    }\n}"
        ],
        "/Users/gbolet/hecbench-roofline/src/tsne-cuda/nbodyfft.cu": [
            "__global__\nvoid copy_to_fft_input(\n    volatile float* __restrict__ fft_input,\n    const    float* w_coefficients_device,\n    const int n_fft_coeffs,\n    const int n_fft_coeffs_half,\n    const int n_terms)\n{\n    int i, j;\n    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (tid >= n_terms * n_fft_coeffs_half * n_fft_coeffs_half)\n        return;\n\n    int current_term = tid / (n_fft_coeffs_half * n_fft_coeffs_half);\n    int current_loc  = tid % (n_fft_coeffs_half * n_fft_coeffs_half);\n\n    i = current_loc / n_fft_coeffs_half;\n    j = current_loc % n_fft_coeffs_half;\n\n    fft_input[current_term * (n_fft_coeffs * n_fft_coeffs) + i * n_fft_coeffs + j] = w_coefficients_device[current_term + current_loc * n_terms];\n}",
            "__global__\nvoid copy_from_fft_output(\n    volatile float* __restrict__ y_tilde_values,\n    const    float* fft_output,\n    const int n_fft_coeffs,\n    const int n_fft_coeffs_half,\n    const int n_terms)\n{\n    int i, j;\n    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (tid >= n_terms * n_fft_coeffs_half * n_fft_coeffs_half)\n        return;\n\n    int current_term = tid / (n_fft_coeffs_half * n_fft_coeffs_half);\n    int current_loc  = tid % (n_fft_coeffs_half * n_fft_coeffs_half);\n\n    i = current_loc / n_fft_coeffs_half + n_fft_coeffs_half;\n    j = current_loc % n_fft_coeffs_half + n_fft_coeffs_half;\n\n    y_tilde_values[current_term + n_terms * current_loc] = fft_output[current_term * (n_fft_coeffs * n_fft_coeffs) + i * n_fft_coeffs + j] / (float)(n_fft_coeffs * n_fft_coeffs);\n}",
            "inline __host__ __device__ uint4 make_uint4(int4 a)\n{\n    return make_uint4(uint(a.x), uint(a.y), uint(a.z), uint(a.w));\n}\n\ninline __host__ __device__ uint4 max(uint4 a, uint4 b)\n{\n    return make_uint4(max(a.x,b.x), max(a.y,b.y), max(a.z,b.z), max(a.w,b.w));\n}\n\ninline __host__ __device__ uint4 min(uint4 a, uint4 b)\n{\n    return make_uint4(min(a.x,b.x), min(a.y,b.y), min(a.z,b.z), min(a.w,b.w));\n}\n\n__global__\nvoid compute_point_box_idx(\n    volatile int*   __restrict__ point_box_idx,\n    volatile float* __restrict__ x_in_box,\n    volatile float* __restrict__ y_in_box,\n    const float* const xs,\n    const float* const ys,\n    const float* const box_lower_bounds,\n    const float min_coord,\n    const float box_width,\n    const int n_boxes,\n    const int n_total_boxes,\n    const int N)\n{\n    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (tid >= N)\n        return;\n\n    int x_idx = (int)((xs[tid] - min_coord) / box_width);\n    int y_idx = (int)((ys[tid] - min_coord) / box_width);\n\n    x_idx = max(0, x_idx);\n    x_idx = min((int)(n_boxes - 1), x_idx);\n\n    y_idx = max(0, y_idx);\n    y_idx = min((int)(n_boxes - 1), y_idx);\n\n    int box_idx = y_idx * n_boxes + x_idx;\n    point_box_idx[tid] = box_idx;\n\n    x_in_box[tid] = (xs[tid] - box_lower_bounds[box_idx])                 / box_width;\n    y_in_box[tid] = (ys[tid] - box_lower_bounds[n_total_boxes + box_idx]) / box_width;\n}",
            "__global__\nvoid interpolate_device(\n    volatile float* __restrict__ interpolated_values,\n    const    float* const y_in_box,\n    const    float* const y_tilde_spacings,\n    const    float* const denominator,\n    const int n_interpolation_points,\n    const int N)\n{\n    int tid, i, j, k;\n    float value, ybox_i;\n\n    tid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (tid >= N * n_interpolation_points)\n        return;\n\n    i = tid % N;\n    j = tid / N;\n\n    value = 1;\n    ybox_i = y_in_box[i];\n\n    for (k = 0; k < n_interpolation_points; k++) {\n        if (j != k) {\n            value *= ybox_i - y_tilde_spacings[k];\n        }\n    }\n\n    interpolated_values[j * N + i] = value / denominator[j];\n}",
            "__global__\nvoid compute_interpolated_indices(\n          float* __restrict__ w_coefficients_device,\n    const int*   const point_box_indices,\n    const float* const chargesQij,\n    const float* const x_interpolated_values,\n    const float* const y_interpolated_values,\n    const int N,\n    const int n_interpolation_points,\n    const int n_boxes,\n    const int n_terms)\n{\n    int tid, current_term, i, interp_i, interp_j, box_idx, box_i, box_j, idx;\n    tid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (tid >= n_terms * n_interpolation_points * n_interpolation_points * N)\n        return;\n\n    current_term = tid % n_terms;\n    i = (tid / n_terms) % N;\n    interp_j = ((tid / n_terms) / N) % n_interpolation_points;\n    interp_i = ((tid / n_terms) / N) / n_interpolation_points;\n\n    box_idx = point_box_indices[i];\n    box_i = box_idx % n_boxes;\n    box_j = box_idx / n_boxes;\n\n    idx = (box_i * n_interpolation_points  + interp_i) * (n_boxes * n_interpolation_points) +\n          (box_j * n_interpolation_points) + interp_j;\n\n    atomicAdd(\n        w_coefficients_device + idx * n_terms + current_term,\n        x_interpolated_values[i + interp_i * N] * y_interpolated_values[i + interp_j * N] * chargesQij[i * n_terms + current_term]);\n}",
            "__global__\nvoid compute_potential_indices(\n          float* __restrict__ potentialsQij,\n    const int*   const point_box_indices,\n    const float* const y_tilde_values,\n    const float* const x_interpolated_values,\n    const float* const y_interpolated_values,\n    const int N,\n    const int n_interpolation_points,\n    const int n_boxes,\n    const int n_terms)\n{\n    int tid, current_term, i, interp_i, interp_j, box_idx, box_i, box_j, idx;\n    tid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (tid >= n_terms * n_interpolation_points * n_interpolation_points * N)\n        return;\n\n    current_term = tid % n_terms;\n    i = (tid / n_terms) % N;\n    interp_j = ((tid / n_terms) / N) % n_interpolation_points;\n    interp_i = ((tid / n_terms) / N) / n_interpolation_points;\n\n    box_idx = point_box_indices[i];\n    box_i = box_idx % n_boxes;\n    box_j = box_idx / n_boxes;\n\n    idx = (box_i * n_interpolation_points + interp_i) * (n_boxes * n_interpolation_points) +\n          (box_j * n_interpolation_points) + interp_j;\n\n    atomicAdd(\n        potentialsQij + i * n_terms + current_term,\n        x_interpolated_values[i + interp_i * N] * y_interpolated_values[i + interp_j * N] * y_tilde_values[idx * n_terms + current_term]);\n}",
            "__host__ __device__\nfloat squared_cauchy_2d(float x1, float x2, float y1, float y2)\n{\n    return powf(1.0f + (x1 - y1) * (x1 - y1) + (x2 - y2) * (x2 - y2), -2.f);\n}\n\n__global__\nvoid compute_kernel_tilde(\n    volatile float* __restrict__ kernel_tilde,   // 780 x 780\n    const    float x_min,\n    const    float y_min,\n    const    float h,\n    const    int   n_interpolation_points_1d,    // 390\n    const    int   n_fft_coeffs)                 // 390 x 2 = 780\n{\n    int tid, i, j;\n    float tmp;\n    tid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (tid >= n_interpolation_points_1d * n_interpolation_points_1d)\n        return;\n\n    i = tid / n_interpolation_points_1d;\n    j = tid % n_interpolation_points_1d;\n\n    // TODO: Possibly issuing a memory pre-fetch here could help the code.\n    tmp = squared_cauchy_2d(y_min + h / 2, x_min + h / 2, y_min + h / 2 + i * h, x_min + h / 2 + j * h);\n    kernel_tilde[(n_interpolation_points_1d + i) * n_fft_coeffs + (n_interpolation_points_1d + j)] = tmp;\n    kernel_tilde[(n_interpolation_points_1d - i) * n_fft_coeffs + (n_interpolation_points_1d + j)] = tmp;\n    kernel_tilde[(n_interpolation_points_1d + i) * n_fft_coeffs + (n_interpolation_points_1d - j)] = tmp;\n    kernel_tilde[(n_interpolation_points_1d - i) * n_fft_coeffs + (n_interpolation_points_1d - j)] = tmp;\n}",
            "__global__\nvoid compute_upper_and_lower_bounds(\n    volatile float* __restrict__ box_upper_bounds,\n    volatile float* __restrict__ box_lower_bounds,\n    const    float box_width,\n    const    float x_min,\n    const    float y_min,\n    const    int   n_boxes,\n    const    int   n_total_boxes)\n{\n    int tid, i, j;\n    tid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (tid >= n_boxes * n_boxes)\n        return;\n\n    i = tid / n_boxes;\n    j = tid % n_boxes;\n\n    box_lower_bounds[i * n_boxes + j] =  j      * box_width + x_min;\n    box_upper_bounds[i * n_boxes + j] = (j + 1) * box_width + x_min;\n\n    box_lower_bounds[n_total_boxes + i * n_boxes + j] =  i      * box_width + y_min;\n    box_upper_bounds[n_total_boxes + i * n_boxes + j] = (i + 1) * box_width + y_min;\n}",
            "__global__\nvoid DFT2D1gpu(float* din, thrust::complex<float>* dout, int num_rows, int num_cols)\n{\n\tint j = blockIdx.y * blockDim.y + threadIdx.y;\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (j >= num_rows || i >= num_cols) {\n        return;\n    }\n    \n    float angle, cosf, sinf; \n    thrust::complex<float> sum, twiddle;\n    angle = -2.0f * PI * fdividef((float)i, (float)num_cols);\n    sum = 0.0f;\n    for (int k = 0; k < num_cols; ++k) {\n        TWIDDLE();\n        sum = sum + din[j * num_cols + k] * twiddle;\n    }\n\n    dout[i * num_rows + j] = sum;\n}",
            "__global__\nvoid DFT2D2gpu(thrust::complex<float>* din, thrust::complex<float>* dout, int num_rows, int num_cols)\n{\n\tint j = blockIdx.y * blockDim.y + threadIdx.y;\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (j >= num_rows || i >= num_cols) {\n        return;\n    }\n    \n    float angle, cosf, sinf;\n    thrust::complex<float> sum, twiddle;\n    angle = -2.0f * PI * fdividef((float)i, (float)num_cols);\n    sum = 0.0f;\n    for (int k = 0; k < num_cols; ++k) {\n        // sincosf(angle * k, &sinf, &cosf);\n        // twiddle = thrust::complex<float>(cosf, sinf);\n        TWIDDLE();\n        sum = sum + din[j * num_cols + k] * twiddle;\n    }\n\n    dout[i * num_rows + j] = sum;\n}",
            "__global__\nvoid iDFT2D1gpu(thrust::complex<float>* din, thrust::complex<float>* dout, int num_rows, int num_cols)\n{\n\tint j = blockIdx.y * blockDim.y + threadIdx.y;\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (j >= num_rows || i >= num_cols) {\n        return;\n    }\n    \n    float angle, cosf, sinf; \n    thrust::complex<float> sum, twiddle;\n    angle = 2.0f * PI * fdividef((float)i, (float)num_cols);\n    sum = 0.0f;\n    for (int k = 0; k < num_cols/2+1; ++k) {\n        // sincosf(angle * k, &sinf, &cosf);\n        // twiddle = thrust::complex<float>(cosf, sinf);\n        TWIDDLE();\n        sum += din[j * (num_cols/2+1) + k] * twiddle;\n    }\n    for (int k = num_cols/2+1; k < num_cols; ++k) {\n        TWIDDLE();\n        sum += thrust::conj(din[((num_rows-j)%num_rows) * (num_cols/2+1) + ((num_cols-k)%num_cols)]) * twiddle;\n    }\n\n    dout[i * num_rows + j] = sum;\n}",
            "__global__\nvoid iDFT2D2gpu(thrust::complex<float>* din, float* dout, int num_rows, int num_cols)\n{\n\tint j = blockIdx.y * blockDim.y + threadIdx.y;\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (j >= num_rows || i >= num_cols) {\n        return;\n    }\n    \n    float angle, sum, cosf, sinf;\n    thrust::complex<float> twiddle;\n    angle = 2.0f * PI * fdividef((float)i, (float)num_cols);\n    sum = 0.0f;\n    for (int k = 0; k < num_cols; ++k) {\n        // sincosf(angle * k, &sinf, &cosf);\n        // twiddle = thrust::complex<float>(cosf, sinf);\n        TWIDDLE();\n        sum = sum + (din[j * num_cols + k] * twiddle).real();\n    }\n\n    dout[i * num_rows + j] = sum;\n}"
        ],
        "/Users/gbolet/hecbench-roofline/src/tsne-cuda/math_utils.cu": [
            "__global__\nvoid syv2k(\n          float* __restrict__ pij_sym,\n    const float* __restrict__ pij_non_sym,\n    const int*   __restrict__ pij_indices,\n    const int num_points,\n    const int num_neighbors)\n{\n    int tid, i, j, jend;\n    float pij_acc;\n\n    tid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (tid >= num_points * num_neighbors) {\n        return;\n    }\n\n    i = tid / num_neighbors;\n    j = pij_indices[tid];\n\n    pij_acc = pij_non_sym[tid];\n    jend = (j + 1) * num_neighbors;\n    for (int jidx = j * num_neighbors; jidx < jend; jidx++) {\n        pij_acc += pij_indices[jidx] == i ? pij_non_sym[jidx] : 0.0f;\n    }\n    pij_sym[tid] = pij_acc / (2.0f * num_points);\n}"
        ],
        "/Users/gbolet/hecbench-roofline/src/tsne-cuda/attr_forces.cu": [
            "__global__ void ComputePijxQijKernelV3(\n          float* __restrict__ workspace_x,\n          float* __restrict__ workspace_y,\n    const float* __restrict__ pij,\n    const int*   __restrict__ pij_ind,\n    const float* __restrict__ points,\n    const int num_points,\n    const int num_neighbors)\n{\n    int tid, i, j;\n    float ix, iy, jx, jy, dx, dy, pijqij;\n    tid = threadIdx.x + blockIdx.x * blockDim.x; // This is the location in the pij matrix\n    if (tid >= num_points * num_neighbors)\n        return;\n\n    i = tid / num_neighbors;\n    j = pij_ind[tid];\n\n    ix = points[i];\n    iy = points[num_points + i];\n    jx = points[j];\n    jy = points[num_points + j];\n    dx = ix - jx; // X distance\n    dy = iy - jy; // Y distance\n    pijqij = pij[tid] / (1 + dx * dx + dy * dy);\n\n    workspace_x[tid] = pijqij * dx;\n    workspace_y[tid] = pijqij * dy;\n}",
            "__global__ void reduce_sum_kernel(\n          float* __restrict__ attractive_forces,\n    const float* __restrict__ workspace_x,\n    const float* __restrict__ workspace_y,\n    const int num_points,\n    const int num_neighbors)\n{\n    int tid, jend, j;\n    float acc_x, acc_y;\n    tid = threadIdx.x + blockIdx.x * blockDim.x; // This is the location in the pij matrix\n    if (tid >= num_points)\n        return;\n\n    acc_x = 0.0f;\n    acc_y = 0.0f;\n    jend = (tid + 1) * num_neighbors;\n    for (j = tid * num_neighbors; j < jend; j++)\n    {\n        acc_x += workspace_x[j];\n        acc_y += workspace_y[j];\n    }\n\n    attractive_forces[tid] = acc_x;\n    attractive_forces[num_points + tid] = acc_y;\n}"
        ],
        "/Users/gbolet/hecbench-roofline/src/tsne-cuda/distance_utils.cu": [
            "__global__\nvoid PostprocessNeighborIndicesKernel(\n    volatile int* __restrict__ pij_indices,\n    const long*   __restrict__ knn_indices,\n    const int num_points,\n    const int num_neighbors)\n{\n    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (tid >= num_points * num_neighbors)\n        return;\n    pij_indices[tid] = (int)knn_indices[tid];\n}"
        ]
    },
    "bh-cuda": {
        "/Users/gbolet/hecbench-roofline/src/bh-cuda/main.cu": [
            "__global__\nvoid IntegrationKernel(\n     const int nbodiesd,\n     const float dtimed,\n     const float dthfd,\n     float4* const __restrict__ posMass,\n     float2* const __restrict__ veld,\n     float4* const __restrict__ accVeld)\n{\n  int i, inc;\n  float dvelx, dvely, dvelz;\n  float velhx, velhy, velhz;\n\n  // iterate over all bodies assigned to thread\n  inc = blockDim.x * gridDim.x;\n  for (i = threadIdx.x + blockIdx.x * blockDim.x; i < nbodiesd; i += inc) {\n    // integrate\n    float4 acc = accVeld[i];\n    dvelx = acc.x * dthfd;\n    dvely = acc.y * dthfd;\n    dvelz = acc.z * dthfd;\n\n    float2 v = veld[i];\n    velhx = v.x + dvelx;\n    velhy = v.y + dvely;\n    velhz = acc.w + dvelz;\n\n    float4 p = posMass[i];\n    p.x += velhx * dtimed;\n    p.y += velhy * dtimed;\n    p.z += velhz * dtimed;\n    posMass[i] = p;\n\n    v.x = velhx + dvelx;\n    v.y = velhy + dvely;\n    acc.w = velhz + dvelz;\n    veld[i] = v;\n    accVeld[i] = acc;\n  }\n}",
            "inline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\ninline __host__ __device__ float4 fmaxf(float4 a, float4 b)\n{\n    return make_float4(fmaxf(a.x,b.x), fmaxf(a.y,b.y), fmaxf(a.z,b.z), fmaxf(a.w,b.w));\n}\n\ninline  __host__ __device__ float4 fminf(float4 a, float4 b)\n{\n    return make_float4(fminf(a.x,b.x), fminf(a.y,b.y), fminf(a.z,b.z), fminf(a.w,b.w));\n}\n\n__global__\nvoid BoundingBoxKernel(\n    const int nnodesd,\n    const int nbodiesd,\n    int* const __restrict__ startd,\n    int* const __restrict__ childd,\n    float4* const __restrict__ posMassd,\n    float3* const __restrict__ maxd,\n    float3* const __restrict__ mind,\n    float* const __restrict__ radiusd,\n    int* const __restrict__ bottomd,\n    int* const __restrict__ stepd,\n    unsigned int* const __restrict__ blkcntd)\n{\n  int i, j, k;\n  float val;\n  float3 min, max;\n  __shared__ float sminx[THREADS1], \n                   smaxx[THREADS1], \n                   sminy[THREADS1],\n                   smaxy[THREADS1],\n                   sminz[THREADS1],\n                   smaxz[THREADS1];\n\n  // initialize with valid data (in case #bodies < #threads)\n  const float4 p0 = posMassd[0];\n  min.x = max.x = p0.x;\n  min.y = max.y = p0.y;\n  min.z = max.z = p0.z;\n\n  // scan all bodies\n  i = threadIdx.x;\n  int inc = THREADS1 * gridDim.x;\n  for (j = i + blockIdx.x * THREADS1; j < nbodiesd; j += inc) {\n    const float4 p = posMassd[j];\n    val = p.x;\n    min.x = fminf(min.x, val);\n    max.x = fmaxf(max.x, val);\n    val = p.y;\n    min.y = fminf(min.y, val);\n    max.y = fmaxf(max.y, val);\n    val = p.z;\n    min.z = fminf(min.z, val);\n    max.z = fmaxf(max.z, val);\n  }\n\n  // reduction in shared memory\n  sminx[i] = min.x;\n  smaxx[i] = max.x;\n  sminy[i] = min.y;\n  smaxy[i] = max.y;\n  sminz[i] = min.z;\n  smaxz[i] = max.z;\n\n  for (j = THREADS1 / 2; j > 0; j /= 2) {\n    __syncthreads();\n    if (i < j) {\n      k = i + j;\n      sminx[i] = min.x = fminf(min.x, sminx[k]);\n      smaxx[i] = max.x = fmaxf(max.x, smaxx[k]);\n      sminy[i] = min.y = fminf(min.y, sminy[k]);\n      smaxy[i] = max.y = fmaxf(max.y, smaxy[k]);\n      sminz[i] = min.z = fminf(min.z, sminz[k]);\n      smaxz[i] = max.z = fmaxf(max.z, smaxz[k]);\n    }\n  }\n\n  // write block result to global memory\n  if (i == 0) {\n    k = blockIdx.x;\n    mind[k] = min;\n    maxd[k] = max;\n    __threadfence();\n\n    inc = gridDim.x - 1;\n    if (inc == atomicInc(blkcntd, inc)) {\n      // I'm the last block, so combine all block results\n      for (j = 0; j <= inc; j++) {\n        float3 minp = mind[j];\n        float3 maxp = maxd[j];\n        min.x = fminf(min.x, minp.x);\n        max.x = fmaxf(max.x, maxp.x);\n        min.y = fminf(min.y, minp.y);\n        max.y = fmaxf(max.y, maxp.y);\n        min.z = fminf(min.z, minp.z);\n        max.z = fmaxf(max.z, maxp.z);\n      }\n\n      // compute radius\n      val = fmaxf(max.x - min.x, max.y - min.y);\n      *radiusd = fmaxf(val, max.z - min.z) * 0.5f;\n\n      // create root node\n      k = nnodesd;\n      *bottomd = k;\n\n      startd[k] = 0;\n      float4 p;\n      p.x = (min.x + max.x) * 0.5f;\n      p.y = (min.y + max.y) * 0.5f;\n      p.z = (min.z + max.z) * 0.5f;\n      p.w = -1.0f;\n      posMassd[k] = p;\n      k *= 8;\n      for (i = 0; i < 8; i++) childd[k + i] = -1;\n      (*stepd)++;\n    }\n  }\n}",
            "__global__\nvoid ClearKernel1(const int nnodesd, const int nbodiesd, int* const __restrict__ childd)\n{\n  int top = 8 * nnodesd;\n  int bottom = 8 * nbodiesd;\n  int inc = blockDim.x * gridDim.x;\n  int k = (bottom & (-WARPSIZE)) + threadIdx.x + blockIdx.x * blockDim.x;  // align to warp size\n  if (k < bottom) k += inc;\n\n  // iterate over all cells assigned to thread\n  while (k < top) {\n    childd[k] = -1;\n    k += inc;\n  }\n}",
            "inline __host__ __device__ uint4 make_uint4(int4 a)\n{\n    return make_uint4(uint(a.x), uint(a.y), uint(a.z), uint(a.w));\n}\n\ninline __host__ __device__ uint4 max(uint4 a, uint4 b)\n{\n    return make_uint4(max(a.x,b.x), max(a.y,b.y), max(a.z,b.z), max(a.w,b.w));\n}\n\n__global__\nvoid TreeBuildingKernel(\n    const int nnodesd,\n    const int nbodiesd,\n    volatile int* const __restrict__ childd,\n    const float4* const __restrict__ posMassd,\n    const float* const __restrict radiusd,\n            int* const __restrict bottomd\n)\n{\n  int i, j, depth, skip, inc;\n  float x, y, z, r;\n  float dx, dy, dz;\n  int ch, n, cell, locked, patch;\n  float radius;\n\n  // cache root data\n  radius = *radiusd * 0.5f;\n  const float4 root = posMassd[nnodesd];\n\n  skip = 1;\n  inc = blockDim.x * gridDim.x;\n  i = threadIdx.x + blockIdx.x * blockDim.x;\n\n  // iterate over all bodies assigned to thread\n  while (i < nbodiesd) {\n    const float4 p = posMassd[i];\n    if (skip != 0) {\n      // new body, so start traversing at root\n      skip = 0;\n      n = nnodesd;\n      depth = 1;\n      r = radius;\n      dx = dy = dz = -r;\n      j = 0;\n      // determine which child to follow\n      if (root.x < p.x) {j = 1; dx = r;}\n      if (root.y < p.y) {j |= 2; dy = r;}\n      if (root.z < p.z) {j |= 4; dz = r;}\n      x = root.x + dx;\n      y = root.y + dy;\n      z = root.z + dz;\n    }\n\n    // follow path to leaf cell\n    ch = childd[n*8+j];\n    while (ch >= nbodiesd) {\n      n = ch;\n      depth++;\n      r *= 0.5f;\n      dx = dy = dz = -r;\n      j = 0;\n      // determine which child to follow\n      if (x < p.x) {j = 1; dx = r;}\n      if (y < p.y) {j |= 2; dy = r;}\n      if (z < p.z) {j |= 4; dz = r;}\n      x += dx;\n      y += dy;\n      z += dz;\n      ch = childd[n*8+j];\n    }\n\n    if (ch != -2) {  // skip if child pointer is locked and try again later\n      locked = n*8+j;\n      if (ch == -1) {\n        if (ch == atomicCAS((int*)&childd[locked], ch, i)) {  // if null, just insert the new body\n          i += inc;  // move on to next body\n          skip = 1;\n        }\n      } else {  // there already is a body at this position\n        if (ch == atomicCAS((int*)&childd[locked], ch, -2)) {  // try to lock\n          patch = -1;\n          const float4 chp = posMassd[ch];\n          // create new cell(s) and insert the old and new bodies\n          do {\n            depth++;\n            cell = atomicSub(bottomd, 1) - 1;\n\n            if (patch != -1) {\n              childd[n*8+j] = cell;\n            }\n            patch = max(patch, cell);\n\n            j = 0;\n            if (x < chp.x) j = 1;\n            if (y < chp.y) j |= 2;\n            if (z < chp.z) j |= 4;\n            childd[cell*8+j] = ch;\n\n            n = cell;\n            r *= 0.5f;\n            dx = dy = dz = -r;\n            j = 0;\n            if (x < p.x) {j = 1; dx = r;}\n            if (y < p.y) {j |= 2; dy = r;}\n            if (z < p.z) {j |= 4; dz = r;}\n            x += dx;\n            y += dy;\n            z += dz;\n\n            ch = childd[n*8+j];\n            // repeat until the two bodies are different children\n          } while (ch >= 0);\n          childd[n*8+j] = i;\n\n          i += inc;  // move on to next body\n          skip = 2;\n        }\n      }\n    }\n    __syncthreads();  // optional barrier for performance\n\n    if (skip == 2) {\n      childd[locked] = patch;\n    }\n  }\n}",
            "__global__\nvoid ClearKernel2(\n    const int nnodesd, \n    int* const __restrict__ startd, \n    float4* const __restrict__ posMassd,\n    int* const __restrict__ bottomd)\n{\n  int k, inc, bottom;\n\n  bottom = *bottomd;\n  inc = blockDim.x * gridDim.x;\n  k = (bottom & (-WARPSIZE)) + threadIdx.x + blockIdx.x * blockDim.x;  // align to warp size\n  if (k < bottom) k += inc;\n\n  // iterate over all cells assigned to thread\n  while (k < nnodesd) {\n    posMassd[k].w = -1.0f;\n    startd[k] = -1;\n    k += inc;\n  }\n}",
            "__global__\nvoid SummarizationKernel(\n    const int nnodesd, \n    const int nbodiesd,\n    volatile int* const __restrict__ countd,\n    const int* const __restrict__ childd,\n    volatile float4* const __restrict__ posMassd, // will cause hanging for 2048 bodies without volatile\n    int* const __restrict bottomd)\n{\n  __shared__ int child[THREADS3 * 8];\n  __shared__ float mass[THREADS3 * 8];\n\n  int i, j, ch, cnt;\n  float cm, px, py, pz, m;\n  int bottom = *bottomd;\n  int inc = blockDim.x * gridDim.x;\n  int k = (bottom & (-WARPSIZE)) + threadIdx.x + blockIdx.x * blockDim.x;  // align to warp size\n  if (k < bottom) k += inc;\n\n  int restart = k;\n  for (j = 0; j < 3; j++) {  // wait-free pre-passes\n    // iterate over all cells assigned to thread\n    while (k <= nnodesd) {\n      if (posMassd[k].w < 0.0f) {\n        for (i = 0; i < 8; i++) {\n          ch = childd[k*8+i];\n          child[i*THREADS3+threadIdx.x] = ch;  // cache children\n          if ((ch >= nbodiesd) && ((mass[i*THREADS3+threadIdx.x] = posMassd[ch].w) < 0.0f)) {\n            break;\n          }\n        }\n        if (i == 8) {\n          // all children are ready\n          cm = 0.0f;\n          px = 0.0f;\n          py = 0.0f;\n          pz = 0.0f;\n          cnt = 0;\n          for (i = 0; i < 8; i++) {\n            ch = child[i*THREADS3+threadIdx.x];\n            if (ch >= 0) {\n              const float chx = posMassd[ch].x;\n              const float chy = posMassd[ch].y;\n              const float chz = posMassd[ch].z;\n              const float chw = posMassd[ch].w;\n              if (ch >= nbodiesd) {  // count bodies (needed later)\n                m = mass[i*THREADS3+threadIdx.x];\n                cnt += countd[ch];\n              } else {\n                m = chw;\n                cnt++;\n              }\n              // add child's contribution\n              cm += m;\n              px += chx * m;\n              py += chy * m;\n              pz += chz * m;\n            }\n          }\n          countd[k] = cnt;\n          m = 1.0f / cm;\n          posMassd[k].x = px * m;\n          posMassd[k].y = py * m;\n          posMassd[k].z = pz * m;\n          posMassd[k].w = cm;\n        }\n      }\n      k += inc;  // move on to next cell\n    }\n    k = restart;\n  }\n\n  j = 0;\n  // iterate over all cells assigned to thread\n  while (k <= nnodesd) {\n    if (posMassd[k].w >= 0.0f) {\n      k += inc;\n    } else {\n      if (j == 0) {\n        j = 8;\n        for (i = 0; i < 8; i++) {\n          ch = childd[k*8+i];\n          child[i*THREADS3+threadIdx.x] = ch;  // cache children\n          if ((ch < nbodiesd) || ((mass[i*THREADS3+threadIdx.x] = posMassd[ch].w) >= 0.0f)) {\n            j--;\n          }\n        }\n      } else {\n        j = 8;\n        for (i = 0; i < 8; i++) {\n          ch = child[i*THREADS3+threadIdx.x];\n          if ((ch < nbodiesd) || (mass[i*THREADS3+threadIdx.x] >= 0.0f) || ((mass[i*THREADS3+threadIdx.x] = posMassd[ch].w) >= 0.0f)) {\n            j--;\n          }\n        }\n      }\n\n      if (j == 0) {\n        // all children are ready\n        cm = 0.0f;\n        px = 0.0f;\n        py = 0.0f;\n        pz = 0.0f;\n        cnt = 0;\n        for (i = 0; i < 8; i++) {\n          ch = child[i*THREADS3+threadIdx.x];\n          if (ch >= 0) {\n            // four reads due to missing copy constructor for \"volatile float4\"\n            const float chx = posMassd[ch].x;\n            const float chy = posMassd[ch].y;\n            const float chz = posMassd[ch].z;\n            const float chw = posMassd[ch].w;\n            if (ch >= nbodiesd) {  // count bodies (needed later)\n              m = mass[i*THREADS3+threadIdx.x];\n              cnt += countd[ch];\n            } else {\n              m = chw;\n              cnt++;\n            }\n            // add child's contribution\n            cm += m;\n            px += chx * m;\n            py += chy * m;\n            pz += chz * m;\n          }\n        }\n        countd[k] = cnt;\n        m = 1.0f / cm;\n        // four writes due to missing copy constructor for \"volatile float4\"\n        posMassd[k].x = px * m;\n        posMassd[k].y = py * m;\n        posMassd[k].z = pz * m;\n        posMassd[k].w = cm;\n        k += inc;\n      }\n    }\n  }\n}",
            "__global__\nvoid SortKernel(\n    const int nnodesd,\n    const int nbodiesd, \n    int* const __restrict__ sortd,\n    const int* const __restrict__ countd,\n    volatile int* const __restrict__ startd,\n    int* const __restrict__ childd,\n    int* const __restrict__ bottomd)\n{\n  int i, j;\n  int bottom = *bottomd;\n  int dec = blockDim.x * gridDim.x;\n  int k = nnodesd + 1 - dec + threadIdx.x + blockIdx.x * blockDim.x;\n\n  // iterate over all cells assigned to thread\n  while (k >= bottom) {\n    int start = startd[k];\n    if (start >= 0) {\n      j = 0;\n      for (i = 0; i < 8; i++) {\n        int ch = childd[k*8+i];\n        if (ch >= 0) {\n          if (i != j) {\n            // move children to front (needed later for speed)\n            childd[k*8+i] = -1;\n            childd[k*8+j] = ch;\n          }\n          j++;\n          if (ch >= nbodiesd) {\n            // child is a cell\n            startd[ch] = start;  // set start ID of child\n            start += countd[ch];  // add #bodies in subtree\n          } else {\n            // child is a body\n            sortd[start] = ch;  // record body in 'sorted' array\n            start++;\n          }\n        }\n      }\n      k -= dec;  // move on to next cell\n    }\n    __syncthreads();  // optional barrier for performance\n  }\n}",
            "__global__\nvoid ForceCalculationKernel(\n    const int nnodesd, \n    const int nbodiesd,\n    const float dthfd,\n    const float itolsqd,\n    const float epssqd,\n    const int* const __restrict__ sortd,\n    const int* const __restrict__ childd,\n    const float4* const __restrict__ posMassd,\n    float2* const __restrict__ veld,\n    float4* const __restrict__ accVeld,\n    const float* const __restrict__ radiusd,\n    const int* const __restrict__ stepd)\n{\n  int i, j, k, n, depth, base, sbase, diff, pd, nd;\n  float ax, ay, az, dx, dy, dz, tmp;\n  __shared__ int pos[THREADS5], node[THREADS5];\n  __shared__ float dq[THREADS5];\n\n  if (0 == threadIdx.x) {\n    tmp = *radiusd * 2;\n    // precompute values that depend only on tree level\n    dq[0] = tmp * tmp * itolsqd;\n    for (i = 1; i < MAXDEPTH; i++) {\n      dq[i] = dq[i - 1] * 0.25f;\n      dq[i - 1] += epssqd;\n    }\n    dq[i - 1] += epssqd;\n  }\n  __syncthreads();\n\n  // figure out first thread in each warp (lane 0)\n  base = threadIdx.x / WARPSIZE;\n  sbase = base * WARPSIZE;\n  j = base * MAXDEPTH;\n\n  diff = threadIdx.x - sbase;\n  // make multiple copies to avoid index calculations later\n  if (diff < MAXDEPTH) {\n    dq[diff+j] = dq[diff];\n  }\n  __syncthreads();\n\n  // iterate over all bodies assigned to thread\n  for (k = threadIdx.x + blockIdx.x * blockDim.x; k < nbodiesd; k += blockDim.x * gridDim.x) {\n    i = sortd[k];  // get permuted/sorted index\n    // cache position info\n    const float4 pi = posMassd[i];\n\n    ax = 0.0f;\n    ay = 0.0f;\n    az = 0.0f;\n\n    // initialize iteration stack, i.e., push root node onto stack\n    depth = j;\n    if (sbase == threadIdx.x) {\n      pos[j] = 0;\n      node[j] = nnodesd * 8;\n    }\n\n    do {\n      // stack is not empty\n      pd = pos[depth];\n      nd = node[depth];\n      while (pd < 8) {\n        // node on top of stack has more children to process\n        n = childd[nd + pd];  // load child pointer\n        pd++;\n\n        if (n >= 0) {\n          const float4 pn = posMassd[n];\n          dx = pn.x - pi.x;\n          dy = pn.y - pi.y;\n          dz = pn.z - pi.z;\n          tmp = dx*dx + (dy*dy + (dz*dz + epssqd));  // compute distance squared (plus softening)\n          if ((n < nbodiesd) || __all_sync(0xffffffff, tmp >= dq[depth])) {  \n          // check if all threads agree that cell is far enough away (or is a body)\n            tmp = rsqrtf(tmp);  // compute distance\n            tmp = pn.w * tmp * tmp * tmp;\n            ax += dx * tmp;\n            ay += dy * tmp;\n            az += dz * tmp;\n          } else {\n            // push cell onto stack\n            if (sbase == threadIdx.x) {\n              pos[depth] = pd;\n              node[depth] = nd;\n            }\n            depth++;\n            pd = 0;\n            nd = n * 8;\n          }\n        } else {\n          pd = 8;  // early out because all remaining children are also zero\n        }\n      }\n      depth--;  // done with this level\n    } while (depth >= j);\n\n    float4 acc = accVeld[i];\n    if (*stepd > 0) {\n      // update velocity\n      float2 v = veld[i];\n      v.x += (ax - acc.x) * dthfd;\n      v.y += (ay - acc.y) * dthfd;\n      acc.w += (az - acc.z) * dthfd;\n      veld[i] = v;\n    }\n\n    // save computed acceleration\n    acc.x = ax;\n    acc.y = ay;\n    acc.z = az;\n    accVeld[i] = acc;\n  }\n}",
            "__global__\nvoid InitializationKernel(int *step, unsigned int *blkcnt)\n{\n  *step = -1;\n  *blkcnt = 0;\n}"
        ]
    },
    "columnarSolver-cuda": {
        "/Users/gbolet/hecbench-roofline/src/columnarSolver-cuda/kernels.cu": [
            "__device__\nvoid LCG_random_init(unsigned int * seed) {\n  const unsigned int m = 2147483648;\n  const unsigned int a = 26757677;\n  const unsigned int c = 1;\n  *seed = (a * (*seed) + c) % m;\n}\n\n__global__\nvoid setupKernel(unsigned int* state) {\n  int idx = blockIdx.x*blockDim.x + threadIdx.x;\n  state[idx] = idx;\n  for (int i = 0; i < idx; i++)\n    LCG_random_init(&state[idx]);\n}",
            "inline __host__ __device__ int4 make_int4(float4 a)\n{\n    return make_int4(int(a.x), int(a.y), int(a.z), int(a.w));\n}\n\ninline __host__ __device__ int4 abs(int4 v)\n{\n    return make_int4(abs(v.x), abs(v.y), abs(v.z), abs(v.w));\n}\n\n__device__\nvoid swapElements(int *key, int posLeft, int posRight) {\n  if (posLeft != posRight)\n  {\n    key[posLeft] -= key[posRight];\n    key[posRight] += key[posLeft];\n    key[posLeft] = key[posRight] - key[posLeft];\n  }\n}\n\n__device__\nfloat LCG_random_float(unsigned int * seed) {\n  const unsigned int m = 2147483648;\n  const unsigned int a = 26757677;\n  const unsigned int c = 1;\n  *seed = (a * (*seed) + c) % m;\n  return (float) (*seed) / (float) m;\n}\n\n__device__\nvoid decrypt(const int* encrypted, const int* key, int* decrypted) {\n\n  int columns[KEY_LENGTH][SECTION_CONSTANT+1];\n  int offset = 0;\n  int colLength[KEY_LENGTH];\n\n  for (int j=0; j<KEY_LENGTH; ++j) {\n    colLength[j] = ENCRYPTEDLEN / KEY_LENGTH;\n    if (j < ENCRYPTEDLEN % KEY_LENGTH)\n      colLength[j]++;\n  }\n\n  for (int keyPos=0; keyPos < KEY_LENGTH; ++keyPos) {\n    offset = 0;\n    for (int i=0; i<KEY_LENGTH; ++i)\n      if (key[i] < key[keyPos])\n        offset += colLength[i];\n\n    for (int j=0; j<colLength[keyPos]; ++j)   \n      columns[key[keyPos]][j] = encrypted[offset+j];          \n  } \n\n  for (int j=0; j<ENCRYPTEDLEN; ++j) \n    decrypted[j] = columns[key[j % KEY_LENGTH]][j / KEY_LENGTH];  \n}\n\n__device__ \nvoid swapBlock(int *key, int posLeft, int posRight, int length) {  \n  for (int i=0; i<length; i++) \n    swapElements(key, (posLeft+i)%KEY_LENGTH, (posRight+i)%KEY_LENGTH);\n}\n\n__global__ \nvoid decode(const float *__restrict d_scores, \n            const int *__restrict d_encrypted,\n            const unsigned int*__restrict  globalState, \n            int *__restrict d_decrypted) {\n\n  __shared__ float shared_scores[ALPHABET*ALPHABET];\n\n  int key[KEY_LENGTH];\n  int localDecrypted[ENCRYPTEDLEN];  \n  int bestLocalDecrypted[ENCRYPTEDLEN];  \n  int leftLetter = 0;\n  int rightLetter = 0;\n  int backupKey[KEY_LENGTH];\n  int shiftHelper[KEY_LENGTH];\n  int blockStart, blockEnd;\n  int l,f,t,t0,n,ff,tt;\n  float tempScore = 0.f;\n  float bestScore = CAP;\n  int j = 0, jj = 0;\n\n  int idx = blockIdx.x*blockDim.x + threadIdx.x;\n  unsigned int localState = globalState[idx];\n\n  if (threadIdx.x == 0) {\n    for (j=0; j<ALPHABET;++j)\n      for (jj=0; jj<ALPHABET; ++jj)\n        shared_scores[j*ALPHABET + jj] = d_scores[j*ALPHABET + jj];\n  }\n\n  __syncthreads();\n\n  for (j=0; j<KEY_LENGTH; ++j) \n    key[j]=j;\n\n  for (j=0; j<KEY_LENGTH; ++j) {\n    swapElements(key, j, LCG_random_float(&localState)*KEY_LENGTH);\n  }\n\n  for (int cycles=0; cycles<CLIMBINGS; ++cycles) {  \n\n    for (j=0; j<KEY_LENGTH;j++)\n      backupKey[j] = key[j];\n\n    tempScore = 0.f;\n\n    int branch = LCG_random_float(&localState)*100; \n\n    if (branch < HEUR_THRESHOLD_OP1)\n    {\n      for (j=0; j<1+LCG_random_float(&localState)*OP1_HOP; j++) \n      {\n        leftLetter = LCG_random_float(&localState)*KEY_LENGTH;   \n        rightLetter = LCG_random_float(&localState)*KEY_LENGTH; \n        swapElements(key, leftLetter, rightLetter);\n      }            \n    }\n\n    else if (branch < HEUR_THRESHOLD_OP2)\n    {\n      for (j=0; j< 1+LCG_random_float(&localState)*OP2_HOP;j++)\n      {\n        blockStart = LCG_random_float(&localState)*KEY_LENGTH;\n        blockEnd = LCG_random_float(&localState)*KEY_LENGTH;\n        swapBlock(key, blockStart, blockEnd, 1+LCG_random_float(&localState)*(abs((blockStart-blockEnd))-1));\n      }\n    }\n\n    else \n    {\n      l = 1 + LCG_random_float(&localState)*(KEY_LENGTH-2);\n      f = LCG_random_float(&localState)*(KEY_LENGTH-1);\n      t = (f+1+(LCG_random_float(&localState)*(KEY_LENGTH-2)));\n      t = t % KEY_LENGTH;\n\n      for (j=0; j< KEY_LENGTH;j++)\n        shiftHelper[j] = key[j];\n\n      t0 = (t-f+KEY_LENGTH) % KEY_LENGTH;\n      n = (t0+l) % KEY_LENGTH;\n\n      for (j=0; j<n;j++) \n      {\n        ff = (f+j) % KEY_LENGTH;\n        tt = (((t0+j)%n)+f)%KEY_LENGTH;\n        key[tt] = shiftHelper[ff];\n      }        \n    }      \n\n    decrypt(d_encrypted, key, localDecrypted);    \n\n    for (j=0; j<ENCRYPTEDLEN-1; ++j) {\n      tempScore += shared_scores[ALPHABET*localDecrypted[j] + localDecrypted[j+1]];\n    }\n\n    if (tempScore < bestScore) {\n      bestScore = tempScore;\n      for (j=0; j<ENCRYPTEDLEN; ++j) {\n        bestLocalDecrypted[j] = localDecrypted[j];\n      }\n    }    \n\n    else \n    {\n      for (j=0; j<KEY_LENGTH;j++)\n        key[j] = backupKey[j];      \n    }\n  }\n\n  for (j=0; j<ENCRYPTEDLEN; ++j)\n    d_decrypted[idx*ENCRYPTEDLEN+j] = bestLocalDecrypted[j];\n}"
        ]
    },
    "jaccard-cuda": {
        "/Users/gbolet/hecbench-roofline/src/jaccard-cuda/main.cu": [
            "#define T ((int)32)\n\n\n__device__\nT parallel_prefix_sum(const int n, const int *ind, const T *w) \n{\n\n  T sum = 0.0;\n  T last;\n\n  int mn =(((n+blockDim.x-1)/blockDim.x)*blockDim.x); //n in multiple of blockDim.x\n  for (int i=threadIdx.x; i<mn; i+=blockDim.x) {\n    //All threads (especially the last one) must always participate\n    //in the shfl instruction, otherwise their sum will be undefined.\n    //So, the loop stopping condition is based on multiple of n in loop increments,\n    //so that all threads enter into the loop and inside we make sure we do not\n    //read out of bounds memory checking for the actual size n.\n\n    //check if the thread is valid\n    bool valid  = i<n;\n\n    //Notice that the last thread is used to propagate the prefix sum.\n    //For all the threads, in the first iteration the last is 0, in the following\n    //iterations it is the value at the last thread of the previous iterations.\n\n    //get the value of the last thread\n    last = __shfl_sync(mask, sum, blockDim.x-1, blockDim.x);\n\n    //if you are valid read the value from memory, otherwise set your value to 0\n    sum = (valid) ? w[ind[i]] : 0.0;\n\n    //do prefix sum (of size warpSize=blockDim.x =< 32)\n    for (int j=1; j<blockDim.x; j*=2) {\n      T v = __shfl_up_sync(mask, sum, j, blockDim.x);\n      if (threadIdx.x >= j) sum += v;\n    }\n    //shift by last\n    sum += last;\n    //notice that no __threadfence or __syncthreads are needed in this implementation\n  }\n  //get the value of the last thread (to all threads)\n  last = __shfl_sync(mask, sum, blockDim.x-1, blockDim.x);\n\n  return last;\n}\n\n__global__ void \njaccard_row_sum(const int n,\n                const int *__restrict__ csrPtr,\n                const int *__restrict__ csrInd,\n                const T *__restrict__ w,\n                      T *__restrict__ work)\n{\n  for (int row=threadIdx.y+blockIdx.y*blockDim.y; row<n; row+=gridDim.y*blockDim.y) {\n    int start = csrPtr[row];\n    int end   = csrPtr[row+1];\n    int length= end-start;\n    //compute row sums \n    if (weighted) {\n      T sum = parallel_prefix_sum(length, csrInd + start, w); \n      if (threadIdx.x == 0) work[row] = sum;\n    }\n    else {\n      work[row] = (T)length;\n    }\n  }\n}",
            "#define T ((int)32)\n\n\n__global__ void\njaccard_is(const int n, const int e,\n           const int *__restrict__ csrPtr,\n           const int *__restrict__ csrInd,\n           const T *__restrict__ v,\n           const T *__restrict__ work,\n                 T *__restrict__ weight_i,\n                 T *__restrict__ weight_s) \n{\n  for (int row=threadIdx.z+blockIdx.z*blockDim.z; row<n; row+=gridDim.z*blockDim.z) {  \n    for (int j=csrPtr[row]+threadIdx.y+blockIdx.y*blockDim.y;\n             j<csrPtr[row+1]; j+=gridDim.y*blockDim.y) { \n      int col = csrInd[j];\n      //find which row has least elements (and call it reference row)\n      int Ni = csrPtr[row+1] - csrPtr[row];\n      int Nj = csrPtr[col+1] - csrPtr[col];\n      int ref= (Ni < Nj) ? row : col;\n      int cur= (Ni < Nj) ? col : row;\n\n      //compute new sum weights\n      weight_s[j] = work[row] + work[col];\n\n      //compute new intersection weights \n      //search for the element with the same column index in the reference row\n      for (int i=csrPtr[ref]+threadIdx.x+blockIdx.x*blockDim.x; i<csrPtr[ref+1]; i+=gridDim.x*blockDim.x) {\n        int match  =-1;           \n        int ref_col = csrInd[i];\n        T ref_val = weighted ? v[ref_col] : (T)1.0;\n\n        //binary search (column indices are sorted within each row)\n        int left = csrPtr[cur]; \n        int right= csrPtr[cur+1]-1; \n        while(left <= right){\n          int middle = (left+right)>>1; \n          int cur_col= csrInd[middle];\n          if (cur_col > ref_col) {\n            right=middle-1;\n          }\n          else if (cur_col < ref_col) {\n            left=middle+1;\n          }\n          else {\n            match = middle; \n            break; \n          }\n        }            \n\n        //if the element with the same column index in the reference row has been found\n        if (match != -1){\n          atomicAdd(&weight_i[j],ref_val);\n        }\n      }\n    }\n  }\n}",
            "#define T ((int)32)\n\n\n__global__ void \njaccard_jw(const int e,\n    const T *__restrict__ csrVal,\n    const T gamma,\n    const T *__restrict__ weight_i,\n    const T *__restrict__ weight_s,\n          T *__restrict__ weight_j) \n{\n  for (int j=threadIdx.x+blockIdx.x*blockDim.x; j<e; j+=gridDim.x*blockDim.x) {  \n    T Wi =  weight_i[j];\n    T Ws =  weight_s[j];\n    weight_j[j] = (gamma*csrVal[j])* (Wi/(Ws-Wi));\n  }\n}"
        ]
    },
    "conversion-cuda": {
        "/Users/gbolet/hecbench-roofline/src/conversion-cuda/main.cu": [
            "__global__\nvoid cvt (      Td *__restrict__ dst,\n          const Ts *__restrict__ src,\n          const int nelems)\n{\n  int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < nelems) {\n    dst[i] = static_cast<Td>(src[i]);\n  }\n}"
        ]
    },
    "dpid-cuda": {
        "/Users/gbolet/hecbench-roofline/src/dpid-cuda/kernels.cu": [
            "#define fp float\n\n\ninline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\nstatic __forceinline__ __device__\nValueType dot(BasicVector<ValueType> a, BasicVector<ValueType> b)\n{\n    return a.dot(b);\n}\n\n__global__\nvoid add(int n, const float *x, float *y)\n{\n  int index = blockIdx.x * blockDim.x + threadIdx.x;\n  int stride = blockDim.x * gridDim.x;\n  for (int i = index; i < n; i += stride)\n    y[i] += x[i];\n}\n\n__device__ __forceinline__\nfloat contribution(const Local& l, float f, const uint32_t x, const uint32_t y) {\n  if(x < l.sx)    f *= 1.f - (l.sx - x);\n  if((x+1.f) > l.ex)  f *= 1.f - ((x+1.f) - l.ex);\n  if(y < l.sy)    f *= 1.f - (l.sy - y);\n  if((y+1.f) > l.ey)  f *= 1.f - ((y+1.f) - l.ey);\n  return f;\n}\n\ninline __host__ __device__ float4 normalize(float4 v)\n{\n    float invLen = rsqrtf(dot(v, v));\n    return v * invLen;\n}\n\n__global__ void reduce(const  long d_Ne,  // number of elements in array\n                    const int d_no,       // number of sums to reduce\n                    const int d_mul,      // increment\n                    fp *d_sums,           // pointer to partial sums variable (DEVICE GLOBAL MEMORY)\n                    fp *d_sums2){\n\n  // indexes\n    int bx = blockIdx.x;                  // get current horizontal block index\n  int tx = threadIdx.x;                   // get current horizontal thread index\n  int ei = (bx*NUMBER_THREADS)+tx;        // unique thread id, more threads than actual elements !!!\n  int nf = NUMBER_THREADS-(gridDim.x*NUMBER_THREADS-d_no);        // number of elements assigned to last block\n  int df = 0;                             // divisibility factor for the last block\n\n  // statistical\n  __shared__ fp d_psum[NUMBER_THREADS];   // data for block calculations allocated by every block in its shared memory\n  __shared__ fp d_psum2[NUMBER_THREADS];\n\n  // counters\n  int i;\n\n  // copy data to shared memory\n  if(ei<d_no){                            // do only for the number of elements, omit extra threads\n\n    d_psum[tx] = d_sums[ei*d_mul];\n    d_psum2[tx] = d_sums2[ei*d_mul];\n\n  }\n\n  // Lingjie Zhang modifited at Nov 1 / 2015\n    __syncthreads();\n    // end Lingjie Zhang's modification\n\n  // reduction of sums if all blocks are full (rare case)  \n  if(nf == NUMBER_THREADS){\n    // sum of every 2, 4, ..., NUMBER_THREADS elements\n    for(i=2; i<=NUMBER_THREADS; i=2*i){\n      // sum of elements\n      if((tx+1) % i == 0){                      // every ith\n        d_psum[tx] = d_psum[tx] + d_psum[tx-i/2];\n        d_psum2[tx] = d_psum2[tx] + d_psum2[tx-i/2];\n      }\n      // synchronization\n      __syncthreads();\n    }\n    // final sumation by last thread in every block\n    if(tx==(NUMBER_THREADS-1)){                      // block result stored in global memory\n      d_sums[bx*d_mul*NUMBER_THREADS] = d_psum[tx];\n      d_sums2[bx*d_mul*NUMBER_THREADS] = d_psum2[tx];\n    }\n  }\n  // reduction of sums if last block is not full (common case)\n  else{ \n    // for full blocks (all except for last block)\n    if(bx != (gridDim.x - 1)){                      //\n      // sum of every 2, 4, ..., NUMBER_THREADS elements\n      for(i=2; i<=NUMBER_THREADS; i=2*i){                //\n        // sum of elements\n        if((tx+1) % i == 0){                    // every ith\n          d_psum[tx] = d_psum[tx] + d_psum[tx-i/2];\n          d_psum2[tx] = d_psum2[tx] + d_psum2[tx-i/2];\n        }\n        // synchronization\n        __syncthreads();                      //\n      }\n      // final sumation by last thread in every block\n      if(tx==(NUMBER_THREADS-1)){                    // block result stored in global memory\n        d_sums[bx*d_mul*NUMBER_THREADS] = d_psum[tx];\n        d_sums2[bx*d_mul*NUMBER_THREADS] = d_psum2[tx];\n      }\n    }\n    // for not full block (last block)\n    else{                                //\n      // figure out divisibility\n      for(i=2; i<=NUMBER_THREADS; i=2*i){                //\n        if(nf >= i){\n          df = i;\n        }\n      }\n      // sum of every 2, 4, ..., NUMBER_THREADS elements\n      for(i=2; i<=df; i=2*i){                      //\n        // sum of elements (only busy threads)\n        if((tx+1) % i == 0 && tx<df){                // every ith\n          d_psum[tx] = d_psum[tx] + d_psum[tx-i/2];\n          d_psum2[tx] = d_psum2[tx] + d_psum2[tx-i/2];\n        }\n        // synchronization (all threads)\n        __syncthreads();                      //\n      }\n      // remainder / final summation by last thread\n      if(tx==(df-1)){                    //\n        // compute the remainder and final summation by last busy thread\n        for(i=(bx*NUMBER_THREADS)+df; i<(bx*NUMBER_THREADS)+nf; i++){            //\n          d_psum[tx] = d_psum[tx] + d_sums[i];\n          d_psum2[tx] = d_psum2[tx] + d_sums2[i];\n        }\n        // final sumation by last thread in every block\n        d_sums[bx*d_mul*NUMBER_THREADS] = d_psum[tx];\n        d_sums2[bx*d_mul*NUMBER_THREADS] = d_psum2[tx];\n      }\n    }\n  }\n\n}\n\n__global__\nvoid kernelGuidance(const uchar3* __restrict__ input,\n                          uchar3* __restrict__ patches, const Params p)\n{\n  if(PX >= p.oWidth || PY >= p.oHeight) return;\n\n  // init\n  const Local l(p);\n  float4 color = make_float4(0.f, 0.f, 0.f, 0.f);\n\n  // iterate pixels\n  for(uint32_t i = WTHREAD; i < l.pixelCount; i += WSIZE) {\n    const uint32_t x = l.sxr + (i % l.xCount);\n    const uint32_t y = l.syr + (i / l.xCount);\n\n    float f = contribution(l, 1.f, x, y);  \n\n    const uchar3& pixel = input[x + y * p.iWidth];\n    add(color, make_float4(pixel.x * f, pixel.y * f, pixel.z * f, f));\n  }\n\n  // reduce warps\n  reduce(color);\n\n  // store results\n  if((TX % 32) == 0) {\n    normalize(color);\n    patches[PX + PY * p.oWidth] = make_uchar3(color.x, color.y, color.z);\n  }\n}",
            "#define PX (blockIdx.x * TSIZE + (TX / WSIZE))\n\n\n#define T ((int)32)\n\n\n#define block_size_x 32\n\n\n#define block_size_y 8\n\n\n#define fp float\n\n\n#define tile_size_x 4\n\n\n#define tile_size_y 4\n\n\ninline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\nstatic __forceinline__ __device__\nValueType dot(BasicVector<ValueType> a, BasicVector<ValueType> b)\n{\n    return a.dot(b);\n}\n\n__global__\nvoid add(int n, const float *x, float *y)\n{\n  int index = blockIdx.x * blockDim.x + threadIdx.x;\n  int stride = blockDim.x * gridDim.x;\n  for (int i = index; i < n; i += stride)\n    y[i] += x[i];\n}\n\ninline __host__ __device__ float4 normalize(float4 v)\n{\n    float invLen = rsqrtf(dot(v, v));\n    return v * invLen;\n}\n\n__device__ __forceinline__\nvoid fill_shared_mem_tiled_1D(\n  T (&sh_mem)[tile_size*stride], \n  const T *__restrict__ d_mem,\n  int sh_offset, int d_offset)\n{\n  #pragma unroll\n  for (int ti=0; ti<tile_size; ti++) {\n    sh_mem[sh_offset+ti*stride] = d_mem[d_offset+ti*stride];\n  }\n}\n\n__device__ __forceinline__\nvoid distance_tiled(\n  const T *__restrict__ A, \n  const T *__restrict__ B,\n  int m, int n, \n  const T *__restrict__ scale_A,\n  const T *__restrict__ scale_B,\n  T *__restrict__ cross_term)\n{\n  int tx = threadIdx.x;\n  int ty = threadIdx.y;\n  int i = tx + blockIdx.x * block_size_x * tile_size_x;\n  int j = ty + blockIdx.y * block_size_y * tile_size_y;\n\n  __shared__ T sh_A[dim][block_size_x*tile_size_x];\n  __shared__ T sh_B[dim][block_size_y*tile_size_y];\n  __shared__ T sh_scale_A[block_size_x*tile_size_x];\n  __shared__ T sh_scale_B[block_size_y*tile_size_y];\n  __shared__ T sum;\n\n  if (tx == 0 && ty == 0) sum = 0;\n\n  #pragma unroll\n  for (int d=0; d<dim; d++) {\n    fill_shared_mem_tiled_1D<tile_size_x, block_size_x>(sh_A[d], A+d*m, tx, i);\n    fill_shared_mem_tiled_1D<tile_size_y, block_size_y>(sh_B[d], B+d*n, ty, j);\n  }\n  fill_shared_mem_tiled_1D<tile_size_x, block_size_x>(sh_scale_A, scale_A, tx, i);\n  fill_shared_mem_tiled_1D<tile_size_y, block_size_y>(sh_scale_B, scale_B, ty, j);\n\n  T s_cross_term = 0.0;\n  #pragma unroll\n  for (int ti=0; ti<tile_size_x; ti++) {\n    #pragma unroll\n    for (int tj=0; tj<tile_size_y; tj++) {\n\n      if ((i+ti*block_size_x < m) && (j+tj*block_size_y < n)) {\n\n        T dist_ij = 0;\n\n        #pragma unroll\n        for (int d=0; d<dim; d++) {\n          dist_ij += (sh_A[d][tx+ti*block_size_x]-sh_B[d][ty+tj*block_size_y])*\n                     (sh_A[d][tx+ti*block_size_x]-sh_B[d][ty+tj*block_size_y]);\n        }\n        s_cross_term += exp(-dist_ij/(sh_scale_A[tx+ti*block_size_x] + sh_scale_B[ty+tj*block_size_y]));\n      }\n    }\n  }\n\n  atomicAdd(&sum, s_cross_term);\n  __syncthreads();\n\n  //write back the per-thread block partial cross term\n  if (tx == 0 && ty == 0) {\n    cross_term[blockIdx.y*gridDim.x+blockIdx.x] = sum;\n  }\n}\n\n__device__ __forceinline__\nfloat4 calcAverage(const Params& p, const uchar3* __restrict__ patches) {\n  const float corner = 1.0;\n  const float edge   = 2.0;\n  const float center = 4.0;\n\n  // calculate average color\n  float4 avg = make_float4(0.f, 0.f, 0.f, 0.f);\n\n  // TOP\n  if(PY > 0) {\n    if(PX > 0) \n      add(avg, patches[(PX - 1) + (PY - 1) * p.oWidth], corner);\n\n    add(avg, patches[(PX) + (PY - 1) * p.oWidth], edge);\n\n    if((PX+1) < p.oWidth)\n      add(avg, patches[(PX + 1) + (PY - 1) * p.oWidth], corner);\n  }\n\n  // LEFT\n  if(PX > 0) \n    add(avg, patches[(PX - 1) + (PY) * p.oWidth], edge);\n\n  // CENTER\n  add(avg, patches[(PX) + (PY) * p.oWidth], center);\n\n  // RIGHT\n  if((PX+1) < p.oWidth)\n    add(avg, patches[(PX + 1) + (PY) * p.oWidth], edge);\n\n  // BOTTOM\n  if((PY+1) < p.oHeight) {\n    if(PX > 0) \n      add(avg, patches[(PX - 1) + (PY + 1) * p.oWidth], corner);\n\n    add(avg, patches[(PX) + (PY + 1) * p.oWidth], edge);\n\n    if((PX+1) < p.oWidth)\n      add(avg, patches[(PX + 1) + (PY + 1) * p.oWidth], corner);\n  }\n\n  normalize(avg);\n\n  return avg;\n}\n\n__device__ __forceinline__\nfloat contribution(const Local& l, float f, const uint32_t x, const uint32_t y) {\n  if(x < l.sx)    f *= 1.f - (l.sx - x);\n  if((x+1.f) > l.ex)  f *= 1.f - ((x+1.f) - l.ex);\n  if(y < l.sy)    f *= 1.f - (l.sy - y);\n  if((y+1.f) > l.ey)  f *= 1.f - ((y+1.f) - l.ey);\n  return f;\n}\n\n__global__ \nvoid distance(\n  const T *__restrict__ A,\n  const T *__restrict__ B,\n  int m, int n,\n  const T *__restrict__ scale_A,\n  const T *__restrict__ scale_B, \n        T *__restrict__ cross_term)\n{\n  //2-dimensional with T precision\n  distance_tiled<T, 2>(A, B, m, n, scale_A, scale_B, cross_term);\n}\n\n__device__ __forceinline__\nfloat lambda(const Params p, const float dist) {\n  if(p.lambda == 0.f)\n    return 1.f;\n  else if(p.lambda == 1.f)\n    return dist;\n  return powf(dist, p.lambda);\n}\n\n__global__ void reduce(const  long d_Ne,  // number of elements in array\n                    const int d_no,       // number of sums to reduce\n                    const int d_mul,      // increment\n                    fp *d_sums,           // pointer to partial sums variable (DEVICE GLOBAL MEMORY)\n                    fp *d_sums2){\n\n  // indexes\n    int bx = blockIdx.x;                  // get current horizontal block index\n  int tx = threadIdx.x;                   // get current horizontal thread index\n  int ei = (bx*NUMBER_THREADS)+tx;        // unique thread id, more threads than actual elements !!!\n  int nf = NUMBER_THREADS-(gridDim.x*NUMBER_THREADS-d_no);        // number of elements assigned to last block\n  int df = 0;                             // divisibility factor for the last block\n\n  // statistical\n  __shared__ fp d_psum[NUMBER_THREADS];   // data for block calculations allocated by every block in its shared memory\n  __shared__ fp d_psum2[NUMBER_THREADS];\n\n  // counters\n  int i;\n\n  // copy data to shared memory\n  if(ei<d_no){                            // do only for the number of elements, omit extra threads\n\n    d_psum[tx] = d_sums[ei*d_mul];\n    d_psum2[tx] = d_sums2[ei*d_mul];\n\n  }\n\n  // Lingjie Zhang modifited at Nov 1 / 2015\n    __syncthreads();\n    // end Lingjie Zhang's modification\n\n  // reduction of sums if all blocks are full (rare case)  \n  if(nf == NUMBER_THREADS){\n    // sum of every 2, 4, ..., NUMBER_THREADS elements\n    for(i=2; i<=NUMBER_THREADS; i=2*i){\n      // sum of elements\n      if((tx+1) % i == 0){                      // every ith\n        d_psum[tx] = d_psum[tx] + d_psum[tx-i/2];\n        d_psum2[tx] = d_psum2[tx] + d_psum2[tx-i/2];\n      }\n      // synchronization\n      __syncthreads();\n    }\n    // final sumation by last thread in every block\n    if(tx==(NUMBER_THREADS-1)){                      // block result stored in global memory\n      d_sums[bx*d_mul*NUMBER_THREADS] = d_psum[tx];\n      d_sums2[bx*d_mul*NUMBER_THREADS] = d_psum2[tx];\n    }\n  }\n  // reduction of sums if last block is not full (common case)\n  else{ \n    // for full blocks (all except for last block)\n    if(bx != (gridDim.x - 1)){                      //\n      // sum of every 2, 4, ..., NUMBER_THREADS elements\n      for(i=2; i<=NUMBER_THREADS; i=2*i){                //\n        // sum of elements\n        if((tx+1) % i == 0){                    // every ith\n          d_psum[tx] = d_psum[tx] + d_psum[tx-i/2];\n          d_psum2[tx] = d_psum2[tx] + d_psum2[tx-i/2];\n        }\n        // synchronization\n        __syncthreads();                      //\n      }\n      // final sumation by last thread in every block\n      if(tx==(NUMBER_THREADS-1)){                    // block result stored in global memory\n        d_sums[bx*d_mul*NUMBER_THREADS] = d_psum[tx];\n        d_sums2[bx*d_mul*NUMBER_THREADS] = d_psum2[tx];\n      }\n    }\n    // for not full block (last block)\n    else{                                //\n      // figure out divisibility\n      for(i=2; i<=NUMBER_THREADS; i=2*i){                //\n        if(nf >= i){\n          df = i;\n        }\n      }\n      // sum of every 2, 4, ..., NUMBER_THREADS elements\n      for(i=2; i<=df; i=2*i){                      //\n        // sum of elements (only busy threads)\n        if((tx+1) % i == 0 && tx<df){                // every ith\n          d_psum[tx] = d_psum[tx] + d_psum[tx-i/2];\n          d_psum2[tx] = d_psum2[tx] + d_psum2[tx-i/2];\n        }\n        // synchronization (all threads)\n        __syncthreads();                      //\n      }\n      // remainder / final summation by last thread\n      if(tx==(df-1)){                    //\n        // compute the remainder and final summation by last busy thread\n        for(i=(bx*NUMBER_THREADS)+df; i<(bx*NUMBER_THREADS)+nf; i++){            //\n          d_psum[tx] = d_psum[tx] + d_sums[i];\n          d_psum2[tx] = d_psum2[tx] + d_sums2[i];\n        }\n        // final sumation by last thread in every block\n        d_sums[bx*d_mul*NUMBER_THREADS] = d_psum[tx];\n        d_sums2[bx*d_mul*NUMBER_THREADS] = d_psum2[tx];\n      }\n    }\n  }\n\n}\n\n__global__\nvoid kernelDownsampling(const uchar3* __restrict__ input,\n                        const uchar3* __restrict__ patches,\n                        const Params p,\n                              uchar3* __restrict__ output)\n{\n  if(PX >= p.oWidth || PY >= p.oHeight) return;\n\n  // init\n  const Local l(p);\n  const float4 avg = calcAverage(p, patches);\n\n  float4 color = make_float4(0.f, 0.f, 0.f, 0.f);\n\n  // iterate pixels\n  for(uint32_t i = WTHREAD; i < l.pixelCount; i += WSIZE) {\n    const uint32_t x = l.sxr + (i % l.xCount);\n    const uint32_t y = l.syr + (i / l.xCount);\n\n    const uchar3& pixel = input[x + y * p.iWidth];\n    float f = distance(avg, pixel);\n\n    f = lambda(p, f);\n    f = contribution(l, f, x, y);\n\n    add(color, pixel, f);\n  }\n\n  // reduce warp\n  reduce(color);\n\n  if(WTHREAD == 0) {\n    uchar3& ref = output[PX + PY * p.oWidth];\n\n    if(color.w == 0.0f)\n      ref = make_uchar3((unsigned char)avg.x, (unsigned char)avg.y, (unsigned char)avg.z);\n    else {\n      normalize(color);\n      ref = make_uchar3((unsigned char)color.x, (unsigned char)color.y, (unsigned char)color.z);\n    }\n  }\n}"
        ]
    },
    "eikonal-cuda": {
        "/Users/gbolet/hecbench-roofline/src/eikonal-cuda/kernel.cu": [
            "#define DOUBLE \t\t2\n\n\ninline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\ninline __host__ __device__ float4 sqrtf(float4 v)\n{\n    return make_float4(sqrtf(v.x), sqrtf(v.y), sqrtf(v.z), sqrtf(v.w));\n}\n\ninline __host__ __device__ uint4 make_uint4(int4 a)\n{\n    return make_uint4(uint(a.x), uint(a.y), uint(a.z), uint(a.w));\n}\n\ninline __host__ __device__ uint4 min(uint4 a, uint4 b)\n{\n    return make_uint4(min(a.x,b.x), min(a.y,b.y), min(a.z,b.z), min(a.w,b.w));\n}\n\n__device__ DOUBLE get_time_eikonal(DOUBLE a, DOUBLE b, DOUBLE c, DOUBLE s)\n{\n  DOUBLE ret, tmp;\n\n  // a > b > c\n  if(a < b) { tmp = a; a = b; b = tmp; }\n  if(b < c) { tmp = b; b = c; c = tmp; }\n  if(a < b) { tmp = a; a = b; b = tmp; }\n\n  ret = INF;\n\n  if(c < INF)\n  {\n    ret = c + s;\n\n    if(ret > b) \n    {  \n      tmp = ((b+c) + sqrtf(2.0f*s*s-(b-c)*(b-c)))*0.5f;\n\n      if(tmp > b) ret = tmp; \n\n      if(ret > a)  {      \n        tmp = (a+b+c)/3.0f + sqrtf(2.0f*(a*(b-a)+b*(c-b)+c*(a-c))+3.0f*s*s)/3.0f; \n\n        if(tmp > a) ret = tmp;\n      }\n    }\n  }\n\n  return ret;\n}\n\n__global__ void run_solver(\n  const double*__restrict__ spd,\n  const bool*__restrict__ mask,\n  const DOUBLE *__restrict__ sol_in,\n  DOUBLE *__restrict__ sol_out,\n  bool *__restrict__ con,\n  const uint*__restrict__ list,\n  int xdim, int ydim, int zdim,\n  int nIter, uint nActiveBlock)\n{\n  uint list_idx = blockIdx.y*gridDim.x + blockIdx.x;\n\n  if(list_idx < nActiveBlock)\n  {\n    // retrieve actual block index from the active list\n    uint block_idx = list[list_idx];\n\n    double F;\n    bool isValid;\n    uint blocksize = BLOCK_LENGTH*BLOCK_LENGTH*BLOCK_LENGTH;\n    uint base_addr = block_idx*blocksize;\n\n    uint xgridlength = xdim/BLOCK_LENGTH;\n    uint ygridlength = ydim/BLOCK_LENGTH;\n    uint zgridlength = zdim/BLOCK_LENGTH;\n\n    // compute block index\n    uint bx = block_idx%xgridlength;\n    uint tmpIdx = (block_idx - bx)/xgridlength;\n    uint by = tmpIdx%ygridlength;\n    uint bz = (tmpIdx-by)/ygridlength;\n\n    uint tx = threadIdx.x;\n    uint ty = threadIdx.y;\n    uint tz = threadIdx.z;\n    uint tIdx = tz*BLOCK_LENGTH*BLOCK_LENGTH + ty*BLOCK_LENGTH + tx;\n\n    __shared__ DOUBLE _sol[BLOCK_LENGTH+2][BLOCK_LENGTH+2][BLOCK_LENGTH+2];\n\n    // copy global to shared memory\n    dim3 idx(tx+1,ty+1,tz+1);\n\n    SOL(idx.x,idx.y,idx.z) = sol_in[base_addr + tIdx];\n    F = spd[base_addr + tIdx];\n    if(F > 0) F = 1.0/F; // F = 1/f\n    isValid = mask[base_addr + tIdx];\n\n    uint new_base_addr, new_tIdx;\n\n    // 1-neighborhood values\n    if(tx == 0) \n    {\n      if(bx == 0) // end of the grid\n      {  \n        new_tIdx = tIdx;\n        new_base_addr = base_addr;\n      }\n      else\n      {\n        new_tIdx = tIdx + BLOCK_LENGTH-1;\n        new_base_addr = (block_idx - 1)*blocksize;  \n      }\n\n      SOL(tx,idx.y,idx.z) = sol_in[new_base_addr + new_tIdx];  \n    }\n\n    if(tx == BLOCK_LENGTH-1)\n    {\n      if(bx == xgridlength-1) // end of the grid\n      {\n        new_tIdx = tIdx;\n        new_base_addr = base_addr;\n      }\n      else\n      {\n        new_tIdx = tIdx - (BLOCK_LENGTH-1);\n        new_base_addr = (block_idx + 1)*blocksize;  \n      }\n      SOL(tx+2,idx.y,idx.z) = sol_in[new_base_addr + new_tIdx];  \n    }\n\n    if(ty == 0)\n    {\n      if(by == 0)\n      {\n        new_tIdx = tIdx;\n        new_base_addr = base_addr;\n      }\n      else\n      {\n        new_tIdx = tIdx + (BLOCK_LENGTH-1)*BLOCK_LENGTH;\n        new_base_addr = (block_idx - xgridlength)*blocksize;\n      }\n\n      SOL(idx.x,ty,idx.z) = sol_in[new_base_addr + new_tIdx];\n    }\n\n    if(ty == BLOCK_LENGTH-1)\n    {\n      if(by == ygridlength-1) \n      {\n        new_tIdx = tIdx;\n        new_base_addr = base_addr;\n      }\n      else\n      {\n        new_tIdx = tIdx - (BLOCK_LENGTH-1)*BLOCK_LENGTH;\n        new_base_addr = (block_idx + xgridlength)*blocksize;\n      }\n\n      SOL(idx.x,ty+2,idx.z) = sol_in[new_base_addr + new_tIdx];\n    }\n\n    if(tz == 0)\n    {\n      if(bz == 0)\n      {\n        new_tIdx = tIdx;\n        new_base_addr = base_addr;\n      }\n      else\n      {\n        new_tIdx = tIdx + (BLOCK_LENGTH-1)*BLOCK_LENGTH*BLOCK_LENGTH;\n        new_base_addr = (block_idx - xgridlength*ygridlength)*blocksize;\n      }\n\n      SOL(idx.x,idx.y,tz) = sol_in[new_base_addr + new_tIdx];\n    }\n\n    if(tz == BLOCK_LENGTH-1)\n    {\n      if(bz == zgridlength-1) \n      {\n        new_tIdx = tIdx;\n        new_base_addr = base_addr;\n      }\n      else\n      {\n        new_tIdx = tIdx - (BLOCK_LENGTH-1)*BLOCK_LENGTH*BLOCK_LENGTH;\n        new_base_addr = (block_idx + xgridlength*ygridlength)*blocksize;\n      }\n\n      SOL(idx.x,idx.y,tz+2) = sol_in[new_base_addr + new_tIdx];\n    }\n\n    __syncthreads();\n\n    DOUBLE a,b,c,oldT,newT;\n\n    for(int iter=0; iter<nIter; iter++)  \n    {\n      //\n      // compute new value\n      //\n      oldT = newT = SOL(idx.x,idx.y,idx.z);\n\n      if(isValid)\n      {\n        a = min(SOL(tx,idx.y,idx.z),SOL(tx+2,idx.y,idx.z));\n        b = min(SOL(idx.x,ty,idx.z),SOL(idx.x,ty+2,idx.z));\n        c = min(SOL(idx.x,idx.y,tz),SOL(idx.x,idx.y,tz+2));\n\n        DOUBLE tmp = (DOUBLE) get_time_eikonal(a, b, c, F);\n\n        newT = min(tmp,oldT);\n      }\n      __syncthreads();  \n\n      if(isValid) SOL(idx.x,idx.y,idx.z) = newT;\n\n      __syncthreads(); // this may not required    \n    }\n\n    DOUBLE residue = oldT - newT;\n\n    // write back to global memory\n    con[base_addr + tIdx] = (residue < EPS) ? true : false;\n    sol_out[base_addr + tIdx] = newT;    \n  }\n}",
            "__global__ void run_reduction(\n  const bool *__restrict__ con,\n  bool *__restrict__ listVol,\n  const uint *__restrict__ list,\n  uint nActiveBlock)\n{\n  uint list_idx = blockIdx.y*gridDim.x + blockIdx.x;\n\n  if(list_idx < nActiveBlock)\n  {\n    uint block_idx = list[list_idx];\n\n    __shared__ bool conv[BLOCK_LENGTH*BLOCK_LENGTH*BLOCK_LENGTH];\n\n    uint blocksize = BLOCK_LENGTH*BLOCK_LENGTH*BLOCK_LENGTH/2;\n    uint base_addr = block_idx*blocksize*2;\n    uint tx = threadIdx.x;\n    uint ty = threadIdx.y;\n    uint tz = threadIdx.z;\n    uint tIdx = tz*BLOCK_LENGTH*BLOCK_LENGTH + ty*BLOCK_LENGTH + tx;\n\n    conv[tIdx] = con[base_addr + tIdx];\n    conv[tIdx + blocksize] = con[base_addr + tIdx + blocksize];\n\n    __syncthreads();\n\n    for(uint i=blocksize; i>0; i/=2)\n    {\n      if(tIdx < i)\n      {\n        bool b1, b2;\n        b1 = conv[tIdx];\n        b2 = conv[tIdx+i];\n        conv[tIdx] = (b1 && b2) ? true : false ;\n      }\n      __syncthreads();\n    }\n\n    if(tIdx == 0) \n    {    \n      listVol[block_idx] = !conv[0]; // active list is negation of tile convergence (active = not converged)\n    }\n  }\n}",
            "#define DOUBLE \t\t2\n\n\ninline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\ninline __host__ __device__ float4 sqrtf(float4 v)\n{\n    return make_float4(sqrtf(v.x), sqrtf(v.y), sqrtf(v.z), sqrtf(v.w));\n}\n\ninline __host__ __device__ uint4 make_uint4(int4 a)\n{\n    return make_uint4(uint(a.x), uint(a.y), uint(a.z), uint(a.w));\n}\n\ninline __host__ __device__ uint4 min(uint4 a, uint4 b)\n{\n    return make_uint4(min(a.x,b.x), min(a.y,b.y), min(a.z,b.z), min(a.w,b.w));\n}\n\n__device__ DOUBLE get_time_eikonal(DOUBLE a, DOUBLE b, DOUBLE c, DOUBLE s)\n{\n  DOUBLE ret, tmp;\n\n  // a > b > c\n  if(a < b) { tmp = a; a = b; b = tmp; }\n  if(b < c) { tmp = b; b = c; c = tmp; }\n  if(a < b) { tmp = a; a = b; b = tmp; }\n\n  ret = INF;\n\n  if(c < INF)\n  {\n    ret = c + s;\n\n    if(ret > b) \n    {  \n      tmp = ((b+c) + sqrtf(2.0f*s*s-(b-c)*(b-c)))*0.5f;\n\n      if(tmp > b) ret = tmp; \n\n      if(ret > a)  {      \n        tmp = (a+b+c)/3.0f + sqrtf(2.0f*(a*(b-a)+b*(c-b)+c*(a-c))+3.0f*s*s)/3.0f; \n\n        if(tmp > a) ret = tmp;\n      }\n    }\n  }\n\n  return ret;\n}\n\n__global__ void run_check_neighbor(\n  const double*__restrict__ spd,\n  const bool*__restrict__ mask,\n  const DOUBLE *__restrict__ sol_in,\n  DOUBLE *__restrict__ sol_out,\n  bool *__restrict__ con,\n  const uint*__restrict__ list,\n  int xdim, int ydim, int zdim,\n  uint nActiveBlock, uint nTotalBlock)\n{\n\n  uint list_idx = blockIdx.y*gridDim.x + blockIdx.x;\n\n  if(list_idx < nTotalBlock)\n  {\n    double F;\n    bool isValid;\n    __shared__ DOUBLE _sol[BLOCK_LENGTH+2][BLOCK_LENGTH+2][BLOCK_LENGTH+2];\n\n    uint block_idx = list[list_idx];\n    uint blocksize = BLOCK_LENGTH*BLOCK_LENGTH*BLOCK_LENGTH;\n    uint base_addr = block_idx*blocksize;\n\n    uint tx = threadIdx.x;\n    uint ty = threadIdx.y;\n    uint tz = threadIdx.z;\n    uint tIdx = tz*BLOCK_LENGTH*BLOCK_LENGTH + ty*BLOCK_LENGTH + tx;\n\n    if(list_idx < nActiveBlock) // copy value\n    {\n      sol_out[base_addr + tIdx] = sol_in[base_addr + tIdx];\n    } \n    else\n    {\n      uint xgridlength = xdim/BLOCK_LENGTH;\n      uint ygridlength = ydim/BLOCK_LENGTH;\n      uint zgridlength = zdim/BLOCK_LENGTH;\n\n      // compute block index\n      uint bx = block_idx%xgridlength;\n      uint tmpIdx = (block_idx - bx)/xgridlength;\n      uint by = tmpIdx%ygridlength;\n      uint bz = (tmpIdx-by)/ygridlength;\n\n      // copy global to shared memory\n      dim3 idx(tx+1,ty+1,tz+1);\n      _sol[idx.x][idx.y][idx.z] = sol_in[base_addr + tIdx];\n      F = spd[base_addr + tIdx];\n      if(F > 0) F = 1.0/F;\n      isValid = mask[base_addr + tIdx];\n\n      uint new_base_addr, new_tIdx;\n\n      // 1-neighborhood values\n      if(tx == 0) \n      {\n        if(bx == 0) // end of the grid\n        {  \n          new_tIdx = tIdx;\n          new_base_addr = base_addr;\n        }\n        else\n        {\n          new_tIdx = tIdx + BLOCK_LENGTH-1;\n          new_base_addr = (block_idx - 1)*blocksize;  \n        }\n        _sol[tx][idx.y][idx.z] = sol_in[new_base_addr + new_tIdx];  \n      }\n\n      if(tx == BLOCK_LENGTH-1)\n      {\n        if(bx == xgridlength-1) // end of the grid\n        {\n          new_tIdx = tIdx;\n          new_base_addr = base_addr;\n        }\n        else\n        {\n          new_tIdx = tIdx - (BLOCK_LENGTH-1);\n          new_base_addr = (block_idx + 1)*blocksize;  \n        }\n        _sol[tx+2][idx.y][idx.z] = sol_in[new_base_addr + new_tIdx];  \n      }\n\n      if(ty == 0)\n      {\n        if(by == 0)\n        {\n          new_tIdx = tIdx;\n          new_base_addr = base_addr;\n        }\n        else\n        {\n          new_tIdx = tIdx + (BLOCK_LENGTH-1)*BLOCK_LENGTH;\n          new_base_addr = (block_idx - xgridlength)*blocksize;\n        }\n        _sol[idx.x][ty][idx.z] = sol_in[new_base_addr + new_tIdx];\n      }\n\n      if(ty == BLOCK_LENGTH-1)\n      {\n        if(by == ygridlength-1) \n        {\n          new_tIdx = tIdx;\n          new_base_addr = base_addr;\n        }\n        else\n        {\n          new_tIdx = tIdx - (BLOCK_LENGTH-1)*BLOCK_LENGTH;\n          new_base_addr = (block_idx + xgridlength)*blocksize;\n        }\n        _sol[idx.x][ty+2][idx.z] = sol_in[new_base_addr + new_tIdx];\n      }\n\n      if(tz == 0)\n      {\n        if(bz == 0)\n        {\n          new_tIdx = tIdx;\n          new_base_addr = base_addr;\n        }\n        else\n        {\n          new_tIdx = tIdx + (BLOCK_LENGTH-1)*BLOCK_LENGTH*BLOCK_LENGTH;\n          new_base_addr = (block_idx - xgridlength*ygridlength)*blocksize;\n        }\n        _sol[idx.x][idx.y][tz] = sol_in[new_base_addr + new_tIdx];\n      }\n\n      if(tz == BLOCK_LENGTH-1)\n      {\n        if(bz == zgridlength-1) \n        {\n          new_tIdx = tIdx;\n          new_base_addr = base_addr;\n        }\n        else\n        {\n          new_tIdx = tIdx - (BLOCK_LENGTH-1)*BLOCK_LENGTH*BLOCK_LENGTH;\n          new_base_addr = (block_idx + xgridlength*ygridlength)*blocksize;\n        }\n        _sol[idx.x][idx.y][tz+2] = sol_in[new_base_addr + new_tIdx];\n      }\n\n      __syncthreads();\n\n\n      DOUBLE a,b,c,oldT,newT;\n\n      //\n      // compute new value\n      //\n      oldT = newT = _sol[idx.x][idx.y][idx.z];\n\n      if(isValid)\n      {\n        a = min(_sol[tx][idx.y][idx.z],_sol[tx+2][idx.y][idx.z]);\n        b = min(_sol[idx.x][ty][idx.z],_sol[idx.x][ty+2][idx.z]);\n        c = min(_sol[idx.x][idx.y][tz],_sol[idx.x][idx.y][tz+2]);\n\n        DOUBLE tmp = (DOUBLE) get_time_eikonal(a, b, c, F);\n        newT = min(tmp,oldT);\n\n        sol_out[base_addr + tIdx] = newT;\n      }\n      // write back to global memory\n      DOUBLE residue = oldT - newT;\n      con[base_addr + tIdx] = (residue < EPS) ? true : false;  \n    }\n  }\n}"
        ]
    },
    "easyWave-cuda": {
        "/Users/gbolet/hecbench-roofline/src/easyWave-cuda/kernels.cuh": [
            "__global__ void\nkernel2(float* node, \n    const float* C1,\n    const float* C2,\n    const float* C3,\n    const float* C4,\n    const int Imin,\n    const int Jmin,\n    const int Imax,\n    const int Jmax,\n    const int NLat,\n    const int NLon )\n{\n  int i, j, m;\n  // open bondary conditions\n  if( Jmin <= 2 ) {\n    for( i=2; i<=(NLon-1); i++ ) {\n      m = idx(1,i);\n      Node(m, iH) = sqrt( pow(Node(m, iN), 2.0f) + \n          0.25f*pow((Node(m, iM)+Node(m-NLat, iM)),2.0f) )*C1[i];\n      if( Node(m, iN) > 0 ) Node(m, iH) = - Node(m, iH);\n    }\n  }\n  if( Imin <= 2 ) {\n    for( j=2; j<=(NLat-1); j++ ) {\n      m = idx(j,1);\n      Node(m, iH) = sqrt( pow(Node(m, iM),2.0f) + \n          0.25f*pow((Node(m, iN)+Node(m-1, iN)),2.0f) )*C2[j];\n      if( Node(m, iM) > 0 ) Node(m, iH) = - Node(m, iH);\n    }\n  }\n  if( Jmax >= (NLat-1) ) {\n    for( i=2; i<=(NLon-1); i++ ) {\n      m = idx(NLat,i);\n      Node(m, iH) = sqrt( pow(Node(m-1, iN),2.0f) + 0.25f*pow((Node(m, iM)+Node(m-1, iM)),2.0f) )*C3[i];\n      if( Node(m-1, iN) < 0 ) Node(m, iH) = - Node(m, iH);\n    }\n  }\n  if( Imax >= (NLon-1) ) {\n    for( j=2; j<=(NLat-1); j++ ) {\n      m = idx(j,NLon);\n      Node(m, iH) = sqrt( pow(Node(m-NLat, iM),2.0f) + 0.25f*pow((Node(m, iN)+Node(m-1, iN)),2.0f) )*C4[j];\n      if( Node(m-NLat, iM) < 0 ) Node(m, iH) = - Node(m, iH);\n    }\n  }\n  if( Jmin <= 2 ) {\n    m = idx(1,1);\n    Node(m, iH) = sqrt( pow(Node(m, iM),2.0f) + pow(Node(m, iN),2.0f) )*C1[1];\n    if( Node(m, iN) > 0 ) Node(m, iH) = - Node(m, iH);\n    m = idx(1,NLon);\n    Node(m, iH) = sqrt( pow(Node(m-NLat, iM),2.0f) + pow(Node(m, iN),2.0f) )*C1[NLon];\n    if( Node(m, iN) > 0 ) Node(m, iH) = - Node(m, iH);\n  }\n  if( Jmin >= (NLat-1) ) {\n    m = idx(NLat,1);\n    Node(m, iH) = sqrt( pow(Node(m, iM),2.0f) + pow(Node(m-1, iN),2.0f) )*C3[1];\n    if( Node(m-1, iN) < 0 ) Node(m, iH) = - Node(m, iH);\n    m = idx(NLat,NLon);\n    Node(m, iH) = sqrt( pow(Node(m-NLat, iM),2.0f) + pow(Node(m-1, iN),2.0f) )*C3[NLon];\n    if( Node(m-1, iN) < 0 ) Node(m, iH) = - Node(m, iH);\n  }\n}",
            "__global__ void\nkernel3(float* node, \n    float* R6, \n    const int Imin,\n    const int Jmin,\n    const int Imax,\n    const int Jmax,\n    const int NLat)\n{\n  int j = blockDim.x*blockIdx.x+threadIdx.x + Jmin;\n  int i = blockDim.y*blockIdx.y+threadIdx.y + Imin;\n  if (i <= Imax && j <= Jmax) {\n    int m = idx(j,i);\n    if( (Node(m, iD)*Node(m+NLat, iD)) != 0 )\n      Node(m, iM) = Node(m, iM) - Node(m, iR2)*(Node(m+NLat, iH)-Node(m, iH));\n\n    if( (Node(m, iD)*Node(m+1, iD)) != 0 )\n      Node(m, iN) = Node(m, iN) - Node(m, iR4)*(Node(m+1, iH)-Node(m, iH));\n  }\n}",
            "inline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\ninline __host__ __device__ float4 fabs(float4 v)\n{\n    return make_float4(fabs(v.x), fabs(v.y), fabs(v.z), fabs(v.w));\n}\n\n__global__ void\nkernel4(float* node, \n    const float* C1,\n    const float* C2,\n    const float* C3,\n    const float* C4,\n    int *Imin,\n    int *Jmin,\n    int *Imax,\n    int *Jmax,\n    const int NLat,\n    const int NLon,\n    const float Par_sshClipThreshold )\n{\n  int i, j, m;\n  int enlarge;\n  if( Jmin[0] <= 2 ) {\n    for( i=1; i<=(NLon-1); i++ ) {\n      m = idx(1,i);\n      Node(m, iM) = Node(m, iM) - Node(m, iR2)*(Node(m+NLat, iH) - Node(m, iH));\n    }\n  }\n  if( Imin[0] <= 2 ) {\n    for( j=1; j<=NLat; j++ ) {\n      m = idx(j,1);\n      Node(m, iM) = Node(m, iM) - Node(m, iR2)*(Node(m+NLat, iH) - Node(m, iH));\n    }\n  }\n  if( Jmax[0] >= (NLat-1) ) {\n    for( i=1; i<=(NLon-1); i++ ) {\n      m = idx(NLat,i);\n      Node(m, iM) = Node(m, iM) - Node(m, iR2)*(Node(m+NLat, iH) - Node(m, iH));\n    }\n  }\n  if( Imin[0] <= 2 ) {\n    for( j=1; j<=(NLat-1); j++ ) {\n      m = idx(j,1);\n      Node(m, iN) = Node(m, iN) - Node(m, iR4)*(Node(m+1, iH) - Node(m, iH));\n    }\n  }\n  if( Jmin[0] <= 2 ) {\n    for( i=1; i<=NLon; i++ ) {\n      m = idx(1,i);\n      Node(m, iN) = Node(m, iN) - Node(m, iR4)*(Node(m+1, iH) - Node(m, iH));\n    }\n  }\n  if( Imax[0] >= (NLon-1) ) {\n    for( j=1; j<=(NLat-1); j++ ) {\n      m = idx(j,NLon);\n      Node(m, iN) = Node(m, iN) - Node(m, iR4)*(Node(m+1, iH) - Node(m, iH));\n    }\n  }\n\n  // calculation area for the next step\n  if( Imin[0] > 2 ) {\n    for( enlarge=0, j=Jmin[0]; j<=Jmax[0]; j++ ) {\n      if( fabs(Node(idx(j,Imin[0]+2), iH)) > Par_sshClipThreshold ) { enlarge = 1; break; }\n    }\n    if( enlarge ) { Imin[0]--; if( Imin[0] < 2 ) Imin[0] = 2; }\n  }\n  if( Imax[0] < (NLon-1) ) {\n    for( enlarge=0, j=Jmin[0]; j<=Jmax[0]; j++ ) {\n      if( fabs(Node(idx(j,Imax[0]-2), iH)) > Par_sshClipThreshold ) { enlarge = 1; break; }\n    }\n    if( enlarge ) { Imax[0]++; if( Imax[0] > (NLon-1) ) Imax[0] = NLon-1; }\n  }\n  if( Jmin[0] > 2 ) {\n    for( enlarge=0, i=Imin[0]; i<=Imax[0]; i++ ) {\n      if( fabs(Node(idx(Jmin[0]+2,i), iH)) > Par_sshClipThreshold ) { enlarge = 1; break; }\n    }\n    if( enlarge ) { Jmin[0]--; if( Jmin[0] < 2 ) Jmin[0] = 2; }\n  }\n  if( Jmax[0] < (NLat-1) ) {\n    for( enlarge=0, i=Imin[0]; i<=Imax[0]; i++ ) {\n      if( fabs(Node(idx(Jmax[0]-2,i), iH)) > Par_sshClipThreshold ) { enlarge = 1; break; }\n    }\n    if( enlarge ) { Jmax[0]++; if( Jmax[0] > (NLat-1) ) Jmax[0] = NLat-1; }\n  }\n}"
        ]
    },
    "sort-cuda": {
        "/Users/gbolet/hecbench-roofline/src/sort-cuda/sort_top_scan.h": [
            "#define T ((int)32)\n\n\n__global__ void\ntop_scan (T* isums, const size_t num_work_groups)\n{\n  __shared__ T lmem[256*2];\n  __shared__ T s_seed;\n  int lid = threadIdx.x;\n  int local_range = blockDim.x;\n\n  if (lid == 0) s_seed = 0; \n  __syncthreads();\n\n  // Decide if this is the last thread that needs to\n  // propagate the seed value\n  int last_thread = (lid < num_work_groups &&\n                    (lid+1) == num_work_groups) ? 1 : 0;\n\n  for (int d = 0; d < 16; d++)\n  {\n    T val = 0;\n    // Load each block's count for digit d\n    if (lid < num_work_groups)\n    {\n      val = isums[(num_work_groups * d) + lid];\n    }\n    // Exclusive scan the counts in local memory\n    // T res = scanLocalMem(val, lmem, 1);\n    int idx = lid;\n    lmem[idx] = 0;\n    idx += local_range;\n    lmem[idx] = val;\n    __syncthreads();\n    for (int i = 1; i < local_range; i *= 2)\n    {\n      T t = lmem[idx -  i]; \n      __syncthreads();\n      lmem[idx] += t;     \n      __syncthreads();\n    }\n    T res = lmem[idx-1];\n\n    // Write scanned value out to global\n    if (lid < num_work_groups)\n    {\n      isums[(num_work_groups * d) + lid] = res + s_seed;\n    }\n    __syncthreads();\n\n    if (last_thread)\n    {\n      s_seed += res + val;\n    }\n    __syncthreads();\n  }\n}"
        ],
        "/Users/gbolet/hecbench-roofline/src/sort-cuda/sort_bottom_scan.h": [
            "#define T ((int)32)\n\n\n__global__ void\nbottom_scan (T* out, const T* in, const T* isums, const size_t size, const unsigned int shift)\n{\n\n  __shared__ T lmem[256*2];\n  __shared__ T l_scanned_seeds[16];\n  __shared__ T l_block_counts[16];\n\n  int group_range = gridDim.x;\n  int group = blockIdx.x;\n  int lid = threadIdx.x;\n  int local_range = blockDim.x;\n\n\n  // Keep a private histogram as well\n  int histogram[16];\n\n  // Prepare for reading 4-element vectors\n  // Assume: divisible by 4\n\n  int n4 = size / 4; //vector type is 4 wide\n\n  int region_size = n4 / group_range;\n  int block_start = group * region_size;\n  // Give the last block any extra elements\n  int block_stop  = (group == group_range - 1) ?\n    n4 : block_start + region_size;\n\n  // Calculate starting index for this thread/work item\n  int i = block_start + lid;\n  int window = block_start;\n\n  // Set the histogram in local memory to zero\n  // and read in the scanned seeds from gmem\n  if (lid < 16)\n  {\n    l_block_counts[lid] = 0;\n    l_scanned_seeds[lid] =\n      isums[(lid*group_range)+group];\n  }\n  __syncthreads();\n\n  // Scan multiple elements per thread\n  while (window < block_stop)\n  {\n    // Reset histogram\n    for (int q = 0; q < 16; q++) histogram[q] = 0;\n    VECTYPE val_4;\n    VECTYPE key_4;\n\n    if (i < block_stop) // Make sure we don't read out of bounds\n    {\n      val_4 = ((VECTYPE*)in)[i];\n\n      // Mask the keys to get the appropriate digit\n      key_4.x = (val_4.x >> shift) & 0xFU;\n      key_4.y = (val_4.y >> shift) & 0xFU;\n      key_4.z = (val_4.z >> shift) & 0xFU;\n      key_4.w = (val_4.w >> shift) & 0xFU;\n\n      // Update the histogram\n      histogram[key_4.x]++;\n      histogram[key_4.y]++;\n      histogram[key_4.z]++;\n      histogram[key_4.w]++;\n    }\n\n    // Scan the digit counts in local memory\n    for (int digit = 0; digit < 16; digit++)\n    {\n      int idx = lid;\n      lmem[idx] = 0;\n      idx += local_range;\n      lmem[idx] = histogram[digit];\n      __syncthreads();\n      for (int i = 1; i < local_range; i *= 2)\n      {\n        T t = lmem[idx -  i]; \n        __syncthreads();\n        lmem[idx] += t;     \n        __syncthreads();\n      }\n      histogram[digit] = lmem[idx-1];\n\n      //histogram[digit] = scanLocalMem(histogram[digit], lmem, 1);\n      __syncthreads();\n    }\n\n    if (i < block_stop) // Make sure we don't write out of bounds\n    {\n      int address;\n      address = histogram[key_4.x] + l_scanned_seeds[key_4.x] + l_block_counts[key_4.x];\n      out[address] = val_4.x;\n      histogram[key_4.x]++;\n\n      address = histogram[key_4.y] + l_scanned_seeds[key_4.y] + l_block_counts[key_4.y];\n      out[address] = val_4.y;\n      histogram[key_4.y]++;\n\n      address = histogram[key_4.z] + l_scanned_seeds[key_4.z] + l_block_counts[key_4.z];\n      out[address] = val_4.z;\n      histogram[key_4.z]++;\n\n      address = histogram[key_4.w] + l_scanned_seeds[key_4.w] + l_block_counts[key_4.w];\n      out[address] = val_4.w;\n      histogram[key_4.w]++;\n    }\n\n    // Before proceeding, make sure everyone has finished their current\n    // indexing computations.\n    __syncthreads();\n    // Now update the seed array.\n    if (lid == local_range-1)\n    {\n      for (int q = 0; q < 16; q++)\n      {\n        l_block_counts[q] += histogram[q];\n      }\n    }\n    __syncthreads();\n\n    // Advance window\n    window += local_range;\n    i += local_range;\n  }\n}"
        ]
    },
    "sosfil-cuda": {
        "/Users/gbolet/hecbench-roofline/src/sosfil-cuda/main.cu": [
            "#define T ((int)32)\n\n\n__global__ void sosfilt( \n    const int n_signals,\n    const int n_samples,\n    const int n_sections,\n    const int zi_width,\n    const T *__restrict__ sos,\n    const T *__restrict__ zi,\n          T *__restrict__ x_in)\n{\n  extern __shared__ char smem[];\n  T *s_out = reinterpret_cast<T *>( smem );\n  T *s_zi = reinterpret_cast<T *>( &s_out[n_sections] ) ;\n  T *s_sos = reinterpret_cast<T *>( &s_zi[n_sections * zi_width] ) ;\n\n  // dim3 blocksPerGrid (1, blocks);\n  // dim3 threadsPerBlock (256, 1);\n  const int tx = static_cast<int>( threadIdx.x ) ;\n  const int ty = static_cast<int>( blockIdx.y * blockDim.y + threadIdx.y ) ;\n\n  // Reset shared memory\n  s_out[tx] = 0;\n\n  // Load zi\n  for ( int i = 0; i < zi_width; i++ ) {\n    s_zi[tx * zi_width + i] = zi[ty * n_sections * zi_width + tx * zi_width + i];\n  }\n\n  // Load SOS\n#pragma unroll \n  for ( int i = 0; i < sos_width; i++ ) {\n    s_sos[tx * sos_width + i] = sos[tx * sos_width + i];\n  }\n\n  __syncthreads( );\n\n  const int load_size = n_sections - 1 ;\n  const int unload_size = n_samples - load_size ;\n\n  T temp;\n  T x_n;\n\n  if ( ty < n_signals ) {\n    // Loading phase\n    for ( int n = 0; n < load_size; n++ ) {\n      if ( tx == 0 ) {\n        x_n = x_in[ty * n_samples + n];\n      } else {\n        x_n = s_out[tx - 1];\n      }\n\n      // Use direct II transposed structure\n      temp = s_sos[tx * sos_width + 0] * x_n + s_zi[tx * zi_width + 0];\n\n      s_zi[tx * zi_width + 0] =\n        s_sos[tx * sos_width + 1] * x_n - s_sos[tx * sos_width + 4] * temp + s_zi[tx * zi_width + 1];\n\n      s_zi[tx * zi_width + 1] = s_sos[tx * sos_width + 2] * x_n - s_sos[tx * sos_width + 5] * temp;\n\n      s_out[tx] = temp;\n\n      __syncthreads( );\n    }\n\n    // Processing phase\n    for ( int n = load_size; n < n_samples; n++ ) {\n      if ( tx == 0 ) {\n        x_n = x_in[ty * n_samples + n];\n      } else {\n        x_n = s_out[tx - 1];\n      }\n\n      // Use direct II transposed structure\n      temp = s_sos[tx * sos_width + 0] * x_n + s_zi[tx * zi_width + 0];\n\n      s_zi[tx * zi_width + 0] =\n        s_sos[tx * sos_width + 1] * x_n - s_sos[tx * sos_width + 4] * temp + s_zi[tx * zi_width + 1];\n\n      s_zi[tx * zi_width + 1] = s_sos[tx * sos_width + 2] * x_n - s_sos[tx * sos_width + 5] * temp;\n\n      if ( tx < load_size ) {\n        s_out[tx] = temp;\n      } else {\n        x_in[ty * n_samples + ( n - load_size )] = temp;\n      }\n\n      __syncthreads( );\n    }\n\n    // Unloading phase\n    for ( int n = 0; n < n_sections; n++ ) {\n      // retire threads that are less than n\n      if ( tx > n ) {\n        x_n = s_out[tx - 1];\n\n        // Use direct II transposed structure\n        temp = s_sos[tx * sos_width + 0] * x_n + s_zi[tx * zi_width + 0];\n\n        s_zi[tx * zi_width + 0] =\n          s_sos[tx * sos_width + 1] * x_n - s_sos[tx * sos_width + 4] * temp + s_zi[tx * zi_width + 1];\n\n        s_zi[tx * zi_width + 1] = s_sos[tx * sos_width + 2] * x_n - s_sos[tx * sos_width + 5] * temp;\n\n        if ( tx < load_size ) {\n          s_out[tx] = temp;\n        } else {\n          x_in[ty * n_samples + ( n + unload_size )] = temp;\n        }\n      }\n      __syncthreads( );\n    }\n  }\n}"
        ]
    },
    "lebesgue-cuda": {
        "/Users/gbolet/hecbench-roofline/src/lebesgue-cuda/kernels.cu": [
            "inline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\ninline __host__ __device__ float4 fabs(float4 v)\n{\n    return make_float4(fabs(v.x), fabs(v.y), fabs(v.z), fabs(v.w));\n}\n\n__device__ __forceinline__\ndouble atomicMax(double *address, double val)\n{\n  unsigned long long ret = __double_as_longlong(*address);\n  while(val > __longlong_as_double(ret))\n  {\n    unsigned long long old = ret;\n    if((ret = atomicCAS((unsigned long long *)address, old, __double_as_longlong(val))) == old)\n      break;\n  }\n  return __longlong_as_double(ret);\n}\n\n__global__\nvoid kernel (double *__restrict__ lmax,\n             double *__restrict__ linterp,\n             const double *__restrict__ xfun, \n             const double *__restrict__ x,\n             const int n, const int nfun) \n{\n  int j = blockIdx.x * blockDim.x + threadIdx.x;\n  if (j >= nfun) return;\n  for (int i = 0; i < n; i++ )\n    linterp[i*nfun+j] = 1.0;\n\n  for (int i1 = 0; i1 < n; i1++ )\n    for (int i2 = 0; i2 < n; i2++ )\n      if ( i1 != i2 )\n        linterp[i1*nfun+j] = linterp[i1*nfun+j] * ( xfun[j] - x[i2] ) / ( x[i1] - x[i2] );\n\n  double t = 0.0;\n  for (int i = 0; i < n; i++ )\n    t += fabs ( linterp[i*nfun+j] );\n\n  atomicMax(lmax, t);\n}"
        ]
    },
    "lombscargle-cuda": {
        "/Users/gbolet/hecbench-roofline/src/lombscargle-cuda/main.cu": [
            "__global__ void       \nlombscargle( const int x_shape,\n    const int freqs_shape,\n    const float *__restrict__ x,\n    const float *__restrict__ y,\n    const float *__restrict__ freqs,\n    float *__restrict__ pgram,\n    const float y_dot )\n{\n  const int tx  = ( blockIdx.x * blockDim.x + threadIdx.x ) ;\n  const int stride = ( blockDim.x * gridDim.x ) ;\n\n  for ( int tid = tx; tid < freqs_shape; tid += stride ) {\n\n    float freq = freqs[tid] ;\n\n    float xc = 0;\n    float xs = 0;\n    float cc = 0;\n    float ss = 0;\n    float cs = 0;\n    float c;\n    float s; \n\n    for ( int j = 0; j < x_shape; j++ ) {\n      sincosf( freq * x[j], &s, &c );\n      xc += y[j] * c;\n      xs += y[j] * s;\n      cc += c * c;\n      ss += s * s;\n      cs += c * s;\n    }\n\n    float c_tau;\n    float s_tau;\n    float tau = atan2f( 2.0f * cs, cc - ss ) / ( 2.0f * freq ) ;\n    sincosf( freq * tau, &s_tau, &c_tau );\n    float c_tau2 = c_tau * c_tau ;\n    float s_tau2 = s_tau * s_tau ;\n    float cs_tau = 2.0f * c_tau * s_tau ;\n\n    pgram[tid] = ( 0.5f * ( ( ( c_tau * xc + s_tau * xs ) * ( c_tau * xc + s_tau * xs ) /\n            ( c_tau2 * cc + cs_tau * cs + s_tau2 * ss ) ) +\n          ( ( c_tau * xs - s_tau * xc ) * ( c_tau * xs - s_tau * xc ) /\n            ( c_tau2 * ss - cs_tau * cs + s_tau2 * cc ) ) ) ) * y_dot;\n  }\n}"
        ]
    },
    "ace-cuda": {
        "/Users/gbolet/hecbench-roofline/src/ace-cuda/main.cu": [
            "__device__\ndouble An(double phix, double phiy, double phiz, double epsilon)\n{\n  if (phix != 0.0 || phiy != 0.0 || phiz != 0.0){\n    return ((1.0 - 3.0 * epsilon) * (1.0 + (((4.0 * epsilon) / (1.0-3.0*epsilon))*\n           ((SQ(phix)*SQ(phix)+SQ(phiy)*SQ(phiy)+SQ(phiz)*SQ(phiz)) /\n           ((SQ(phix)+SQ(phiy)+SQ(phiz))*(SQ(phix)+SQ(phiy)+SQ(phiz)))))));\n  }\n  else\n  {\n    return (1.0-((5.0/3.0)*epsilon));\n  }\n}\n\n__device__ double GradientX(const double phi[][DATAYSIZE][DATAXSIZE], \n                            double dx, double dy, double dz, int x, int y, int z)\n{\n  int nx = (int)DATAXSIZE - 1;\n  int xp = x+1;\n  int xn = x-1;\n\n  if (xp > nx) xp = 0;\n  if (xn < 0)  xn = nx;\n\n  return (phi[z][y][xp] - phi[z][y][xn]) / (2.0*dx);\n}\n\n__device__ double GradientY(const double phi[][DATAYSIZE][DATAXSIZE], \n                            double dx, double dy, double dz, int x, int y, int z)\n{\n  int ny = (int)DATAYSIZE - 1;\n  int yp = y+1;\n  int yn = y-1;\n\n  if (yp > ny) yp = 0;\n  if (yn < 0)  yn = ny;\n\n  return (phi[z][yp][x] - phi[z][yn][x]) / (2.0*dy);\n}\n\n__device__ double GradientZ(const double phi[][DATAYSIZE][DATAXSIZE],\n                            double dx, double dy, double dz, int x, int y, int z)\n{\n  int nz = (int)DATAZSIZE - 1;\n  int zp = z+1;\n  int zn = z-1;\n\n  if (zp > nz) zp = 0;\n  if (zn < 0)  zn = nz;\n\n  return (phi[zp][y][x] - phi[zn][y][x]) / (2.0*dz);\n}\n\n__device__\ndouble Wn(double phix, double phiy, double phiz, double epsilon, double W0)\n{\n  return (W0*An(phix,phiy,phiz,epsilon));\n}\n\n__device__\ndouble dFunc(double l, double m, double n)\n{\n  if (l != 0.0 || m != 0.0 || n != 0.0){\n    return (((l*l*l*(SQ(m)+SQ(n)))-(l*(SQ(m)*SQ(m)+SQ(n)*SQ(n)))) /\n            ((SQ(l)+SQ(m)+SQ(n))*(SQ(l)+SQ(m)+SQ(n))));\n  }\n  else\n  {\n    return 0.0;\n  }\n}\n\n__global__\nvoid calculateForce(double phi[][DATAYSIZE][DATAXSIZE], \n                    double Fx[][DATAYSIZE][DATAXSIZE],\n                    double Fy[][DATAYSIZE][DATAXSIZE],\n                    double Fz[][DATAYSIZE][DATAXSIZE],\n                    double dx, double dy, double dz,\n                    double epsilon, double W0, double tau0)\n{\n\n  unsigned iz = blockIdx.x*blockDim.x + threadIdx.x;\n  unsigned iy = blockIdx.y*blockDim.y + threadIdx.y;\n  unsigned ix = blockIdx.z*blockDim.z + threadIdx.z;\n\n  if ((ix < (DATAXSIZE-1)) && (iy < (DATAYSIZE-1)) && \n      (iz < (DATAZSIZE-1)) && (ix > (0)) && \n      (iy > (0)) && (iz > (0))) {\n\n    double phix = GradientX(phi,dx,dy,dz,ix,iy,iz);\n    double phiy = GradientY(phi,dx,dy,dz,ix,iy,iz);\n    double phiz = GradientZ(phi,dx,dy,dz,ix,iy,iz);\n    double sqGphi = SQ(phix) + SQ(phiy) + SQ(phiz);\n    double c = 16.0 * W0 * epsilon;\n    double w = Wn(phix,phiy,phiz,epsilon,W0);\n    double w2 = SQ(w);\n    \n\n    Fx[ix][iy][iz] = w2 * phix + sqGphi * w * c * dFunc(phix,phiy,phiz);\n    Fy[ix][iy][iz] = w2 * phiy + sqGphi * w * c * dFunc(phiy,phiz,phix);\n    Fz[ix][iy][iz] = w2 * phiz + sqGphi * w * c * dFunc(phiz,phix,phiy);\n  }\n  else\n  {\n    Fx[ix][iy][iz] = 0.0;\n    Fy[ix][iy][iz] = 0.0;\n    Fz[ix][iy][iz] = 0.0;\n  }\n\n}",
            "__device__ double GradientX(const double phi[][DATAYSIZE][DATAXSIZE], \n                            double dx, double dy, double dz, int x, int y, int z)\n{\n  int nx = (int)DATAXSIZE - 1;\n  int xp = x+1;\n  int xn = x-1;\n\n  if (xp > nx) xp = 0;\n  if (xn < 0)  xn = nx;\n\n  return (phi[z][y][xp] - phi[z][y][xn]) / (2.0*dx);\n}\n\n__device__ double GradientY(const double phi[][DATAYSIZE][DATAXSIZE], \n                            double dx, double dy, double dz, int x, int y, int z)\n{\n  int ny = (int)DATAYSIZE - 1;\n  int yp = y+1;\n  int yn = y-1;\n\n  if (yp > ny) yp = 0;\n  if (yn < 0)  yn = ny;\n\n  return (phi[z][yp][x] - phi[z][yn][x]) / (2.0*dy);\n}\n\n__device__ double GradientZ(const double phi[][DATAYSIZE][DATAXSIZE],\n                            double dx, double dy, double dz, int x, int y, int z)\n{\n  int nz = (int)DATAZSIZE - 1;\n  int zp = z+1;\n  int zn = z-1;\n\n  if (zp > nz) zp = 0;\n  if (zn < 0)  zn = nz;\n\n  return (phi[zp][y][x] - phi[zn][y][x]) / (2.0*dz);\n}\n\n__device__\ndouble An(double phix, double phiy, double phiz, double epsilon)\n{\n  if (phix != 0.0 || phiy != 0.0 || phiz != 0.0){\n    return ((1.0 - 3.0 * epsilon) * (1.0 + (((4.0 * epsilon) / (1.0-3.0*epsilon))*\n           ((SQ(phix)*SQ(phix)+SQ(phiy)*SQ(phiy)+SQ(phiz)*SQ(phiz)) /\n           ((SQ(phix)+SQ(phiy)+SQ(phiz))*(SQ(phix)+SQ(phiy)+SQ(phiz)))))));\n  }\n  else\n  {\n    return (1.0-((5.0/3.0)*epsilon));\n  }\n}\n\n__device__\ndouble Divergence(double phix[][DATAYSIZE][DATAXSIZE], \n                  double phiy[][DATAYSIZE][DATAXSIZE],\n                  double phiz[][DATAYSIZE][DATAXSIZE], \n                  double dx, double dy, double dz, int x, int y, int z)\n{\n  return GradientX(phix,dx,dy,dz,x,y,z) + \n         GradientY(phiy,dx,dy,dz,x,y,z) +\n         GradientZ(phiz,dx,dy,dz,x,y,z);\n}\n\n__device__\ndouble dFphi(double phi, double u, double lambda)\n{\n  return (-phi*(1.0-phi*phi)+lambda*u*(1.0-phi*phi)*(1.0-phi*phi));\n}\n\n__device__\ndouble taun(double phix, double phiy, double phiz, double epsilon, double tau0)\n{\n  return tau0 * SQ(An(phix,phiy,phiz,epsilon));\n}\n\n__global__\nvoid allenCahn(double phinew[][DATAYSIZE][DATAXSIZE], \n               double phiold[][DATAYSIZE][DATAXSIZE],\n               double uold[][DATAYSIZE][DATAXSIZE],\n               double Fx[][DATAYSIZE][DATAXSIZE],\n               double Fy[][DATAYSIZE][DATAXSIZE],\n               double Fz[][DATAYSIZE][DATAXSIZE],\n               double epsilon, double W0, double tau0, double lambda,\n               double dt, double dx, double dy, double dz)\n{\n  unsigned iz = blockIdx.x*blockDim.x + threadIdx.x;\n  unsigned iy = blockIdx.y*blockDim.y + threadIdx.y;\n  unsigned ix = blockIdx.z*blockDim.z + threadIdx.z;\n\n  if ((ix < (DATAXSIZE-1)) && (iy < (DATAYSIZE-1)) && \n      (iz < (DATAZSIZE-1)) && (ix > (0)) && \n      (iy > (0)) && (iz > (0))) {\n\n    double phix = GradientX(phiold,dx,dy,dz,ix,iy,iz);\n    double phiy = GradientY(phiold,dx,dy,dz,ix,iy,iz);\n    double phiz = GradientZ(phiold,dx,dy,dz,ix,iy,iz); \n\n    phinew[ix][iy][iz] = phiold[ix][iy][iz] + \n     (dt / taun(phix,phiy,phiz,epsilon,tau0)) * \n     (Divergence(Fx,Fy,Fz,dx,dy,dz,ix,iy,iz) - \n      dFphi(phiold[ix][iy][iz], uold[ix][iy][iz],lambda));\n  }\n}",
            "__global__\nvoid boundaryConditionsPhi(double phinew[][DATAYSIZE][DATAXSIZE])\n{\n  unsigned iz = blockIdx.x*blockDim.x + threadIdx.x;\n  unsigned iy = blockIdx.y*blockDim.y + threadIdx.y;\n  unsigned ix = blockIdx.z*blockDim.z + threadIdx.z;\n\n  if (ix == 0){\n    phinew[ix][iy][iz] = -1.0;\n  }\n  else if (ix == DATAXSIZE-1){\n    phinew[ix][iy][iz] = -1.0;\n  }\n  else if (iy == 0){\n    phinew[ix][iy][iz] = -1.0;\n  }\n  else if (iy == DATAYSIZE-1){\n    phinew[ix][iy][iz] = -1.0;\n  }\n  else if (iz == 0){\n    phinew[ix][iy][iz] = -1.0;\n  }\n  else if (iz == DATAZSIZE-1){\n    phinew[ix][iy][iz] = -1.0;\n  }\n}",
            "__device__ double Laplacian(const double c[][DATAYSIZE][DATAXSIZE],\n                            double dx, double dy, double dz, int x, int y, int z)\n{\n  int xp, xn, yp, yn, zp, zn;\n\n  int nx = (int)DATAXSIZE - 1;\n  int ny = (int)DATAYSIZE - 1;\n  int nz = (int)DATAZSIZE - 1;\n\n  xp = x+1;\n  xn = x-1;\n  yp = y+1;\n  yn = y-1;\n  zp = z+1;\n  zn = z-1;\n\n  if (xp > nx) xp = 0;\n  if (yp > ny) yp = 0;\n  if (zp > nz) zp = 0;\n  if (xn < 0)  xn = nx;\n  if (yn < 0)  yn = ny;\n  if (zn < 0)  zn = nz;\n\n  double cxx = (c[z][y][xp] + c[z][y][xn] - 2.0*c[z][y][x]) / (dx*dx);\n  double cyy = (c[z][yp][x] + c[z][yn][x] - 2.0*c[z][y][x]) / (dy*dy);\n  double czz = (c[zp][y][x] + c[zn][y][x] - 2.0*c[z][y][x]) / (dz*dz);\n\n  return cxx + cyy + czz;\n}\n\n__global__\nvoid thermalEquation(double unew[][DATAYSIZE][DATAXSIZE],\n                     double uold[][DATAYSIZE][DATAXSIZE],\n                     double phinew[][DATAYSIZE][DATAXSIZE],\n                     double phiold[][DATAYSIZE][DATAXSIZE],\n                     double D, double dt, double dx, double dy, double dz)\n{\n  unsigned iz = blockIdx.x*blockDim.x + threadIdx.x;\n  unsigned iy = blockIdx.y*blockDim.y + threadIdx.y;\n  unsigned ix = blockIdx.z*blockDim.z + threadIdx.z;\n\n  if ((ix < (DATAXSIZE-1)) && (iy < (DATAYSIZE-1)) && \n      (iz < (DATAZSIZE-1)) && (ix > (0)) && \n      (iy > (0)) && (iz > (0))){\n    unew[ix][iy][iz] = uold[ix][iy][iz] + \n      0.5*(phinew[ix][iy][iz]- phiold[ix][iy][iz]) +\n      dt * D * Laplacian(uold,dx,dy,dz,ix,iy,iz);\n  }\n}",
            "__global__\nvoid boundaryConditionsU(double unew[][DATAYSIZE][DATAXSIZE], double delta)\n{\n  unsigned iz = blockIdx.x*blockDim.x + threadIdx.x;\n  unsigned iy = blockIdx.y*blockDim.y + threadIdx.y;\n  unsigned ix = blockIdx.z*blockDim.z + threadIdx.z;\n\n  if (ix == 0){\n    unew[ix][iy][iz] =  -delta;\n  }\n  else if (ix == DATAXSIZE-1){\n    unew[ix][iy][iz] =  -delta;\n  }\n  else if (iy == 0){\n    unew[ix][iy][iz] =  -delta;\n  }\n  else if (iy == DATAYSIZE-1){\n    unew[ix][iy][iz] =  -delta;\n  }\n  else if (iz == 0){\n    unew[ix][iy][iz] =  -delta;\n  }\n  else if (iz == DATAZSIZE-1){\n    unew[ix][iy][iz] =  -delta;\n  }\n}",
            "__global__\nvoid swapGrid(double cnew[][DATAYSIZE][DATAXSIZE],\n              double cold[][DATAYSIZE][DATAXSIZE])\n{\n  unsigned iz = blockIdx.x*blockDim.x + threadIdx.x;\n  unsigned iy = blockIdx.y*blockDim.y + threadIdx.y;\n  unsigned ix = blockIdx.z*blockDim.z + threadIdx.z;\n\n  if ((ix < (DATAXSIZE)) && \n      (iy < (DATAYSIZE)) &&\n      (iz < (DATAZSIZE))) {\n    double tmp = cnew[ix][iy][iz];\n    cnew[ix][iy][iz] = cold[ix][iy][iz];\n    cold[ix][iy][iz] = tmp;\n  }\n}"
        ]
    },
    "gerbil-cuda": {
        "/Users/gbolet/hecbench-roofline/src/gerbil-cuda/src/cuda_ds/AddKernel.cu": [
            "inline __host__ __device__ uint4 make_uint4(int4 a)\n{\n    return make_uint4(uint(a.x), uint(a.y), uint(a.z), uint(a.w));\n}\n\ninline __host__ __device__ uint4 min(uint4 a, uint4 b)\n{\n    return make_uint4(min(a.x,b.x), min(a.y,b.y), min(a.z,b.z), min(a.w,b.w));\n}\n\n__global__ void addKernel(\n\t\tvolatile KeyValuePair<intsPerKey>* table,\t// pointer to hash table\n\t\tconst uint64_t numEntries,\t\t\t// number of entries in hash table\n\t\tconst Key<intsPerKey>* keyBundle,\t// pointer to keys which are to be inserted\n\t\tconst uint64_t numKeys,\t\t\t\t// number of keys in key bundle\n\t\tKey<intsPerKey>* noSuccessArea,\t\t// pointer to area where keys are copied to when they\n\t\t\t\t\t\t\t\t\t\t\t// cannot be inserted after MAXTRIALS trials\n\t\tuint64_t* numNoSuccessPtr,\t\t\t// number of unsuccessfully inserted keys\n\t\tconst uint64_t maxNumNoSuccess\t\t// maximal number of keys in noSuccessArea\n\t\t) {\n\n\t/**************************************************************************\n\t * Declare Variables\n\t *************************************************************************/\n\n\t// sizes of structures, etc.\n\tconst uint32_t intsPerEntry = intsPerKey + 1;\n\tconst uint32_t entriesPerBlock = blocksize / intsPerEntry;\n\tconst uint32_t counterPosition = intsPerKey;\n\n\t// integer-wise interpretation of hash table and kmerBundle\n\tvolatile uint32_t* tableInts = (volatile uint32_t*) table;\n\tconst uint32_t* keyBundleInts = (const uint32_t*) keyBundle;\n\n\t// Thread-Indices, etc.\n\tconst uint32_t blockID = blockIdx.x;\n\tconst uint32_t threadID = threadIdx.x;\n\tconst uint32_t subBlockID = threadID % intsPerEntry;\n\tconst uint32_t subBlock = threadID / intsPerEntry;\n\n\t// shared variables and pointers to it\n\t__shared__ uint32_t sharedData[2 + 2 * entriesPerBlock\n\t\t\t+ intsPerKey * keysPerBlock];\n\tvolatile uint32_t* success = &sharedData[0];\n\tuint32_t* matchOffset = &sharedData[1];\n\tuint32_t* matchStatus = &sharedData[2];\n\tuint32_t* lockStatus = &sharedData[2 + entriesPerBlock];\n\tuint32_t* keyInts = &sharedData[2 + 2 * entriesPerBlock];\n\n\t// Local Variables\n\tuint32_t x;\n\tuint32_t basePosition;\n\tuint32_t numEntriesLoaded, probingOffset, numTrials, i;\n\tuint32_t currentKey;\n\tvolatile uint32_t* addr;\n\tKey<intsPerKey>* key;\n\n\t/**********************************************************************\n\t * Load kmers into shared memory\n\t *********************************************************************/\n\ti = threadID;\n\twhile (i < intsPerKey * keysPerBlock && blockID * keysPerBlock * intsPerKey + i < numKeys * intsPerKey) {\n\t\tkeyInts[i] = keyBundleInts[intsPerKey * blockID * keysPerBlock + i];\n#ifdef DEBUG\n\t\tprintf(\"blockID=%i threadID=%i loading kmer %i, i=%i, bundlesize=%i\\n\",\n\t\t\t\tblockID, threadID, keyInts[i], i, numKeys);\n#endif\n\t\ti += blocksize;\n\t}\n\n\t/************************************************************************\n\t * Try to add keys into hash table\n\t ***********************************************************************/\n\n\t// for each key in this bundle\n\tfor (currentKey = 0;\n\t\t\tcurrentKey < keysPerBlock\n\t\t\t\t\t&& blockID * keysPerBlock + currentKey < numKeys;\n\t\t\tcurrentKey++) {\n\n\t\t// Kmer-View on the data in shared memory\n\t\tkey = (Key<intsPerKey>*) &keyInts[currentKey * intsPerKey];\n\n\t\tif (threadID == 0)\n\t\t\t*success = false;\n\n\t\t/**************************************************************************\n\t\t * Try to add currentKey to hash table\n\t\t *************************************************************************/\n\t\tfor(numTrials = 0; numTrials < MAXTRIALS && !(*success); numTrials++) {\n\n\t\t\t// determine probing position\n\t\t\tbasePosition = key->hash(numTrials) % numEntries;\n\n#ifdef DEBUG\n\t\t\tprintf(\n\t\t\t\t\t\"blockID=%i, threadID=%i, currentKeyCounter=%i, key[0]=%i, basePosition=%u\\n\",\n\t\t\t\t\tblockID, threadID, currentKey, *((int*) key), basePosition);\n#endif\n\n\t\t\t// number of entries probed in this trial\n\t\t\tnumEntriesLoaded = min(entriesPerBlock, (uint32_t) (numEntries - basePosition));\n\n\t\t\t// thread-own probing address\n\t\t\taddr = &tableInts[basePosition * intsPerEntry + threadID];\n\n\t\t\t// init flags\n\t\t\tif (threadID < entriesPerBlock) {\n\t\t\t\tmatchStatus[threadID] = true;\n\t\t\t}\n\n\t\t\tif (threadID == 0)\n\t\t\t\t*matchOffset = (uint32_t) -1;\n\n#ifdef DEBUG\n\t\t\tprintf(\n\t\t\t\t\t\"blockID=%i, threadID=%i, currentKeyCounter=%i, success=%i, numTrials=%i, basePosition=%i, numEntriesLoaded=%i\\n\",\n\t\t\t\t\tblockID, threadID, currentKey, *success, numTrials,\n\t\t\t\t\tbasePosition, numEntriesLoaded);\n#endif\n\n\t\t\t// almost all threads participate\n\t\t\tif (threadID < intsPerEntry * numEntriesLoaded) {\n\n\t\t\t\t// Coalesced reading of a stripe of memory from hash table:\n\t\t\t\t// Each Thread reads one integer starting at basePosition\n\t\t\t\tx = *addr;\n\n#ifdef DEBUG\n\t\t\t\tprintf(\n\t\t\t\t\t\t\"blockID=%i, threadID=%i, subwarp=%i, subwarpID=%i: x=%i, subwarpFlags=%i, trial=%i, currentKey=%i\\n\",\n\t\t\t\t\t\tblockID, threadID, subBlock, subBlockID, x,\n\t\t\t\t\t\tmatchStatus[subBlock], numTrials, currentKey);\n#endif\n\n\t\t\t\t/**************************************************************\n\t\t\t\t * See if any position matches currentKey\n\t\t\t\t *************************************************************/\n\n\t\t\t\t// parallel int-wise comparision with shared kmer\n\t\t\t\tif (subBlockID < counterPosition) {\n\t\t\t\t\tif (x != keyInts[currentKey * intsPerKey + subBlockID])\n\t\t\t\t\t\tmatchStatus[subBlock] = false;\n\t\t\t\t}\n\t\t\t\t// the last thread of each subwarp checks the counter\n\t\t\t\telse {\n\n\t\t\t\t\t// determine lock status\n\t\t\t\t\tif (x == 0)\n\t\t\t\t\t\tlockStatus[subBlock] = STATUS_FREE;\n\t\t\t\t\telse if (x == 0xffffffff)\n\t\t\t\t\t\tlockStatus[subBlock] = STATUS_LOCKED;\n\t\t\t\t\telse\n\t\t\t\t\t\tlockStatus[subBlock] = STATUS_OCCUPIED;\n\t\t\t\t}\n\n\t\t\t\t// if match has been found\n\t\t\t\tif (subBlockID\n\t\t\t\t\t\t== 0 && matchStatus[subBlock] && lockStatus[subBlock] == STATUS_OCCUPIED) {\n\t\t\t\t\t*matchOffset = subBlock;\n\t\t\t\t}\n\n\n#ifdef DEBUG\n\t\t\t\tprintf(\n\t\t\t\t\t\t\"blockID=%i, threadID=%i, subwarp=%i, subwarpID=%i: matchOffset=%i\\n\",\n\t\t\t\t\t\tblockID, threadID, subBlock, subBlockID, *matchOffset);\n#endif\n\n\t\t\t\t/**************************************************************\n\t\t\t\t * If matching kmer has been found: increase counter\n\t\t\t\t *************************************************************/\n\n\t\t\t\t// found existing entry in hash table\n\t\t\t\tif (*matchOffset != (uint32_t) -1) {\n\n\t\t\t\t\t// increase counter\n\t\t\t\t\tif (subBlock == *matchOffset\n\t\t\t\t\t\t\t&& subBlockID == counterPosition) {\n\n\t\t\t\t\t\tatomicAdd((uint32_t*) addr, 1);\n\t\t\t\t\t\t//*addr = *addr + 1;\n#ifdef DEBUG\n\t\t\t\t\t\tprintf(\n\t\t\t\t\t\t\t\t\"blockID = %i: found match at offset = %i (addr=%p)\\n\",\n\t\t\t\t\t\t\t\tblockID, *matchOffset, addr);\n#endif\n\n\t\t\t\t\t\t*success = true;\n\t\t\t\t\t}\n\t\t\t\t}\n#ifdef DEBUG\n\t\t\t\telse {\n\t\t\t\t\tif (threadID == 0)\n\t\t\t\t\tprintf(\"blockID = %i: no match found\\n\", blockID);\n\t\t\t\t}\n#endif\n\n\t\t\t\t/**************************************************************\n\t\t\t\t * If no matching position has been found:\n\t\t\t\t * Try to add entry to hash table by linear probing of the\n\t\t\t\t * scanned part.\n\t\t\t\t **************************************************************/\n\t\t\t\tfor(probingOffset = 0; probingOffset < numEntriesLoaded && !(*success); probingOffset++) {\n\n#ifdef DEBUG\n\t\t\t\t\tif (threadID == 0) {\n\t\t\t\t\t\tprintf(\"blockID=%i: probing position %i\\n\", blockID,\n\t\t\t\t\t\t\t\tbasePosition + probingOffset);\n\t\t\t\t\t}\n#endif\n\n\t\t\t\t\t//__threadfence();\n\n\t\t\t\t\t// wait until entry becomes unlocked\n\t\t\t\t\twhile (lockStatus[probingOffset] == STATUS_LOCKED) {\n\t\t\t\t\t\t\n\t\t\t\t\t\t// re-load table entries from global memory\n\t\t\t\t\t\tx = *addr;\n\n\t\t\t\t\t\t// init flags\n\t\t\t\t\t\tif (subBlockID == counterPosition)\n\t\t\t\t\t\t\tmatchStatus[subBlock] = true;\n\n#ifdef DEBUG\n\t\t\t\t\t\tprintf(\n\t\t\t\t\t\t\t\t\"blockID=%i, threadID=%i, subwarp=%i, subwarpID=%i: re-loading table address: %p, x=%i, subwarpFlags=%i, trial=%i, currentKey=%i\\n\",\n\t\t\t\t\t\t\t\tblockID, threadID, subBlock, subBlockID, addr, x,\n\t\t\t\t\t\t\t\tmatchStatus[subBlock], numTrials, currentKey);\n#endif\n\n\t\t\t\t\t\t// parallel int-wise comparision with shared kmer\n\t\t\t\t\t\tif (subBlockID < counterPosition) {\n\t\t\t\t\t\t\tif (x != keyInts[currentKey * intsPerKey\n\t\t\t\t\t\t\t\t\t\t\t+ subBlockID])\n\t\t\t\t\t\t\t\tmatchStatus[subBlock] = false;\n\t\t\t\t\t\t}\n\t\t\t\t\t\t// the last thread of each subwarp checks the counter\n\t\t\t\t\t\telse {\n\t\t\t\t\t\t\t// determine lock status\n\t\t\t\t\t\t\tif (x == 0)\n\t\t\t\t\t\t\t\tlockStatus[subBlock] = STATUS_FREE;\n\t\t\t\t\t\t\telse if (x == 0xffffffff)\n\t\t\t\t\t\t\t\tlockStatus[subBlock] = STATUS_LOCKED;\n\t\t\t\t\t\t\telse\n\t\t\t\t\t\t\t\tlockStatus[subBlock] = STATUS_OCCUPIED;\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\n\t\t\t\t\t// entry is now free or occupied\n\n\t\t\t\t\t// entry is occupied by...\n\t\t\t\t\tif (lockStatus[probingOffset] == STATUS_OCCUPIED) {\n\n\t\t\t\t\t\t// ...matching key\n\t\t\t\t\t\tif(matchStatus[probingOffset]) {\n\n\t\t\t\t\t\t\t// increase counter by 1\n\t\t\t\t\t\t\tif (subBlock == probingOffset && subBlockID == counterPosition) {\n\n\t\t\t\t\t\t\t\tatomicAdd((uint32_t*) addr, 1);\n\n#ifdef DEBUG\n                \t            printf(\n           \t                 \t\t\"blockID = %i: found match at position = %i while reprobing\\n\",\n                                     blockID, basePosition + probingOffset);\n#endif\n\n\t\t\t\t\t\t\t\t*success = true;\n \t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t\t// ...non-matching key\n\t\t\t\t\t\telse {\n\t\t\t\t\t\t\t// probe next position\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\n\t\t\t\t\t// if entry is free\n\t\t\t\t\telse if (lockStatus[probingOffset] == STATUS_FREE) {\n\n\t\t\t\t\t\t//  try to lock entry\n\t\t\t\t\t\tif (subBlock == probingOffset\n\t\t\t\t\t\t\t\t&& subBlockID == counterPosition) {\n\n\t\t\t\t\t\t\tif (atomicCAS((uint32_t*) addr, 0, 0xffffffff) == 0) {\n\t\t\t\t\t\t\t\tlockStatus[probingOffset] = STATUS_OCCUPIED;\n\t\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\t\tlockStatus[probingOffset] = STATUS_LOCKED;\n\t\t\t\t\t\t\t}\n#ifdef DEBUG\n\t\t\t\t\t\t\t// if lock was successful\n\t\t\t\t\t\t\tif (lockStatus[probingOffset] == STATUS_OCCUPIED) {\n\t\t\t\t\t\t\t\tprintf(\n\t\t\t\t\t\t\t\t\t\t\"blockID=%i, threadID=%i, subwarp=%i, subwarpID=%i, currentKeyCounter=%i: position %i (address %p) was locked\\n\",\n\t\t\t\t\t\t\t\t\t\tblockID, threadID, subBlock, subBlockID,\n\t\t\t\t\t\t\t\t\t\tcurrentKey,\n\t\t\t\t\t\t\t\t\t\tbasePosition + probingOffset, addr);\n\t\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\t\tprintf(\n\t\t\t\t\t\t\t\t\t\t\"blockID=%i, threadID=%i, subwarp=%i, subwarpID=%i, currentKeyCounter=%i: position %i (address %p) was NOT locked\\n\",\n\t\t\t\t\t\t\t\t\t\tblockID, threadID, subBlock, subBlockID,\n\t\t\t\t\t\t\t\t\t\tcurrentKey,\n\t\t\t\t\t\t\t\t\t\tbasePosition + probingOffset, addr);\n\t\t\t\t\t\t\t}\n#endif\n\t\t\t\t\t\t}\n\n\t\t\t\t\t\t// if entry could be locked\n\t\t\t\t\t\tif (lockStatus[probingOffset] == STATUS_OCCUPIED) {\n\n\t\t\t\t\t\t\tif (subBlock == probingOffset) {\n\n\t\t\t\t\t\t\t\t// copy shared kmer to global memory\n\t\t\t\t\t\t\t\tif (subBlockID < counterPosition) {\n\t\t\t\t\t\t\t\t\t*addr = keyInts[currentKey * intsPerKey\n\t\t\t\t\t\t\t\t\t\t\t+ subBlockID];\n#ifdef DEBUG\n\t\t\t\t\t\t\t\t\tprintf(\n\t\t\t\t\t\t\t\t\t\t\t\"blockID=%i, threadID=%i, subwarp=%i, subwarpID=%i, currentKeyCounter=%i: add entry at position %i (address %p): x=%i\\n\",\n\t\t\t\t\t\t\t\t\t\t\tblockID, threadID, subBlock,\n\t\t\t\t\t\t\t\t\t\t\tsubBlockID, currentKey,\n\t\t\t\t\t\t\t\t\t\t\tbasePosition + probingOffset, addr,\n\t\t\t\t\t\t\t\t\t\t\tkeyInts[currentKey * intsPerKey\n\t\t\t\t\t\t\t\t\t\t\t+ subBlockID]);\n#endif\n\t\t\t\t\t\t\t\t}\n\n\t\t\t\t\t\t\t\t__threadfence();\n\n\t\t\t\t\t\t\t\tif(subBlockID == counterPosition) {\n\n\t\t\t\t\t\t\t\t\t// set counter to one\n\t\t\t\t\t\t\t\t\t*addr = 1;\n\n\t\t\t\t\t\t\t\t\t*success = true;\n#ifdef DEBUG\n\t\t\t\t\t\t\t\t\tprintf(\n\t\t\t\t\t\t\t\t\t\t\t\"blockID=%i, threadID=%i, subwarp=%i, subwarpID=%i currentKeyCounter=%i, : set counter at position %i (address %p) to 1 -> %i\\n\",\n\t\t\t\t\t\t\t\t\t\t\tblockID, threadID, subBlock,\n\t\t\t\t\t\t\t\t\t\t\tsubBlockID, currentKey,\n\t\t\t\t\t\t\t\t\t\t\tbasePosition + probingOffset, addr, *addr);\n#endif\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t\t// if entry could not be locked\n\t\t\t\t\t\telse {\n\t\t\t\t\t\t\t// probe the same entry again\n\t\t\t\t\t\t\tprobingOffset--;\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\t// error\n\t\t\t\t\telse {\n\t\t\t\t\t\tprintf(\"ERROR CASE: %i\\n\", lockStatus[probingOffset]);\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t}\n\t\t}\n\t\t// if kmer could not be insered after MAXTRIALS trials\n\t\tif (!*success) {\n\n\t\t\t// write kmer to area of shame\n\t\t\tif (threadID == 0) {\n#ifdef DEBUG\n\t\t\t\tprintf(\n\t\t\t\t\t\t\"blockID=%i, threadID=%i, subwarp=%i, subwarpID=%i:, currentKeyCounter=%i no success after %i trials\\n\",\n\t\t\t\t\t\tblockID, threadID, subBlock, subBlockID, currentKey,\n\t\t\t\t\t\tnumTrials);\n#endif\n\n\t\t\t\t// get address where to write kmer\n\t\t\t\tuint64_t numNoSucces = atomicAdd((unsigned long long int*) numNoSuccessPtr, (unsigned long long int) 1);\n\t\t\t\tif (numNoSucces < maxNumNoSuccess) {\n\t\t\t\t\taddr = (uint32_t*) (noSuccessArea + numNoSucces);\n\t\t\t\t\t// write kmer to area\n\t\t\t\t\tfor (uint32_t i = 0; i < intsPerKey; i++)\n\t\t\t\t\t\taddr[i] = keyInts[currentKey * intsPerKey + i];\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}"
        ],
        "/Users/gbolet/hecbench-roofline/src/gerbil-cuda/src/cuda_ds/CompressKernel.cu": [
            "inline __host__ __device__ uint4 make_uint4(int4 a)\n{\n    return make_uint4(uint(a.x), uint(a.y), uint(a.z), uint(a.w));\n}\n\ninline __host__ __device__ uint4 min(uint4 a, uint4 b)\n{\n    return make_uint4(min(a.x,b.x), min(a.y,b.y), min(a.z,b.z), min(a.w,b.w));\n}\n\n__device__  inline uint64_t firstFreePosition(KeyValuePair<intsPerKey>* table, const uint64_t numEntries,\n\t\tconst uint64_t l, const uint64_t r, uint32_t* shared) {\n\n\tconst uint32_t intsPerEntry = intsPerKey + 1;\n\tconst uint32_t entriesPerBlock = blocksize / intsPerEntry;\n\tconst uint32_t tId = threadIdx.x;\n\n\t// Shared Memory and views on it.\n\t// This is the same physical memory as defined in compressKernel!\n\tuint32_t* leftPart = &shared[0];\n\tvolatile uint64_t *result = (volatile uint64_t*) &shared[2 * blocksize];\n\n\tif (tId == 0)\n\t\t*result = (uint64_t) -1;\n\t__syncthreads();\n\n\tuint64_t i = l;\n\twhile (i < r) {\n\n\t\tuint64_t j = i % entriesPerBlock;\n\n\t\tif(j == 0) {\n\n\t\t\t// read the next stripe of hash table from global memory\n\t\t\tconst uint32_t* startAddr = (uint32_t*) (table + i);\n\t\t\tconst uint32_t* stopAddr = (uint32_t*) min((uint64_t) (table + i + entriesPerBlock), (uint64_t) (table + numEntries));\n\t\t\tconst uint32_t* addr = startAddr + tId;\n\t\t\tif(addr < stopAddr) {\n\t\t\t\tleftPart[tId] = *addr;\n\t\t\t\t//printf(\"thread %u: i=%u loading memory: %u\\n\", tId, i, leftPart[tId]);\n\t\t\t}\n\t\t\t__syncthreads();\n\t\t}\n\n\t\t// linear scan of the part of the hash table\n\t\tfor (; j < entriesPerBlock && i < r && (*result) == (uint64_t) -1; j++, i++) {\n\t\t\t// only one thread participates\n\t\t\tif (tId == 0) {\n\t\t\t\t// the first free position is returned\n\t\t\t\tif (leftPart[j * intsPerEntry + intsPerKey] == 0)\n\t\t\t\t\t*result = i;\n\t\t\t}\n\t\t\t__syncthreads();\n\t\t}\n\n\t\tif (*result != (uint64_t) -1)\n\t\t\treturn *result;\n\t}\n\n\t// no free position found\n\treturn r;\n}\n\n__device__  inline uint64_t lastOccupiedPosition(KeyValuePair<intsPerKey>* table, const uint64_t numEntries, const uint64_t l,\n\t\tconst uint64_t r, uint32_t* shared) {\n\n\tconst uint32_t intsPerEntry = intsPerKey + 1;\n\tconst uint32_t entriesPerBlock = blocksize / intsPerEntry;\n\tconst uint32_t tId = threadIdx.x;\n\n\t// Views on the shared memory\n\tvolatile uint32_t* rightPart = &shared[blocksize];\n\tvolatile uint64_t *result = (volatile uint64_t*) &shared[2 * blocksize + 3];\n\n\tif (tId == 0)\n\t\t*result = (uint64_t) -1;\n\t__syncthreads();\n\n\t// the position from the end of the table\n\tuint64_t i = numEntries - r - 1;\n\n\t// Run to front\n\twhile (numEntries-i-1 > l) {\n\n\t\tuint64_t j = i % entriesPerBlock;\n\n\t\tif(j == 0) {\n\n\t\t\tconst uint64_t numEntriesLoaded = min((uint64_t) entriesPerBlock, numEntries-i);\n\t\t\tconst uint32_t* startAddr = (uint32_t*) (table + numEntries - i - numEntriesLoaded);\n\t\t\tconst uint32_t* stopAddr = (uint32_t*) (table + numEntries - i);\n\t\t\tconst uint32_t* addr = (uint32_t*) (table + numEntries - i - entriesPerBlock) + tId;\n\n\t\t\t// read the next stripe of hash table from global memory\n\t\t\tif(addr >= startAddr && addr < stopAddr) {\n\t\t\t\trightPart[tId] = *addr;\n\t\t\t\t//printf(\"thread %u: i=%u loading memory: %u\\n\", tId, i, rightPart[tId]);\n\t\t\t}\n\t\t\t__syncthreads();\n\t\t}\n\n\t\t// linear scan of the part of the hash table\n\t\tfor (; j < entriesPerBlock && numEntries-i-1 > l && *result == (uint64_t) -1; j++, i++) {\n\t\t\t// only one thread participates\n\t\t\tif (tId == 0) {\n\t\t\t\t// the last occupied position is returned\n\t\t\t\tif (rightPart[(entriesPerBlock - j - 1) * intsPerEntry + intsPerKey] != 0)\n\t\t\t\t\t*result = numEntries - i -1;\n\t\t\t}\n\t\t\t__syncthreads();\n\t\t}\n\n\t\tif (*result != (uint64_t) -1) {\n\t\t\t//printf(\"r is %u\\n\", *result);\n\t\t\treturn *result;\n\t\t}\n\t}\n\n\t// no free position found\n\treturn l;\n}\n\n__device__ inline void my_swap(KeyValuePair<intsPerKey>* table, const uint64_t numEntries,\n\t\tconst uint64_t l, const uint64_t r, uint32_t* shared) {\n\n\tconst uint32_t tId = threadIdx.x;\n\tconst uint32_t intsPerEntry = intsPerKey + 1;\n\tconst uint32_t entriesPerBlock = blocksize / intsPerEntry;\n\tuint32_t* tableInts = (uint32_t*) table;\n\n\t//if(tId == 0)\n\t//\tprintf(\"swap %u %u\\n\", l, r);\n\n\t// Views on the shared memory\n\tuint32_t* leftPart = &shared[0];\n\tuint32_t* rightPart = &shared[blocksize];\n\n\tif (tId < intsPerEntry) {\n\n\t\t// swap in global memory\n\t\tconst uint64_t x = numEntries - r - 1;\n\t\tconst uint64_t pos = entriesPerBlock - (x % entriesPerBlock) - 1;\n\t\ttableInts[intsPerEntry * l + tId] = rightPart[pos * intsPerEntry + tId];\n\t\ttableInts[intsPerEntry * r + tId] = leftPart[(l % entriesPerBlock) * intsPerEntry + tId];\n\t}\n\t__syncthreads();\n\n}\n\n__global__ void compressKernel(KeyValuePair<intsPerKey>* table,\n\t\tconst uint64_t numEntries, uint64_t* res) {\n\n\t// prepare shared memory to hold parts of the hash table containing l and r\n\t__shared__ uint32_t shared[2 * blocksize + 4];\n\n\t// init left and right marker\n\tuint64_t l = firstFreePosition<intsPerKey, blocksize>(table, numEntries, 0, numEntries - 1, shared);\t// l points to first zero position\n\tuint64_t r = lastOccupiedPosition<intsPerKey, blocksize>(table, numEntries, 0, numEntries - 1, shared);\t// r points to last nonzero position\n\n\t/* invariant:\n\t *   l points to first zero position.\n\t *   r points to last nonzero position.\n\t */\n\n\twhile (l < r) {\n\n\t//\tprintf(\"l=%u, r=%u\\n\", l, r);\n\n\t\t// swap table[l] with table[r])\n\t\tmy_swap<intsPerKey, blocksize>(table, numEntries, l, r, shared);\n\n\t\t// repair invariant\n\t\tl = firstFreePosition<intsPerKey, blocksize>(table, numEntries, l+1, r, shared);\n\t\tr = lastOccupiedPosition<intsPerKey, blocksize>(table, numEntries, l, r-1, shared);\n\t}\n\n//\tprintf(\"l=%u, r=%u\\n\", l, r);\n\n\t*res = l;\n}"
        ]
    },
    "kmeans-cuda": {
        "/Users/gbolet/hecbench-roofline/src/kmeans-cuda/cluster.cu": [
            "__global__\nvoid feature_transpose (float* feature_swap,\n                        const float* feature,\n                        const int nfeatures,\n                        const int npoints)\n{\n  int tid = blockIdx.x * blockDim.x + threadIdx.x; \n  if (tid < npoints) {\n    for(int i = 0; i <  nfeatures; i++)\n      feature_swap[i * npoints + tid] = feature[tid * nfeatures + i];\n  }\n}",
            "__global__\nvoid find_membership (const float*__restrict__ feature,\n                      const float*__restrict__ cluster,\n                              int*__restrict__ member, \n                      const int nclusters,\n                      const int nfeatures,\n                      const int npoints)\n{\n  int point_id = blockIdx.x * blockDim.x + threadIdx.x; \n  if (point_id < npoints) {\n    int index = 0;\n    float min_dist = FLT_MAX;\n    for (int i = 0; i < nclusters; i++) {\n      float dist = 0;\n      float ans  = 0;\n      for (int l = 0; l < nfeatures; l++) {\n        ans += (feature[l * npoints + point_id] - cluster[i * nfeatures + l]) * \n               (feature[l * npoints + point_id] - cluster[i * nfeatures + l]) ; \n      }\n      dist = ans;\n      if (dist < min_dist) {\n        min_dist = dist;\n        index    = i;\n      }\n    }\n    member[point_id] = index;\n  }\n}"
        ]
    },
    "pnpoly-cuda": {
        "/Users/gbolet/hecbench-roofline/src/pnpoly-cuda/kernel.h": [
            "__device__ __forceinline__ int is_between(float a, float b, float c) {\n  return (b > a) != (c > a);\n}\n\n__global__\nvoid pnpoly_opt(\n    int*__restrict__ bitmap,\n    const float2*__restrict__ point,\n    const float2*__restrict__ vertex,\n    int n) \n{\n  int i = blockIdx.x * BLOCK_SIZE_X * tile_size + threadIdx.x;\n  if (i < n) {\n    int c[tile_size];\n    float2 lpoint[tile_size];\n    #pragma unroll\n    for (int ti=0; ti<tile_size; ti++) {\n      c[ti] = 0;\n      if (i+BLOCK_SIZE_X*ti < n) {\n        lpoint[ti] = point[i+BLOCK_SIZE_X*ti];\n      }\n    }\n\n    int k = VERTICES-1;\n\n    for (int j=0; j<VERTICES; k = j++) {    // edge from vj to vk\n      float2 vj = vertex[j]; \n      float2 vk = vertex[k]; \n\n      float slope = (vk.x-vj.x) / (vk.y-vj.y);\n\n      #pragma unroll\n      for (int ti=0; ti<tile_size; ti++) {\n\n        float2 p = lpoint[ti];\n\n        if (is_between(p.y, vj.y, vk.y) &&         //if p is between vj and vk vertically\n            (p.x < slope * (p.y-vj.y) + vj.x)\n           ) {  //if p.x crosses the line vj-vk when moved in positive x-direction\n          c[ti] = !c[ti];\n        }\n      }\n    }\n\n    #pragma unroll\n    for (int ti=0; ti<tile_size; ti++) {\n      //could do an if statement here if 1s are expected to be rare\n      if (i+BLOCK_SIZE_X*ti < n)\n        bitmap[i+BLOCK_SIZE_X*ti] = c[ti];\n    }\n  }\n}",
            "__global__\nvoid pnpoly_base(\n    int*__restrict__ bitmap,\n    const float2*__restrict__ point,\n    const float2*__restrict__ vertex,\n    int n) \n{\n  int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < n) {\n    int c = 0;\n    float2 p = point[i];\n\n    int k = VERTICES-1;\n\n    for (int j=0; j<VERTICES; k = j++) {    // edge from v to vp\n      float2 vj = vertex[j]; \n      float2 vk = vertex[k]; \n\n      float slope = (vk.x-vj.x) / (vk.y-vj.y);\n\n      if (((vj.y>p.y) != (vk.y>p.y)) &&            //if p is between vj and vk vertically\n          (p.x < slope * (p.y-vj.y) + vj.x)) {   //if p.x crosses the line vj-vk when moved in positive x-direction\n        c = !c;\n      }\n    }\n\n    bitmap[i] = c; // 0 if even (out), and 1 if odd (in)\n  }\n}"
        ]
    },
    "addBiasResidualLayerNorm-cuda": {
        "/Users/gbolet/hecbench-roofline/src/addBiasResidualLayerNorm-cuda/kernels.h": [
            "#define T ((int)32)\n\n\n#define T2 double2\n\n\n__inline__ __device__ T warpReduceSum(T val)\n{\n#pragma unroll\n  for (int mask = 16; mask > 0; mask >>= 1)\n    val += __shfl_xor_sync(FINAL_MASK, val, mask, 32);\n  return val;\n}\n\n__global__\nvoid add(int n, const float *x, float *y)\n{\n  int index = blockIdx.x * blockDim.x + threadIdx.x;\n  int stride = blockDim.x * gridDim.x;\n  for (int i = index; i < n; i += stride)\n    y[i] += x[i];\n}\n\n__inline__ __device__ T blockReduceSum(T val)\n{\n  static __shared__ T shared[32];\n  int lane = threadIdx.x & 0x1f;\n  int wid = threadIdx.x >> 5;\n\n  val = warpReduceSum<T>(val);\n\n  if (lane == 0)\n    shared[wid] = val;\n\n  __syncthreads();\n\n  // Modify from blockDim.x << 5 to blockDim.x / 32. to prevent\n  // blockDim.x is not divided by 32\n  val = (threadIdx.x < (blockDim.x / 32.f)) ? shared[lane] : (T)(0.0f);\n  val = warpReduceSum<T>(val);\n\n  return val;\n}\n\ninline __device__ __nv_bfloat162 floatToType2(float a) {\n  return __float2bfloat162_rn(a);\n}\n\ninline __device__ __nv_bfloat162 fma(__nv_bfloat162 a, __nv_bfloat162 b, __nv_bfloat162 c, __nv_bfloat162 d) {\n    return __hadd2(__hmul2(__hmul2(a, b), c), d);\n}\n\ninline __device__ __nv_bfloat162 sub(__nv_bfloat162 a, __nv_bfloat162 b) {\n  return __hsub2(a, b);\n}\n\ninline __host__ __device__ float typeToFloat(half a) {\n  return __half2float(a);\n}\n\n__global__ void addBiasResidualPostLayerNormV2(\n          T* out,\n    const T* __restrict__ input,\n    const T* __restrict__ bias,\n    const T* __restrict__ gamma,\n    const T* __restrict__ beta,\n    const float layernorm_eps,\n    const int n)\n{\n  using T2             = typename TypeConverter<T>::Type;\n  const int        ite = 4;\n  const int        tid = threadIdx.x;\n  const int        bid = blockIdx.x;\n  __shared__ float s_mean;\n  __shared__ float s_variance;\n  float            mean     = 0.0f;\n  float            variance = 0.0f;\n  T2               local_out_half2[ite];\n\n  T2*       out_ptr   = (T2*)out;\n  const T2* input_ptr = (const T2*)input;\n  const T2* bias_ptr  = (const T2*)bias;\n  const T2* gamma_ptr = (const T2*)gamma;\n  const T2* beta_ptr  = (const T2*)beta;\n\n  T2 sum = floatToType2<T2>(0.0f);\n\n  // ite = 4 and blockDim.x = n / 8\n  // When n = 1024, blockDim.x = 128\n  // col_id range: [0-127], [128-255], [256-383], [384-511]\n  // block stride = n / 2 = 512\n#pragma unroll\n  for (int i = 0; i < ite; i++) {\n    int col_id         = i * blockDim.x + tid;\n    int id             = bid * n / 2 + col_id;\n    local_out_half2[i] = add(out_ptr[id], input_ptr[id], bias_ptr[col_id]);\n    sum                = add(sum, local_out_half2[i]);\n  }\n\n  mean = blockReduceSum<float>(typeToFloat(__hadd(sum.x , sum.y)));\n  if (threadIdx.x == 0) {\n    s_mean = mean / n;\n  }\n  __syncthreads();\n\n  float var      = 0.0f;\n  T2    s_mean_2 = floatToType2<T2>(s_mean);\n\n#pragma unroll\n  for (int i = 0; i < ite; i++) {\n    local_out_half2[i] = sub(local_out_half2[i], s_mean_2);\n    float v1           = typeToFloat(local_out_half2[i].x);\n    float v2           = typeToFloat(local_out_half2[i].y);\n    var += v1 * v1 + v2 * v2;\n  }\n\n  variance = blockReduceSum<float>(var);\n  if (tid == 0) {\n    s_variance = rsqrtf(variance / n + layernorm_eps);\n  }\n  __syncthreads();\n\n  T2 s_var_2 = floatToType2<T2>(s_variance);\n#pragma unroll\n  for (int i = 0; i < ite; i++) {\n    int col_id  = i * blockDim.x + tid;\n    int id      = bid * n / 2 + col_id;\n    out_ptr[id] = fma(local_out_half2[i], s_var_2,\n                      gamma_ptr[col_id], beta_ptr[col_id]);\n  }\n}",
            "#define T ((int)32)\n\n\n__inline__ __device__ T warpReduceSum(T val)\n{\n#pragma unroll\n  for (int mask = 16; mask > 0; mask >>= 1)\n    val += __shfl_xor_sync(FINAL_MASK, val, mask, 32);\n  return val;\n}\n\n__global__\nvoid add(int n, const float *x, float *y)\n{\n  int index = blockIdx.x * blockDim.x + threadIdx.x;\n  int stride = blockDim.x * gridDim.x;\n  for (int i = index; i < n; i += stride)\n    y[i] += x[i];\n}\n\n__inline__ __device__ T blockReduceSum(T val)\n{\n  static __shared__ T shared[32];\n  int lane = threadIdx.x & 0x1f;\n  int wid = threadIdx.x >> 5;\n\n  val = warpReduceSum<T>(val);\n\n  if (lane == 0)\n    shared[wid] = val;\n\n  __syncthreads();\n\n  // Modify from blockDim.x << 5 to blockDim.x / 32. to prevent\n  // blockDim.x is not divided by 32\n  val = (threadIdx.x < (blockDim.x / 32.f)) ? shared[lane] : (T)(0.0f);\n  val = warpReduceSum<T>(val);\n\n  return val;\n}\n\ninline __host__ __device__ __nv_bfloat16 floatToType(float a) {\n  return __float2bfloat16(a);\n}\n\ninline __host__ __device__ float typeToFloat(half a) {\n  return __half2float(a);\n}\n\n__global__ void addBiasResidualPostLayerNorm(\n          T* out, \n    const T* __restrict__ input,\n    const T* __restrict__ bias,\n    const T* __restrict__ gamma,\n    const T* __restrict__ beta,\n    const float layernorm_eps,\n    const int n)\n{\n  __shared__ float s_mean;\n  __shared__ float s_variance;\n  float            mean     = 0.0f;\n  float            variance = 0.0f;\n  float            local_out_cache[N];\n\n#pragma unroll N\n  for (int idx = threadIdx.x, i = 0; idx < n && i < N; ++i) {\n    float local_out = typeToFloat(add(out[blockIdx.x * n + idx], input[blockIdx.x * n + idx], bias[idx]));\n    mean += local_out;\n    // save local_out to local_out_cache to save some recompute\n    local_out_cache[i] = local_out;\n    idx += blockDim.x;\n  }\n\n  mean = blockReduceSum<float>(mean);\n  if (threadIdx.x == 0) {\n    s_mean = mean / n;\n  }\n  __syncthreads();\n\n#pragma unroll N\n  for (int idx = threadIdx.x, i = 0; idx < n && i < N; ++i) {\n    float local_out = local_out_cache[i];\n    variance += (local_out - s_mean) * (local_out - s_mean);\n    idx += blockDim.x;\n  }\n  variance = blockReduceSum<float>(variance);\n  if (threadIdx.x == 0) {\n    s_variance = variance / n + layernorm_eps;\n  }\n  __syncthreads();\n\n#pragma unroll N\n  for (int idx = threadIdx.x, i = 0; idx < n && i < N; ++i) {\n    float local_out = local_out_cache[i];\n    out[blockIdx.x * n + idx] =\n      floatToType<T>(((local_out - s_mean) * rsqrtf(s_variance)) * typeToFloat(gamma[idx]) + typeToFloat(beta[idx]));\n    idx += blockDim.x;\n  }\n}",
            "#define T ((int)32)\n\n\n#define T2 double2\n\n\n__inline__ __device__ T warpReduceSum(T val)\n{\n#pragma unroll\n  for (int mask = 16; mask > 0; mask >>= 1)\n    val += __shfl_xor_sync(FINAL_MASK, val, mask, 32);\n  return val;\n}\n\n__global__\nvoid add(int n, const float *x, float *y)\n{\n  int index = blockIdx.x * blockDim.x + threadIdx.x;\n  int stride = blockDim.x * gridDim.x;\n  for (int i = index; i < n; i += stride)\n    y[i] += x[i];\n}\n\n__inline__ __device__ T blockReduceSum(T val)\n{\n  static __shared__ T shared[32];\n  int lane = threadIdx.x & 0x1f;\n  int wid = threadIdx.x >> 5;\n\n  val = warpReduceSum<T>(val);\n\n  if (lane == 0)\n    shared[wid] = val;\n\n  __syncthreads();\n\n  // Modify from blockDim.x << 5 to blockDim.x / 32. to prevent\n  // blockDim.x is not divided by 32\n  val = (threadIdx.x < (blockDim.x / 32.f)) ? shared[lane] : (T)(0.0f);\n  val = warpReduceSum<T>(val);\n\n  return val;\n}\n\ninline __device__ __nv_bfloat162 float2ToType2(float2 a) {\n  return __float22bfloat162_rn(a);\n}\n\ninline __device__ float2 type2ToFloat2(__nv_bfloat162 a) {\n  return __bfloat1622float2(a);\n}\n\n__global__ void generalAddBiasResidualPostLayerNorm(\n          T* out, \n    const T* __restrict__ input,\n    const T* __restrict__ bias,\n    const T* __restrict__ gamma,\n    const T* __restrict__ beta,\n    const float layernorm_eps,\n    const int n)\n{\n  using T2 = typename TypeConverter<T>::Type;\n  __shared__ float s_mean;\n  __shared__ float s_variance;\n  float            mean     = 0.0f;\n  float            variance = 0.0f;\n\n  T2*       out_ptr   = (T2*)out;\n  const T2* input_ptr = (const T2*)input;\n  const T2* bias_ptr  = (const T2*)bias;\n  const T2* gamma_ptr = (const T2*)gamma;\n  const T2* beta_ptr  = (const T2*)beta;\n\n  float local_out = 0.0f;\n  for (int idx = threadIdx.x; idx < n / 2; idx += blockDim.x) {\n    int    id            = blockIdx.x * n / 2 + idx;\n    T2     tmp           = add(add(out_ptr[id], input_ptr[id]), bias_ptr[idx]);\n    float2 local_out_fp2 = type2ToFloat2(tmp);\n    local_out += local_out_fp2.x;\n    local_out += local_out_fp2.y;\n    // save tmp to out_ptr to save some recomputation\n    out_ptr[id] = tmp;\n  }\n\n  mean = blockReduceSum<float>(local_out);\n  if (threadIdx.x == 0) {\n    s_mean = mean / n;\n  }\n  __syncthreads();\n\n  for (int idx = threadIdx.x; idx < n / 2; idx += blockDim.x) {\n    int    id            = blockIdx.x * n / 2 + idx;\n    float2 local_out_fp2 = type2ToFloat2(out_ptr[id]);\n    variance += (local_out_fp2.x - s_mean) * (local_out_fp2.x - s_mean);\n    variance += (local_out_fp2.y - s_mean) * (local_out_fp2.y - s_mean);\n  }\n\n  variance = blockReduceSum<float>(variance);\n  if (threadIdx.x == 0) {\n    s_variance = rsqrtf(variance / n + layernorm_eps);\n  }\n  __syncthreads();\n\n  for (int idx = threadIdx.x; idx < n / 2; idx += blockDim.x) {\n    int    id            = blockIdx.x * n / 2 + idx;\n    float2 local_out_fp2 = type2ToFloat2(out_ptr[id]);\n    float2 gamma_val     = type2ToFloat2(gamma_ptr[idx]);\n    float2 beta_val      = type2ToFloat2(beta_ptr[idx]);\n    local_out_fp2.x      = (local_out_fp2.x - s_mean) * s_variance * gamma_val.x + beta_val.x;\n    local_out_fp2.y      = (local_out_fp2.y - s_mean) * s_variance * gamma_val.y + beta_val.y;\n    out_ptr[id]          = float2ToType2<T2>(local_out_fp2);\n  }\n}"
        ]
    },
    "multimaterial-cuda": {
        "/Users/gbolet/hecbench-roofline/src/multimaterial-cuda/compact.cu": [
            "__global__ void ccc_loop1_2(\n  const double * __restrict__ rho_compact_list,\n  const double * __restrict__  Vf_compact_list,\n  const double * __restrict__  V,\n  double * __restrict__ rho_ave_compact,\n  const int * __restrict__ mmc_index,\n  const int  mmc_cells,\n  const int * __restrict__ mmc_i,\n  const int * __restrict__ mmc_j,\n  int sizex, int sizey)\n{\n  int c = threadIdx.x + blockIdx.x * blockDim.x;\n  if (c >= mmc_cells) return;\n  double ave = 0.0;\n  for (int m = mmc_index[c]; m < mmc_index[c+1]; m++) {\n    ave +=  rho_compact_list[m] * Vf_compact_list[m];\n  }\n  rho_ave_compact[mmc_i[c]+sizex*mmc_j[c]] = ave/V[mmc_i[c]+sizex*mmc_j[c]];\n}",
            "__global__ void ccc_loop2(\n  const int * __restrict__ imaterial,\n  const int * __restrict__ matids,\n  const int * __restrict__ nextfrac,\n  const double * __restrict__ rho_compact,\n  const double * __restrict__ rho_compact_list, \n  const double * __restrict__ t_compact,\n  const double * __restrict__ t_compact_list, \n  const double * __restrict__  Vf_compact_list,\n  const double * __restrict__ n,\n  double * __restrict__  p_compact,\n  double * __restrict__ p_compact_list,\n  int sizex, int sizey,\n  int * __restrict__ mmc_index)\n{\n  int i = threadIdx.x + blockIdx.x * blockDim.x;\n  int j = threadIdx.y + blockIdx.y * blockDim.y;\n  if (i >= sizex || j >= sizey) return;\n\n  int ix = imaterial[i+sizex*j];\n  if (ix <= 0) {\n#ifdef FUSED\n    // NOTE: I think the paper describes this algorithm (Alg. 9) wrong.\n    // The solution below is what I believe to good.\n\n    // condition is 'ix >= 0', this is the equivalent of\n    // 'until ix < 0' from the paper\n#ifdef LINKED\n    for (ix = -ix; ix >= 0; ix = nextfrac[ix]) {\n      double nm = n[matids[ix]];\n      p_compact_list[ix] = (nm * rho_compact_list[ix] * t_compact_list[ix]) / Vf_compact_list[ix];\n    }\n#else\n    for (int idx = mmc_index[-ix]; idx < mmc_index[-ix+1]; idx++) {\n      double nm = n[matids[idx]];\n      p_compact_list[idx] = (nm * rho_compact_list[idx] * t_compact_list[idx]) / Vf_compact_list[idx];\n    }\n#endif\n#endif\n  }\n  else {\n    // NOTE: HACK: we index materials from zero, but zero can be a list index\n    int mat = ix - 1;\n    // NOTE: There is no division by Vf here, because the fractional volume is 1.0 in the pure cell case.\n    p_compact[i+sizex*j] = n[mat] * rho_compact[i+sizex*j] * t_compact[i+sizex*j];;\n  }\n}",
            "__global__ void ccc_loop2_2(\n  const int * __restrict__ matids,\n  const double * __restrict__ rho_compact_list, \n  const double * __restrict__ t_compact_list,\n  const double * __restrict__ Vf_compact_list,\n  const double * __restrict__ n,\n  double * __restrict__ p_compact_list,\n  int * __restrict__ mmc_index,\n  int mmc_cells)\n{\n  int idx = threadIdx.x + blockIdx.x * blockDim.x;\n  if (idx >= mmc_cells) return;\n  double nm = n[matids[idx]];\n  p_compact_list[idx] = (nm * rho_compact_list[idx] * t_compact_list[idx]) / Vf_compact_list[idx];\n}",
            "__global__ void ccc_loop3(\n  const int * __restrict__ imaterial,\n  const int * __restrict__ nextfrac,\n  const int * __restrict__ matids,\n  const double * __restrict__ rho_compact, \n  const double * __restrict__ rho_compact_list, \n  double * __restrict__ rho_mat_ave_compact, \n  double * __restrict__ rho_mat_ave_compact_list, \n  const double * __restrict__ x,\n  const double * __restrict__ y,\n  int sizex, int sizey,\n  int * __restrict__ mmc_index)\n{\n  int i = threadIdx.x + blockIdx.x * blockDim.x;\n  int j = threadIdx.y + blockIdx.y * blockDim.y;\n  if (i >= sizex-1 || j >= sizey-1 || i < 1 || j < 1) return;\n\n  // o: outer\n  double xo = x[i+sizex*j];\n  double yo = y[i+sizex*j];\n\n  // There are at most 9 neighbours in 2D case.\n  double dsqr[9];\n\n  // for all neighbours\n  for (int nj = -1; nj <= 1; nj++) {\n\n    for (int ni = -1; ni <= 1; ni++) {\n\n      dsqr[(nj+1)*3 + (ni+1)] = 0.0;\n\n      // i: inner\n      double xi = x[(i+ni)+sizex*(j+nj)];\n      double yi = y[(i+ni)+sizex*(j+nj)];\n\n      dsqr[(nj+1)*3 + (ni+1)] += (xo - xi) * (xo - xi);\n      dsqr[(nj+1)*3 + (ni+1)] += (yo - yi) * (yo - yi);\n    }\n  }\n\n  int ix = imaterial[i+sizex*j];\n\n  if (ix <= 0) {\n    // condition is 'ix >= 0', this is the equivalent of\n    // 'until ix < 0' from the paper\n#ifdef LINKED\n    for (ix = -ix; ix >= 0; ix = nextfrac[ix]) {\n#else\n      for (int ix = mmc_index[-imaterial[i+sizex*j]]; ix < mmc_index[-imaterial[i+sizex*j]+1]; ix++) {\n#endif\n        int mat = matids[ix];\n        double rho_sum = 0.0;\n        int Nn = 0;\n\n        // for all neighbours\n        for (int nj = -1; nj <= 1; nj++) {\n\n          for (int ni = -1; ni <= 1; ni++) {\n\n            int ci = i+ni, cj = j+nj;\n            int jx = imaterial[ci+sizex*cj];\n\n            if (jx <= 0) {\n              // condition is 'jx >= 0', this is the equivalent of\n              // 'until jx < 0' from the paper\n#ifdef LINKED\n              for (jx = -jx; jx >= 0; jx = nextfrac[jx]) {\n#else\n                for (int jx = mmc_index[-imaterial[ci+sizex*cj]]; jx < mmc_index[-imaterial[ci+sizex*cj]+1]; jx++) {\n#endif\n                  if (matids[jx] == mat) {\n                    rho_sum += rho_compact_list[jx] / dsqr[(nj+1)*3 + (ni+1)];\n                    Nn += 1;\n\n                    // The loop has an extra condition: \"and not found\".\n                    // This makes sense, if the material is found, there won't be any more of the same.\n                    break;\n                  }\n                }\n              }\n              else {\n                // NOTE: In this case, the neighbour is a pure cell, its material index is in jx.\n                // In contrast, Algorithm 10 loads matids[jx] which I think is wrong.\n\n                // NOTE: HACK: we index materials from zero, but zero can be a list index\n                int mat_neighbour = jx - 1;\n                if (mat == mat_neighbour) {\n                  rho_sum += rho_compact[ci+sizex*cj] / dsqr[(nj+1)*3 + (ni+1)];\n                  Nn += 1;\n                }\n              } // end if (jx <= 0)\n            } // end for (int ni)\n          } // end for (int nj)\n\n          rho_mat_ave_compact_list[ix] = rho_sum / Nn;\n        } // end for (ix = -ix)\n      } // end if (ix <= 0)\n      else {\n        // NOTE: In this case, the cell is a pure cell, its material index is in ix.\n        // In contrast, Algorithm 10 loads matids[ix] which I think is wrong.\n\n        // NOTE: HACK: we index materials from zero, but zero can be a list index\n        int mat = ix - 1;\n\n        double rho_sum = 0.0;\n        int Nn = 0;\n\n        // for all neighbours\n        for (int nj = -1; nj <= 1; nj++) {\n          if ((j + nj < 0) || (j + nj >= sizey)) // TODO: better way?\n            continue;\n\n          for (int ni = -1; ni <= 1; ni++) {\n            if ((i + ni < 0) || (i + ni >= sizex)) // TODO: better way?\n              continue;\n\n            int ci = i+ni, cj = j+nj;\n            int jx = imaterial[ci+sizex*cj];\n\n            if (jx <= 0) {\n              // condition is 'jx >= 0', this is the equivalent of\n              // 'until jx < 0' from the paper\n#ifdef LINKED\n              for (jx = -jx; jx >= 0; jx = nextfrac[jx]) {\n#else\n                for (int jx = mmc_index[-imaterial[ci+sizex*cj]]; jx < mmc_index[-imaterial[ci+sizex*cj]+1]; jx++) {\n#endif\n                  if (matids[jx] == mat) {\n                    rho_sum += rho_compact_list[jx] / dsqr[(nj+1)*3 + (ni+1)];\n                    Nn += 1;\n\n                    // The loop has an extra condition: \"and not found\".\n                    // This makes sense, if the material is found, there won't be any more of the same.\n                    break;\n                  }\n                }\n              }\n              else {\n                // NOTE: In this case, the neighbour is a pure cell, its material index is in jx.\n                // In contrast, Algorithm 10 loads matids[jx] which I think is wrong.\n\n                // NOTE: HACK: we index materials from zero, but zero can be a list index\n                int mat_neighbour = jx - 1;\n                if (mat == mat_neighbour) {\n                  rho_sum += rho_compact[ci+sizex*cj] / dsqr[(nj+1)*3 + (ni+1)];\n                  Nn += 1;\n                }\n              } // end if (jx <= 0)\n            } // end for (int ni)\n          } // end for (int nj)\n\n          rho_mat_ave_compact[i+sizex*j] = rho_sum / Nn;\n        } // end else\n      }"
        ]
    },
    "matern-cuda": {
        "/Users/gbolet/hecbench-roofline/src/matern-cuda/main.cu": [
            "inline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\ninline __host__ __device__ float4 expf(float4 v)\n{\n    return make_float4(expf(v.x), expf(v.y), expf(v.z), expf(v.w));\n}\n\ninline __host__ __device__ float4 sqrtf(float4 v)\n{\n    return make_float4(sqrtf(v.x), sqrtf(v.y), sqrtf(v.z), sqrtf(v.w));\n}\n\n__global__ void matern_kernel (\n  const int num_targets,\n  const float l,\n  const float *__restrict__ sources,\n  const float *__restrict__ targets,\n  const float *__restrict__ weights,\n        float *__restrict__ result)\n{\n  int tx = threadIdx.x;\n  int px = blockIdx.x * blockDim.x + tx; // range [0:ntargets)\n  if (px >= num_targets) return;\n\n  int ty = threadIdx.y;\n  int py = ty; // range [0:nsources)\n  if (py >= SY) return;\n\n  __shared__ float local_result[SX * SY];\n  __shared__ float local_targets[SX * 3];\n  __shared__ float local_sources[SY * 3];\n  __shared__ float local_weights[SY];\n\n  if (ty == 0) {\n    for (int i = 0; i < 3; i++)\n      local_targets[tx * 3 + i] = targets[px * 3 + i];\n  }\n\n  if (tx == 0) {\n    for (int i = 0; i < 3; i++)\n      local_sources[ty * 3 + i] = sources[py * 3 + i];\n    local_weights[ty] = weights[ty];\n  }\n\n  __syncthreads();\n\n  float squared_diff = 0.f;\n  \n  for (int i = 0; i < 3; i++) {\n    squared_diff += (local_targets[tx * 3 + i] - local_sources[ty * 3 + i]) *\n                    (local_targets[tx * 3 + i] - local_sources[ty * 3 + i]);\n  } \n  float diff = sqrtf(squared_diff);\n\n  local_result[tx * SY + ty] = \n    (1.f + sqrtf(5.f) * diff / l + 5.f * squared_diff / (3.f * l * l)) *  \n    expf(-sqrtf(5.f) * diff  / l) * local_weights[ty];\n   \n  __syncthreads();\n\n  if (ty == 0) {\n    float res = 0.f;\n    for (int i = 0; i < SY; i++)\n      res += local_result[tx * SY + i];\n    result[px] = res;\n  }\n}"
        ]
    },
    "distort-cuda": {
        "/Users/gbolet/hecbench-roofline/src/distort-cuda/distort.cu": [
            "inline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\ninline __host__ __device__ float4 floorf(float4 v)\n{\n    return make_float4(floorf(v.x), floorf(v.y), floorf(v.z), floorf(v.w));\n}\n\n__host__ __device__\ninline float getRadialX(float x, float y, const struct Properties* prop)\n{\n  x = (x * prop->xscale + prop->xshift);\n  y = (y * prop->yscale + prop->yshift);\n  float result = x + ((x - prop->centerX) * prop->K *\n    ((x - prop->centerX) * (x - prop->centerX) + (y - prop->centerY) * (y - prop->centerY)));\n  return result;\n}\n\n__host__ __device__\ninline float getRadialY(float x, float y, const struct Properties* prop)\n{\n  x = (x * prop->xscale + prop->xshift);\n  y = (y * prop->yscale + prop->yshift);\n  float result = y + ((y - prop->centerY) * prop->K * \n    ((x - prop->centerX) * (x - prop->centerX) + (y - prop->centerY) * (y - prop->centerY)));\n  return result;\n}\n\n__host__ __device__\ninline void sampleImageTest(const uchar3* src, float idx0, float idx1,\n                            uchar3& result, const struct Properties* prop)\n{\n  // out-of-bound check\n  if((idx0 < 0) || (idx1 < 0) || (idx0 > prop->height - 1) || (idx1 > prop->width - 1))\n  {\n    result.x = 0;\n    result.y = 0;\n    result.z = 0;\n    return;\n  }\n\n  int idx0_floor = (int)floorf(idx0);\n  int idx0_ceil = (int)ceilf(idx0);\n  int idx1_floor = (int)floorf(idx1);\n  int idx1_ceil = (int)ceilf(idx1);\n\n  uchar3 s1 = src[(idx0_floor * prop->width) + idx1_floor];\n  uchar3 s2 = src[(idx0_floor * prop->width) + idx1_ceil];\n  uchar3 s3 = src[(idx0_ceil * prop->width) + idx1_ceil];\n  uchar3 s4 = src[(idx0_ceil * prop->width) + idx1_floor];\n\n  float x = idx0 - idx0_floor;\n  float y = idx1 - idx1_floor;\n\n  result.x = s1.x * (1.f - x) * (1.f - y) + s2.x * (1.f - x) * y + s3.x * x * y + s4.x * x * (1.f - y);\n  result.y = s1.y * (1.f - x) * (1.f - y) + s2.y * (1.f - x) * y + s3.y * x * y + s4.y * x * (1.f - y);\n  result.z = s1.z * (1.f - x) * (1.f - y) + s2.z * (1.f - x) * y + s3.z * x * y + s4.z * x * (1.f - y);\n}\n\n__global__ void barrel_distort (\n  const uchar3 *__restrict__ src,\n        uchar3 *__restrict__ dst,\n  const struct Properties *__restrict__ prop)\n{\n  int h = blockIdx.y * blockDim.y + threadIdx.y;\n  int w = blockIdx.x * blockDim.x + threadIdx.x;\n  if (w < prop->width && h < prop->height) {\n    float x = getRadialX((float)w, (float)h, prop);\n    float y = getRadialY((float)w, (float)h, prop);\n    uchar3 temp;\n    sampleImageTest(src, y, x, temp, prop);\n    dst[(h * prop->width) + w] = temp;\n  }\n}"
        ]
    },
    "pitch-cuda": {
        "/Users/gbolet/hecbench-roofline/src/pitch-cuda/main.cu": [
            "inline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\ninline __host__ __device__ float4 expf(float4 v)\n{\n    return make_float4(expf(v.x), expf(v.y), expf(v.z), expf(v.w));\n}\n\n__device__\nfloat sigmoid(const float x) { return 1.0f / (1.0f + expf(-x)); }\n\n__global__\nvoid parallelPitched2DAccess (float* devPtr, size_t pitch, int width, int height)\n{\n  int r = blockIdx.y * blockDim.y + threadIdx.y;\n  int c = blockIdx.x * blockDim.x + threadIdx.x;\n  if (r < height && c < width) {\n    float* row = (float*)((char*)devPtr + r * pitch);\n    row[c] = sigmoid(row[c]);\n  }\n}",
            "inline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\ninline __host__ __device__ float4 expf(float4 v)\n{\n    return make_float4(expf(v.x), expf(v.y), expf(v.z), expf(v.w));\n}\n\n__device__\nfloat sigmoid(const float x) { return 1.0f / (1.0f + expf(-x)); }\n\n__global__\nvoid parallelSimple2DAccess (float* elem, int width, int height)\n{\n  int r = blockIdx.y * blockDim.y + threadIdx.y;\n  int c = blockIdx.x * blockDim.x + threadIdx.x;\n  if (r < height && c < width) {\n    elem[r * width + c] = sigmoid(elem[r * width + c]);\n  }\n}",
            "inline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\ninline __host__ __device__ float4 expf(float4 v)\n{\n    return make_float4(expf(v.x), expf(v.y), expf(v.z), expf(v.w));\n}\n\n__device__\nfloat sigmoid(const float x) { return 1.0f / (1.0f + expf(-x)); }\n\n__global__\nvoid parallelPitched3DAccess (cudaPitchedPtr devPitchedPtr, int width, int height, int depth)\n{\n  int z = blockIdx.z * blockDim.z + threadIdx.z;\n  int y = blockIdx.y * blockDim.y + threadIdx.y;\n  int x = blockIdx.x * blockDim.x + threadIdx.x;\n  if (z < depth && y < height && x < width) {\n    char* devPtr = (char*)devPitchedPtr.ptr;\n    size_t pitch = devPitchedPtr.pitch;\n    size_t slicePitch = pitch * height;\n    char* slice = devPtr + z * slicePitch;\n    float* row = (float*)(slice + y * pitch);\n    row[x] = sigmoid(row[x]);\n  }\n}",
            "inline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\ninline __host__ __device__ float4 expf(float4 v)\n{\n    return make_float4(expf(v.x), expf(v.y), expf(v.z), expf(v.w));\n}\n\n__device__\nfloat sigmoid(const float x) { return 1.0f / (1.0f + expf(-x)); }\n\n__global__\nvoid parallelSimple3DAccess (float* elem, int width, int height, int depth)\n{\n  int z = blockIdx.z * blockDim.z + threadIdx.z;\n  int y = blockIdx.y * blockDim.y + threadIdx.y;\n  int x = blockIdx.x * blockDim.x + threadIdx.x;\n  if (z < depth && y < height && x < width) {\n    float element = elem[z * height * width + y * width + x];\n    elem[z * height * width + y * width + x] = sigmoid(element);\n  }\n}"
        ]
    },
    "lud-cuda": {
        "/Users/gbolet/hecbench-roofline/src/lud-cuda/lud_kernels.cu": [
            "__global__ void\nlud_diagonal (float *m, const int matrix_dim, const int offset) {\n  __shared__ float shadow [BLOCK_SIZE*BLOCK_SIZE];\n  int i,j;\n  int tx = threadIdx.x;\n\n  int array_offset = offset*matrix_dim+offset;\n  for(i=0; i < BLOCK_SIZE; i++){\n    shadow[i * BLOCK_SIZE + tx]=m[array_offset + tx];\n    array_offset += matrix_dim;\n  }\n\n  __syncthreads();\n\n  for(i=0; i < BLOCK_SIZE-1; i++) {\n\n    if (tx>i){\n      for(j=0; j < i; j++)\n        shadow[tx * BLOCK_SIZE + i] -= shadow[tx * BLOCK_SIZE + j] * shadow[j * BLOCK_SIZE + i];\n      shadow[tx * BLOCK_SIZE + i] /= shadow[i * BLOCK_SIZE + i];\n    }\n\n    __syncthreads();\n    if (tx>i){\n\n      for(j=0; j < i+1; j++)\n        shadow[(i+1) * BLOCK_SIZE + tx] -= shadow[(i+1) * BLOCK_SIZE + j]*shadow[j * BLOCK_SIZE + tx];\n    }\n\n    __syncthreads();\n  }\n\n  array_offset = (offset+1)*matrix_dim+offset;\n  for(i=1; i < BLOCK_SIZE; i++){\n    m[array_offset+tx]=shadow[i * BLOCK_SIZE + tx];\n    array_offset += matrix_dim;\n  }\n}",
            "__global__ void\nlud_perimeter (float *m, const int matrix_dim, const int offset) {\n  __shared__ float dia [BLOCK_SIZE*BLOCK_SIZE];\n  __shared__ float peri_row [BLOCK_SIZE*BLOCK_SIZE];\n  __shared__ float peri_col [BLOCK_SIZE*BLOCK_SIZE];\n\n  int i,j, array_offset;\n  int idx;\n\n  int  bx = blockIdx.x;  \n  int  tx = threadIdx.x;\n\n  if (tx < BLOCK_SIZE) {\n    idx = tx;\n    array_offset = offset*matrix_dim+offset;\n    for (i=0; i < BLOCK_SIZE/2; i++){\n      dia[i * BLOCK_SIZE + idx]=m[array_offset+idx];\n      array_offset += matrix_dim;\n    }\n\n    array_offset = offset*matrix_dim+offset;\n    for (i=0; i < BLOCK_SIZE; i++) {\n      peri_row[i * BLOCK_SIZE+ idx]=m[array_offset+(bx+1)*BLOCK_SIZE+idx];\n      array_offset += matrix_dim;\n    }\n\n  } else {\n    idx = tx-BLOCK_SIZE;\n\n    array_offset = (offset+BLOCK_SIZE/2)*matrix_dim+offset;\n    for (i=BLOCK_SIZE/2; i < BLOCK_SIZE; i++){\n      dia[i * BLOCK_SIZE + idx]=m[array_offset+idx];\n      array_offset += matrix_dim;\n    }\n\n    array_offset = (offset+(bx+1)*BLOCK_SIZE)*matrix_dim+offset;\n    for (i=0; i < BLOCK_SIZE; i++) {\n      peri_col[i * BLOCK_SIZE + idx] = m[array_offset+idx];\n      array_offset += matrix_dim;\n    }\n\n  }\n  __syncthreads();\n\n  if (tx < BLOCK_SIZE) { //peri-row\n    idx=tx;\n    for(i=1; i < BLOCK_SIZE; i++){\n      for (j=0; j < i; j++)\n        peri_row[i * BLOCK_SIZE + idx]-=dia[i * BLOCK_SIZE+ j]*peri_row[j * BLOCK_SIZE + idx];\n    }\n  } else { //peri-col\n    idx=tx - BLOCK_SIZE;\n    for(i=0; i < BLOCK_SIZE; i++){\n      for(j=0; j < i; j++)\n        peri_col[idx * BLOCK_SIZE + i]-=peri_col[idx * BLOCK_SIZE+ j]*dia[j * BLOCK_SIZE + i];\n      peri_col[idx * BLOCK_SIZE + i] /= dia[i * BLOCK_SIZE+ i];\n    }\n  }\n\n  __syncthreads();\n\n  if (tx < BLOCK_SIZE) { //peri-row\n    idx=tx;\n    array_offset = (offset+1)*matrix_dim+offset;\n    for(i=1; i < BLOCK_SIZE; i++){\n      m[array_offset+(bx+1)*BLOCK_SIZE+idx] = peri_row[i*BLOCK_SIZE+idx];\n      array_offset += matrix_dim;\n    }\n  } else { //peri-col\n    idx=tx - BLOCK_SIZE;\n    array_offset = (offset+(bx+1)*BLOCK_SIZE)*matrix_dim+offset;\n    for(i=0; i < BLOCK_SIZE; i++){\n      m[array_offset+idx] =  peri_col[i*BLOCK_SIZE+idx];\n      array_offset += matrix_dim;\n    }\n  }\n}",
            "__global__ void\nlud_internal (float *m, const int matrix_dim, const int offset) {\n  __shared__ float peri_row [BLOCK_SIZE*BLOCK_SIZE];\n  __shared__ float peri_col [BLOCK_SIZE*BLOCK_SIZE];\n  int  bx = blockIdx.x;  \n  int  by = blockIdx.y;  \n\n  int  tx = threadIdx.x;\n  int  ty = threadIdx.y;\n\n  float sum;\n\n  int global_row_id = offset + (by+1)*BLOCK_SIZE;\n  int global_col_id = offset + (bx+1)*BLOCK_SIZE;\n\n  peri_row[ty * BLOCK_SIZE + tx] = m[(offset+ty)*matrix_dim+global_col_id+tx];\n  peri_col[ty * BLOCK_SIZE + tx] = m[(global_row_id+ty)*matrix_dim+offset+tx];\n\n  __syncthreads();\n\n  int i;\n  sum = 0;\n  for (i=0; i < BLOCK_SIZE; i++)\n    sum += peri_col[ty * BLOCK_SIZE + i]; // * peri_row[i * BLOCK_SIZE + tx];\n\n  m[(global_row_id+ty)*matrix_dim+global_col_id+tx] -= sum;\n}"
        ]
    },
    "contract-cuda": {
        "/Users/gbolet/hecbench-roofline/src/contract-cuda/kernel.h": [
            "#define T ((int)32)\n\n\n__global__ void contraction (\n  const T *__restrict__ tensor,\n  const T *__restrict__ adj,\n        T *__restrict__ value,\n  const int output_size, \n  const int N, \n  const int nChanels)\n{\n  int tid = blockIdx.x * blockDim.x + threadIdx.x;\n\n  if (tid < output_size) {  \n    int C = nChanels;\n    int B = N * C;\n    int A = N * B;\n    int Y = nChanels * nContractions;\n\n    int f = (tid % Y) % nChanels;\n    int Case = (tid % Y) / nChanels + 1;\n    int y = (tid / Y) % N;\n    int x = (tid / Y) / N;\n\n    int a, b, c, d, e;\n    T adj_value;\n\n    T sum = (T)0;\n\n    // +-----------+\n    // | 1 + 1 + 1 |\n    // +-----------+\n\n    // Case 1 (1/50): Fix a, b. Contract c, d, e.\n    if (Case == 1) {\n      a = x;\n      b = y;\n\n      for (d = 0; d < N; ++d) {\n        for (e = 0; e < N; ++e) {\n          adj_value = adj[d * N + e];\n          if (adj_value > 0) {\n            for (c = 0; c < N; ++c) {\n              sum += tensor[a * A + b * B + c * C + f] * adj_value;\n            }\n          }\n        }\n      }\n    }\n\n    // Case 2 (3/50): Fix a, d. Contract b, c, e.\n    if (Case == 2) {    \n      a = x;\n      d = y;\n\n      for (e = 0; e < N; ++e) {\n        adj_value = adj[d * N + e];\n        if (adj_value > 0) {\n          for (b = 0; b < N; ++b) {\n            for (c = 0; c < N; ++c) {\n              sum += tensor[a * A + b * B + c * C + f] * adj_value;\n            }\n          }\n        }\n      }  \n    }\n\n    // Case 3 (5/50): Fix b, c. Contract a, d, e.\n    if (Case == 3) {    \n      b = x;\n      c = y;\n\n      for (d = 0; d < N; ++d) {\n        for (e = 0; e < N; ++e) {\n          adj_value = adj[d * N + e];\n          if (adj_value > 0) {\n            for (a = 0; a < N; ++a) {\n              sum += tensor[a * A + b * B + c * C + f] * adj_value;\n            }\n          }\n        }\n      }  \n    }\n\n    // Case 4 (6/50): Fix b, d. Contract a, c, e.\n    if (Case == 4) {\n      b = x;\n      d = y;\n\n      for (e = 0; e < N; ++e) {\n        adj_value = adj[d * N + e];\n        if (adj_value > 0) {\n          for (a = 0; a < N; ++a) {\n            for (c = 0; c < N; ++c) {\n              sum += tensor[a * A + b * B + c * C + f] * adj_value;\n            }\n          }\n        }\n      }\n    }\n\n    // Case 5 (10/50): Fix d, e. Contract a, b, c.\n    if (Case == 5) {    \n      d = x;\n      e = y;\n\n      adj_value = adj[d * N + e];\n      if (adj_value > 0) {\n        for (a = 0; a < N; ++a) {\n          for (b = 0; b < N; ++b) {\n            for (c = 0; c < N; ++c) {\n              sum += tensor[a * A + b * B + c * C + f] * adj_value;\n            }\n          }\n        }\n      }\n    }\n\n    // +-------+\n    // | 1 + 2 |\n    // +-------+\n\n    // Case 6 (11/50): (a, b). Contract (c, d). Singleton (e).\n    if (Case == 6) {\n      a = x;\n      b = y;\n\n      for (d = 0; d < N; ++d) {\n        for (e = 0; e < N; ++e) {\n          adj_value = adj[d * N + e];\n          c = d;\n          sum += tensor[a * A + b * B + c * C + f] * adj_value;\n        }\n      }\n    }\n\n    // Case 7 (13/50): (a, b). Contract (d, e). Singleton (c).\n    if (Case == 7) {\n      a = x;\n      b = y;\n\n      for (d = 0; d < N; ++d) {\n        e = d;\n        adj_value = adj[d * N + e];\n        if (adj_value > 0) {\n          for (c = 0; c < N; ++c) {\n            sum += tensor[a * A + b * B + c * C + f] * adj_value;\n          }\n        }\n      }\n    }\n\n    // Case 8 (17/50): (a, d). Contract (b, c). Singleton (e).\n    if (Case == 8) {\n      a = x;\n      d = y;\n\n      for (e = 0; e < N; ++e) {\n        adj_value = adj[d * N + e];\n        if (adj_value > 0) {\n          for (b = 0; b < N; ++b) {\n            c = b;\n            sum += tensor[a * A + b * B + c * C + f] * adj_value;\n          }\n        }\n      }\n    }\n\n    // Case 9 (18/50): (a, d). Contract (b, e). Singleton (c).\n    if (Case == 9) {\n      a = x;\n      d = y;\n\n      for (e = 0; e < N; ++e) {\n        adj_value = adj[d * N + e];\n        if (adj_value > 0) {\n          b = e;\n          for (c = 0; c < N; ++c) {\n            sum += tensor[a * A + b * B + c * C + f] * adj_value;\n          }\n        }\n      }\n    }\n\n    // Case 10 (23/50): (b, c). Contract (a, d). Singleton (e).\n    if (Case == 10) {\n      b = x;\n      c = y;\n\n      for (d = 0; d < N; ++d) {\n        for (e = 0; e < N; ++e) {\n          adj_value = adj[d * N + e];\n          if (adj_value > 0) {\n            a = d;\n            sum += tensor[a * A + b * B + c * C + f] * adj_value;\n          }\n        }\n      }\n    }\n\n    // Case 11 (26/50): (b, d). Contract (a, c). Singleton (e).\n    if (Case == 11) {\n      b = x;\n      d = y;\n\n      for (e = 0; e < N; ++e) {\n        adj_value = adj[d * N + e];\n        if (adj_value > 0) {\n          for (a = 0; a < N; ++a) {\n            c = a;\n            sum += tensor[a * A + b * B + c * C + f] * adj_value;\n          }\n        }\n      }\n    }\n\n    // Case 12 (27/50): (b, d). Contract (a, e). Singleton (c).\n    if (Case == 12) {\n      b = x;\n      d = y;\n\n      for (e = 0; e < N; ++e) {\n        adj_value = adj[d * N + e];\n        if (adj_value > 0) {\n          a = e;\n          for (int c = 0; c < N; ++c) {\n            sum += tensor[a * A + b * B + c * C + f] * adj_value;\n          }\n        }\n      }\n    }\n\n    // Case 13 (28/50): (b, d). Contract (c, e). Singleton (a).\n    if (Case == 13) {\n      b = x;\n      d = y;\n\n      for (e = 0; e < N; ++e) {\n        adj_value = adj[d * N + e];\n        if (adj_value > 0) {\n          c = e;\n          for (int a = 0; a < N; ++a) {\n            sum += tensor[a * A + b * B + c * C + f] * adj_value;\n          }\n        }\n      }\n    }\n\n    // Case 14 (38/50): (d, e). Contract (a, b). Singleton (c).\n    if (Case == 14) {\n      d = x;\n      e = y;\n\n      adj_value = adj[d * N + e];\n      if (adj_value > 0) {\n        for (int a = 0; a < N; ++a) {\n          b = a;\n          for (int c = 0; c < N; ++c) {\n            sum += tensor[a * A + b * B + c * C + f] * adj_value;\n          }\n        }\n      }\n    }\n\n    // Case 15 (40/50): (d, e). Contract (b, c). Singleton (a).\n    if (Case == 15) {\n      d = x;\n      e = y;\n\n      adj_value = adj[d * N + e];\n      if (adj_value > 0) {\n        for (int b = 0; b < N; ++b) {\n          c = b;\n          for (int a = 0; a < N; ++a) {\n            sum += tensor[a * A + b * B + c * C + f] * adj_value;\n          }\n        }\n      }\n    }\n\n    // +---+\n    // | 3 |\n    // +---+\n\n    // Case 16 (43/50): (a, d). Contract (b, c, e).\n    if (Case == 16) {\n      a = x;\n      d = y;\n\n      for (int e = 0; e < N; ++e) {\n        adj_value = adj[d * N + e];\n        if (adj_value > 0) {\n          b = e;\n          c = e;\n          sum += tensor[a * A + b * B + c * C + f] * adj_value;\n        }\n      }\n    }  \n\n    // Case 17 (46/50): (b, d). Contract (a, c, e).\n    if (Case == 17) {\n      b = x;\n      d = y;\n\n      for (int e = 0; e < N; ++e) {\n        adj_value = adj[d * N + e];\n        if (adj_value > 0) {\n          a = e;\n          c = e;\n          sum += tensor[a * A + b * B + c * C + f] * adj_value;\n        }\n      }\n    }\n\n    // Case 18 (50/50): (d, e). Contract (a, b, c).\n    if (Case == 18) {\n      d = x;\n      e = y;\n\n      adj_value = adj[d * N + e];\n      if (adj_value > 0) {\n        for (int a = 0; a < N; ++a) {\n          b = a;\n          c = a;\n          sum += tensor[a * A + b * B + c * C + f] * adj_value;\n        }\n      }\n    }\n\n    value[tid] = sum;\n  }\n}"
        ]
    },
    "rushlarsen-cuda": {
        "/Users/gbolet/hecbench-roofline/src/rushlarsen-cuda/kernels.cu": [
            "#define T ((int)32)\n\n\ninline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\ninline __host__ __device__ float4 fabs(float4 v)\n{\n    return make_float4(fabs(v.x), fabs(v.y), fabs(v.z), fabs(v.w));\n}\n\n__host__ __device__ __forceinline__ constexpr T floor(T a, T b) {\n  return (a - b + 1) / b;\n}\n\n__global__\nvoid k_forward_rush_larsen(double* states, const double t, const double dt,\n                           const double* parameters, const int n)\n{\n  int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < n) {\n    // Assign states\n    const double Xr1 = states[n * STATE_Xr1 + i];\n    const double Xr2 = states[n * STATE_Xr2 + i];\n    const double Xs = states[n * STATE_Xs + i];\n    const double m = states[n * STATE_m + i];\n    const double h = states[n * STATE_h + i];\n    const double j = states[n * STATE_j + i];\n    const double d = states[n * STATE_d + i];\n    const double f = states[n * STATE_f + i];\n    const double f2 = states[n * STATE_f2 + i];\n    const double fCass = states[n * STATE_fCass + i];\n    const double s = states[n * STATE_s + i];\n    const double r = states[n * STATE_r + i];\n    const double Ca_i = states[n * STATE_Ca_i + i];\n    const double R_prime = states[n * STATE_R_prime + i];\n    const double Ca_SR = states[n * STATE_Ca_SR + i];\n    const double Ca_ss = states[n * STATE_Ca_ss + i];\n    const double Na_i = states[n * STATE_Na_i + i];\n    const double V = states[n * STATE_V + i];\n    const double K_i = states[n * STATE_K_i + i];\n\n    // Assign parameters\n    const double P_kna = parameters[n * PARAM_P_kna + i];\n    const double g_K1 = parameters[n * PARAM_g_K1 + i];\n    const double g_Kr = parameters[n * PARAM_g_Kr + i];\n    const double g_Ks = parameters[n * PARAM_g_Ks + i];\n    const double g_Na = parameters[n * PARAM_g_Na + i];\n    const double g_bna = parameters[n * PARAM_g_bna + i];\n    const double g_CaL = parameters[n * PARAM_g_CaL + i];\n    const double g_bca = parameters[n * PARAM_g_bca + i];\n    const double g_to = parameters[n * PARAM_g_to + i];\n    const double K_mNa = parameters[n * PARAM_K_mNa + i];\n    const double K_mk = parameters[n * PARAM_K_mk + i];\n    const double P_NaK = parameters[n * PARAM_P_NaK + i];\n    const double K_NaCa = parameters[n * PARAM_K_NaCa + i];\n    const double K_sat = parameters[n * PARAM_K_sat + i];\n    const double Km_Ca = parameters[n * PARAM_Km_Ca + i];\n    const double Km_Nai = parameters[n * PARAM_Km_Nai + i];\n    const double alpha = parameters[n * PARAM_alpha + i];\n    const double gamma = parameters[n * PARAM_gamma + i];\n    const double K_pCa = parameters[n * PARAM_K_pCa + i];\n    const double g_pCa = parameters[n * PARAM_g_pCa + i];\n    const double g_pK = parameters[n * PARAM_g_pK + i];\n    const double Buf_c = parameters[n * PARAM_Buf_c + i];\n    const double Buf_sr = parameters[n * PARAM_Buf_sr + i];\n    const double Buf_ss = parameters[n * PARAM_Buf_ss + i];\n    const double Ca_o = parameters[n * PARAM_Ca_o + i];\n    const double EC = parameters[n * PARAM_EC + i];\n    const double K_buf_c = parameters[n * PARAM_K_buf_c + i];\n    const double K_buf_sr = parameters[n * PARAM_K_buf_sr + i];\n    const double K_buf_ss = parameters[n * PARAM_K_buf_ss + i];\n    const double K_up = parameters[n * PARAM_K_up + i];\n    const double V_leak = parameters[n * PARAM_V_leak + i];\n    const double V_rel = parameters[n * PARAM_V_rel + i];\n    const double V_sr = parameters[n * PARAM_V_sr + i];\n    const double V_ss = parameters[n * PARAM_V_ss + i];\n    const double V_xfer = parameters[n * PARAM_V_xfer + i];\n    const double Vmax_up = parameters[n * PARAM_Vmax_up + i];\n    const double k1_prime = parameters[n * PARAM_k1_prime + i];\n    const double k2_prime = parameters[n * PARAM_k2_prime + i];\n    const double k3 = parameters[n * PARAM_k3 + i];\n    const double k4 = parameters[n * PARAM_k4 + i];\n    const double max_sr = parameters[n * PARAM_max_sr + i];\n    const double min_sr = parameters[n * PARAM_min_sr + i];\n    const double Na_o = parameters[n * PARAM_Na_o + i];\n    const double Cm = parameters[n * PARAM_Cm + i];\n    const double F = parameters[n * PARAM_F + i];\n    const double R = parameters[n * PARAM_R + i];\n    const double T = parameters[n * PARAM_T + i];\n    const double V_c = parameters[n * PARAM_V_c + i];\n    const double stim_amplitude = parameters[n * PARAM_stim_amplitude + i];\n    const double stim_duration = parameters[n * PARAM_stim_duration + i];\n    const double stim_period = parameters[n * PARAM_stim_period + i];\n    const double stim_start = parameters[n * PARAM_stim_start + i];\n    const double K_o = parameters[n * PARAM_K_o + i];\n\n    // Expressions for the Reversal potentials component\n    const double E_Na = R*T*log(Na_o/Na_i)/F;\n    const double E_K = R*T*log(K_o/K_i)/F;\n    const double E_Ks = R*T*log((K_o + Na_o*P_kna)/(P_kna*Na_i + K_i))/F;\n    const double E_Ca = 0.5*R*T*log(Ca_o/Ca_i)/F;\n\n    // Expressions for the Inward rectifier potassium current component\n    const double alpha_K1 = 0.1/(1. + 6.14421235332821e-6*exp(0.06*V -\n          0.06*E_K));\n    const double beta_K1 = (0.367879441171442*exp(0.1*V - 0.1*E_K) +\n        3.06060402008027*exp(0.0002*V - 0.0002*E_K))/(1. + exp(0.5*E_K\n          - 0.5*V));\n    const double xK1_inf = alpha_K1/(alpha_K1 + beta_K1);\n    const double i_K1 = 0.430331482911935*g_K1*sqrt(K_o)*(-E_K + V)*xK1_inf;\n\n    // Expressions for the Rapid time dependent potassium current component\n    const double i_Kr = 0.430331482911935*g_Kr*sqrt(K_o)*(-E_K + V)*Xr1*Xr2;\n\n    // Expressions for the Xr1 gate component\n    const double xr1_inf = 1.0/(1. + exp(-26./7. - V/7.));\n    const double alpha_xr1 = 450./(1. + exp(-9./2. - V/10.));\n    const double beta_xr1 = 6./(1. +\n        13.5813245225782*exp(0.0869565217391304*V));\n    const double tau_xr1 = alpha_xr1*beta_xr1;\n    const double dXr1_dt = (-Xr1 + xr1_inf)/tau_xr1;\n    const double dXr1_dt_linearized = -1./tau_xr1;\n    states[n * STATE_Xr1 + i] = (fabs(dXr1_dt_linearized) > 1.0e-8 ? (-1.0 +\n          exp(dt*dXr1_dt_linearized))*dXr1_dt/dXr1_dt_linearized : dt*dXr1_dt)\n      + Xr1;\n\n    // Expressions for the Xr2 gate component\n    const double xr2_inf = 1.0/(1. + exp(11./3. + V/24.));\n    const double alpha_xr2 = 3./(1. + exp(-3. - V/20.));\n    const double beta_xr2 = 1.12/(1. + exp(-3. + V/20.));\n    const double tau_xr2 = alpha_xr2*beta_xr2;\n    const double dXr2_dt = (-Xr2 + xr2_inf)/tau_xr2;\n    const double dXr2_dt_linearized = -1./tau_xr2;\n    states[n * STATE_Xr2 + i] = (fabs(dXr2_dt_linearized) > 1.0e-8 ? (-1.0 +\n          exp(dt*dXr2_dt_linearized))*dXr2_dt/dXr2_dt_linearized : dt*dXr2_dt)\n      + Xr2;\n\n    // Expressions for the Slow time dependent potassium current component\n    const double i_Ks = g_Ks*(Xs*Xs)*(-E_Ks + V);\n\n    // Expressions for the Xs gate component\n    const double xs_inf = 1.0/(1. + exp(-5./14. - V/14.));\n    const double alpha_xs = 1400./sqrt(1. + exp(5./6. - V/6.));\n    const double beta_xs = 1.0/(1. + exp(-7./3. + V/15.));\n    const double tau_xs = 80. + alpha_xs*beta_xs;\n    const double dXs_dt = (-Xs + xs_inf)/tau_xs;\n    const double dXs_dt_linearized = -1./tau_xs;\n    states[n * STATE_Xs + i] = (fabs(dXs_dt_linearized) > 1.0e-8 ? (-1.0 +\n          exp(dt*dXs_dt_linearized))*dXs_dt/dXs_dt_linearized : dt*dXs_dt) +\n      Xs;\n\n    // Expressions for the Fast sodium current component\n    const double i_Na = g_Na*(m*m*m)*(-E_Na + V)*h*j;\n\n    // Expressions for the m gate component\n    const double m_inf = 1.0/((1. +\n          0.00184221158116513*exp(-0.110741971207087*V))*(1. +\n          0.00184221158116513*exp(-0.110741971207087*V)));\n    const double alpha_m = 1.0/(1. + exp(-12. - V/5.));\n    const double beta_m = 0.1/(1. + exp(7. + V/5.)) + 0.1/(1. +\n        exp(-1./4. + V/200.));\n    const double tau_m = alpha_m*beta_m;\n    const double dm_dt = (-m + m_inf)/tau_m;\n    const double dm_dt_linearized = -1./tau_m;\n    states[n * STATE_m + i] = (fabs(dm_dt_linearized) > 1.0e-8 ? (-1.0 +\n          exp(dt*dm_dt_linearized))*dm_dt/dm_dt_linearized : dt*dm_dt) + m;\n\n    // Expressions for the h gate component\n    const double h_inf = 1.0/((1. +\n          15212.5932856544*exp(0.134589502018843*V))*(1. +\n          15212.5932856544*exp(0.134589502018843*V)));\n    const double alpha_h = (V < -40. ?\n        4.43126792958051e-7*exp(-0.147058823529412*V) : 0.);\n    const double beta_h = (V < -40. ? 310000.*exp(0.3485*V) +\n        2.7*exp(0.079*V) : 0.77/(0.13 +\n          0.0497581410839387*exp(-0.0900900900900901*V)));\n    const double tau_h = 1.0/(alpha_h + beta_h);\n    const double dh_dt = (-h + h_inf)/tau_h;\n    const double dh_dt_linearized = -1./tau_h;\n    states[n * STATE_h + i] = (fabs(dh_dt_linearized) > 1.0e-8 ? (-1.0 +\n          exp(dt*dh_dt_linearized))*dh_dt/dh_dt_linearized : dt*dh_dt) + h;\n\n    // Expressions for the j gate component\n    const double j_inf = 1.0/((1. +\n          15212.5932856544*exp(0.134589502018843*V))*(1. +\n          15212.5932856544*exp(0.134589502018843*V)));\n    const double alpha_j = (V < -40. ? (37.78 + V)*(-25428.*exp(0.2444*V)\n          - 6.948e-6*exp(-0.04391*V))/(1. + 50262745825.954*exp(0.311*V))\n        : 0.);\n    const double beta_j = (V < -40. ? 0.02424*exp(-0.01052*V)/(1. +\n          0.00396086833990426*exp(-0.1378*V)) : 0.6*exp(0.057*V)/(1. +\n          0.0407622039783662*exp(-0.1*V)));\n    const double tau_j = 1.0/(alpha_j + beta_j);\n    const double dj_dt = (-j + j_inf)/tau_j;\n    const double dj_dt_linearized = -1./tau_j;\n    states[n * STATE_j + i] = (fabs(dj_dt_linearized) > 1.0e-8 ? (-1.0 +\n          exp(dt*dj_dt_linearized))*dj_dt/dj_dt_linearized : dt*dj_dt) + j;\n\n    // Expressions for the Sodium background current component\n    const double i_b_Na = g_bna*(-E_Na + V);\n\n    // Expressions for the L_type Ca current component\n    const double V_eff = (fabs(-15. + V) < 0.01 ? 0.01 : -15. + V);\n    const double i_CaL = 4.*g_CaL*(F*F)*(-Ca_o +\n        0.25*Ca_ss*exp(2.*F*V_eff/(R*T)))*V_eff*d*f*f2*fCass/(R*T*(-1. +\n          exp(2.*F*V_eff/(R*T))));\n\n    // Expressions for the d gate component\n    const double d_inf = 1.0/(1. +\n        0.344153786865412*exp(-0.133333333333333*V));\n    const double alpha_d = 0.25 + 1.4/(1. + exp(-35./13. - V/13.));\n    const double beta_d = 1.4/(1. + exp(1. + V/5.));\n    const double gamma_d = 1.0/(1. + exp(5./2. - V/20.));\n    const double tau_d = alpha_d*beta_d + gamma_d;\n    const double dd_dt = (-d + d_inf)/tau_d;\n    const double dd_dt_linearized = -1./tau_d;\n    states[n * STATE_d + i] = (fabs(dd_dt_linearized) > 1.0e-8 ? (-1.0 +\n          exp(dt*dd_dt_linearized))*dd_dt/dd_dt_linearized : dt*dd_dt) + d;\n\n    // Expressions for the f gate component\n    const double f_inf = 1.0/(1. + exp(20./7. + V/7.));\n    const double tau_f = 20. + 180./(1. + exp(3. + V/10.)) + 200./(1. +\n        exp(13./10. - V/10.)) + 1102.5*exp(-((27. + V)*(27. + V))/225.);\n    const double df_dt = (-f + f_inf)/tau_f;\n    const double df_dt_linearized = -1./tau_f;\n    states[n * STATE_f + i] = (fabs(df_dt_linearized) > 1.0e-8 ? (-1.0 +\n          exp(dt*df_dt_linearized))*df_dt/df_dt_linearized : dt*df_dt) + f;\n\n    // Expressions for the F2 gate component\n    const double f2_inf = 0.33 + 0.67/(1. + exp(5. + V/7.));\n    const double tau_f2 = 31./(1. + exp(5./2. - V/10.)) + 80./(1. +\n        exp(3. + V/10.)) + 562.*exp(-((27. + V)*(27. + V))/240.);\n    const double df2_dt = (-f2 + f2_inf)/tau_f2;\n    const double df2_dt_linearized = -1./tau_f2;\n    states[n * STATE_f2 + i] = (fabs(df2_dt_linearized) > 1.0e-8 ? (-1.0 +\n          exp(dt*df2_dt_linearized))*df2_dt/df2_dt_linearized : dt*df2_dt) +\n      f2;\n\n    // Expressions for the FCass gate component\n    const double fCass_inf = 0.4 + 0.6/(1. + 400.0*(Ca_ss*Ca_ss));\n    const double tau_fCass = 2. + 80./(1. + 400.0*(Ca_ss*Ca_ss));\n    const double dfCass_dt = (-fCass + fCass_inf)/tau_fCass;\n    const double dfCass_dt_linearized = -1./tau_fCass;\n    states[n * STATE_fCass + i] = (fabs(dfCass_dt_linearized) > 1.0e-8 ? (-1.0 +\n          exp(dt*dfCass_dt_linearized))*dfCass_dt/dfCass_dt_linearized :\n        dt*dfCass_dt) + fCass;\n\n    // Expressions for the Calcium background current component\n    const double i_b_Ca = g_bca*(-E_Ca + V);\n\n    // Expressions for the Transient outward current component\n    const double i_to = g_to*(-E_K + V)*r*s;\n\n    // Expressions for the s gate component\n    const double s_inf = 1.0/(1. + exp(4. + V/5.));\n    const double tau_s = 3. + 5./(1. + exp(-4. + V/5.)) +\n      85.*exp(-((45. + V)*(45. + V))/320.);\n    const double ds_dt = (-s + s_inf)/tau_s;\n    const double ds_dt_linearized = -1./tau_s;\n    states[n * STATE_s + i] = (fabs(ds_dt_linearized) > 1.0e-8 ? (-1.0 +\n          exp(dt*ds_dt_linearized))*ds_dt/ds_dt_linearized : dt*ds_dt) + s;\n\n    // Expressions for the r gate component\n    const double r_inf = 1.0/(1. + exp(10./3. - V/6.));\n    const double tau_r = 0.8 + 9.5*exp(-((40. + V)*(40. + V))/1800.);\n    const double dr_dt = (-r + r_inf)/tau_r;\n    const double dr_dt_linearized = -1./tau_r;\n    states[n * STATE_r + i] = (fabs(dr_dt_linearized) > 1.0e-8 ? (-1.0 +\n          exp(dt*dr_dt_linearized))*dr_dt/dr_dt_linearized : dt*dr_dt) + r;\n\n    // Expressions for the Sodium potassium pump current component\n    const double i_NaK = K_o*P_NaK*Na_i/((K_mNa + Na_i)*(K_mk + K_o)*(1. +\n          0.0353*exp(-F*V/(R*T)) + 0.1245*exp(-0.1*F*V/(R*T))));\n\n    // Expressions for the Sodium calcium exchanger current component\n    const double i_NaCa =\n      K_NaCa*(Ca_o*(Na_i*Na_i*Na_i)*exp(F*gamma*V/(R*T)) -\n          alpha*(Na_o*Na_o*Na_o)*Ca_i*exp(F*(-1. + gamma)*V/(R*T)))/((1. +\n            K_sat*exp(F*(-1. + gamma)*V/(R*T)))*(Ca_o +\n            Km_Ca)*((Km_Nai*Km_Nai*Km_Nai) + (Na_o*Na_o*Na_o)));\n\n    // Expressions for the Calcium pump current component\n    const double i_p_Ca = g_pCa*Ca_i/(K_pCa + Ca_i);\n\n    // Expressions for the Potassium pump current component\n    const double i_p_K = g_pK*(-E_K + V)/(1. +\n        65.4052157419383*exp(-0.167224080267559*V));\n\n    // Expressions for the Calcium dynamics component\n    const double i_up = Vmax_up/(1. + (K_up*K_up)/(Ca_i*Ca_i));\n    const double i_leak = V_leak*(-Ca_i + Ca_SR);\n    const double i_xfer = V_xfer*(-Ca_i + Ca_ss);\n    const double kcasr = max_sr - (max_sr - min_sr)/(1. + (EC*EC)/(Ca_SR*Ca_SR));\n    const double Ca_i_bufc = 1.0/(1. + Buf_c*K_buf_c/((K_buf_c + Ca_i)*(K_buf_c\n            + Ca_i)));\n    const double Ca_sr_bufsr = 1.0/(1. + Buf_sr*K_buf_sr/((K_buf_sr +\n            Ca_SR)*(K_buf_sr + Ca_SR)));\n    const double Ca_ss_bufss = 1.0/(1. + Buf_ss*K_buf_ss/((K_buf_ss +\n            Ca_ss)*(K_buf_ss + Ca_ss)));\n    const double dCa_i_dt = (V_sr*(-i_up + i_leak)/V_c - Cm*(-2.*i_NaCa +\n          i_b_Ca + i_p_Ca)/(2.*F*V_c) + i_xfer)*Ca_i_bufc;\n    const double dCa_i_bufc_dCa_i = 2.*Buf_c*K_buf_c/(((1. +\n            Buf_c*K_buf_c/((K_buf_c + Ca_i)*(K_buf_c + Ca_i)))*(1. +\n            Buf_c*K_buf_c/((K_buf_c + Ca_i)*(K_buf_c + Ca_i))))*((K_buf_c +\n            Ca_i)*(K_buf_c + Ca_i)*(K_buf_c + Ca_i)));\n    const double di_NaCa_dCa_i = -K_NaCa*alpha*(Na_o*Na_o*Na_o)*exp(F*(-1.\n          + gamma)*V/(R*T))/((1. + K_sat*exp(F*(-1. + gamma)*V/(R*T)))*(Ca_o +\n            Km_Ca)*((Km_Nai*Km_Nai*Km_Nai) + (Na_o*Na_o*Na_o)));\n    const double di_up_dCa_i = 2.*Vmax_up*(K_up*K_up)/(((1. +\n            (K_up*K_up)/(Ca_i*Ca_i))*(1. +\n            (K_up*K_up)/(Ca_i*Ca_i)))*(Ca_i*Ca_i*Ca_i));\n    const double di_p_Ca_dCa_i = g_pCa/(K_pCa + Ca_i) - g_pCa*Ca_i/((K_pCa +\n          Ca_i)*(K_pCa + Ca_i));\n    const double dE_Ca_dCa_i = -0.5*R*T/(F*Ca_i);\n    const double dCa_i_dt_linearized = (-V_xfer + V_sr*(-V_leak -\n          di_up_dCa_i)/V_c - Cm*(-2.*di_NaCa_dCa_i - g_bca*dE_Ca_dCa_i +\n            di_p_Ca_dCa_i)/(2.*F*V_c))*Ca_i_bufc + (V_sr*(-i_up + i_leak)/V_c -\n            Cm*(-2.*i_NaCa + i_b_Ca + i_p_Ca)/(2.*F*V_c) + i_xfer)*dCa_i_bufc_dCa_i;\n    states[n * STATE_Ca_i + i] = Ca_i + (fabs(dCa_i_dt_linearized) > 1.0e-8 ?\n        (-1.0 + exp(dt*dCa_i_dt_linearized))*dCa_i_dt/dCa_i_dt_linearized :\n        dt*dCa_i_dt);\n    const double k1 = k1_prime/kcasr;\n    const double k2 = k2_prime*kcasr;\n    const double O = (Ca_ss*Ca_ss)*R_prime*k1/(k3 + (Ca_ss*Ca_ss)*k1);\n    const double dR_prime_dt = k4*(1. - R_prime) - Ca_ss*R_prime*k2;\n    const double dR_prime_dt_linearized = -k4 - Ca_ss*k2;\n    states[n * STATE_R_prime + i] = (fabs(dR_prime_dt_linearized) > 1.0e-8 ? (-1.0 +\n          exp(dt*dR_prime_dt_linearized))*dR_prime_dt/dR_prime_dt_linearized :\n        dt*dR_prime_dt) + R_prime;\n    const double i_rel = V_rel*(-Ca_ss + Ca_SR)*O;\n    const double dCa_SR_dt = (-i_leak - i_rel + i_up)*Ca_sr_bufsr;\n    const double dkcasr_dCa_SR = -2.*(EC*EC)*(max_sr - min_sr)/(((1. +\n            (EC*EC)/(Ca_SR*Ca_SR))*(1. + (EC*EC)/(Ca_SR*Ca_SR)))*(Ca_SR*Ca_SR*Ca_SR));\n    const double dCa_sr_bufsr_dCa_SR = 2.*Buf_sr*K_buf_sr/(((1. +\n            Buf_sr*K_buf_sr/((K_buf_sr + Ca_SR)*(K_buf_sr + Ca_SR)))*(1. +\n            Buf_sr*K_buf_sr/((K_buf_sr + Ca_SR)*(K_buf_sr + Ca_SR))))*((K_buf_sr +\n            Ca_SR)*(K_buf_sr + Ca_SR)*(K_buf_sr + Ca_SR)));\n    const double di_rel_dO = V_rel*(-Ca_ss + Ca_SR);\n    const double dk1_dkcasr = -k1_prime/(kcasr*kcasr);\n    const double dO_dk1 = (Ca_ss*Ca_ss)*R_prime/(k3 + (Ca_ss*Ca_ss)*k1) -\n      pow(Ca_ss, 4.)*R_prime*k1/((k3 + (Ca_ss*Ca_ss)*k1)*(k3 +\n            (Ca_ss*Ca_ss)*k1));\n    const double di_rel_dCa_SR = V_rel*O + V_rel*(-Ca_ss +\n        Ca_SR)*dO_dk1*dk1_dkcasr*dkcasr_dCa_SR;\n    const double dCa_SR_dt_linearized = (-V_leak - di_rel_dCa_SR -\n        dO_dk1*di_rel_dO*dk1_dkcasr*dkcasr_dCa_SR)*Ca_sr_bufsr + (-i_leak - i_rel\n          + i_up)*dCa_sr_bufsr_dCa_SR;\n    states[n * STATE_Ca_SR + i] = Ca_SR + (fabs(dCa_SR_dt_linearized) > 1.0e-8 ?\n        (-1.0 + exp(dt*dCa_SR_dt_linearized))*dCa_SR_dt/dCa_SR_dt_linearized\n        : dt*dCa_SR_dt);\n    const double dCa_ss_dt = (V_sr*i_rel/V_ss - V_c*i_xfer/V_ss -\n        Cm*i_CaL/(2.*F*V_ss))*Ca_ss_bufss;\n    const double dO_dCa_ss = -2.*(Ca_ss*Ca_ss*Ca_ss)*(k1*k1)*R_prime/((k3 +\n          (Ca_ss*Ca_ss)*k1)*(k3 + (Ca_ss*Ca_ss)*k1)) + 2.*Ca_ss*R_prime*k1/(k3 +\n          (Ca_ss*Ca_ss)*k1);\n    const double di_rel_dCa_ss = -V_rel*O + V_rel*(-Ca_ss + Ca_SR)*dO_dCa_ss;\n    const double dCa_ss_bufss_dCa_ss = 2.*Buf_ss*K_buf_ss/(((1. +\n            Buf_ss*K_buf_ss/((K_buf_ss + Ca_ss)*(K_buf_ss + Ca_ss)))*(1. +\n            Buf_ss*K_buf_ss/((K_buf_ss + Ca_ss)*(K_buf_ss + Ca_ss))))*((K_buf_ss +\n            Ca_ss)*(K_buf_ss + Ca_ss)*(K_buf_ss + Ca_ss)));\n    const double di_CaL_dCa_ss =\n      1.0*g_CaL*(F*F)*V_eff*d*exp(2.*F*V_eff/(R*T))*f*f2*fCass/(R*T*(-1. +\n            exp(2.*F*V_eff/(R*T))));\n    const double dCa_ss_dt_linearized = (V_sr*(dO_dCa_ss*di_rel_dO +\n          di_rel_dCa_ss)/V_ss - V_c*V_xfer/V_ss -\n        Cm*di_CaL_dCa_ss/(2.*F*V_ss))*Ca_ss_bufss + (V_sr*i_rel/V_ss -\n        V_c*i_xfer/V_ss - Cm*i_CaL/(2.*F*V_ss))*dCa_ss_bufss_dCa_ss;\n    states[n * STATE_Ca_ss + i] = Ca_ss + (fabs(dCa_ss_dt_linearized) > 1.0e-8 ?\n        (-1.0 + exp(dt*dCa_ss_dt_linearized))*dCa_ss_dt/dCa_ss_dt_linearized\n        : dt*dCa_ss_dt);\n\n    // Expressions for the Sodium dynamics component\n    const double dNa_i_dt = Cm*(-i_Na - i_b_Na - 3.*i_NaCa - 3.*i_NaK)/(F*V_c);\n    const double dE_Na_dNa_i = -R*T/(F*Na_i);\n    const double di_NaCa_dNa_i =\n      3.*Ca_o*K_NaCa*(Na_i*Na_i)*exp(F*gamma*V/(R*T))/((1. +\n            K_sat*exp(F*(-1. + gamma)*V/(R*T)))*(Ca_o +\n            Km_Ca)*((Km_Nai*Km_Nai*Km_Nai) + (Na_o*Na_o*Na_o)));\n    const double di_Na_dE_Na = -g_Na*(m*m*m)*h*j;\n    const double di_NaK_dNa_i = K_o*P_NaK/((K_mNa + Na_i)*(K_mk + K_o)*(1. +\n          0.0353*exp(-F*V/(R*T)) + 0.1245*exp(-0.1*F*V/(R*T)))) -\n      K_o*P_NaK*Na_i/(((K_mNa + Na_i)*(K_mNa + Na_i))*(K_mk + K_o)*(1. +\n            0.0353*exp(-F*V/(R*T)) + 0.1245*exp(-0.1*F*V/(R*T))));\n    const double dNa_i_dt_linearized = Cm*(-3.*di_NaCa_dNa_i - 3.*di_NaK_dNa_i\n        + g_bna*dE_Na_dNa_i - dE_Na_dNa_i*di_Na_dE_Na)/(F*V_c);\n    states[n * STATE_Na_i + i] = Na_i + (fabs(dNa_i_dt_linearized) > 1.0e-8 ?\n        (-1.0 + exp(dt*dNa_i_dt_linearized))*dNa_i_dt/dNa_i_dt_linearized :\n        dt*dNa_i_dt);\n\n    // Expressions for the Membrane component\n    const double i_Stim = (t - stim_period*floor(t/stim_period) <=\n        stim_duration + stim_start && t - stim_period*floor(t/stim_period)\n        >= stim_start ? -stim_amplitude : 0.);\n    const double dV_dt = -i_CaL - i_K1 - i_Kr - i_Ks - i_Na - i_NaCa - i_NaK -\n      i_Stim - i_b_Ca - i_b_Na - i_p_Ca - i_p_K - i_to;\n    const double dalpha_K1_dV = -3.68652741199693e-8*exp(0.06*V -\n        0.06*E_K)/((1. + 6.14421235332821e-6*exp(0.06*V - 0.06*E_K))*(1. +\n            6.14421235332821e-6*exp(0.06*V - 0.06*E_K)));\n    const double di_CaL_dV_eff = 4.*g_CaL*(F*F)*(-Ca_o +\n        0.25*Ca_ss*exp(2.*F*V_eff/(R*T)))*d*f*f2*fCass/(R*T*(-1. +\n          exp(2.*F*V_eff/(R*T)))) - 8.*g_CaL*(F*F*F)*(-Ca_o +\n        0.25*Ca_ss*exp(2.*F*V_eff/(R*T)))*V_eff*d*exp(2.*F*V_eff/(R*T))*f*f2*fCass/((R*R)*(T*T)*((-1.\n            + exp(2.*F*V_eff/(R*T)))*(-1. + exp(2.*F*V_eff/(R*T))))) +\n        2.0*g_CaL*(F*F*F)*Ca_ss*V_eff*d*exp(2.*F*V_eff/(R*T))*f*f2*fCass/((R*R)*(T*T)*(-1.\n              + exp(2.*F*V_eff/(R*T))));\n    const double di_Ks_dV = g_Ks*(Xs*Xs);\n    const double di_p_K_dV = g_pK/(1. +\n        65.4052157419383*exp(-0.167224080267559*V)) +\n      10.9373270471469*g_pK*(-E_K + V)*exp(-0.167224080267559*V)/((1. +\n            65.4052157419383*exp(-0.167224080267559*V))*(1. +\n            65.4052157419383*exp(-0.167224080267559*V)));\n    const double di_to_dV = g_to*r*s;\n    const double dxK1_inf_dbeta_K1 = -alpha_K1/((alpha_K1 + beta_K1)*(alpha_K1 +\n          beta_K1));\n    const double dxK1_inf_dalpha_K1 = 1.0/(alpha_K1 + beta_K1) -\n      alpha_K1/((alpha_K1 + beta_K1)*(alpha_K1 + beta_K1));\n    const double dbeta_K1_dV = (0.000612120804016053*exp(0.0002*V -\n          0.0002*E_K) + 0.0367879441171442*exp(0.1*V - 0.1*E_K))/(1. +\n          exp(0.5*E_K - 0.5*V)) + 0.5*(0.367879441171442*exp(0.1*V -\n            0.1*E_K) + 3.06060402008027*exp(0.0002*V -\n              0.0002*E_K))*exp(0.5*E_K - 0.5*V)/((1. + exp(0.5*E_K -\n                  0.5*V))*(1. + exp(0.5*E_K - 0.5*V)));\n    const double di_K1_dV = 0.430331482911935*g_K1*sqrt(K_o)*xK1_inf +\n      0.430331482911935*g_K1*sqrt(K_o)*(-E_K +\n          V)*(dalpha_K1_dV*dxK1_inf_dalpha_K1 + dbeta_K1_dV*dxK1_inf_dbeta_K1);\n    const double dV_eff_dV = (fabs(-15. + V) < 0.01 ? 0. : 1.);\n    const double di_Na_dV = g_Na*(m*m*m)*h*j;\n    const double di_Kr_dV = 0.430331482911935*g_Kr*sqrt(K_o)*Xr1*Xr2;\n    const double di_NaK_dV = K_o*P_NaK*(0.0353*F*exp(-F*V/(R*T))/(R*T) +\n        0.01245*F*exp(-0.1*F*V/(R*T))/(R*T))*Na_i/((K_mNa + Na_i)*(K_mk +\n          K_o)*((1. + 0.0353*exp(-F*V/(R*T)) +\n              0.1245*exp(-0.1*F*V/(R*T)))*(1. + 0.0353*exp(-F*V/(R*T)) +\n              0.1245*exp(-0.1*F*V/(R*T)))));\n    const double di_K1_dxK1_inf = 0.430331482911935*g_K1*sqrt(K_o)*(-E_K +\n        V);\n    const double di_NaCa_dV =\n      K_NaCa*(Ca_o*F*gamma*(Na_i*Na_i*Na_i)*exp(F*gamma*V/(R*T))/(R*T) -\n          F*alpha*(Na_o*Na_o*Na_o)*(-1. + gamma)*Ca_i*exp(F*(-1. +\n              gamma)*V/(R*T))/(R*T))/((1. + K_sat*exp(F*(-1. +\n                  gamma)*V/(R*T)))*(Ca_o + Km_Ca)*((Km_Nai*Km_Nai*Km_Nai) +\n                (Na_o*Na_o*Na_o))) - F*K_NaCa*K_sat*(-1. +\n                gamma)*(Ca_o*(Na_i*Na_i*Na_i)*exp(F*gamma*V/(R*T)) -\n                  alpha*(Na_o*Na_o*Na_o)*Ca_i*exp(F*(-1. +\n                      gamma)*V/(R*T)))*exp(F*(-1. + gamma)*V/(R*T))/(R*T*((1. +\n                        K_sat*exp(F*(-1. + gamma)*V/(R*T)))*(1. + K_sat*exp(F*(-1. +\n                            gamma)*V/(R*T))))*(Ca_o + Km_Ca)*((Km_Nai*Km_Nai*Km_Nai) +\n                        (Na_o*Na_o*Na_o)));\n    const double dV_dt_linearized = -g_bca - g_bna - di_K1_dV - di_Kr_dV -\n      di_Ks_dV - di_NaCa_dV - di_NaK_dV - di_Na_dV - di_p_K_dV - di_to_dV -\n      (dalpha_K1_dV*dxK1_inf_dalpha_K1 +\n       dbeta_K1_dV*dxK1_inf_dbeta_K1)*di_K1_dxK1_inf - dV_eff_dV*di_CaL_dV_eff;\n    states[n * STATE_V + i] = (fabs(dV_dt_linearized) > 1.0e-8 ? (-1.0 +\n          exp(dt*dV_dt_linearized))*dV_dt/dV_dt_linearized : dt*dV_dt) + V;\n\n    // Expressions for the Potassium dynamics component\n    const double dK_i_dt = Cm*(-i_K1 - i_Kr - i_Ks - i_Stim - i_p_K - i_to +\n        2.*i_NaK)/(F*V_c);\n    const double dE_Ks_dK_i = -R*T/(F*(P_kna*Na_i + K_i));\n    const double dbeta_K1_dE_K = (-0.000612120804016053*exp(0.0002*V -\n          0.0002*E_K) - 0.0367879441171442*exp(0.1*V - 0.1*E_K))/(1. +\n          exp(0.5*E_K - 0.5*V)) - 0.5*(0.367879441171442*exp(0.1*V -\n            0.1*E_K) + 3.06060402008027*exp(0.0002*V -\n              0.0002*E_K))*exp(0.5*E_K - 0.5*V)/((1. + exp(0.5*E_K -\n                  0.5*V))*(1. + exp(0.5*E_K - 0.5*V)));\n    const double di_Kr_dE_K = -0.430331482911935*g_Kr*sqrt(K_o)*Xr1*Xr2;\n    const double dE_K_dK_i = -R*T/(F*K_i);\n    const double di_Ks_dE_Ks = -g_Ks*(Xs*Xs);\n    const double di_to_dE_K = -g_to*r*s;\n    const double dalpha_K1_dE_K = 3.68652741199693e-8*exp(0.06*V -\n        0.06*E_K)/((1. + 6.14421235332821e-6*exp(0.06*V - 0.06*E_K))*(1. +\n            6.14421235332821e-6*exp(0.06*V - 0.06*E_K)));\n    const double di_K1_dE_K = -0.430331482911935*g_K1*sqrt(K_o)*xK1_inf +\n      0.430331482911935*g_K1*sqrt(K_o)*(-E_K +\n          V)*(dalpha_K1_dE_K*dxK1_inf_dalpha_K1 + dbeta_K1_dE_K*dxK1_inf_dbeta_K1);\n    const double di_p_K_dE_K = -g_pK/(1. +\n        65.4052157419383*exp(-0.167224080267559*V));\n    const double dK_i_dt_linearized =\n      Cm*(-(dE_K_dK_i*dalpha_K1_dE_K*dxK1_inf_dalpha_K1 +\n            dE_K_dK_i*dbeta_K1_dE_K*dxK1_inf_dbeta_K1)*di_K1_dxK1_inf -\n          dE_K_dK_i*di_K1_dE_K - dE_K_dK_i*di_Kr_dE_K - dE_K_dK_i*di_p_K_dE_K -\n          dE_K_dK_i*di_to_dE_K - dE_Ks_dK_i*di_Ks_dE_Ks)/(F*V_c);\n    states[n * STATE_K_i + i] = K_i + (fabs(dK_i_dt_linearized) > 1.0e-8 ? (-1.0 +\n          exp(dt*dK_i_dt_linearized))*dK_i_dt/dK_i_dt_linearized : dt*dK_i_dt);\n  }\n}"
        ]
    },
    "popcount-cuda": {
        "/Users/gbolet/hecbench-roofline/src/popcount-cuda/main.cu": [
            "__global__ void pc1 (const unsigned long* __restrict__ data,int* __restrict__ r, const int length)\n{\n  int i = blockIdx.x*blockDim.x+threadIdx.x;\n  if (i >= length) return;\n  unsigned long x = data[i];\n  x -= (x >> 1) & m1;             //put count of each 2 bits into those 2 bits\n  x = (x & m2) + ((x >> 2) & m2); //put count of each 4 bits into those 4 bits \n  x = (x + (x >> 4)) & m4;        //put count of each 8 bits into those 8 bits \n  x += x >>  8;  //put count of each 16 bits into their lowest 8 bits\n  x += x >> 16;  //put count of each 32 bits into their lowest 8 bits\n  x += x >> 32;  //put count of each 64 bits into their lowest 8 bits\n  r[i] = x & 0x7f;\n}",
            "__global__ void pc2 (const unsigned long* __restrict__ data, int* __restrict__ r, const int length)\n{\n  int i = blockIdx.x*blockDim.x+threadIdx.x;\n  if (i >= length) return;\n  unsigned long x = data[i];\n  x -= (x >> 1) & m1;             //put count of each 2 bits into those 2 bits\n  x = (x & m2) + ((x >> 2) & m2); //put count of each 4 bits into those 4 bits \n  x = (x + (x >> 4)) & m4;        //put count of each 8 bits into those 8 bits \n  r[i] = (x * h01) >> 56;  //returns left 8 bits of x + (x<<8) + (x<<16) + (x<<24) + ... \n}",
            "__global__ void pc3 (const unsigned long* __restrict__ data, int* __restrict__ r, const int length)\n{\n  int i = blockIdx.x*blockDim.x+threadIdx.x;\n  if (i >= length) return;\n  char count;\n  unsigned long x = data[i];\n  for (count=0; x; count++) x &= x - 1;\n  r[i] = count;\n}",
            "__global__ void pc4 (const unsigned long* __restrict__ data, int* __restrict__ r, const int length)\n{\n  int i = blockIdx.x*blockDim.x+threadIdx.x;\n  if (i >= length) return;\n  unsigned long x = data[i];\n  char cnt = 0;\n  for (char i = 0; i < 64; i++)\n  {\n    cnt = cnt + (x & 0x1);\n    x = x >> 1;\n  }\n  r[i] = cnt;\n}",
            "__global__ void pc5 (const unsigned long* __restrict__ data, int* __restrict__ r, const int length)\n{\n  int i = blockIdx.x*blockDim.x+threadIdx.x;\n  if (i >= length) return;\n  unsigned long x = data[i];\n  const unsigned char a[256] = { 0,1,1,2,1,2,2,3,1,2,2,3,2,3,3,4,1,2,2,3,2,3,3,4,2,3,3,4,3,4,4,5,1,2,2,3,2,3,3,4,2,3,3,4,3,4,4,5,2,3,3,4,3,4,4,5,3,4,4,5,4,5,5,6,1,2,2,3,2,3,3,4,2,3,3,4,3,4,4,5,2,3,3,4,3,4,4,5,3,4,4,5,4,5,5,6,2,3,3,4,3,4,4,5,3,4,4,5,4,5,5,6,3,4,4,5,4,5,5,6,4,5,5,6,5,6,6,7,1,2,2,3,2,3,3,4,2,3,3,4,3,4,4,5,2,3,3,4,3,4,4,5,3,4,4,5,4,5,5,6,2,3,3,4,3,4,4,5,3,4,4,5,4,5,5,6,3,4,4,5,4,5,5,6,4,5,5,6,5,6,6,7,2,3,3,4,3,4,4,5,3,4,4,5,4,5,5,6,3,4,4,5,4,5,5,6,4,5,5,6,5,6,6,7,3,4,4,5,4,5,5,6,4,5,5,6,5,6,6,7,4,5,5,6,5,6,6,7,5,6,6,7,6,7,7,8};\n  const unsigned char b[256] = { 0,1,1,2,1,2,2,3,1,2,2,3,2,3,3,4,1,2,2,3,2,3,3,4,2,3,3,4,3,4,4,5,1,2,2,3,2,3,3,4,2,3,3,4,3,4,4,5,2,3,3,4,3,4,4,5,3,4,4,5,4,5,5,6,1,2,2,3,2,3,3,4,2,3,3,4,3,4,4,5,2,3,3,4,3,4,4,5,3,4,4,5,4,5,5,6,2,3,3,4,3,4,4,5,3,4,4,5,4,5,5,6,3,4,4,5,4,5,5,6,4,5,5,6,5,6,6,7,1,2,2,3,2,3,3,4,2,3,3,4,3,4,4,5,2,3,3,4,3,4,4,5,3,4,4,5,4,5,5,6,2,3,3,4,3,4,4,5,3,4,4,5,4,5,5,6,3,4,4,5,4,5,5,6,4,5,5,6,5,6,6,7,2,3,3,4,3,4,4,5,3,4,4,5,4,5,5,6,3,4,4,5,4,5,5,6,4,5,5,6,5,6,6,7,3,4,4,5,4,5,5,6,4,5,5,6,5,6,6,7,4,5,5,6,5,6,6,7,5,6,6,7,6,7,7,8};\n  const unsigned char c[256] = { 0,1,1,2,1,2,2,3,1,2,2,3,2,3,3,4,1,2,2,3,2,3,3,4,2,3,3,4,3,4,4,5,1,2,2,3,2,3,3,4,2,3,3,4,3,4,4,5,2,3,3,4,3,4,4,5,3,4,4,5,4,5,5,6,1,2,2,3,2,3,3,4,2,3,3,4,3,4,4,5,2,3,3,4,3,4,4,5,3,4,4,5,4,5,5,6,2,3,3,4,3,4,4,5,3,4,4,5,4,5,5,6,3,4,4,5,4,5,5,6,4,5,5,6,5,6,6,7,1,2,2,3,2,3,3,4,2,3,3,4,3,4,4,5,2,3,3,4,3,4,4,5,3,4,4,5,4,5,5,6,2,3,3,4,3,4,4,5,3,4,4,5,4,5,5,6,3,4,4,5,4,5,5,6,4,5,5,6,5,6,6,7,2,3,3,4,3,4,4,5,3,4,4,5,4,5,5,6,3,4,4,5,4,5,5,6,4,5,5,6,5,6,6,7,3,4,4,5,4,5,5,6,4,5,5,6,5,6,6,7,4,5,5,6,5,6,6,7,5,6,6,7,6,7,7,8};\n  const unsigned char d[256] = { 0,1,1,2,1,2,2,3,1,2,2,3,2,3,3,4,1,2,2,3,2,3,3,4,2,3,3,4,3,4,4,5,1,2,2,3,2,3,3,4,2,3,3,4,3,4,4,5,2,3,3,4,3,4,4,5,3,4,4,5,4,5,5,6,1,2,2,3,2,3,3,4,2,3,3,4,3,4,4,5,2,3,3,4,3,4,4,5,3,4,4,5,4,5,5,6,2,3,3,4,3,4,4,5,3,4,4,5,4,5,5,6,3,4,4,5,4,5,5,6,4,5,5,6,5,6,6,7,1,2,2,3,2,3,3,4,2,3,3,4,3,4,4,5,2,3,3,4,3,4,4,5,3,4,4,5,4,5,5,6,2,3,3,4,3,4,4,5,3,4,4,5,4,5,5,6,3,4,4,5,4,5,5,6,4,5,5,6,5,6,6,7,2,3,3,4,3,4,4,5,3,4,4,5,4,5,5,6,3,4,4,5,4,5,5,6,4,5,5,6,5,6,6,7,3,4,4,5,4,5,5,6,4,5,5,6,5,6,6,7,4,5,5,6,5,6,6,7,5,6,6,7,6,7,7,8};\n\n  unsigned char i1 = a[(x & 0xFF)];\n  unsigned char i2 = a[(x >> 8) & 0xFF];\n  unsigned char i3 = b[(x >> 16) & 0xFF];\n  unsigned char i4 = b[(x >> 24) & 0xFF];\n  unsigned char i5 = c[(x >> 32) & 0xFF];\n  unsigned char i6 = c[(x >> 40) & 0xFF];\n  unsigned char i7 = d[(x >> 48) & 0xFF];\n  unsigned char i8 = d[(x >> 56) & 0xFF];\n  r[i] = (i1+i2)+(i3+i4)+(i5+i6)+(i7+i8);\n}",
            "__global__ void pc6 (const unsigned long* __restrict__ data, int* __restrict__ r, const int length)\n{\n  int i = blockIdx.x*blockDim.x+threadIdx.x;\n  if (i >= length) return;\n  r[i] = __popcll(data[i]);\n}"
        ]
    },
    "sparkler-cuda": {
        "/Users/gbolet/hecbench-roofline/src/sparkler-cuda/main.cu": [
            "#define P double\n\n\n__host__ __device__ size_t nonzero_stride(const size_t& i) {\n  enum {MAX = 499}; // Use prime number to randomize against sizes.\n  return 1 + i % MAX;\n}\n\n__global__ void set_input_matrix_kernel(\n  size_t nr, size_t nc, size_t nru, typename Matrix_t::P* d,\n  size_t base_vector_num, typename Matrix_t::P value) {\n\n  const size_t index = threadIdx.x + blockDim.x * blockIdx.x;\n  if (index >= nr * nc)\n    return;\n\n  const size_t r = index % nr;\n  const size_t c = index / nr;\n\n  typedef typename Matrix_t::P P;\n  const P zero = TCBufTypes<P>::zero();\n\n  const size_t stride = nonzero_stride(r + base_vector_num);\n\n  Matrix_t::eltd(r, c, d, nru) = c % stride ? zero : value;\n}"
        ]
    },
    "overlap-cuda": {
        "/Users/gbolet/hecbench-roofline/src/overlap-cuda/main.cu": [
            "__global__\nvoid incKernel(int *g_out, const int *g_in, int N, int inner_reps) {\n  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n\n  if (idx < N) {\n    for (int i = 0; i < inner_reps; ++i) {\n      g_out[idx] = (i == 0 ? g_in[idx] : g_out[idx]) + 1;\n    }\n  }\n}"
        ]
    },
    "sw4ck-cuda": {
        "/Users/gbolet/hecbench-roofline/src/sw4ck-cuda/kernel5.cpp": [
            "#define float_sw4 double\n\n\n__global__ \nvoid kernel5(\n    const int start0, const int N0, \n    const int start1, const int N1,\n    const int start2, const int N2,\n    const int ifirst, const int ilast,\n    const int jfirst, const int jlast,\n    const int kfirst, const int klast,\n    const int nk,\n    const float_sw4 a1, const float_sw4 sgn,\n    const float_sw4* __restrict__ a_u, \n    const float_sw4* __restrict__ a_mu,\n    const float_sw4* __restrict__ a_lambda,\n    const float_sw4* __restrict__ a_met,\n    const float_sw4* __restrict__ a_jac,\n          float_sw4* __restrict__ a_lu, \n    const float_sw4* __restrict__ a_acof, \n    const float_sw4* __restrict__ a_bope,\n    const float_sw4* __restrict__ a_ghcof, \n    const float_sw4* __restrict__ a_acof_no_gp,\n    const float_sw4* __restrict__ a_ghcof_no_gp, \n    const float_sw4* __restrict__ a_strx,\n    const float_sw4* __restrict__ a_stry ) \n{\n\n  int i = start0 + threadIdx.x + blockIdx.x * blockDim.x;\n  int j = start1 + threadIdx.y + blockIdx.y * blockDim.y;\n  int k = start2 + threadIdx.z + blockIdx.z * blockDim.z;\n  if ((i < N0) && (j < N1) && (k < N2)) {\n    // 5 ops\n    float_sw4 ijac = strx(i) * stry(j) / jac(i, j, k);\n    float_sw4 istry = 1 / (stry(j));\n    float_sw4 istrx = 1 / (strx(i));\n    float_sw4 istrxy = istry * istrx;\n\n    float_sw4 r1 = 0, r2 = 0, r3 = 0;\n\n    // pp derivative (u) (u-eq)\n    // 53 ops, tot=58\n    float_sw4 cof1 = (2 * mu(i - 2, j, k) + la(i - 2, j, k)) *\n      met(1, i - 2, j, k) * met(1, i - 2, j, k) *\n      strx(i - 2);\n    float_sw4 cof2 = (2 * mu(i - 1, j, k) + la(i - 1, j, k)) *\n      met(1, i - 1, j, k) * met(1, i - 1, j, k) *\n      strx(i - 1);\n    float_sw4 cof3 = (2 * mu(i, j, k) + la(i, j, k)) * met(1, i, j, k) *\n      met(1, i, j, k) * strx(i);\n    float_sw4 cof4 = (2 * mu(i + 1, j, k) + la(i + 1, j, k)) *\n      met(1, i + 1, j, k) * met(1, i + 1, j, k) *\n      strx(i + 1);\n    float_sw4 cof5 = (2 * mu(i + 2, j, k) + la(i + 2, j, k)) *\n      met(1, i + 2, j, k) * met(1, i + 2, j, k) *\n      strx(i + 2);\n\n    float_sw4 mux1 = cof2 - tf * (cof3 + cof1);\n    float_sw4 mux2 = cof1 + cof4 + 3 * (cof3 + cof2);\n    float_sw4 mux3 = cof2 + cof5 + 3 * (cof4 + cof3);\n    float_sw4 mux4 = cof4 - tf * (cof3 + cof5);\n\n    r1 = r1 + i6 *\n      (mux1 * (u(1, i - 2, j, k) - u(1, i, j, k)) +\n       mux2 * (u(1, i - 1, j, k) - u(1, i, j, k)) +\n       mux3 * (u(1, i + 1, j, k) - u(1, i, j, k)) +\n       mux4 * (u(1, i + 2, j, k) - u(1, i, j, k))) *\n      istry;\n\n    // qq derivative (u) (u-eq)\n    // 43 ops, tot=101\n    cof1 = (mu(i, j - 2, k)) * met(1, i, j - 2, k) * met(1, i, j - 2, k) *\n      stry(j - 2);\n    cof2 = (mu(i, j - 1, k)) * met(1, i, j - 1, k) * met(1, i, j - 1, k) *\n      stry(j - 1);\n    cof3 = (mu(i, j, k)) * met(1, i, j, k) * met(1, i, j, k) * stry(j);\n    cof4 = (mu(i, j + 1, k)) * met(1, i, j + 1, k) * met(1, i, j + 1, k) *\n      stry(j + 1);\n    cof5 = (mu(i, j + 2, k)) * met(1, i, j + 2, k) * met(1, i, j + 2, k) *\n      stry(j + 2);\n\n    mux1 = cof2 - tf * (cof3 + cof1);\n    mux2 = cof1 + cof4 + 3 * (cof3 + cof2);\n    mux3 = cof2 + cof5 + 3 * (cof4 + cof3);\n    mux4 = cof4 - tf * (cof3 + cof5);\n\n    r1 = r1 + i6 *\n      (mux1 * (u(1, i, j - 2, k) - u(1, i, j, k)) +\n       mux2 * (u(1, i, j - 1, k) - u(1, i, j, k)) +\n       mux3 * (u(1, i, j + 1, k) - u(1, i, j, k)) +\n       mux4 * (u(1, i, j + 2, k) - u(1, i, j, k))) *\n      istrx;\n\n    // pp derivative (v) (v-eq)\n    // 43 ops, tot=144\n    cof1 = (mu(i - 2, j, k)) * met(1, i - 2, j, k) * met(1, i - 2, j, k) *\n      strx(i - 2);\n    cof2 = (mu(i - 1, j, k)) * met(1, i - 1, j, k) * met(1, i - 1, j, k) *\n      strx(i - 1);\n    cof3 = (mu(i, j, k)) * met(1, i, j, k) * met(1, i, j, k) * strx(i);\n    cof4 = (mu(i + 1, j, k)) * met(1, i + 1, j, k) * met(1, i + 1, j, k) *\n      strx(i + 1);\n    cof5 = (mu(i + 2, j, k)) * met(1, i + 2, j, k) * met(1, i + 2, j, k) *\n      strx(i + 2);\n\n    mux1 = cof2 - tf * (cof3 + cof1);\n    mux2 = cof1 + cof4 + 3 * (cof3 + cof2);\n    mux3 = cof2 + cof5 + 3 * (cof4 + cof3);\n    mux4 = cof4 - tf * (cof3 + cof5);\n\n    r2 = r2 + i6 *\n      (mux1 * (u(2, i - 2, j, k) - u(2, i, j, k)) +\n       mux2 * (u(2, i - 1, j, k) - u(2, i, j, k)) +\n       mux3 * (u(2, i + 1, j, k) - u(2, i, j, k)) +\n       mux4 * (u(2, i + 2, j, k) - u(2, i, j, k))) *\n      istry;\n\n    // qq derivative (v) (v-eq)\n    // 53 ops, tot=197\n    cof1 = (2 * mu(i, j - 2, k) + la(i, j - 2, k)) * met(1, i, j - 2, k) *\n      met(1, i, j - 2, k) * stry(j - 2);\n    cof2 = (2 * mu(i, j - 1, k) + la(i, j - 1, k)) * met(1, i, j - 1, k) *\n      met(1, i, j - 1, k) * stry(j - 1);\n    cof3 = (2 * mu(i, j, k) + la(i, j, k)) * met(1, i, j, k) *\n      met(1, i, j, k) * stry(j);\n    cof4 = (2 * mu(i, j + 1, k) + la(i, j + 1, k)) * met(1, i, j + 1, k) *\n      met(1, i, j + 1, k) * stry(j + 1);\n    cof5 = (2 * mu(i, j + 2, k) + la(i, j + 2, k)) * met(1, i, j + 2, k) *\n      met(1, i, j + 2, k) * stry(j + 2);\n    mux1 = cof2 - tf * (cof3 + cof1);\n    mux2 = cof1 + cof4 + 3 * (cof3 + cof2);\n    mux3 = cof2 + cof5 + 3 * (cof4 + cof3);\n    mux4 = cof4 - tf * (cof3 + cof5);\n\n    r2 = r2 + i6 *\n      (mux1 * (u(2, i, j - 2, k) - u(2, i, j, k)) +\n       mux2 * (u(2, i, j - 1, k) - u(2, i, j, k)) +\n       mux3 * (u(2, i, j + 1, k) - u(2, i, j, k)) +\n       mux4 * (u(2, i, j + 2, k) - u(2, i, j, k))) *\n      istrx;\n\n    // pp derivative (w) (w-eq)\n    // 43 ops, tot=240\n    cof1 = (mu(i - 2, j, k)) * met(1, i - 2, j, k) * met(1, i - 2, j, k) *\n      strx(i - 2);\n    cof2 = (mu(i - 1, j, k)) * met(1, i - 1, j, k) * met(1, i - 1, j, k) *\n      strx(i - 1);\n    cof3 = (mu(i, j, k)) * met(1, i, j, k) * met(1, i, j, k) * strx(i);\n    cof4 = (mu(i + 1, j, k)) * met(1, i + 1, j, k) * met(1, i + 1, j, k) *\n      strx(i + 1);\n    cof5 = (mu(i + 2, j, k)) * met(1, i + 2, j, k) * met(1, i + 2, j, k) *\n      strx(i + 2);\n\n    mux1 = cof2 - tf * (cof3 + cof1);\n    mux2 = cof1 + cof4 + 3 * (cof3 + cof2);\n    mux3 = cof2 + cof5 + 3 * (cof4 + cof3);\n    mux4 = cof4 - tf * (cof3 + cof5);\n\n    r3 = r3 + i6 *\n      (mux1 * (u(3, i - 2, j, k) - u(3, i, j, k)) +\n       mux2 * (u(3, i - 1, j, k) - u(3, i, j, k)) +\n       mux3 * (u(3, i + 1, j, k) - u(3, i, j, k)) +\n       mux4 * (u(3, i + 2, j, k) - u(3, i, j, k))) *\n      istry;\n\n    // qq derivative (w) (w-eq)\n    // 43 ops, tot=283\n    cof1 = (mu(i, j - 2, k)) * met(1, i, j - 2, k) * met(1, i, j - 2, k) *\n      stry(j - 2);\n    cof2 = (mu(i, j - 1, k)) * met(1, i, j - 1, k) * met(1, i, j - 1, k) *\n      stry(j - 1);\n    cof3 = (mu(i, j, k)) * met(1, i, j, k) * met(1, i, j, k) * stry(j);\n    cof4 = (mu(i, j + 1, k)) * met(1, i, j + 1, k) * met(1, i, j + 1, k) *\n      stry(j + 1);\n    cof5 = (mu(i, j + 2, k)) * met(1, i, j + 2, k) * met(1, i, j + 2, k) *\n      stry(j + 2);\n    mux1 = cof2 - tf * (cof3 + cof1);\n    mux2 = cof1 + cof4 + 3 * (cof3 + cof2);\n    mux3 = cof2 + cof5 + 3 * (cof4 + cof3);\n    mux4 = cof4 - tf * (cof3 + cof5);\n\n    r3 = r3 + i6 *\n      (mux1 * (u(3, i, j - 2, k) - u(3, i, j, k)) +\n       mux2 * (u(3, i, j - 1, k) - u(3, i, j, k)) +\n       mux3 * (u(3, i, j + 1, k) - u(3, i, j, k)) +\n       mux4 * (u(3, i, j + 2, k) - u(3, i, j, k))) *\n      istrx;\n\n    // All rr-derivatives at once\n    // averaging the coefficient\n    // 54*8*8+25*8 = 3656 ops, tot=3939\n    float_sw4 mucofu2, mucofuv, mucofuw, mucofvw, mucofv2, mucofw2;\n    //#pragma unroll 8\n    for (int q = nk - 7; q <= nk; q++) {\n      mucofu2 = 0;\n      mucofuv = 0;\n      mucofuw = 0;\n      mucofvw = 0;\n      mucofv2 = 0;\n      mucofw2 = 0;\n      //#pragma unroll 8\n      for (int m = nk - 7; m <= nk; m++) {\n        mucofu2 += acof_no_gp(nk - k + 1, nk - q + 1, nk - m + 1) *\n          ((2 * mu(i, j, m) + la(i, j, m)) * met(2, i, j, m) *\n           strx(i) * met(2, i, j, m) * strx(i) +\n           mu(i, j, m) * (met(3, i, j, m) * stry(j) *\n             met(3, i, j, m) * stry(j) +\n             met(4, i, j, m) * met(4, i, j, m)));\n        mucofv2 += acof_no_gp(nk - k + 1, nk - q + 1, nk - m + 1) *\n          ((2 * mu(i, j, m) + la(i, j, m)) * met(3, i, j, m) *\n           stry(j) * met(3, i, j, m) * stry(j) +\n           mu(i, j, m) * (met(2, i, j, m) * strx(i) *\n             met(2, i, j, m) * strx(i) +\n             met(4, i, j, m) * met(4, i, j, m)));\n        mucofw2 +=\n          acof_no_gp(nk - k + 1, nk - q + 1, nk - m + 1) *\n          ((2 * mu(i, j, m) + la(i, j, m)) * met(4, i, j, m) *\n           met(4, i, j, m) +\n           mu(i, j, m) *\n           (met(2, i, j, m) * strx(i) * met(2, i, j, m) * strx(i) +\n            met(3, i, j, m) * stry(j) * met(3, i, j, m) * stry(j)));\n        mucofuv += acof_no_gp(nk - k + 1, nk - q + 1, nk - m + 1) *\n          (mu(i, j, m) + la(i, j, m)) * met(2, i, j, m) *\n          met(3, i, j, m);\n        mucofuw += acof_no_gp(nk - k + 1, nk - q + 1, nk - m + 1) *\n          (mu(i, j, m) + la(i, j, m)) * met(2, i, j, m) *\n          met(4, i, j, m);\n        mucofvw += acof_no_gp(nk - k + 1, nk - q + 1, nk - m + 1) *\n          (mu(i, j, m) + la(i, j, m)) * met(3, i, j, m) *\n          met(4, i, j, m);\n      }\n\n      // Computing the second derivative,\n      r1 += istrxy * mucofu2 * u(1, i, j, q) + mucofuv * u(2, i, j, q) +\n        istry * mucofuw * u(3, i, j, q);\n      r2 += mucofuv * u(1, i, j, q) + istrxy * mucofv2 * u(2, i, j, q) +\n        istrx * mucofvw * u(3, i, j, q);\n      r3 += istry * mucofuw * u(1, i, j, q) +\n        istrx * mucofvw * u(2, i, j, q) +\n        istrxy * mucofw2 * u(3, i, j, q);\n    }\n\n    // Ghost point values, only nonzero for k=nk.\n    // 72 ops., tot=4011\n    mucofu2 = ghcof_no_gp(nk - k + 1) *\n      ((2 * mu(i, j, nk) + la(i, j, nk)) * met(2, i, j, nk) *\n       strx(i) * met(2, i, j, nk) * strx(i) +\n       mu(i, j, nk) * (met(3, i, j, nk) * stry(j) *\n         met(3, i, j, nk) * stry(j) +\n         met(4, i, j, nk) * met(4, i, j, nk)));\n    mucofv2 = ghcof_no_gp(nk - k + 1) *\n      ((2 * mu(i, j, nk) + la(i, j, nk)) * met(3, i, j, nk) *\n       stry(j) * met(3, i, j, nk) * stry(j) +\n       mu(i, j, nk) * (met(2, i, j, nk) * strx(i) *\n         met(2, i, j, nk) * strx(i) +\n         met(4, i, j, nk) * met(4, i, j, nk)));\n    mucofw2 =\n      ghcof_no_gp(nk - k + 1) *\n      ((2 * mu(i, j, nk) + la(i, j, nk)) * met(4, i, j, nk) *\n       met(4, i, j, nk) +\n       mu(i, j, nk) *\n       (met(2, i, j, nk) * strx(i) * met(2, i, j, nk) * strx(i) +\n        met(3, i, j, nk) * stry(j) * met(3, i, j, nk) * stry(j)));\n    mucofuv = ghcof_no_gp(nk - k + 1) * (mu(i, j, nk) + la(i, j, nk)) *\n      met(2, i, j, nk) * met(3, i, j, nk);\n    mucofuw = ghcof_no_gp(nk - k + 1) * (mu(i, j, nk) + la(i, j, nk)) *\n      met(2, i, j, nk) * met(4, i, j, nk);\n    mucofvw = ghcof_no_gp(nk - k + 1) * (mu(i, j, nk) + la(i, j, nk)) *\n      met(3, i, j, nk) * met(4, i, j, nk);\n    r1 += istrxy * mucofu2 * u(1, i, j, nk + 1) +\n      mucofuv * u(2, i, j, nk + 1) +\n      istry * mucofuw * u(3, i, j, nk + 1);\n    r2 += mucofuv * u(1, i, j, nk + 1) +\n      istrxy * mucofv2 * u(2, i, j, nk + 1) +\n      istrx * mucofvw * u(3, i, j, nk + 1);\n    r3 += istry * mucofuw * u(1, i, j, nk + 1) +\n      istrx * mucofvw * u(2, i, j, nk + 1) +\n      istrxy * mucofw2 * u(3, i, j, nk + 1);\n\n    // pq-derivatives (u-eq)\n    // 38 ops., tot=4049\n    r1 +=\n      c2 *\n      (mu(i, j + 2, k) * met(1, i, j + 2, k) * met(1, i, j + 2, k) *\n       (c2 * (u(2, i + 2, j + 2, k) - u(2, i - 2, j + 2, k)) +\n        c1 * (u(2, i + 1, j + 2, k) - u(2, i - 1, j + 2, k))) -\n       mu(i, j - 2, k) * met(1, i, j - 2, k) * met(1, i, j - 2, k) *\n       (c2 * (u(2, i + 2, j - 2, k) - u(2, i - 2, j - 2, k)) +\n        c1 * (u(2, i + 1, j - 2, k) - u(2, i - 1, j - 2, k)))) +\n      c1 *\n      (mu(i, j + 1, k) * met(1, i, j + 1, k) * met(1, i, j + 1, k) *\n       (c2 * (u(2, i + 2, j + 1, k) - u(2, i - 2, j + 1, k)) +\n        c1 * (u(2, i + 1, j + 1, k) - u(2, i - 1, j + 1, k))) -\n       mu(i, j - 1, k) * met(1, i, j - 1, k) * met(1, i, j - 1, k) *\n       (c2 * (u(2, i + 2, j - 1, k) - u(2, i - 2, j - 1, k)) +\n        c1 * (u(2, i + 1, j - 1, k) - u(2, i - 1, j - 1, k))));\n\n    // qp-derivatives (u-eq)\n    // 38 ops. tot=4087\n    r1 +=\n      c2 *\n      (la(i + 2, j, k) * met(1, i + 2, j, k) * met(1, i + 2, j, k) *\n       (c2 * (u(2, i + 2, j + 2, k) - u(2, i + 2, j - 2, k)) +\n        c1 * (u(2, i + 2, j + 1, k) - u(2, i + 2, j - 1, k))) -\n       la(i - 2, j, k) * met(1, i - 2, j, k) * met(1, i - 2, j, k) *\n       (c2 * (u(2, i - 2, j + 2, k) - u(2, i - 2, j - 2, k)) +\n        c1 * (u(2, i - 2, j + 1, k) - u(2, i - 2, j - 1, k)))) +\n      c1 *\n      (la(i + 1, j, k) * met(1, i + 1, j, k) * met(1, i + 1, j, k) *\n       (c2 * (u(2, i + 1, j + 2, k) - u(2, i + 1, j - 2, k)) +\n        c1 * (u(2, i + 1, j + 1, k) - u(2, i + 1, j - 1, k))) -\n       la(i - 1, j, k) * met(1, i - 1, j, k) * met(1, i - 1, j, k) *\n       (c2 * (u(2, i - 1, j + 2, k) - u(2, i - 1, j - 2, k)) +\n        c1 * (u(2, i - 1, j + 1, k) - u(2, i - 1, j - 1, k))));\n\n    // pq-derivatives (v-eq)\n    // 38 ops. , tot=4125\n    r2 +=\n      c2 *\n      (la(i, j + 2, k) * met(1, i, j + 2, k) * met(1, i, j + 2, k) *\n       (c2 * (u(1, i + 2, j + 2, k) - u(1, i - 2, j + 2, k)) +\n        c1 * (u(1, i + 1, j + 2, k) - u(1, i - 1, j + 2, k))) -\n       la(i, j - 2, k) * met(1, i, j - 2, k) * met(1, i, j - 2, k) *\n       (c2 * (u(1, i + 2, j - 2, k) - u(1, i - 2, j - 2, k)) +\n        c1 * (u(1, i + 1, j - 2, k) - u(1, i - 1, j - 2, k)))) +\n      c1 *\n      (la(i, j + 1, k) * met(1, i, j + 1, k) * met(1, i, j + 1, k) *\n       (c2 * (u(1, i + 2, j + 1, k) - u(1, i - 2, j + 1, k)) +\n        c1 * (u(1, i + 1, j + 1, k) - u(1, i - 1, j + 1, k))) -\n       la(i, j - 1, k) * met(1, i, j - 1, k) * met(1, i, j - 1, k) *\n       (c2 * (u(1, i + 2, j - 1, k) - u(1, i - 2, j - 1, k)) +\n        c1 * (u(1, i + 1, j - 1, k) - u(1, i - 1, j - 1, k))));\n\n    //* qp-derivatives (v-eq)\n    // 38 ops., tot=4163\n    r2 +=\n      c2 *\n      (mu(i + 2, j, k) * met(1, i + 2, j, k) * met(1, i + 2, j, k) *\n       (c2 * (u(1, i + 2, j + 2, k) - u(1, i + 2, j - 2, k)) +\n        c1 * (u(1, i + 2, j + 1, k) - u(1, i + 2, j - 1, k))) -\n       mu(i - 2, j, k) * met(1, i - 2, j, k) * met(1, i - 2, j, k) *\n       (c2 * (u(1, i - 2, j + 2, k) - u(1, i - 2, j - 2, k)) +\n        c1 * (u(1, i - 2, j + 1, k) - u(1, i - 2, j - 1, k)))) +\n      c1 *\n      (mu(i + 1, j, k) * met(1, i + 1, j, k) * met(1, i + 1, j, k) *\n       (c2 * (u(1, i + 1, j + 2, k) - u(1, i + 1, j - 2, k)) +\n        c1 * (u(1, i + 1, j + 1, k) - u(1, i + 1, j - 1, k))) -\n       mu(i - 1, j, k) * met(1, i - 1, j, k) * met(1, i - 1, j, k) *\n       (c2 * (u(1, i - 1, j + 2, k) - u(1, i - 1, j - 2, k)) +\n        c1 * (u(1, i - 1, j + 1, k) - u(1, i - 1, j - 1, k))));\n\n    // rp - derivatives\n    // 24*8 = 192 ops, tot=4355\n    float_sw4 dudrm2 = 0, dudrm1 = 0, dudrp1 = 0, dudrp2 = 0;\n    float_sw4 dvdrm2 = 0, dvdrm1 = 0, dvdrp1 = 0, dvdrp2 = 0;\n    float_sw4 dwdrm2 = 0, dwdrm1 = 0, dwdrp1 = 0, dwdrp2 = 0;\n    //#pragma unroll 8\n    for (int q = nk - 7; q <= nk; q++) {\n      dudrm2 -= bope(nk - k + 1, nk - q + 1) * u(1, i - 2, j, q);\n      dvdrm2 -= bope(nk - k + 1, nk - q + 1) * u(2, i - 2, j, q);\n      dwdrm2 -= bope(nk - k + 1, nk - q + 1) * u(3, i - 2, j, q);\n      dudrm1 -= bope(nk - k + 1, nk - q + 1) * u(1, i - 1, j, q);\n      dvdrm1 -= bope(nk - k + 1, nk - q + 1) * u(2, i - 1, j, q);\n      dwdrm1 -= bope(nk - k + 1, nk - q + 1) * u(3, i - 1, j, q);\n      dudrp2 -= bope(nk - k + 1, nk - q + 1) * u(1, i + 2, j, q);\n      dvdrp2 -= bope(nk - k + 1, nk - q + 1) * u(2, i + 2, j, q);\n      dwdrp2 -= bope(nk - k + 1, nk - q + 1) * u(3, i + 2, j, q);\n      dudrp1 -= bope(nk - k + 1, nk - q + 1) * u(1, i + 1, j, q);\n      dvdrp1 -= bope(nk - k + 1, nk - q + 1) * u(2, i + 1, j, q);\n      dwdrp1 -= bope(nk - k + 1, nk - q + 1) * u(3, i + 1, j, q);\n    }\n\n    // rp derivatives (u-eq)\n    // 67 ops, tot=4422\n    r1 += (c2 * ((2 * mu(i + 2, j, k) + la(i + 2, j, k)) *\n          met(2, i + 2, j, k) * met(1, i + 2, j, k) *\n          strx(i + 2) * dudrp2 +\n          la(i + 2, j, k) * met(3, i + 2, j, k) *\n          met(1, i + 2, j, k) * dvdrp2 * stry(j) +\n          la(i + 2, j, k) * met(4, i + 2, j, k) *\n          met(1, i + 2, j, k) * dwdrp2 -\n          ((2 * mu(i - 2, j, k) + la(i - 2, j, k)) *\n           met(2, i - 2, j, k) * met(1, i - 2, j, k) *\n           strx(i - 2) * dudrm2 +\n           la(i - 2, j, k) * met(3, i - 2, j, k) *\n           met(1, i - 2, j, k) * dvdrm2 * stry(j) +\n           la(i - 2, j, k) * met(4, i - 2, j, k) *\n           met(1, i - 2, j, k) * dwdrm2)) +\n        c1 * ((2 * mu(i + 1, j, k) + la(i + 1, j, k)) *\n          met(2, i + 1, j, k) * met(1, i + 1, j, k) *\n          strx(i + 1) * dudrp1 +\n          la(i + 1, j, k) * met(3, i + 1, j, k) *\n          met(1, i + 1, j, k) * dvdrp1 * stry(j) +\n          la(i + 1, j, k) * met(4, i + 1, j, k) *\n          met(1, i + 1, j, k) * dwdrp1 -\n          ((2 * mu(i - 1, j, k) + la(i - 1, j, k)) *\n           met(2, i - 1, j, k) * met(1, i - 1, j, k) *\n           strx(i - 1) * dudrm1 +\n           la(i - 1, j, k) * met(3, i - 1, j, k) *\n           met(1, i - 1, j, k) * dvdrm1 * stry(j) +\n           la(i - 1, j, k) * met(4, i - 1, j, k) *\n           met(1, i - 1, j, k) * dwdrm1))) *\n           istry;\n\n    // rp derivatives (v-eq)\n    // 42 ops, tot=4464\n    r2 +=\n      c2 * (mu(i + 2, j, k) * met(3, i + 2, j, k) *\n          met(1, i + 2, j, k) * dudrp2 +\n          mu(i + 2, j, k) * met(2, i + 2, j, k) *\n          met(1, i + 2, j, k) * dvdrp2 * strx(i + 2) * istry -\n          (mu(i - 2, j, k) * met(3, i - 2, j, k) *\n           met(1, i - 2, j, k) * dudrm2 +\n           mu(i - 2, j, k) * met(2, i - 2, j, k) *\n           met(1, i - 2, j, k) * dvdrm2 * strx(i - 2) * istry)) +\n      c1 * (mu(i + 1, j, k) * met(3, i + 1, j, k) *\n          met(1, i + 1, j, k) * dudrp1 +\n          mu(i + 1, j, k) * met(2, i + 1, j, k) *\n          met(1, i + 1, j, k) * dvdrp1 * strx(i + 1) * istry -\n          (mu(i - 1, j, k) * met(3, i - 1, j, k) *\n           met(1, i - 1, j, k) * dudrm1 +\n           mu(i - 1, j, k) * met(2, i - 1, j, k) *\n           met(1, i - 1, j, k) * dvdrm1 * strx(i - 1) * istry));\n\n    // rp derivatives (w-eq)\n    // 38 ops, tot=4502\n    r3 +=\n      istry * (c2 * (mu(i + 2, j, k) * met(4, i + 2, j, k) *\n            met(1, i + 2, j, k) * dudrp2 +\n            mu(i + 2, j, k) * met(2, i + 2, j, k) *\n            met(1, i + 2, j, k) * dwdrp2 * strx(i + 2) -\n            (mu(i - 2, j, k) * met(4, i - 2, j, k) *\n             met(1, i - 2, j, k) * dudrm2 +\n             mu(i - 2, j, k) * met(2, i - 2, j, k) *\n             met(1, i - 2, j, k) * dwdrm2 * strx(i - 2))) +\n          c1 * (mu(i + 1, j, k) * met(4, i + 1, j, k) *\n            met(1, i + 1, j, k) * dudrp1 +\n            mu(i + 1, j, k) * met(2, i + 1, j, k) *\n            met(1, i + 1, j, k) * dwdrp1 * strx(i + 1) -\n            (mu(i - 1, j, k) * met(4, i - 1, j, k) *\n             met(1, i - 1, j, k) * dudrm1 +\n             mu(i - 1, j, k) * met(2, i - 1, j, k) *\n             met(1, i - 1, j, k) * dwdrm1 * strx(i - 1))));\n\n    // rq - derivatives\n    // 24*8 = 192 ops , tot=4694\n\n    dudrm2 = 0;\n    dudrm1 = 0;\n    dudrp1 = 0;\n    dudrp2 = 0;\n    dvdrm2 = 0;\n    dvdrm1 = 0;\n    dvdrp1 = 0;\n    dvdrp2 = 0;\n    dwdrm2 = 0;\n    dwdrm1 = 0;\n    dwdrp1 = 0;\n    dwdrp2 = 0;\n    //#pragma unroll 8\n    for (int q = nk - 7; q <= nk; q++) {\n      dudrm2 -= bope(nk - k + 1, nk - q + 1) * u(1, i, j - 2, q);\n      dvdrm2 -= bope(nk - k + 1, nk - q + 1) * u(2, i, j - 2, q);\n      dwdrm2 -= bope(nk - k + 1, nk - q + 1) * u(3, i, j - 2, q);\n      dudrm1 -= bope(nk - k + 1, nk - q + 1) * u(1, i, j - 1, q);\n      dvdrm1 -= bope(nk - k + 1, nk - q + 1) * u(2, i, j - 1, q);\n      dwdrm1 -= bope(nk - k + 1, nk - q + 1) * u(3, i, j - 1, q);\n      dudrp2 -= bope(nk - k + 1, nk - q + 1) * u(1, i, j + 2, q);\n      dvdrp2 -= bope(nk - k + 1, nk - q + 1) * u(2, i, j + 2, q);\n      dwdrp2 -= bope(nk - k + 1, nk - q + 1) * u(3, i, j + 2, q);\n      dudrp1 -= bope(nk - k + 1, nk - q + 1) * u(1, i, j + 1, q);\n      dvdrp1 -= bope(nk - k + 1, nk - q + 1) * u(2, i, j + 1, q);\n      dwdrp1 -= bope(nk - k + 1, nk - q + 1) * u(3, i, j + 1, q);\n    }\n\n    // rq derivatives (u-eq)\n    // 42 ops, tot=4736\n    r1 += c2 * (mu(i, j + 2, k) * met(3, i, j + 2, k) *\n        met(1, i, j + 2, k) * dudrp2 * stry(j + 2) * istrx +\n        mu(i, j + 2, k) * met(2, i, j + 2, k) *\n        met(1, i, j + 2, k) * dvdrp2 -\n        (mu(i, j - 2, k) * met(3, i, j - 2, k) *\n         met(1, i, j - 2, k) * dudrm2 * stry(j - 2) * istrx +\n         mu(i, j - 2, k) * met(2, i, j - 2, k) *\n         met(1, i, j - 2, k) * dvdrm2)) +\n      c1 * (mu(i, j + 1, k) * met(3, i, j + 1, k) *\n          met(1, i, j + 1, k) * dudrp1 * stry(j + 1) * istrx +\n          mu(i, j + 1, k) * met(2, i, j + 1, k) *\n          met(1, i, j + 1, k) * dvdrp1 -\n          (mu(i, j - 1, k) * met(3, i, j - 1, k) *\n           met(1, i, j - 1, k) * dudrm1 * stry(j - 1) * istrx +\n           mu(i, j - 1, k) * met(2, i, j - 1, k) *\n           met(1, i, j - 1, k) * dvdrm1));\n\n    // rq derivatives (v-eq)\n    // 70 ops, tot=4806\n    r2 += c2 * (la(i, j + 2, k) * met(2, i, j + 2, k) *\n        met(1, i, j + 2, k) * dudrp2 +\n        (2 * mu(i, j + 2, k) + la(i, j + 2, k)) *\n        met(3, i, j + 2, k) * met(1, i, j + 2, k) * dvdrp2 *\n        stry(j + 2) * istrx +\n        la(i, j + 2, k) * met(4, i, j + 2, k) *\n        met(1, i, j + 2, k) * dwdrp2 * istrx -\n        (la(i, j - 2, k) * met(2, i, j - 2, k) *\n         met(1, i, j - 2, k) * dudrm2 +\n         (2 * mu(i, j - 2, k) + la(i, j - 2, k)) *\n         met(3, i, j - 2, k) * met(1, i, j - 2, k) * dvdrm2 *\n         stry(j - 2) * istrx +\n         la(i, j - 2, k) * met(4, i, j - 2, k) *\n         met(1, i, j - 2, k) * dwdrm2 * istrx)) +\n      c1 * (la(i, j + 1, k) * met(2, i, j + 1, k) *\n          met(1, i, j + 1, k) * dudrp1 +\n          (2 * mu(i, j + 1, k) + la(i, j + 1, k)) *\n          met(3, i, j + 1, k) * met(1, i, j + 1, k) * dvdrp1 *\n          stry(j + 1) * istrx +\n          la(i, j + 1, k) * met(4, i, j + 1, k) *\n          met(1, i, j + 1, k) * dwdrp1 * istrx -\n          (la(i, j - 1, k) * met(2, i, j - 1, k) *\n           met(1, i, j - 1, k) * dudrm1 +\n           (2 * mu(i, j - 1, k) + la(i, j - 1, k)) *\n           met(3, i, j - 1, k) * met(1, i, j - 1, k) * dvdrm1 *\n           stry(j - 1) * istrx +\n           la(i, j - 1, k) * met(4, i, j - 1, k) *\n           met(1, i, j - 1, k) * dwdrm1 * istrx));\n\n    // rq derivatives (w-eq)\n    // 39 ops, tot=4845\n    r3 += (c2 * (mu(i, j + 2, k) * met(3, i, j + 2, k) *\n          met(1, i, j + 2, k) * dwdrp2 * stry(j + 2) +\n          mu(i, j + 2, k) * met(4, i, j + 2, k) *\n          met(1, i, j + 2, k) * dvdrp2 -\n          (mu(i, j - 2, k) * met(3, i, j - 2, k) *\n           met(1, i, j - 2, k) * dwdrm2 * stry(j - 2) +\n           mu(i, j - 2, k) * met(4, i, j - 2, k) *\n           met(1, i, j - 2, k) * dvdrm2)) +\n        c1 * (mu(i, j + 1, k) * met(3, i, j + 1, k) *\n          met(1, i, j + 1, k) * dwdrp1 * stry(j + 1) +\n          mu(i, j + 1, k) * met(4, i, j + 1, k) *\n          met(1, i, j + 1, k) * dvdrp1 -\n          (mu(i, j - 1, k) * met(3, i, j - 1, k) *\n           met(1, i, j - 1, k) * dwdrm1 * stry(j - 1) +\n           mu(i, j - 1, k) * met(4, i, j - 1, k) *\n           met(1, i, j - 1, k) * dvdrm1))) *\n      istrx;\n\n    // pr and qr derivatives at once\n    // in loop: 8*(53+53+43) = 1192 ops, tot=6037\n    //#pragma unroll 8\n    for (int q = nk - 7; q <= nk; q++) {\n      // (u-eq)\n      // 53 ops\n      r1 -= bope(nk - k + 1, nk - q + 1) *\n        (\n         // pr\n         (2 * mu(i, j, q) + la(i, j, q)) * met(2, i, j, q) *\n         met(1, i, j, q) *\n         (c2 * (u(1, i + 2, j, q) - u(1, i - 2, j, q)) +\n          c1 * (u(1, i + 1, j, q) - u(1, i - 1, j, q))) *\n         strx(i) * istry +\n         mu(i, j, q) * met(3, i, j, q) * met(1, i, j, q) *\n         (c2 * (u(2, i + 2, j, q) - u(2, i - 2, j, q)) +\n          c1 * (u(2, i + 1, j, q) - u(2, i - 1, j, q))) +\n         mu(i, j, q) * met(4, i, j, q) * met(1, i, j, q) *\n         (c2 * (u(3, i + 2, j, q) - u(3, i - 2, j, q)) +\n          c1 * (u(3, i + 1, j, q) - u(3, i - 1, j, q))) *\n         istry\n         // qr\n         + mu(i, j, q) * met(3, i, j, q) * met(1, i, j, q) *\n         (c2 * (u(1, i, j + 2, q) - u(1, i, j - 2, q)) +\n          c1 * (u(1, i, j + 1, q) - u(1, i, j - 1, q))) *\n         stry(j) * istrx +\n         la(i, j, q) * met(2, i, j, q) * met(1, i, j, q) *\n         (c2 * (u(2, i, j + 2, q) - u(2, i, j - 2, q)) +\n          c1 * (u(2, i, j + 1, q) - u(2, i, j - 1, q))));\n\n      // (v-eq)\n      // 53 ops\n      r2 -= bope(nk - k + 1, nk - q + 1) *\n        (\n         // pr\n         la(i, j, q) * met(3, i, j, q) * met(1, i, j, q) *\n         (c2 * (u(1, i + 2, j, q) - u(1, i - 2, j, q)) +\n          c1 * (u(1, i + 1, j, q) - u(1, i - 1, j, q))) +\n         mu(i, j, q) * met(2, i, j, q) * met(1, i, j, q) *\n         (c2 * (u(2, i + 2, j, q) - u(2, i - 2, j, q)) +\n          c1 * (u(2, i + 1, j, q) - u(2, i - 1, j, q))) *\n         strx(i) * istry\n         // qr\n         + mu(i, j, q) * met(2, i, j, q) * met(1, i, j, q) *\n         (c2 * (u(1, i, j + 2, q) - u(1, i, j - 2, q)) +\n          c1 * (u(1, i, j + 1, q) - u(1, i, j - 1, q))) +\n         (2 * mu(i, j, q) + la(i, j, q)) * met(3, i, j, q) *\n         met(1, i, j, q) *\n         (c2 * (u(2, i, j + 2, q) - u(2, i, j - 2, q)) +\n          c1 * (u(2, i, j + 1, q) - u(2, i, j - 1, q))) *\n         stry(j) * istrx +\n         mu(i, j, q) * met(4, i, j, q) * met(1, i, j, q) *\n         (c2 * (u(3, i, j + 2, q) - u(3, i, j - 2, q)) +\n          c1 * (u(3, i, j + 1, q) - u(3, i, j - 1, q))) *\n         istrx);\n\n      // (w-eq)\n      // 43 ops\n      r3 -= bope(nk - k + 1, nk - q + 1) *\n        (\n         // pr\n         la(i, j, q) * met(4, i, j, q) * met(1, i, j, q) *\n         (c2 * (u(1, i + 2, j, q) - u(1, i - 2, j, q)) +\n          c1 * (u(1, i + 1, j, q) - u(1, i - 1, j, q))) *\n         istry +\n         mu(i, j, q) * met(2, i, j, q) * met(1, i, j, q) *\n         (c2 * (u(3, i + 2, j, q) - u(3, i - 2, j, q)) +\n          c1 * (u(3, i + 1, j, q) - u(3, i - 1, j, q))) *\n         strx(i) * istry\n         // qr\n         + mu(i, j, q) * met(3, i, j, q) * met(1, i, j, q) *\n         (c2 * (u(3, i, j + 2, q) - u(3, i, j - 2, q)) +\n          c1 * (u(3, i, j + 1, q) - u(3, i, j - 1, q))) *\n         stry(j) * istrx +\n         la(i, j, q) * met(4, i, j, q) * met(1, i, j, q) *\n         (c2 * (u(2, i, j + 2, q) - u(2, i, j - 2, q)) +\n          c1 * (u(2, i, j + 1, q) - u(2, i, j - 1, q))) *\n         istrx);\n    }\n\n    // 12 ops, tot=6049\n    lu(1, i, j, k) = a1 * lu(1, i, j, k) + sgn * r1 * ijac;\n    lu(2, i, j, k) = a1 * lu(2, i, j, k) + sgn * r2 * ijac;\n    lu(3, i, j, k) = a1 * lu(3, i, j, k) + sgn * r3 * ijac;\n  }\n}"
        ]
    },
    "loopback-cuda": {
        "/Users/gbolet/hecbench-roofline/src/loopback-cuda/kernels.cu": [
            "inline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\ninline __host__ __device__ float4 expf(float4 v)\n{\n    return make_float4(expf(v.x), expf(v.y), expf(v.z), expf(v.w));\n}\n\ninline __host__ __device__ float4 fmaxf(float4 a, float4 b)\n{\n    return make_float4(fmaxf(a.x,b.x), fmaxf(a.y,b.y), fmaxf(a.z,b.z), fmaxf(a.w,b.w));\n}\n\ninline  __host__ __device__ float4 fminf(float4 a, float4 b)\n{\n    return make_float4(fminf(a.x,b.x), fminf(a.y,b.y), fminf(a.z,b.z), fminf(a.w,b.w));\n}\n\ninline __host__ __device__ float4 sqrtf(float4 v)\n{\n    return make_float4(sqrtf(v.x), sqrtf(v.y), sqrtf(v.z), sqrtf(v.w));\n}\n\n__device__ unsigned LCGStep(unsigned &z)\n{\n  return z = (1664525 * z + 1013904223);\n}\n\n__device__ unsigned TausStep(unsigned &z, int S1, int S2, int S3, unsigned M)\n{\n  unsigned b = (((z << S1) ^ z) >> S2);\n  return z = (((z & M) << S3) ^ b);\n}\n\n__device__ void boxMuller(float u1, float u2, float &uo1, float &uo2)\n{\n  float z1 = sqrtf(-2.0f * logf(u1));\n  float s1 = sinf(2.0f * PI * u2);\n  float s2 = cosf(2.0f * PI * u2);\n  uo1 = z1 * s1;\n  uo2 = z1 * s2;\n}\n\n__device__ float getRandomValueTauswortheUniform(unsigned &z1, unsigned &z2, unsigned &z3, unsigned &z4)\n{\n  unsigned taus = TausStep(z1, 13, 19, 12, 4294967294U) ^ \n                  TausStep(z2, 2, 25, 4, 4294967288U) ^ TausStep(z3, 3, 11, 17, 4294967280U);\n  unsigned lcg = LCGStep(z4);\n\n  return 2.3283064365387e-10f * (taus ^ lcg);  // taus+\n}\n\n__device__ float getRandomValueTausworthe(unsigned &z1, unsigned &z2, unsigned &z3, \n                                          unsigned &z4, float &temporary, unsigned phase)\n{\n  if (phase & 1)\n  {\n    // Return second value of pair\n    return temporary;\n  }\n  else\n  {\n    float t1, t2, t3;\n    // Phase is even, generate pair, return first of values, store second\n    t1 = getRandomValueTauswortheUniform(z1, z2, z3, z4);\n    t2 = getRandomValueTauswortheUniform(z1, z2, z3, z4);\n    boxMuller(t1, t2, t3, temporary);\n    return t3;\n  }\n}\n\n__device__ float tausworthe_lookback_sim(\n    unsigned T, float VOL_0, float EPS_0, \n    float A_0, float A_1, float A_2, float S_0,\n    float MU, unsigned &z1, unsigned &z2,\n    unsigned &z3, unsigned &z4, float* path)\n{\n  float temp_random_value;\n  float vol = VOL_0, eps = EPS_0;\n  float s = S_0;\n  int base = threadIdx.x;\n\n  for (unsigned t = 0; t < T; t++)\n  {\n    // store the current asset price\n    path[base] = s;\n    base += LOOKBACK_TAUSWORTHE_NUM_THREADS;\n\n    // time-varying volatility in the GARCH model\n    vol = sqrtf(A_0 + A_1 * vol * vol + A_2 * eps * eps);\n\n    // size of next asset movement depends in part on the size of the most recent movement\n    eps = getRandomValueTausworthe(z1, z2, z3, z4, temp_random_value, t) * vol;\n    // s may become infinite\n    eps = fmaxf(fminf(eps, 1.f), -1.f);\n\n    // next price\n    s *= expf(MU + eps);\n  }\n\n  // Look back at path to find payoff\n  float sum = 0;\n  for (unsigned t = 0; t < T; t++)\n  {\n    base -= LOOKBACK_TAUSWORTHE_NUM_THREADS;\n    sum += fmaxf(path[base] - s, 0.f);\n  }\n  return sum;\n}\n\n__global__ void tausworthe_lookback(\n    unsigned num_cycles,\n    const unsigned int *__restrict__ seedValues,\n    float *__restrict__ simulationResultsMean,\n    float *__restrict__ simulationResultsVariance,\n    const float *__restrict__ g_VOL_0,\n    const float *__restrict__ g_EPS_0,\n    const float *__restrict__ g_A_0,\n    const float *__restrict__ g_A_1,\n    const float *__restrict__ g_A_2,\n    const float *__restrict__ g_S_0,\n    const float *__restrict__ g_MU)\n{\n  __shared__ float path[LOOKBACK_TAUSWORTHE_NUM_THREADS*LOOKBACK_MAX_T];\n\n  unsigned address = blockIdx.x * blockDim.x + threadIdx.x;\n\n  // Initialise tausworth with seeds\n  unsigned z1 = seedValues[address];\n  unsigned z2 = seedValues[address +     TAUSWORTHE_TOTAL_NUM_THREADS];\n  unsigned z3 = seedValues[address + 2 * TAUSWORTHE_TOTAL_NUM_THREADS];\n  unsigned z4 = seedValues[address + 3 * TAUSWORTHE_TOTAL_NUM_THREADS];\n\n  float VOL_0, EPS_0, A_0, A_1, A_2, S_0, MU;\n  VOL_0 = g_VOL_0[address];\n  EPS_0 = g_EPS_0[address];\n  A_0 = g_A_0[address];\n  A_1 = g_A_1[address];\n  A_2 = g_A_2[address];\n  S_0 = g_S_0[address];\n  MU = g_MU[address];\n\n  float mean = 0, variance = 0;\n  for (unsigned i = 1; i <= LOOKBACK_PATHS_PER_SIM; i++)\n  {\n    // simulate a path for num_cyles cyles \n    float res = tausworthe_lookback_sim(num_cycles, VOL_0, EPS_0,\n        A_0, A_1, A_2, S_0,\n        MU,\n        z1, z2, z3, z4,  // rng state variables\n        path);\n\n    // update mean and variance in a numerically stable way\n    float delta = res - mean;\n    mean += delta / i;\n    variance += delta * (res - mean);\n  }\n\n  simulationResultsMean[address] = mean;\n  simulationResultsVariance[address] = variance / (LOOKBACK_PATHS_PER_SIM - 1);\n}"
        ]
    },
    "bincount-cuda": {
        "/Users/gbolet/hecbench-roofline/src/bincount-cuda/main.cu": [
            "__device__ static IndexType\ngetBin(input_t v, input_t minvalue, input_t maxvalue, IndexType nbins)\n{\n  IndexType bin = (v - minvalue) * nbins / (maxvalue - minvalue);\n  // (only applicable for histc)\n  // while each bin is inclusive at the lower end and exclusive at the higher,\n  // i.e. [start, end) the last bin is inclusive at both, i.e. [start, end], in\n  // order to include maxvalue if exists therefore when bin == nbins, adjust bin\n  // to the last bin\n  if (bin == nbins) bin--;\n  return bin;\n}\n\nstatic inline __device__\nvoid gpuAtomicAddNoReturn(int *address, int val) {\n  atomicAdd(address, val);\n}\n\n__global__ void bincount (\n       output_t *output,\n  const input_t *input,\n  IndexType nbins,\n  input_t minvalue,\n  input_t maxvalue,\n  IndexType input_size,\n  IndexType output_size)\n{\n  extern __shared__ unsigned char my_smem[];\n  output_t* smem = nullptr;\n\n  if (MemoryType == DeviceMemoryType::SHARED) {\n    // atomically add to block specific shared memory\n    // then atomically add to the global output tensor\n    smem = reinterpret_cast<output_t*>(my_smem);\n    for (IndexType i = threadIdx.x; i < nbins; i += blockDim.x) {\n      smem[i] = 0;\n    }\n    __syncthreads();\n\n    FOR_KERNEL_LOOP(linearIndex, input_size) {\n      const auto v = input[linearIndex];\n\n      if (v >= minvalue && v <= maxvalue) {\n        const IndexType bin = getBin<input_t, IndexType>(\n                              v, minvalue, maxvalue, nbins);\n        gpuAtomicAddNoReturn(&smem[bin], 1);\n      }\n    }\n    __syncthreads();\n\n    // Atomically update output bin count.\n    for (IndexType i = threadIdx.x; i < nbins; i += blockDim.x) {\n      gpuAtomicAddNoReturn(&output[i], smem[i]);\n    }\n\n  } else {\n    ////////////////////////// Global memory //////////////////////////\n    // atomically add to the output tensor\n    // compute histogram for the block\n    FOR_KERNEL_LOOP(linearIndex, input_size) {\n      const auto v = input[linearIndex];\n      if (v >= minvalue && v <= maxvalue) {\n        const IndexType bin = getBin<input_t, IndexType>(\n                              v, minvalue, maxvalue, nbins);\n        gpuAtomicAddNoReturn(&output[bin], 1);\n      }\n    }\n  }\n}"
        ]
    },
    "maxFlops-cuda": {
        "/Users/gbolet/hecbench-roofline/src/maxFlops-cuda/kernels.h": [
            "#define MULMADD2_MOP20  \\\n     MULMADD2_OP MULMADD2_OP MULMADD2_OP MULMADD2_OP MULMADD2_OP MULMADD2_OP MULMADD2_OP MULMADD2_OP MULMADD2_OP MULMADD2_OP \\\n     MULMADD2_OP MULMADD2_OP MULMADD2_OP MULMADD2_OP MULMADD2_OP MULMADD2_OP MULMADD2_OP MULMADD2_OP MULMADD2_OP MULMADD2_OP\n\n\n#define T ((int)32)\n\n\n__global__ void MulMAdd2(T *data, int nIters, T v1, T v2) {\n  int gid = blockIdx.x*blockDim.x + threadIdx.x;\n  register T s = data[gid], s2=10.0f-s;\n  for (int j=0 ; j<nIters ; ++j) {\n     /* Each macro op has 20 operations.\n        Unroll 4 more times for 80 operations total.\n      */\n     MULMADD2_MOP20 MULMADD2_MOP20\n     MULMADD2_MOP20 MULMADD2_MOP20\n  }\n  data[gid] = s+s2;\n}",
            "#define MULMADD4_MOP10  \\\n     MULMADD4_OP MULMADD4_OP MULMADD4_OP MULMADD4_OP MULMADD4_OP \\\n     MULMADD4_OP MULMADD4_OP MULMADD4_OP MULMADD4_OP MULMADD4_OP\n\n\n#define T ((int)32)\n\n\n__global__ void MulMAdd4(T *data, int nIters, T v1, T v2) {\n  int gid = blockIdx.x*blockDim.x + threadIdx.x;\n  register T s = data[gid], s2=10.0f-s, s3=9.0f-s, s4=9.0f-s2;\n  for (int j=0 ; j<nIters ; ++j) {\n     /* Each macro op has 10 operations.\n        Unroll 4 more times for 40 operations total.\n      */\n     MULMADD4_MOP10 MULMADD4_MOP10\n     MULMADD4_MOP10 MULMADD4_MOP10\n  }\n  data[gid] = (s+s2)+(s3+s4);\n}",
            "#define MULMADD8_MOP5  \\\n     MULMADD8_OP MULMADD8_OP MULMADD8_OP MULMADD8_OP MULMADD8_OP\n\n\n#define T ((int)32)\n\n\n__global__ void MulMAdd8(T *data, int nIters, T v1, T v2) {\n  int gid = blockIdx.x*blockDim.x + threadIdx.x;\n  T s = data[gid], s2=10.0f-s, s3=9.0f-s, s4=9.0f-s2,\n    s5=8.0f-s, s6=8.0f-s2, s7=7.0f-s, s8=7.0f-s2;\n  for (int j=0 ; j<nIters ; ++j) {\n     /* Each macro op has 5 operations.\n        Unroll 4 more times for 20 operations total.\n      */\n     MULMADD8_MOP5 MULMADD8_MOP5\n     MULMADD8_MOP5 MULMADD8_MOP5\n  }\n  data[gid] = ((s+s2)+(s3+s4))+((s5+s6)+(s7+s8));\n}"
        ]
    },
    "qrg-cuda": {
        "/Users/gbolet/hecbench-roofline/src/qrg-cuda/main.cu": [
            "__global__ void  \nqrng (float* output, const unsigned int* table, const unsigned int seed, const unsigned int N)\n{\n  unsigned int globalID_x   = blockIdx.x * blockDim.x + threadIdx.x;\n  unsigned int localID_y    = threadIdx.y;\n  unsigned int globalSize_x = gridDim.x * blockDim.x;\n\n  for (unsigned int pos = globalID_x; pos < N; pos += globalSize_x) {\n    unsigned int result = 0;\n    unsigned int data = seed + pos;\n    for(int bit = 0; bit < QRNG_RESOLUTION; bit++, data >>= 1)\n      if(data & 1) result ^= table[bit+localID_y*QRNG_RESOLUTION];\n    output[__mul24(localID_y,N) + pos] = (float)(result + 1) * INT_SCALE;\n  }\n}",
            "__device__\nfloat MoroInvCNDgpu(unsigned int x)\n{\n  const float a1 = 2.50662823884f;\n  const float a2 = -18.61500062529f;\n  const float a3 = 41.39119773534f;\n  const float a4 = -25.44106049637f;\n  const float b1 = -8.4735109309f;\n  const float b2 = 23.08336743743f;\n  const float b3 = -21.06224101826f;\n  const float b4 = 3.13082909833f;\n  const float c1 = 0.337475482272615f;\n  const float c2 = 0.976169019091719f;\n  const float c3 = 0.160797971491821f;\n  const float c4 = 2.76438810333863E-02f;\n  const float c5 = 3.8405729373609E-03f;\n  const float c6 = 3.951896511919E-04f;\n  const float c7 = 3.21767881768E-05f;\n  const float c8 = 2.888167364E-07f;\n  const float c9 = 3.960315187E-07f;\n\n  float z;\n\n  bool negate = false;\n\n  // Ensure the conversion to floating point will give a value in the\n  // range (0,0.5] by restricting the input to the bottom half of the\n  // input domain. We will later reflect the result if the input was\n  // originally in the top half of the input domain\n  if (x >= 0x80000000UL)\n  {\n    x = 0xffffffffUL - x;\n    negate = true;\n  }\n\n  // x is now in the range [0,0x80000000) (i.e. [0,0x7fffffff])\n  // Convert to floating point in (0,0.5]\n  const float x1 = 1.0f / (float)0xffffffffUL;\n  const float x2 = x1 / 2.0f;\n  float p1 = x * x1 + x2;\n  // Convert to floating point in (-0.5,0]\n  float p2 = p1 - 0.5f;\n\n  // The input to the Moro inversion is p2 which is in the range\n  // (-0.5,0]. This means that our output will be the negative side\n  // of the bell curve (which we will reflect if \"negate\" is true).\n\n  // Main body of the bell curve for |p| < 0.42\n  if (p2 > -0.42f)\n  {\n    z = p2 * p2;\n    z = p2 * (((a4 * z + a3) * z + a2) * z + a1) / ((((b4 * z + b3) * z + b2) * z + b1) * z + 1.0f);\n  }\n  // Special case (Chebychev) for tail\n  else\n  {\n    z = logf(-logf(p1));\n    z = - (c1 + z * (c2 + z * (c3 + z * (c4 + z * (c5 + z * (c6 + z * (c7 + z * (c8 + z * c9))))))));\n  }\n\n  // If the original input (x) was in the top half of the range, reflect\n  // to get the positive side of the bell curve\n  return negate ? -z : z;\n}\n\n__global__ void  \nicnd (float* output, const unsigned int pathN, const unsigned int distance)\n{\n  const unsigned int globalID   = blockIdx.x * blockDim.x + threadIdx.x;\n  const unsigned int globalSize = gridDim.x * blockDim.x;\n\n  for(unsigned int pos = globalID; pos < pathN; pos += globalSize){\n    unsigned int d = (pos + 1) * distance;\n    output[pos] = MoroInvCNDgpu(d);\n  }\n}"
        ]
    },
    "bonds-cuda": {
        "/Users/gbolet/hecbench-roofline/src/bonds-cuda/bondsKernelsGpu.cu": [
            "#define dataType double\n\n\n__device__ bool isLeapKernelGpu(int y) \n{\n  bool YearIsLeap[121];\n\n  YearIsLeap[0] = 1;;\n  YearIsLeap[1] = 0;;\n  YearIsLeap[2] = 0;\n  YearIsLeap[3] = 0;//1096;\n  YearIsLeap[4] = 1;//1461;\n  YearIsLeap[5] = 0;//1827;\n  YearIsLeap[6] = 0;//2192;\n  YearIsLeap[7] = 0;//2557;\n  YearIsLeap[8] = 1;//2922;\n  YearIsLeap[9] = 0;//3288;\n  YearIsLeap[10] = 0;//3653;\n  YearIsLeap[11] = 0;//4018;\n  YearIsLeap[12] = 1;//4383;\n  YearIsLeap[13] = 0;//4749;\n  YearIsLeap[14] = 0;//5114;\n  YearIsLeap[15] = 0;//5479;\n  YearIsLeap[16] = 1;//5844;\n  YearIsLeap[17] = 0;//6210;\n  YearIsLeap[18] = 0;//6575;\n  YearIsLeap[19] = 0;//6940;\n  YearIsLeap[20] = 1;//7305;\n  YearIsLeap[21] = 0;//7671;\n  YearIsLeap[22] = 0;//8036;\n  YearIsLeap[23] = 0;//8401;\n  YearIsLeap[24] = 1;//8766;\n  YearIsLeap[25] = 0;//9132;\n  YearIsLeap[26] = 0;//9497;\n  YearIsLeap[27] = 0;//9862;\n  YearIsLeap[28] = 1;//10227;\n  YearIsLeap[29] = 0;//10593;\n  YearIsLeap[30] = 0;//10958;\n  YearIsLeap[31] = 0;//11323;\n  YearIsLeap[32] = 1;//11688;\n  YearIsLeap[33] = 0;//12054;\n  YearIsLeap[34] = 0;//12419;\n  YearIsLeap[35] = 0;//12784;\n  YearIsLeap[36] = 1;//13149;\n  YearIsLeap[37] = 0;//13515;\n  YearIsLeap[38] = 0;//13880;\n  YearIsLeap[39] = 0;//14245;\n  YearIsLeap[40] = 1;//14610;\n  YearIsLeap[41] = 0;//14976;\n  YearIsLeap[42] = 0;//15341;\n  YearIsLeap[43] = 0;//15706;\n  YearIsLeap[44] = 1;//16071;\n  YearIsLeap[45] = 0;//16437;\n  YearIsLeap[46] = 0;//16802;\n  YearIsLeap[47] = 0;//17167;\n  YearIsLeap[48] = 1;//17532;\n  YearIsLeap[49] = 0;//17898;\n  YearIsLeap[50] = 0;//18263;\n  YearIsLeap[51] = 0;//18628;\n  YearIsLeap[52] = 1;//18993;\n  YearIsLeap[53] = 0;//19359;\n  YearIsLeap[54] = 0;//19724;\n  YearIsLeap[55] = 0;//20089;\n  YearIsLeap[56] = 1;//20454;\n  YearIsLeap[57] = 0;//20820;\n  YearIsLeap[58] = 0;//21185;\n  YearIsLeap[59] = 0;//21550;\n  YearIsLeap[60] = 1;//21915;\n  YearIsLeap[61] = 0;//22281;\n  YearIsLeap[62] = 0;//22646;\n  YearIsLeap[63] = 0;//23011;\n  YearIsLeap[64] = 1;//23376;\n  YearIsLeap[65] = 0;//23742;\n  YearIsLeap[66] = 0;//24107;\n  YearIsLeap[67] = 0;//24472;\n  YearIsLeap[68] = 1;//24837;\n  YearIsLeap[69] = 0;//25203;\n  YearIsLeap[70] = 0;//25568;\n  YearIsLeap[71] = 0;//25933;\n  YearIsLeap[72] = 1;//26298;\n  YearIsLeap[73] = 0;//26664;\n  YearIsLeap[74] = 0;//27029;\n  YearIsLeap[75] = 0;//27394;\n  YearIsLeap[76] = 1;//27759;\n  YearIsLeap[77] = 0;//28125;\n  YearIsLeap[78] = 0;//28490;\n  YearIsLeap[79] = 0;//28855;\n  YearIsLeap[80] = 1;//29220;\n  YearIsLeap[81] = 0;//29586;\n  YearIsLeap[82] = 0;//29951;\n  YearIsLeap[83] = 0;//30316;\n  YearIsLeap[84] = 1;//30681;\n  YearIsLeap[85] = 0;//31047;\n  YearIsLeap[86] = 0;//31412;\n  YearIsLeap[87] = 0;//31777;\n  YearIsLeap[88] = 1;//32142;\n  YearIsLeap[89] = 0;//32508;\n  YearIsLeap[90] = 0;//32873;\n  YearIsLeap[91] = 0;//33238;\n  YearIsLeap[92] = 1;//33603;\n  YearIsLeap[93] = 0;//33969;\n  YearIsLeap[94] = 0;//34334;\n  YearIsLeap[95] = 0;//34699;\n  YearIsLeap[96] = 1;//35064;\n  YearIsLeap[97] = 0;//35430;\n  YearIsLeap[98] = 0;//35795;\n  YearIsLeap[99] = 0;//36160;\n  YearIsLeap[100] = 1;// 36525;\n  YearIsLeap[101] = 0;// 36891;\n  YearIsLeap[102] = 0;// 37256;\n  YearIsLeap[103] = 0;// 37621;\n  YearIsLeap[104] = 1;// 37986;\n  YearIsLeap[105] = 0;// 38352;\n  YearIsLeap[106] = 0;//38717;\n  YearIsLeap[107] = 0;//39082;\n  YearIsLeap[108] = 1;//39447;\n  YearIsLeap[109] = 0;//39813;\n  YearIsLeap[110] = 0;//40178;\n  YearIsLeap[111] = 0;//40543;\n  YearIsLeap[112] = 1;//40908;\n  YearIsLeap[113] = 0;//41274;\n  YearIsLeap[114] = 0;//41639;\n  YearIsLeap[115] = 0;//42004;\n  YearIsLeap[116] = 1;//42369;\n  YearIsLeap[117] = 0;//42735;\n  YearIsLeap[118] = 0;//43100;\n  YearIsLeap[119] = 0;//42735;\n  YearIsLeap[120] = 1;//43830;\n\n  return YearIsLeap[y-1900];\n}\n\n__device__ int monthOffsetKernelGpu(int m, bool leapYear) \n{\n  int MonthOffset[13];\n  MonthOffset[0]=0;\n  MonthOffset[1]=31;\n  MonthOffset[2]=59;\n  MonthOffset[3]=90;\n  MonthOffset[4]=120;\n  MonthOffset[5]=151;\n  MonthOffset[6]=181;\n  MonthOffset[7]=212;\n  MonthOffset[8]=243;\n  MonthOffset[9]=273;\n  MonthOffset[10]=304;\n  MonthOffset[11]=334;\n  MonthOffset[12]=365;\n\n  int MonthLeapOffset[13];\n  MonthLeapOffset[0]=0;\n  MonthLeapOffset[1]=31;\n  MonthLeapOffset[2]=60;\n  MonthLeapOffset[3]=91;\n  MonthLeapOffset[4]=121;\n  MonthLeapOffset[5]=152;\n  MonthLeapOffset[6]=182;\n  MonthLeapOffset[7]=213;\n  MonthLeapOffset[8]=244;\n  MonthLeapOffset[9]=274;\n  MonthLeapOffset[10]=305;\n  MonthLeapOffset[11]=335;\n  MonthLeapOffset[12]=366;\n\n  return (leapYear? MonthLeapOffset[m-1] : MonthOffset[m-1]);\n}\n\n__device__ int yearOffsetKernelGpu(int y)\n{\n\n  int YearOffset[121];\n  YearOffset[0] = 0;;\n  YearOffset[1] = 366;;\n  YearOffset[2] = 731;\n  YearOffset[3] = 1096;\n  YearOffset[4] = 1461;\n  YearOffset[5] = 1827;\n  YearOffset[6] = 2192;\n  YearOffset[7] = 2557;\n  YearOffset[8] = 2922;\n  YearOffset[9] = 3288;\n  YearOffset[10] = 3653;\n  YearOffset[11] = 4018;\n  YearOffset[12] = 4383;\n  YearOffset[13] = 4749;\n  YearOffset[14] = 5114;\n  YearOffset[15] = 5479;\n  YearOffset[16] = 5844;\n  YearOffset[17] = 6210;\n  YearOffset[18] = 6575;\n  YearOffset[19] = 6940;\n  YearOffset[20] = 7305;\n  YearOffset[21] = 7671;\n  YearOffset[22] = 8036;\n  YearOffset[23] = 8401;\n  YearOffset[24] = 8766;\n  YearOffset[25] = 9132;\n  YearOffset[26] = 9497;\n  YearOffset[27] = 9862;\n  YearOffset[28] = 10227;\n  YearOffset[29] = 10593;\n  YearOffset[30] = 10958;\n  YearOffset[31] = 11323;\n  YearOffset[32] = 11688;\n  YearOffset[33] = 12054;\n  YearOffset[34] = 12419;\n  YearOffset[35] = 12784;\n  YearOffset[36] = 13149;\n  YearOffset[37] = 13515;\n  YearOffset[38] = 13880;\n  YearOffset[39] = 14245;\n  YearOffset[40] = 14610;\n  YearOffset[41] = 14976;\n  YearOffset[42] = 15341;\n  YearOffset[43] = 15706;\n  YearOffset[44] = 16071;\n  YearOffset[45] = 16437;\n  YearOffset[46] = 16802;\n  YearOffset[47] = 17167;\n  YearOffset[48] = 17532;\n  YearOffset[49] = 17898;\n  YearOffset[50] = 18263;\n  YearOffset[51] = 18628;\n  YearOffset[52] = 18993;\n  YearOffset[53] = 19359;\n  YearOffset[54] = 19724;\n  YearOffset[55] = 20089;\n  YearOffset[56] = 20454;\n  YearOffset[57] = 20820;\n  YearOffset[58] = 21185;\n  YearOffset[59] = 21550;\n  YearOffset[60] = 21915;\n  YearOffset[61] = 22281;\n  YearOffset[62] = 22646;\n  YearOffset[63] = 23011;\n  YearOffset[64] = 23376;\n  YearOffset[65] = 23742;\n  YearOffset[66] = 24107;\n  YearOffset[67] = 24472;\n  YearOffset[68] = 24837;\n  YearOffset[69] = 25203;\n  YearOffset[70] = 25568;\n  YearOffset[71] = 25933;\n  YearOffset[72] = 26298;\n  YearOffset[73] = 26664;\n  YearOffset[74] = 27029;\n  YearOffset[75] = 27394;\n  YearOffset[76] = 27759;\n  YearOffset[77] = 28125;\n  YearOffset[78] = 28490;\n  YearOffset[79] = 28855;\n  YearOffset[80] = 29220;\n  YearOffset[81] = 29586;\n  YearOffset[82] = 29951;\n  YearOffset[83] = 30316;\n  YearOffset[84] = 30681;\n  YearOffset[85] = 31047;\n  YearOffset[86] = 31412;\n  YearOffset[87] = 31777;\n  YearOffset[88] = 32142;\n  YearOffset[89] = 32508;\n  YearOffset[90] = 32873;\n  YearOffset[91] = 33238;\n  YearOffset[92] = 33603;\n  YearOffset[93] = 33969;\n  YearOffset[94] = 34334;\n  YearOffset[95] = 34699;\n  YearOffset[96] = 35064;\n  YearOffset[97] = 35430;\n  YearOffset[98] = 35795;\n  YearOffset[99] = 36160;\n  YearOffset[100] = 36525;\n  YearOffset[101] = 36891;\n  YearOffset[102] = 37256;\n  YearOffset[103] = 37621;\n  YearOffset[104] = 37986;\n  YearOffset[105] = 38352;\n  YearOffset[106] = 38717;\n  YearOffset[107] = 39082;\n  YearOffset[108] = 39447;\n  YearOffset[109] = 39813;\n  YearOffset[110] = 40178;\n  YearOffset[111] = 40543;\n  YearOffset[112] = 40908;\n  YearOffset[113] = 41274;\n  YearOffset[114] = 41639;\n  YearOffset[115] = 42004;\n  YearOffset[116] = 42369;\n  YearOffset[117] = 42735;\n  YearOffset[118] = 43100;\n  YearOffset[119] = 42735;\n  YearOffset[120] = 43830;\n\n  return YearOffset[y-1900];\n}\n\n__device__ bondsDateStruct intializeDateKernelGpu(int d, int m, int y) \n{\n  bondsDateStruct currDate;\n\n  currDate.day = d;\n  currDate.month = m;\n  currDate.year = y;\n\n  bool leap = isLeapKernelGpu(y);\n  int offset = monthOffsetKernelGpu(m,leap);\n\n  currDate.dateSerialNum = d + offset + yearOffsetKernelGpu(y);\n\n  return currDate;\n}\n\n__device__ int monthLengthKernelGpu(int month, bool leapYear) \n{\n  int MonthLength[12];\n  MonthLength[0]=31;\n  MonthLength[1]=28;\n  MonthLength[2]=31;\n  MonthLength[3]=30;\n  MonthLength[4]=31;\n  MonthLength[5]=30;\n  MonthLength[6]=31;\n  MonthLength[7]=31;\n  MonthLength[8]=30;\n  MonthLength[9]=31;\n  MonthLength[10]=30;\n  MonthLength[11]=31;\n\n  int MonthLeapLength[12];\n  MonthLeapLength[0]=31;\n  MonthLeapLength[1]=29;\n  MonthLeapLength[2]=31;\n  MonthLeapLength[3]=30;\n  MonthLeapLength[4]=31;\n  MonthLeapLength[5]=30;\n  MonthLeapLength[6]=31;\n  MonthLeapLength[7]=31;\n  MonthLeapLength[8]=30;\n  MonthLeapLength[9]=31;\n  MonthLeapLength[10]=30;\n  MonthLeapLength[11]=31;\n\n  return (leapYear? MonthLeapLength[month-1] : MonthLength[month-1]);\n}\n\n__device__ bool eventHasOccurredGpu(bondsDateStruct currDate, bondsDateStruct eventDate)\n{\n  return eventDate.dateSerialNum > currDate.dateSerialNum;\n}\n\n__device__ bool cashFlowHasOccurredGpu(bondsDateStruct refDate, bondsDateStruct eventDate)\n{\n  return eventHasOccurredGpu(refDate, eventDate);\n}\n\ninline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\ninline __host__ __device__ float4 fmaxf(float4 a, float4 b)\n{\n    return make_float4(fmaxf(a.x,b.x), fmaxf(a.y,b.y), fmaxf(a.z,b.z), fmaxf(a.w,b.w));\n}\n\ninline  __host__ __device__ float4 fminf(float4 a, float4 b)\n{\n    return make_float4(fminf(a.x,b.x), fminf(a.y,b.y), fminf(a.z,b.z), fminf(a.w,b.w));\n}\n\n__device__ __forceinline__ float MAX<float>(float in, float in2)\n{\n    return fmaxf(in, in2);\n}\n\n__device__ __forceinline__ float MIN<float>(float in, float in2)\n{\n    return fminf(in, in2);\n}\n\n__device__ int dayCountGpu(bondsDateStruct d1, bondsDateStruct d2, int dayCounter) \n{\n  if (dayCounter == USE_EXACT_DAY)\n  {\n    int dd1 = d1.day, dd2 = d2.day;\n    int mm1 = d1.month, mm2 = d2.month;\n    int yy1 = d1.year, yy2 = d2.year;\n\n    if (dd2 == 31 && dd1 < 30) \n    { \n      dd2 = 1; mm2++; \n    }\n\n    return 360*(yy2-yy1) + 30*(mm2-mm1-1) + MAX(0, 30-dd1) + MIN(30, dd2);\n  }\n  else\n  {\n    return (d2.dateSerialNum - d1.dateSerialNum);\n  }\n}\n\n__device__ dataType interestRateCompoundFactorGpuTwoArgs(intRateStruct intRate, dataType t) \n{\n  if (intRate.comp == SIMPLE_INTEREST)\n    return (dataType)1.0 + intRate.rate*t;\n  else if (intRate.comp == COMPOUNDED_INTEREST)\n    return pow((dataType)1.0+intRate.rate/intRate.freq, intRate.freq*t);\n  else if (intRate.comp == CONTINUOUS_INTEREST)\n    return exp(intRate.rate*t);\n  return (dataType)0.0;\n}\n\n__device__ dataType yearFractionGpu(bondsDateStruct d1,\n    bondsDateStruct d2, int dayCounter)\n{\n  return dayCountGpu(d1, d2, dayCounter) / (dataType)360.0; \n}\n\n__device__ dataType fixedRateCouponNominalGpu()\n{\n  return (dataType)100.0;\n}\n\n__device__ dataType interestRateCompoundFactorGpu(intRateStruct intRate, bondsDateStruct d1,\n    bondsDateStruct d2, int dayCounter)\n{\n  dataType t = yearFractionGpu(d1, d2, dayCounter);\n  return interestRateCompoundFactorGpuTwoArgs(intRate, t);\n}\n\n__device__ int cashFlowsNextCashFlowNumGpu(cashFlowsStruct cashFlows,\n    bondsDateStruct currDate,\n    int numLegs) \n{\n  int i;\n  for (i = 0; i < numLegs; ++i) \n  {\n    if ( ! (cashFlowHasOccurredGpu(cashFlows.legs[i].paymentDate, currDate) ))\n      return i;\n  }\n\n  return (numLegs-1);\n}\n\n__device__ dataType fixedRateCouponAccruedAmountGpu(cashFlowsStruct cashFlows, int numLeg, bondsDateStruct d, inArgsStruct inArgs, int bondNum) \n{\n  if (d.dateSerialNum <= cashFlows.legs[numLeg].accrualStartDate.dateSerialNum || d.dateSerialNum > inArgs.maturityDate[bondNum].dateSerialNum) \n  {\n    return (dataType)0.0;\n  }\n  else\n  {\n    bondsDateStruct endDate = cashFlows.legs[numLeg].accrualEndDate;\n    if (d.dateSerialNum < cashFlows.legs[numLeg].accrualEndDate.dateSerialNum)\n    {\n      endDate = d;\n    }\n\n    return fixedRateCouponNominalGpu()*(interestRateCompoundFactorGpu(cashFlows.intRate, \n           cashFlows.legs[numLeg].accrualStartDate, endDate, cashFlows.dayCounter) - (dataType)1.0);\n  }\n}\n\n__device__ dataType bondNotionalGpu()\n{\n  return (dataType)100.0;\n}\n\n__device__ dataType cashFlowsAccruedAmountGpu(cashFlowsStruct cashFlows,\n    bool includecurrDateFlows,\n    bondsDateStruct currDate,\n    int numLegs, inArgsStruct inArgs, int bondNum) \n{\n  int legComputeNum = cashFlowsNextCashFlowNumGpu(cashFlows,\n      currDate, numLegs); \n\n  dataType result = 0.0;\n\n  int i;\n\n  for (i = legComputeNum; i < (numLegs); ++i)\n  {\n    result += fixedRateCouponAccruedAmountGpu(cashFlows, i, currDate, inArgs, bondNum);\n  }\n\n  return result;\n}\n\n__device__ dataType bondFunctionsAccruedAmountGpu(inArgsStruct inArgs, bondsDateStruct date, int bondNum, cashFlowsStruct cashFlows, int numLegs) \n{\n  return cashFlowsAccruedAmountGpu(cashFlows,\n      false, date, numLegs, inArgs, bondNum) * (dataType)100.0 / bondNotionalGpu();\n}\n\n__device__ dataType bondAccruedAmountGpu(inArgsStruct inArgs, bondsDateStruct date, int bondNum, cashFlowsStruct cashFlows, int numLegs)\n{\n  dataType currentNotional = bondNotionalGpu();\n  if (currentNotional == (dataType)0.0)\n    return (dataType)0.0;\n\n  return bondFunctionsAccruedAmountGpu(inArgs, date, bondNum, cashFlows, numLegs);\n}\n\ninline __host__ __device__ float4 fabs(float4 v)\n{\n    return make_float4(fabs(v.x), fabs(v.y), fabs(v.z), fabs(v.w));\n}\n\n__device__ bool closeGpuThreeArgs(dataType x, dataType y, int n)\n{\n  dataType diff = fabs(x-y);\n  dataType tolerance = n*QL_EPSILON_GPU;\n\n  return diff <= tolerance*fabs(x) &&\n    diff <= tolerance*fabs(y);\n}\n\n__device__ bondsDateStruct advanceDateGpu(bondsDateStruct date, int numMonthsAdvance) \n{\n  int d = date.day;\n  int m = date.month+numMonthsAdvance;\n  int y = date.year;\n\n  while (m > 12) \n  {\n    m -= 12;\n    y += 1;\n  }\n\n  while (m < 1) \n  {\n    m += 12;\n    y -= 1;\n  }\n\n  int length = monthLengthKernelGpu(m, isLeapKernelGpu(y));\n  if (d > length)\n    d = length;\n\n  bondsDateStruct newDate = intializeDateKernelGpu(d, m, y);\n\n  return newDate;\n}\n\n__device__ dataType fixedRateCouponAmountGpu(cashFlowsStruct cashFlows, int numLeg) \n{\n  if (cashFlows.legs[numLeg].amount == COMPUTE_AMOUNT)\n  {\n    return fixedRateCouponNominalGpu()*(interestRateCompoundFactorGpu(cashFlows.intRate, cashFlows.legs[numLeg].accrualStartDate,\n          cashFlows.legs[numLeg].accrualEndDate, cashFlows.dayCounter) - (dataType)1.0);\n  }\n  else\n  {\n    return cashFlows.legs[numLeg].amount;\n  }\n}\n\n__device__ dataType interestRateDiscountFactorGpu(intRateStruct intRate, dataType t) \n{\n  return (dataType)1.0/interestRateCompoundFactorGpuTwoArgs(intRate, t);\n}\n\n__device__ dataType cashFlowsNpvYieldGpu(cashFlowsStruct cashFlows,\n    intRateStruct y,\n    bool includecurrDateFlows,\n    bondsDateStruct currDate,\n    bondsDateStruct npvDate,\n    int numLegs) \n{\n  dataType npv = 0.0;\n  dataType discount = 1.0;\n  bondsDateStruct lastDate;\n  bool first = true;\n\n  int i;\n  for (i=0; i<numLegs; ++i) \n  {\n    if (cashFlowHasOccurredGpu(cashFlows.legs[i].paymentDate, currDate))\n      continue;\n\n    bondsDateStruct couponDate = cashFlows.legs[i].paymentDate;\n    dataType amount = fixedRateCouponAmountGpu(cashFlows, i);\n    if (first) \n    {\n      first = false;\n      if (i > 0) {\n        lastDate = advanceDateGpu(cashFlows.legs[i].paymentDate, -1*6); \n      } else {\n        lastDate = cashFlows.legs[i].accrualStartDate;\n      }\n      discount *= interestRateDiscountFactorGpu(y, yearFractionGpu(npvDate, couponDate, y.dayCounter));\n    } \n    else  \n    {\n      discount *= interestRateDiscountFactorGpu(y, yearFractionGpu(lastDate, couponDate, y.dayCounter));\n    }\n\n    lastDate = couponDate;\n\n    npv += amount * discount;\n  }\n\n  return npv;\n}\n\n__device__ dataType modifiedDurationGpu(cashFlowsStruct cashFlows,\n    intRateStruct y,\n    bool includecurrDateFlows,\n    bondsDateStruct currDate,\n    bondsDateStruct npvDate,\n    int numLegs)\n{\n  dataType P = 0.0;\n  dataType dPdy = 0.0;\n  dataType r = y.rate;\n  dataType N = y.freq;\n  int dc = y.dayCounter;\n\n  int i;\n  for (i=0; i<numLegs; ++i) \n  {\n    if (!cashFlowHasOccurredGpu(cashFlows.legs[i].paymentDate, currDate)) \n    {\n      dataType t = yearFractionGpu(npvDate,\n          cashFlows.legs[i].paymentDate, dc);\n      dataType c = fixedRateCouponAmountGpu(cashFlows, i);  \n      dataType B = interestRateDiscountFactorGpu(y, t); \n\n      P += c * B;\n      if (y.comp == SIMPLE_INTEREST)\n        dPdy -= c * B*B * t;\n      if (y.comp == COMPOUNDED_INTEREST)\n        dPdy -= c * t * B/(1+r/N);\n      if (y.comp == CONTINUOUS_INTEREST)\n        dPdy -= c * B * t;\n      if (y.comp == SIMPLE_THEN_COMPOUNDED_INTEREST)\n      {\n        if (t<=(dataType)1.0/N)\n          dPdy -= c * B*B * t;\n        else\n          dPdy -= c * t * B/((dataType)1+r/N);\n      }\n    }\n  }\n\n  if (P == (dataType)0.0) // no cashflows\n  {\n    return (dataType)0.0;\n  }\n  return (-1*dPdy)/P; // reverse derivative sign\n}\n\n__device__ dataType fDerivativeGpu(irrFinderStruct f, dataType y, cashFlowsStruct cashFlows, int numLegs)\n{\n  intRateStruct yield;\n  yield.rate = y;\n  yield.dayCounter = f.dayCounter;\n  yield.comp = f.comp;\n  yield.freq = f.freq;\n\n  return modifiedDurationGpu(cashFlows, yield,\n      f.includecurrDateFlows,\n      f.currDate, f.npvDate, numLegs);\n}\n\n__device__ dataType fOpGpu(irrFinderStruct f, dataType y, cashFlowsStruct cashFlows, int numLegs)\n{\n  intRateStruct yield;\n\n  yield.rate = y;\n  yield.comp = f.comp;\n  yield.freq = f.freq;\n  yield.dayCounter = f.dayCounter;\n\n  dataType NPV = cashFlowsNpvYieldGpu(cashFlows,\n      yield,\n      false,\n      f.currDate,\n      f.npvDate, numLegs);\n\n  return (f.npv - NPV);\n}\n\n__device__ bool closeGpu(dataType x, dataType y)\n{\n  return closeGpuThreeArgs(x,y,42);\n}\n\n__device__ dataType solveImplGpu(solverStruct solver, irrFinderStruct f,\n    dataType xAccuracy, cashFlowsStruct cashFlows, int numLegs)\n{\n  dataType froot, dfroot, dx, dxold;\n  dataType xh, xl;\n\n  // Orient the search so that f(xl) < 0\n  if (solver.fxMin_ < (dataType)0.0) \n  {\n    xl = solver.xMin_;\n    xh = solver.xMax_;\n  } \n  else \n  {\n    xh = solver.xMin_;\n    xl = solver.xMax_;\n  }\n\n  // the \"stepsize before last\"\n  dxold = solver.xMax_ - solver.xMin_;\n  // it was dxold=std::fabs(xMax_-xMin_); in Numerical Recipes\n  // here (xMax_-xMin_ > 0) is verified in the constructor\n\n  // and the last step\n  dx = dxold;\n\n  froot = fOpGpu(f, solver.root_, cashFlows, numLegs);\n  dfroot = fDerivativeGpu(f, solver.root_, cashFlows, numLegs);\n\n  ++solver.evaluationNumber_;\n\n  while (solver.evaluationNumber_<=solver.maxEvaluations_) \n  {\n    // Bisect if (out of range || not decreasing fast enough)\n    if ((((solver.root_-xh)*dfroot-froot)*\n          ((solver.root_-xl)*dfroot-froot) > (dataType)0.0)\n        || (fabs((dataType)2.0*froot) > fabs(dxold*dfroot))) \n    {\n      dxold = dx;\n      dx = (xh-xl)/(dataType)2.0;\n      solver.root_=xl+dx;\n    } \n    else \n    {\n      dxold = dx;\n      dx = froot/dfroot;\n      solver.root_ -= dx;\n    }\n\n    // Convergence criterion\n    if (fabs(dx) < xAccuracy)\n      return solver.root_;\n    froot = fOpGpu(f, solver.root_, cashFlows, numLegs);\n    dfroot = fDerivativeGpu(f, solver.root_, cashFlows, numLegs);\n    ++solver.evaluationNumber_;\n    if (froot < (dataType)0.0)\n      xl=solver.root_;\n    else\n      xh=solver.root_;\n  }\n\n  return solver.root_;\n}\n\n__device__ dataType solverSolveGpu(solverStruct solver,\n    irrFinderStruct f,\n    dataType accuracy,\n    dataType guess,\n    dataType step,\n    cashFlowsStruct cashFlows,\n    int numLegs)\n{\n  // check whether we really want to use epsilon\n  accuracy = MAX(accuracy, QL_EPSILON_GPU);\n\n  dataType growthFactor = (dataType)1.6;\n  int flipflop = -1;\n\n  solver.root_ = guess;\n  solver.fxMax_ = fOpGpu(f, solver.root_, cashFlows, numLegs);\n\n  // monotonically crescent bias, as in optionValue(volatility)\n  if (closeGpu(solver.fxMax_,(dataType)0.0))\n  {\n    return solver.root_;\n  }\n  else if (closeGpu(solver.fxMax_, (dataType)0.0)) \n  {\n    solver.xMin_ = /*enforceBounds*/(solver.root_ - step);\n    solver.fxMin_ = fOpGpu(f, solver.xMin_, cashFlows, numLegs);\n    solver.xMax_ = solver.root_;\n  } \n  else \n  {\n    solver.xMin_ = solver.root_;\n    solver.fxMin_ = solver.fxMax_;\n    solver.xMax_ = /*enforceBounds*/(solver.root_+step);\n    solver.fxMax_ = fOpGpu(f, solver.xMax_, cashFlows, numLegs);\n  }\n\n  solver.evaluationNumber_ = 2;\n  while (solver.evaluationNumber_ <= solver.maxEvaluations_) \n  {\n    if (solver.fxMin_*solver.fxMax_ <= (dataType)0.0) \n    {\n      if (closeGpu(solver.fxMin_, (dataType)0.0))\n        return solver.xMin_;\n      if (closeGpu(solver.fxMax_, (dataType)0.0))\n        return solver.xMax_;\n      solver.root_ = (solver.xMax_+solver.xMin_)/(dataType)2.0;\n      return solveImplGpu(solver, f, accuracy, cashFlows, numLegs);\n    }\n    if (fabs(solver.fxMin_) < fabs(solver.fxMax_)) \n    {\n      solver.xMin_ = /*enforceBounds*/(solver.xMin_+growthFactor*(solver.xMin_ - solver.xMax_));\n      solver.fxMin_= fOpGpu(f, solver.xMin_, cashFlows, numLegs);\n    } \n    else if (fabs(solver.fxMin_) > fabs(solver.fxMax_)) \n    {\n      solver.xMax_ = /*enforceBounds*/(solver.xMax_+growthFactor*(solver.xMax_ - solver.xMin_));\n      solver.fxMax_= fOpGpu(f, solver.xMax_, cashFlows, numLegs);\n    } \n    else if (flipflop == -1) \n    {\n      solver.xMin_ = /*enforceBounds*/(solver.xMin_+growthFactor*(solver.xMin_ - solver.xMax_));\n      solver.fxMin_= fOpGpu(f, solver.xMin_, cashFlows, numLegs);\n      solver.evaluationNumber_++;\n      flipflop = 1;\n    } \n    else if (flipflop == 1) \n    {\n      solver.xMax_ = /*enforceBounds*/(solver.xMax_+growthFactor*(solver.xMax_ - solver.xMin_));\n      solver.fxMax_= fOpGpu(f, solver.xMax_, cashFlows, numLegs);\n      flipflop = -1;\n    }\n    solver.evaluationNumber_++;\n  }\n\n  return (dataType)0.0;\n}\n\n__device__ dataType getCashFlowsYieldGpu(cashFlowsStruct leg,\n    dataType npv,\n    int dayCounter,\n    int compounding,\n    dataType frequency,\n    bool includecurrDateFlows,\n    bondsDateStruct currDate,\n    bondsDateStruct npvDate,\n    int numLegs,\n    dataType accuracy,\n    int maxIterations,\n    dataType guess)\n{\n  //Brent solver;\n  solverStruct solver;\n  solver.maxEvaluations_ = maxIterations;\n  irrFinderStruct objFunction;\n\n  objFunction.npv = npv;\n  objFunction.dayCounter = dayCounter;\n  objFunction.comp = compounding;\n  objFunction.freq = frequency;\n  objFunction.includecurrDateFlows = includecurrDateFlows;\n  objFunction.currDate = currDate;\n  objFunction.npvDate = npvDate;\n\n  return solverSolveGpu(solver, objFunction, accuracy, guess, guess/(dataType)10.0, leg, numLegs);\n}\n\n__device__ dataType getBondFunctionsYieldGpu(dataType cleanPrice,\n    int dc,\n    int comp,\n    dataType freq,\n    bondsDateStruct settlement,\n    dataType accuracy,\n    int maxEvaluations,\n    inArgsStruct currInArgs, int bondNum, cashFlowsStruct cashFlows, int numLegs)\n{\n  dataType dirtyPrice = cleanPrice + bondFunctionsAccruedAmountGpu(currInArgs, settlement, bondNum, cashFlows, numLegs); \n  dirtyPrice /= (dataType)100.0 / bondNotionalGpu();\n\n  return getCashFlowsYieldGpu(cashFlows, dirtyPrice,\n      dc, comp, freq,\n      false, settlement, settlement, numLegs,\n      accuracy, maxEvaluations, (dataType)0.05);\n}\n\n__device__ dataType flatForwardDiscountImplGpu(intRateStruct intRate, dataType t) \n{\n  return interestRateDiscountFactorGpu(intRate, t);\n}\n\n__device__ dataType bondsYieldTermStructureDiscountGpu(bondsYieldTermStruct ytStruct, bondsDateStruct t)\n{\n  ytStruct.intRate.rate = ytStruct.forward;\n  ytStruct.intRate.freq = ytStruct.frequency;\n  ytStruct.intRate.comp = ytStruct.compounding;\n  return flatForwardDiscountImplGpu(ytStruct.intRate, yearFractionGpu(ytStruct.refDate, t, ytStruct.dayCounter));\n}\n\n__device__ dataType cashFlowsNpvGpu(cashFlowsStruct cashFlows,\n    bondsYieldTermStruct discountCurve,\n    bool includecurrDateFlows,\n    bondsDateStruct currDate,\n    bondsDateStruct npvDate,\n    int numLegs) \n{\n  npvDate = currDate;\n\n  dataType totalNPV = 0.0;\n\n  int i;\n\n  for (i=0; i<numLegs; ++i) {\n    if (!(cashFlowHasOccurredGpu(cashFlows.legs[i].paymentDate, currDate)))\n      totalNPV += fixedRateCouponAmountGpu(cashFlows, i) *\n        bondsYieldTermStructureDiscountGpu(discountCurve, cashFlows.legs[i].paymentDate);\n  }\n\n  return totalNPV/bondsYieldTermStructureDiscountGpu(discountCurve, npvDate);\n}\n\n__device__ dataType discountingBondEngineCalculateSettlementValueGpu(inArgsStruct inArgs, int bondNum, cashFlowsStruct cashFlows, int numLegs)  \n{\n\n  bondsDateStruct currDate = inArgs.currDate[bondNum];\n\n  if (currDate.dateSerialNum < inArgs.bond[bondNum].startDate.dateSerialNum)\n  {\n    currDate = inArgs.bond[bondNum].startDate;\n  }\n\n  return cashFlowsNpvGpu(cashFlows,\n      inArgs.discountCurve[bondNum],\n      false,\n      currDate,\n      currDate, \n      numLegs);\n}\n\n__device__ dataType getAccruedAmountGpu(inArgsStruct inArgs, bondsDateStruct date, int bondNum, cashFlowsStruct cashFlows, int numLegs)\n{\n  return bondAccruedAmountGpu(inArgs, date, bondNum, cashFlows, numLegs);\n}\n\n__device__ dataType getBondYieldGpu(dataType cleanPrice,\n    int dc,\n    int comp,\n    dataType freq,\n    bondsDateStruct settlement,\n    dataType accuracy,\n    int maxEvaluations,\n    inArgsStruct currInArgs, int bondNum, cashFlowsStruct cashFlows, int numLegs)\n{\n  dataType currentNotional = bondNotionalGpu();\n\n  if (currentNotional == (dataType)0.0)\n    return (dataType)0.0;\n\n  if (currInArgs.bond[bondNum].startDate.dateSerialNum > settlement.dateSerialNum)\n  {\n    settlement = currInArgs.bond[bondNum].startDate;\n  }\n\n  return getBondFunctionsYieldGpu(cleanPrice, dc, comp, freq,\n      settlement, accuracy, maxEvaluations,\n      currInArgs, bondNum, cashFlows, numLegs);\n}\n\n__device__ dataType getDirtyPriceGpu(inArgsStruct inArgs, int bondNum, cashFlowsStruct cashFlows, int numLegs)\n{\n  dataType currentNotional = bondNotionalGpu();\n  return discountingBondEngineCalculateSettlementValueGpu(inArgs, bondNum, cashFlows, numLegs) * (dataType)100.0 / currentNotional;\n}\n\n__global__ void bonds(inArgsStruct inArgs, resultsStruct results, int n)\n{\n  int bondNum = blockIdx.x*blockDim.x + threadIdx.x;\n  if (bondNum < n)\n  {\n    int numLegs;\n\n    int numCashFlows = 0;\n\n    //bondsDateStruct endDate = inArgs.bond[bondNum].maturityDate;\n    bondsDateStruct currCashflowDate = inArgs.bond[bondNum].maturityDate;\n\n    while (currCashflowDate.dateSerialNum > inArgs.bond[bondNum].startDate.dateSerialNum)\n    {\n      numCashFlows++;\n      currCashflowDate = advanceDateGpu(currCashflowDate, -6); \n    }\n\n    numLegs = numCashFlows+1;\n\n    cashFlowsStruct cashFlows; \n    couponStruct cashLegs[9];\n    cashFlows.legs = cashLegs;\n\n    cashFlows.intRate.dayCounter = USE_EXACT_DAY;\n    cashFlows.intRate.rate  = inArgs.bond[bondNum].rate;\n    cashFlows.intRate.freq  = ANNUAL_FREQ;\n    cashFlows.intRate.comp  = SIMPLE_INTEREST;\n    cashFlows.dayCounter  = USE_EXACT_DAY;\n    cashFlows.nominal  = (dataType)100.0;\n\n    //bondsDateStruct currPaymentDate;\n    bondsDateStruct currStartDate = advanceDateGpu(inArgs.bond[bondNum].maturityDate, (numLegs - 1)*-6);\n    bondsDateStruct currEndDate = advanceDateGpu(currStartDate, 6); \n\n    int cashFlowNum;\n    for (cashFlowNum = 0; cashFlowNum < numLegs-1; cashFlowNum++)\n    {\n      cashFlows.legs[cashFlowNum].paymentDate = currEndDate;\n\n\n      cashFlows.legs[cashFlowNum].accrualStartDate  = currStartDate;\n      cashFlows.legs[cashFlowNum].accrualEndDate  = currEndDate;\n\n      cashFlows.legs[cashFlowNum].amount = COMPUTE_AMOUNT;\n\n      currStartDate = currEndDate;\n      currEndDate = advanceDateGpu(currEndDate, 6); \n    }\n\n    cashFlows.legs[numLegs-1].paymentDate  = inArgs.bond[bondNum].maturityDate;\n    cashFlows.legs[numLegs-1].accrualStartDate = inArgs.currDate[bondNum];\n    cashFlows.legs[numLegs-1].accrualEndDate  = inArgs.currDate[bondNum];\n    cashFlows.legs[numLegs-1].amount = (dataType)100.0;\n\n    results.bondForwardVal[bondNum] = getBondYieldGpu(inArgs.bondCleanPrice[bondNum],\n        USE_EXACT_DAY,\n        COMPOUNDED_INTEREST,\n        (dataType)2.0,\n        inArgs.currDate[bondNum],\n        ACCURACY,\n        100,\n        inArgs, bondNum, cashFlows, numLegs);\n    inArgs.discountCurve[bondNum].forward = results.bondForwardVal[bondNum];\n    results.dirtyPrice[bondNum] = getDirtyPriceGpu(inArgs, bondNum, cashFlows, numLegs);\n    results.accruedAmountCurrDate[bondNum] = getAccruedAmountGpu(inArgs, inArgs.currDate[bondNum], bondNum, cashFlows, numLegs);\n    results.cleanPrice[bondNum] = results.dirtyPrice[bondNum] - results.accruedAmountCurrDate[bondNum];\n  }\n}"
        ]
    },
    "fhd-cuda": {
        "/Users/gbolet/hecbench-roofline/src/fhd-cuda/main.cu": [
            "__global__\nvoid cmpfhd(const float*__restrict__ rmu, \n            const float*__restrict__ imu,\n                  float*__restrict__ rfhd,\n                  float*__restrict__ ifhd,\n            const float*__restrict__ x, \n            const float*__restrict__ y,\n            const float*__restrict__ z,\n            const int samples,\n            const int voxels) \n{\n  int n = blockIdx.x * blockDim.x + threadIdx.x;\n\n  if (n < samples) {\n    float xn = x[n], yn = y[n], zn = z[n];\n    float rfhdn = rfhd[n], ifhdn = ifhd[n];\n    for (int m = 0; m < voxels; m++) {\n      float e = 2.f * (float)M_PI * (k[m].x * xn + k[m].y * yn + k[m].z * zn);\n      float c = __cosf(e);\n      float s = __sinf(e);\n      rfhdn += rmu[m] * c - imu[m] * s;\n      ifhdn += imu[m] * c + rmu[m] * s;\n    }\n    rfhd[n] = rfhdn, ifhd[n] = ifhdn;\n  }\n}"
        ]
    },
    "diamond-cuda": {
        "/Users/gbolet/hecbench-roofline/src/diamond-cuda/masking.cu": [
            "inline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\ninline __host__ __device__ float4 fabs(float4 v)\n{\n    return make_float4(fabs(v.x), fabs(v.y), fabs(v.z), fabs(v.w));\n}\n\n__device__\nint calcRepeatProbs(float *letterProbs,\n    const unsigned char *seqBeg, \n    const int size, \n    const int maxRepeatOffset,\n    const double *likelihoodRatioMatrix, // 64 by 64 matrix,\n    const double b2b,\n    const double f2f0,\n    const double f2b,\n    const double b2fLast_inv,\n    const double *pow_lkp,\n    double *foregroundProbs,\n    const int scaleStepSize,\n    double *scaleFactors)\t\t      \t\n{\n\n  double backgroundProb = 1.0;\n  for (int k=0; k < size ; k++) {\n\n    const int v0 = seqBeg[k];\n    const int k_cap = k < maxRepeatOffset ? k : maxRepeatOffset;\n\n    const int pad1 = k_cap - 1;\n    const int pad2 = maxRepeatOffset - k_cap; // maxRepeatOffset - k, then 0                   when k > maxRepeatOffset\n    const int pad3 = k - k_cap;               // 0                  , then maxRepeatOffset - k when k > maxRepeatOffset\n\n    double accu = 0;\n\n    for (int i = 0; i < k; i++) {\n\n      const int idx1 = pad1 - i;\n      const int idx2 = pad2 + i;\n      const int idx3 = pad3 + i;\n\n      const int v1 = seqBeg[idx3];\n      accu += foregroundProbs[idx1];\n      foregroundProbs[idx1] = ( (f2f0 * foregroundProbs[idx1]) +  \n          (backgroundProb * pow_lkp[idx2]) ) * \n        likelihoodRatioMatrix[v0*size+v1];\n    }\n\n    backgroundProb = (backgroundProb * b2b) + (accu * f2b);\n\n    if (k % scaleStepSize == scaleStepSize - 1) {\n      const double scale = 1 / backgroundProb;\n      scaleFactors[k / scaleStepSize] = scale;\n\n      for (int i=0; i< k_cap; i++)\n        foregroundProbs[i] = foregroundProbs[i] * scale;\n\n      backgroundProb = 1;\n    }\n\n    letterProbs[k] = (float)(backgroundProb);\n  }\n\n  double accu = 0;\n  for (int i=0 ; i < maxRepeatOffset; i++) {\n    accu += foregroundProbs[i];\n    foregroundProbs[i] = f2b;\n  }\n\n  const double fTot = backgroundProb * b2b + accu * f2b;\n  backgroundProb = b2b;\n\n  const double fTot_inv = 1/ fTot ;\n  for (int k=(size-1) ; k >= 0 ; k--){\n\n\n    double nonRepeatProb = letterProbs[k] * backgroundProb * fTot_inv;\n    letterProbs[k] = 1 - (float)(nonRepeatProb);\n\n    //const int k_cap  = std::min(k, maxRepeatOffset);\n    const int k_cap = k < maxRepeatOffset ? k : maxRepeatOffset;\n\n    if (k % scaleStepSize == scaleStepSize - 1) {\n      const double scale = scaleFactors[k/ scaleStepSize];\n\n      for (int i=0; i< k_cap; i++)\n        foregroundProbs[i] = foregroundProbs[i] * scale;\n\n      backgroundProb *= scale;\n    }\n\n    const double c0 = f2b * backgroundProb;\n    const int v0= seqBeg[k];\n\n    double accu = 0;\n    for (int i = 0; i < k_cap; i++) {\n\n\n      const int v1 =  seqBeg[k-(i+1)];\n      const double f = foregroundProbs[i] * likelihoodRatioMatrix[v0*size+v1];\n\n      accu += pow_lkp[k_cap-(i+1)]*f;\n      foregroundProbs[i] = c0 + f2f0 * f;\n    }\n\n    const double p = k > maxRepeatOffset ? 1. : pow_lkp[maxRepeatOffset - k]*b2fLast_inv;\n    backgroundProb = (b2b * backgroundProb) + accu*p;\n  }\n\n  const double bTot = backgroundProb;\n  return (fabs(fTot - bTot) > fmax(fTot, bTot) / 1e6);\n}\n\n__device__\ninline double firstRepeatOffsetProb(const double probMult, const int maxRepeatOffset) {\n  if (probMult < 1 || probMult > 1)\n    return (1 - probMult) / (1 - pow(probMult, (double)maxRepeatOffset));\n  else\n    return 1.0 / maxRepeatOffset;\n}\n\n__device__\nvoid maskProbableLetters(const int size,\n    unsigned char *seqBeg,\n    const float *probabilities, \n    const unsigned char *maskTable) {\n\n  const double minMaskProb = 0.5;\n  for (int i=0; i<size; i++)\n    if (probabilities[i] >= minMaskProb)\n      seqBeg[i] = maskTable[seqBeg[i]];\n}\n\n__global__ void\nmaskSequences(unsigned char * seqs, \n    const double * likelihoodRatioMatrix,\n    const unsigned char * maskTable,\n    const int size                     ,\n    const int maxRepeatOffset          ,\n    const double repeatProb            ,\n    const double repeatEndProb         ,\n    const double repeatOffsetProbDecay ,\n    const double firstGapProb          ,\n    const double otherGapProb          ,\n    const double minMaskProb           ,\n    int seqs_len )\n{\n  int gid = blockIdx.x*blockDim.x+threadIdx.x;\n  if (gid >= seqs_len) return;\n\n  unsigned char* seqBeg = seqs+gid*33;\n\n  float probabilities[SEQ_LEN];\n\n  const double b2b = 1 - repeatProb;\n  const double f2f0 = 1 - repeatEndProb;\n  const double f2b = repeatEndProb;\n\n  const double b2fGrowth = 1 / repeatOffsetProbDecay;\n\n  const double  b2fLast = repeatProb * firstRepeatOffsetProb(b2fGrowth, maxRepeatOffset);\n  const double b2fLast_inv = 1 / b2fLast ;\n\n  double p = b2fLast;\n  double ar_1[50];\n\n  for (int i=0 ; i < maxRepeatOffset; i++){\n    ar_1[i] = p ;\n    p *= b2fGrowth;\n  }\n\n  const int scaleStepSize = 16;\n\n  double scaleFactors[SEQ_LEN / scaleStepSize];\n\n  double foregroundProbs[50];\n\n  for (int i=0 ; i < maxRepeatOffset; i++){\n    foregroundProbs[i] = 0;\n  };\n\n  const int err  = calcRepeatProbs(probabilities,seqBeg, size, \n      maxRepeatOffset, likelihoodRatioMatrix,\n      b2b, f2f0, f2b,\n      b2fLast_inv,ar_1,foregroundProbs,scaleStepSize, scaleFactors);\n\n  //if (err)  printf(\"tantan: warning: possible numeric inaccuracy\\n\");\n\n  maskProbableLetters(size,seqBeg, probabilities, maskTable);\n}"
        ]
    },
    "miniFE-cuda": {
        "/Users/gbolet/hecbench-roofline/src/miniFE-cuda/basic/optional/cuda/CudaNode.cuh": [
            "__global__ void\nTkern1D(int length, WDP wd, int stride)\n{\n  unsigned int i = blockIdx.x*blockDim.x + threadIdx.x;\n  while(i < length) {\n    wd(i);\n    i += stride;\n  }\n}"
        ]
    },
    "sad-cuda": {
        "/Users/gbolet/hecbench-roofline/src/sad-cuda/main.cu": [
            "inline __host__ __device__ int4 make_int4(float4 a)\n{\n    return make_int4(int(a.x), int(a.y), int(a.z), int(a.w));\n}\n\ninline __host__ __device__ int4 abs(int4 v)\n{\n    return make_int4(abs(v.x), abs(v.y), abs(v.z), abs(v.w));\n}\n\ninline __host__ __device__ uint4 make_uint4(int4 a)\n{\n    return make_uint4(uint(a.x), uint(a.y), uint(a.z), uint(a.w));\n}\n\ninline __host__ __device__ uint4 min(uint4 a, uint4 b)\n{\n    return make_uint4(min(a.x,b.x), min(a.y,b.y), min(a.z,b.z), min(a.w,b.w));\n}\n\n__global__ void compute_sad_array(\n                    int*__restrict__ sad_array,\n    const unsigned char*__restrict__ image,\n    const unsigned char*__restrict__ kernel,\n    const int sad_array_size,\n    const int image_width,\n    const int image_height,\n    const int kernel_width,\n    const int kernel_height,\n    const int kernel_size)\n{\n  int col = blockIdx.x * blockDim.x + threadIdx.x;\n  int row = blockIdx.y * blockDim.y + threadIdx.y;\n  int sad_result = 0;\n\n  if (row < image_height && col < image_width) {\n    const int overlap_width = min(image_width - col, kernel_width);\n    const int overlap_height = min(image_height - row, kernel_height);\n    #pragma unroll 4\n    for (int kr = 0; kr < overlap_height; kr++) {\n      #pragma unroll 4\n      for (int kc = 0; kc < overlap_width; kc++) {\n        const int image_addr = ((row + kr) * image_width + (col + kc)) * 3;\n        const int kernel_addr = (kr * kernel_width + kc) * 3;\n        const int m_r = (int)(image[image_addr + 0]);\n        const int m_g = (int)(image[image_addr + 1]);\n        const int m_b = (int)(image[image_addr + 2]);\n        const int t_r = (int)(kernel[kernel_addr + 0]);\n        const int t_g = (int)(kernel[kernel_addr + 1]);\n        const int t_b = (int)(kernel[kernel_addr + 2]);\n        const int error = abs(m_r - t_r) + abs(m_g - t_g) + abs(m_b - t_b);\n        sad_result += error;\n      }\n    }\n\n    int norm_sad = (int)(sad_result / (float)kernel_size);\n\n    int my_index_in_sad_array = row * image_width + col;\n    if (my_index_in_sad_array < sad_array_size) {\n      sad_array[my_index_in_sad_array] = norm_sad;\n    }\n  }\n}",
            "inline __host__ __device__ uint4 make_uint4(int4 a)\n{\n    return make_uint4(uint(a.x), uint(a.y), uint(a.z), uint(a.w));\n}\n\ninline __host__ __device__ uint4 min(uint4 a, uint4 b)\n{\n    return make_uint4(min(a.x,b.x), min(a.y,b.y), min(a.z,b.z), min(a.w,b.w));\n}\n\n__global__ void find_min_in_sad_array(\n    const int sad_array_size,\n    const int* __restrict__ sad_array,\n          int* __restrict__ min_sad)\n{\n  unsigned int gid = blockIdx.x * blockDim.x + threadIdx.x;\n  unsigned int stride = gridDim.x * blockDim.x;\n  unsigned int offset = 0;\n\n  __shared__ int cache[BLOCK_SIZE];\n\n  int temp = FOUND_MIN;\n  while (gid + offset < sad_array_size) {\n    temp = min(temp, sad_array[gid + offset]);\n    offset += stride;\n  }\n\n  cache[threadIdx.x] = temp;\n\n  __syncthreads();\n\n  unsigned int i = blockDim.x / 2;\n  while (i != 0) {\n    if (threadIdx.x < i)\n      cache[threadIdx.x] = min(cache[threadIdx.x], cache[threadIdx.x + i]);\n    __syncthreads();\n    i /= 2;\n  }\n\n  // Update global min for each block\n  if (threadIdx.x == 0)\n    atomicMin(min_sad, cache[0]);\n}",
            "__global__ void get_num_of_occurrences(\n    const int sad_array_size,\n    const int*__restrict__ sad_array,\n    const int*__restrict__ min_sad,\n          int*__restrict__ num_occurrences)\n{\n  unsigned int gid = threadIdx.x + blockIdx.x * blockDim.x;\n\n  __shared__ int s;\n\n  if (gid < sad_array_size) {\n\n    if (threadIdx.x == 0) s = 0;\n\n    __syncthreads();\n\n    if (sad_array[gid] == *min_sad)\n      atomicAdd(&s, 1);\n\n    __syncthreads();\n\n    // Update global occurance for each block\n    if (threadIdx.x == 0)\n      atomicAdd(num_occurrences, s);\n  }\n}"
        ]
    },
    "lda-cuda": {
        "/Users/gbolet/hecbench-roofline/src/lda-cuda/kernel.h": [
            "__device__ __forceinline__\nfloat4 __shfl_down(const float4 var, const uint32_t srcLane, const uint32_t width = 32) {\n  float4 output;\n  output.x = __shfl_down_sync(0xFFFFFFFF, var.x, srcLane, width);\n  output.y = __shfl_down_sync(0xFFFFFFFF, var.y, srcLane, width);\n  output.z = __shfl_down_sync(0xFFFFFFFF, var.z, srcLane, width);\n  output.w = __shfl_down_sync(0xFFFFFFFF, var.w, srcLane, width);\n  return output;\n}\n\n__inline__ __device__\nfloat warp_reduce_sum(float val) {\n  #if __CUDACC_VER_MAJOR__ >= 9\n  // __shfl_down is deprecated with cuda 9+\n  unsigned int active = __activemask();\n  #pragma unroll\n  for (int offset = WARP_SIZE / 2; offset > 0; offset /= 2) {\n      val += __shfl_down_sync(active, val, offset);\n  }\n  #else\n  #pragma unroll\n  for (int offset = WARP_SIZE / 2; offset > 0; offset /= 2) {\n    val += __shfl_down(val, offset);\n  }\n  #endif\n  return val;\n}\n\ninline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\ninline __host__ __device__ float4 expf(float4 v)\n{\n    return make_float4(expf(v.x), expf(v.y), expf(v.z), expf(v.w));\n}\n\ninline __host__ __device__ float4 fmaxf(float4 a, float4 b)\n{\n    return make_float4(fmaxf(a.x,b.x), fmaxf(a.y,b.y), fmaxf(a.z,b.z), fmaxf(a.w,b.w));\n}\n\n__inline__ __device__\nfloat Digamma(float x) {\n  float result = 0.0f, xx, xx2, xx4;\n  for ( ; x < 7.0f; ++x)\n    result -= 1.0f / x;\n  x -= 0.5f;\n  xx = 1.0f / x;\n  xx2 = xx * xx;\n  xx4 = xx2 * xx2;\n  result += logf(x) + 1.0f / 24.0f * xx2 \n    - 7.0f / 960.0f * xx4 + 31.0f / 8064.0f * xx4 * xx2 \n    - 127.0f / 30720.0f * xx4 * xx4;\n  return result;\n}\n\n__inline__ __device__\nfloat ReduceSum(const float* vec, const int length) {\n  \n  __shared__ float shared[32];\n\n  // figure out the warp/ position inside the warp\n  int warp =  threadIdx.x / WARP_SIZE;\n  int lane = threadIdx.x % WARP_SIZE;\n  \n  // paritial sum\n  float val = 0.0f;\n  for (int i = threadIdx.x; i < length; i += blockDim.x) \n    val += vec[i];\n  val = warp_reduce_sum(val);\n  \n  // write out the partial reduction to shared memory if appropiate\n  if (lane == 0) {\n    shared[warp] = val;\n  }\n  __syncthreads();\n  \n  // if we we don't have multiple warps, we're done\n  if (blockDim.x <= WARP_SIZE) {\n    return shared[0];\n  }\n\n  // otherwise reduce again in the first warp\n  val = (threadIdx.x < blockDim.x / WARP_SIZE) ? shared[lane]: 0.0f;\n  if (warp == 0) {\n    val = warp_reduce_sum(val);\n    // broadcast back to shared memory\n    if (threadIdx.x == 0) {\n        shared[0] = val;\n    }\n  }\n  __syncthreads();\n  return shared[0];\n}\n\n__global__ void EstepKernel(\n  const int*__restrict__ cols,\n  const int*__restrict__ indptr, \n  const bool*__restrict__ vali,\n  const float*__restrict__ counts,\n  const bool init_gamma,\n  const int num_cols,\n  const int num_indptr, \n  const int num_topics,\n  const int num_iters,\n  const float*__restrict__ alpha,\n  const float*__restrict__ beta,\n  float*__restrict__ gamma,\n  float*__restrict__ grad_alpha,\n  float*__restrict__ new_beta, \n  float*__restrict__ train_losses,\n  float*__restrict__ vali_losses,\n  int*__restrict__ locks)\n{  \n  // storage for block\n  extern __shared__ float shared_memory[];\n  float*__restrict__  _new_gamma = &shared_memory[0];\n  float*__restrict__  _phi = &shared_memory[num_topics];\n  float*__restrict__  _loss_vec = &shared_memory[num_topics * 2];\n  float*__restrict__  _vali_phi_sum = &shared_memory[num_topics * 3];\n\n  float* _grad_alpha = grad_alpha + num_topics * blockIdx.x;\n\n  for (int i = blockIdx.x; i < num_indptr; i += gridDim.x) {\n    int beg = indptr[i], end = indptr[i + 1];\n    float* _gamma = gamma + num_topics * i;\n    if (init_gamma) {\n      for (int j = threadIdx.x; j < num_topics; j += blockDim.x) {\n        _gamma[j] = alpha[j] + (end - beg) / num_topics;\n      }\n    }\n    __syncthreads();\n    \n    // initiate phi sum for validation data for computing vali loss \n    for (int j = threadIdx.x; j < num_topics; j += blockDim.x)\n      _vali_phi_sum[j] = 0.0f;\n\n    // iterate E step\n    for (int j = 0; j < num_iters; ++j) {\n      // initialize new gamma\n      for (int k = threadIdx.x; k < num_topics; k += blockDim.x)\n        _new_gamma[k] = 0.0f;\n      __syncthreads();\n\n      // compute phi from gamma\n      for (int k = beg; k < end; ++k) {\n        const int w = cols[k];  // word\n        const bool _vali = vali[k];\n        const float c = counts[k]; \n        // compute phi\n        if (not _vali or j + 1 == num_iters) {\n          for (int l = threadIdx.x; l < num_topics; l += blockDim.x)\n            _phi[l] = beta[w * num_topics + l] * expf(Digamma(_gamma[l]));\n          __syncthreads();\n          \n          // normalize phi and add it to new gamma and new beta\n          float phi_sum = ReduceSum(_phi, num_topics);\n\n          for (int l = threadIdx.x; l < num_topics; l += blockDim.x) {\n            _phi[l] /= phi_sum;\n            \n            // update gamma for train data and phi_sum for computing loss\n            if (_vali) \n              _vali_phi_sum[l] += _phi[l] * c;\n            else\n              _new_gamma[l] += _phi[l] * c;\n          \n          }\n          __syncthreads();\n        }\n        \n        if (j + 1 == num_iters) {\n          // update beta for train data\n          if (not _vali) {\n            // write access of w th vector of new_beta \n            if (threadIdx.x == 0) {\n              while (atomicCAS(&locks[w], 0, 1)) {}\n            } \n\n            __syncthreads();\n            for (int l = threadIdx.x; l < num_topics; l += blockDim.x)\n              new_beta[w * num_topics + l] += _phi[l] * c;\n            __syncthreads();\n\n            // release lock\n            if (threadIdx.x == 0) locks[w] = 0;\n            __syncthreads();\n          }\n          \n          // comput loss and reset shared mem\n          // see Eq (15) in https://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf\n          for (int l = threadIdx.x; l < num_topics; l += blockDim.x) {\n            _loss_vec[l] = logf(fmaxf(beta[w * num_topics + l], EPS));\n            _loss_vec[l] -= logf(fmaxf(_phi[l], EPS));\n            _loss_vec[l] *= _phi[l];\n          }\n          __syncthreads();\n          float _loss = ReduceSum(_loss_vec, num_topics) * c;\n          if (threadIdx.x == 0) {\n            if (_vali) \n              vali_losses[blockIdx.x] += _loss;\n            else\n              train_losses[blockIdx.x] += _loss;\n          }\n          __syncthreads();\n\n        }\n        __syncthreads();\n      }\n\n      // update gamma\n      for (int k = threadIdx.x; k < num_topics; k += blockDim.x)\n        _gamma[k] = _new_gamma[k] + alpha[k];\n      __syncthreads();\n    }\n\n    // update gradient of alpha and loss from E[log(theta)]\n    float gamma_sum = ReduceSum(_gamma, num_topics);\n    for (int j = threadIdx.x; j < num_topics; j += blockDim.x) {\n      float Elogthetad = Digamma(_gamma[j]) - Digamma(gamma_sum);\n      _grad_alpha[j] += Elogthetad;\n      _new_gamma[j] *= Elogthetad;\n      _vali_phi_sum[j] *= Elogthetad;\n    }\n    \n    // see Eq (15) in https://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf\n    float train_loss = ReduceSum(_new_gamma, num_topics);\n    float vali_loss = ReduceSum(_vali_phi_sum, num_topics);\n    if (threadIdx.x == 0) {\n      train_losses[blockIdx.x] += train_loss;\n      vali_losses[blockIdx.x] += vali_loss;\n    }\n\n    __syncthreads();\n  } \n}"
        ]
    },
    "particles-cuda": {
        "/Users/gbolet/hecbench-roofline/src/particles-cuda/bitonicSort_kernels.cu": [
            "__device__\ninline void ComparatorLocal(\n    unsigned int* keyA,\n    unsigned int* valA,\n    unsigned int* keyB,\n    unsigned int* valB,\n    const unsigned int dir)\n{\n  if( (*keyA > *keyB) == dir ){\n    unsigned int t;\n    t = *keyA; *keyA = *keyB; *keyB = t;\n    t = *valA; *valA = *valB; *valB = t;\n  }\n}\n\n__global__ void bitonicSortLocal(\n    unsigned int*__restrict__ d_DstKey,\n    unsigned int*__restrict__ d_DstVal,\n    const unsigned int*__restrict__ d_SrcKey,\n    const unsigned int*__restrict__ d_SrcVal,\n    const unsigned int arrayLength,\n    const unsigned int dir)\n{\n  __shared__  unsigned int l_key[LOCAL_SIZE_LIMIT];\n  __shared__  unsigned int l_val[LOCAL_SIZE_LIMIT];\n\n  //Offset to the beginning of subbatch and load data\n  d_SrcKey += blockIdx.x * LOCAL_SIZE_LIMIT + threadIdx.x;\n  d_SrcVal += blockIdx.x * LOCAL_SIZE_LIMIT + threadIdx.x;\n  d_DstKey += blockIdx.x * LOCAL_SIZE_LIMIT + threadIdx.x;\n  d_DstVal += blockIdx.x * LOCAL_SIZE_LIMIT + threadIdx.x;\n  l_key[threadIdx.x +                      0] = d_SrcKey[                     0];\n  l_val[threadIdx.x +                      0] = d_SrcVal[                     0];\n  l_key[threadIdx.x + (LOCAL_SIZE_LIMIT / 2)] = d_SrcKey[(LOCAL_SIZE_LIMIT / 2)];\n  l_val[threadIdx.x + (LOCAL_SIZE_LIMIT / 2)] = d_SrcVal[(LOCAL_SIZE_LIMIT / 2)];\n\n  for(unsigned int size = 2; size < arrayLength; size <<= 1){\n    //Bitonic merge\n    unsigned int ddd = dir ^ ( (threadIdx.x & (size / 2)) != 0 );\n    for(unsigned int stride = size / 2; stride > 0; stride >>= 1){\n      __syncthreads();\n      unsigned int pos = 2 * threadIdx.x - (threadIdx.x & (stride - 1));\n      ComparatorLocal(\n          &l_key[pos +      0], &l_val[pos +      0],\n          &l_key[pos + stride], &l_val[pos + stride],\n          ddd);\n    }\n  }\n\n  //ddd == dir for the last bitonic merge step\n  {\n    for(unsigned int stride = arrayLength / 2; stride > 0; stride >>= 1){\n      __syncthreads();\n      unsigned int pos = 2 * threadIdx.x - (threadIdx.x & (stride - 1));\n      ComparatorLocal(\n          &l_key[pos +      0], &l_val[pos +      0],\n          &l_key[pos + stride], &l_val[pos + stride],\n          dir);\n    }\n  }\n\n  __syncthreads();\n  d_DstKey[                     0] = l_key[threadIdx.x +                      0];\n  d_DstVal[                     0] = l_val[threadIdx.x +                      0];\n  d_DstKey[(LOCAL_SIZE_LIMIT / 2)] = l_key[threadIdx.x + (LOCAL_SIZE_LIMIT / 2)];\n  d_DstVal[(LOCAL_SIZE_LIMIT / 2)] = l_val[threadIdx.x + (LOCAL_SIZE_LIMIT / 2)];\n}",
            "__device__\ninline void ComparatorLocal(\n    unsigned int* keyA,\n    unsigned int* valA,\n    unsigned int* keyB,\n    unsigned int* valB,\n    const unsigned int dir)\n{\n  if( (*keyA > *keyB) == dir ){\n    unsigned int t;\n    t = *keyA; *keyA = *keyB; *keyB = t;\n    t = *valA; *valA = *valB; *valB = t;\n  }\n}\n\n__global__ void bitonicSortLocal1(\n    unsigned int*__restrict__ d_DstKey,\n    unsigned int*__restrict__ d_DstVal,\n    const unsigned int*__restrict__ d_SrcKey,\n    const unsigned int*__restrict__ d_SrcVal)\n{\n  __shared__ unsigned int l_key[LOCAL_SIZE_LIMIT];\n  __shared__ unsigned int l_val[LOCAL_SIZE_LIMIT];\n\n  //Offset to the beginning of subarray and load data\n  d_SrcKey += blockIdx.x * LOCAL_SIZE_LIMIT + threadIdx.x;\n  d_SrcVal += blockIdx.x * LOCAL_SIZE_LIMIT + threadIdx.x;\n  d_DstKey += blockIdx.x * LOCAL_SIZE_LIMIT + threadIdx.x;\n  d_DstVal += blockIdx.x * LOCAL_SIZE_LIMIT + threadIdx.x;\n  l_key[threadIdx.x +                      0] = d_SrcKey[                     0];\n  l_val[threadIdx.x +                      0] = d_SrcVal[                     0];\n  l_key[threadIdx.x + (LOCAL_SIZE_LIMIT / 2)] = d_SrcKey[(LOCAL_SIZE_LIMIT / 2)];\n  l_val[threadIdx.x + (LOCAL_SIZE_LIMIT / 2)] = d_SrcVal[(LOCAL_SIZE_LIMIT / 2)];\n\n  unsigned int comparatorI = (blockIdx.x * blockDim.x + threadIdx.x) & ((LOCAL_SIZE_LIMIT / 2) - 1);\n\n  for(unsigned int size = 2; size < LOCAL_SIZE_LIMIT; size <<= 1){\n    //Bitonic merge\n    unsigned int ddd = (comparatorI & (size / 2)) != 0;\n    for(unsigned int stride = size / 2; stride > 0; stride >>= 1){\n      __syncthreads();\n      unsigned int pos = 2 * threadIdx.x - (threadIdx.x & (stride - 1));\n      ComparatorLocal(\n          &l_key[pos +      0], &l_val[pos +      0],\n          &l_key[pos + stride], &l_val[pos + stride],\n          ddd\n               );\n    }\n  }\n\n  //Odd / even arrays of LOCAL_SIZE_LIMIT elements\n  //sorted in opposite directions\n  {\n    unsigned int ddd = (blockIdx.x & 1);\n    for(unsigned int stride = LOCAL_SIZE_LIMIT / 2; stride > 0; stride >>= 1){\n      __syncthreads();\n      unsigned int pos = 2 * threadIdx.x - (threadIdx.x & (stride - 1));\n      ComparatorLocal(\n          &l_key[pos +      0], &l_val[pos +      0],\n          &l_key[pos + stride], &l_val[pos + stride],\n          ddd\n               );\n    }\n  }\n\n  __syncthreads();\n  d_DstKey[                     0] = l_key[threadIdx.x +                      0];\n  d_DstVal[                     0] = l_val[threadIdx.x +                      0];\n  d_DstKey[(LOCAL_SIZE_LIMIT / 2)] = l_key[threadIdx.x + (LOCAL_SIZE_LIMIT / 2)];\n  d_DstVal[(LOCAL_SIZE_LIMIT / 2)] = l_val[threadIdx.x + (LOCAL_SIZE_LIMIT / 2)];\n}",
            "__device__\ninline void ComparatorPrivate(\n    unsigned int *keyA,\n    unsigned int *valA,\n    unsigned int *keyB,\n    unsigned int *valB,\n    unsigned int dir)\n{\n  if( (*keyA > *keyB) == dir ){\n    unsigned int t;\n    t = *keyA; *keyA = *keyB; *keyB = t;\n    t = *valA; *valA = *valB; *valB = t;\n  }\n}\n\n__global__ void bitonicMergeGlobal(\n    unsigned int*__restrict__ d_DstKey,\n    unsigned int*__restrict__ d_DstVal,\n    const unsigned int*__restrict__ d_SrcKey,\n    const unsigned int*__restrict__ d_SrcVal,\n    const unsigned int arrayLength,\n    const unsigned int size,\n    const unsigned int stride,\n    const unsigned int dir)\n{\n  unsigned int global_comparatorI = blockIdx.x * blockDim.x + threadIdx.x;\n  unsigned int        comparatorI = global_comparatorI & (arrayLength / 2 - 1);\n\n  //Bitonic merge\n  unsigned int ddd = dir ^ ( (comparatorI & (size / 2)) != 0 );\n  unsigned int pos = 2 * global_comparatorI - (global_comparatorI & (stride - 1));\n\n  unsigned int keyA = d_SrcKey[pos +      0];\n  unsigned int valA = d_SrcVal[pos +      0];\n  unsigned int keyB = d_SrcKey[pos + stride];\n  unsigned int valB = d_SrcVal[pos + stride];\n\n  ComparatorPrivate(\n      &keyA, &valA,\n      &keyB, &valB,\n      ddd);\n\n  d_DstKey[pos +      0] = keyA;\n  d_DstVal[pos +      0] = valA;\n  d_DstKey[pos + stride] = keyB;\n  d_DstVal[pos + stride] = valB;\n}",
            "__device__\ninline void ComparatorLocal(\n    unsigned int* keyA,\n    unsigned int* valA,\n    unsigned int* keyB,\n    unsigned int* valB,\n    const unsigned int dir)\n{\n  if( (*keyA > *keyB) == dir ){\n    unsigned int t;\n    t = *keyA; *keyA = *keyB; *keyB = t;\n    t = *valA; *valA = *valB; *valB = t;\n  }\n}\n\n__global__ void bitonicMergeLocal(\n    unsigned int*__restrict__ d_DstKey,\n    unsigned int*__restrict__ d_DstVal,\n    const unsigned int*__restrict__ d_SrcKey,\n    const unsigned int*__restrict__ d_SrcVal,\n    const unsigned int arrayLength,\n    const unsigned int size,\n    unsigned int stride,\n    const unsigned int dir)\n{\n  __shared__ unsigned int l_key[LOCAL_SIZE_LIMIT];\n  __shared__ unsigned int l_val[LOCAL_SIZE_LIMIT];\n\n  d_SrcKey += blockIdx.x * LOCAL_SIZE_LIMIT + threadIdx.x;\n  d_SrcVal += blockIdx.x * LOCAL_SIZE_LIMIT + threadIdx.x;\n  d_DstKey += blockIdx.x * LOCAL_SIZE_LIMIT + threadIdx.x;\n  d_DstVal += blockIdx.x * LOCAL_SIZE_LIMIT + threadIdx.x;\n  l_key[threadIdx.x +                      0] = d_SrcKey[                     0];\n  l_val[threadIdx.x +                      0] = d_SrcVal[                     0];\n  l_key[threadIdx.x + (LOCAL_SIZE_LIMIT / 2)] = d_SrcKey[(LOCAL_SIZE_LIMIT / 2)];\n  l_val[threadIdx.x + (LOCAL_SIZE_LIMIT / 2)] = d_SrcVal[(LOCAL_SIZE_LIMIT / 2)];\n\n  //Bitonic merge\n  unsigned int comparatorI = (blockIdx.x * blockDim.x + threadIdx.x) & ((arrayLength / 2) - 1);\n  unsigned int         ddd = dir ^ ( (comparatorI & (size / 2)) != 0 );\n  for(; stride > 0; stride >>= 1){\n    __syncthreads();\n    unsigned int pos = 2 * threadIdx.x - (threadIdx.x & (stride - 1));\n    ComparatorLocal(\n        &l_key[pos +      0], &l_val[pos +      0],\n        &l_key[pos + stride], &l_val[pos + stride],\n        ddd);\n  }\n\n  __syncthreads();\n  d_DstKey[                     0] = l_key[threadIdx.x +                      0];\n  d_DstVal[                     0] = l_val[threadIdx.x +                      0];\n  d_DstKey[(LOCAL_SIZE_LIMIT / 2)] = l_key[threadIdx.x + (LOCAL_SIZE_LIMIT / 2)];\n  d_DstVal[(LOCAL_SIZE_LIMIT / 2)] = l_val[threadIdx.x + (LOCAL_SIZE_LIMIT / 2)];\n}"
        ],
        "/Users/gbolet/hecbench-roofline/src/particles-cuda/particles_kernels.cu": [
            "__global__ void integrateSystemK(\n    float4*__restrict__ d_Pos,  //input/output\n    float4*__restrict__ d_Vel,  //input/output\n    const simParams_t params,\n    const float deltaTime,\n    const unsigned int numParticles)\n{\n  const unsigned int index = blockIdx.x * blockDim.x + threadIdx.x;\n  if(index >= numParticles) return;\n\n  float4 pos = d_Pos[index];\n  float4 vel = d_Vel[index];\n\n  pos.w = 1.0f;\n  vel.w = 0.0f;\n\n  //Gravity\n  float4 g = {params.gravity.x, params.gravity.y, params.gravity.z, 0};\n  vel += g * deltaTime;\n  vel *= params.globalDamping;\n\n  //Advance pos\n  pos += vel * deltaTime;\n\n  //printf(\"before %d %3.f %3.f %3.f\\n\", index, pos.x, pos.y, pos.z);\n\n  //Collide with cube\n  if(pos.x < -1.0f + params.particleRadius){\n    pos.x = -1.0f + params.particleRadius;\n    vel.x *= params.boundaryDamping;\n  }\n  if(pos.x > 1.0f - params.particleRadius){\n    pos.x = 1.0f - params.particleRadius;\n    vel.x *= params.boundaryDamping;\n  }\n\n  if(pos.y < -1.0f + params.particleRadius){\n    pos.y = -1.0f + params.particleRadius;\n    vel.y *= params.boundaryDamping;\n  }\n  if(pos.y > 1.0f - params.particleRadius){\n    pos.y = 1.0f - params.particleRadius;\n    vel.y *= params.boundaryDamping;\n  }\n\n  if(pos.z < -1.0f + params.particleRadius){\n    pos.z = -1.0f + params.particleRadius;\n    vel.z *= params.boundaryDamping;\n  }\n  if(pos.z > 1.0f - params.particleRadius){\n    pos.z = 1.0f - params.particleRadius;\n    vel.z *= params.boundaryDamping;\n  }\n\n  //Store new position and velocity\n  d_Pos[index] = pos;\n  d_Vel[index] = vel;\n  //printf(\"after %d %3.f %3.f %3.f\\n\", index, pos.x, pos.y, pos.z);\n}",
            "#define T ((int)32)\n\n\n__host__ __device__ __forceinline__ constexpr T floor(T a, T b) {\n  return (a - b + 1) / b;\n}\n\n__device__\nunsigned int getGridHash(int4 gridPos, const simParams_t &params)\n{\n  //Wrap addressing, assume power-of-two grid dimensions\n  gridPos.x = gridPos.x & (params.gridSize.x - 1);\n  gridPos.y = gridPos.y & (params.gridSize.y - 1);\n  gridPos.z = gridPos.z & (params.gridSize.z - 1);\n  return UMAD( UMAD(gridPos.z, params.gridSize.y, gridPos.y), params.gridSize.x, gridPos.x );\n}\n\n__device__\nint4 getGridPos(const float4 p, const simParams_t &params)\n{\n  int4 gridPos;\n  gridPos.x = (int)floor((p.x - params.worldOrigin.x) / params.cellSize.x);\n  gridPos.y = (int)floor((p.y - params.worldOrigin.y) / params.cellSize.y);\n  gridPos.z = (int)floor((p.z - params.worldOrigin.z) / params.cellSize.z);\n  gridPos.w = 0;\n  return gridPos;\n}\n\n__global__ void calcHashK(\n    unsigned int*__restrict__ d_Hash, //output\n    unsigned int*__restrict__ d_Index, //output\n    const float4*__restrict__ d_Pos, //input: positions\n    const simParams_t params,\n    unsigned int numParticles)\n{\n  const unsigned int index = blockIdx.x * blockDim.x + threadIdx.x;\n  if(index >= numParticles) return;\n\n  float4 p = d_Pos[index];\n\n  //Get address in grid\n  int4  gridPos = getGridPos(p, params);\n  unsigned int gridHash = getGridHash(gridPos, params);\n\n  //Store grid hash and particle index\n  d_Hash[index] = gridHash;\n  d_Index[index] = index;\n}",
            "__global__ void memSetK(\n    unsigned int* d_Data,\n    const unsigned int val,\n    const unsigned int N)\n{\n  unsigned int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if(i < N) d_Data[i] = val;\n}",
            "__global__ void findCellBoundsAndReorderK(\n    unsigned int*__restrict__ d_CellStart,     //output: cell start index\n    unsigned int*__restrict__ d_CellEnd,       //output: cell end index\n    float4*__restrict__ d_ReorderedPos,  //output: reordered by cell hash positions\n    float4*__restrict__ d_ReorderedVel,  //output: reordered by cell hash velocities\n    const unsigned int*__restrict__ d_Hash,    //input: sorted grid hashes\n    const unsigned int*__restrict__ d_Index,   //input: particle indices sorted by hash\n    const float4*__restrict__ d_Pos,     //input: positions array sorted by hash\n    const float4*__restrict__ d_Vel,     //input: velocity array sorted by hash\n    const unsigned int numParticles)\n{\n  unsigned int hash;\n  const unsigned int index = blockIdx.x * blockDim.x + threadIdx.x;\n  const unsigned int lid = threadIdx.x;\n\n  extern __shared__ unsigned int localHash[];\n\n  //Handle case when no. of particles not multiple of block size\n  if(index < numParticles){\n    hash = d_Hash[index];\n\n    //Load hash data into local memory so that we can look \n    //at neighboring particle's hash value without loading\n    //two hash values per thread\n    localHash[lid + 1] = hash;\n\n    //First thread in block must load neighbor particle hash\n    if(index > 0 && lid == 0)\n      localHash[0] = d_Hash[index - 1];\n  }\n\n  __syncthreads();\n\n  if(index < numParticles){\n    //Border case\n    if(index == 0)\n      d_CellStart[hash] = 0;\n\n    //Main case\n    else{\n      if(hash != localHash[lid])\n        d_CellEnd[localHash[lid]]  = d_CellStart[hash] = index;\n    };\n\n    //Another border case\n    if(index == numParticles - 1)\n      d_CellEnd[hash] = numParticles;\n\n\n    //Now use the sorted index to reorder the pos and vel arrays\n    unsigned int sortedIndex = d_Index[index];\n    float4 pos = d_Pos[sortedIndex];\n    float4 vel = d_Vel[sortedIndex];\n\n    d_ReorderedPos[index] = pos;\n    d_ReorderedVel[index] = vel;\n  }\n}",
            "#define T ((int)32)\n\n\n__host__ __device__ __forceinline__ constexpr T floor(T a, T b) {\n  return (a - b + 1) / b;\n}\n\n__device__\nfloat4 collideSpheres(\n    float4 posA,\n    float4 posB,\n    float4 velA,\n    float4 velB,\n    float radiusA,\n    float radiusB,\n    float spring,\n    float damping,\n    float shear,\n    float attraction)\n{\n  //Calculate relative position\n  float4     relPos = {posB.x - posA.x, posB.y - posA.y, posB.z - posA.z, 0};\n  float        dist = sqrt(relPos.x * relPos.x + relPos.y * relPos.y + relPos.z * relPos.z);\n  float collideDist = radiusA + radiusB;\n\n  float4 force = {0, 0, 0, 0};\n  if(dist < collideDist){\n    float4 norm = {relPos.x / dist, relPos.y / dist, relPos.z / dist, 0};\n\n    //Relative velocity\n    float4 relVel = {velB.x - velA.x, velB.y - velA.y, velB.z - velA.z, 0};\n\n    //Relative tangential velocity\n    float relVelDotNorm = relVel.x * norm.x + relVel.y * norm.y + relVel.z * norm.z;\n    float4 tanVel = {relVel.x - relVelDotNorm * norm.x, relVel.y - relVelDotNorm * norm.y, \n      relVel.z - relVelDotNorm * norm.z, 0};\n\n    //Spring force (potential)\n    float springFactor = -spring * (collideDist - dist);\n    force = {\n      springFactor * norm.x + damping * relVel.x + shear * tanVel.x + attraction * relPos.x,\n      springFactor * norm.y + damping * relVel.y + shear * tanVel.y + attraction * relPos.y,\n      springFactor * norm.z + damping * relVel.z + shear * tanVel.z + attraction * relPos.z,\n      0\n    };\n  }\n\n  return force;\n}\n\n__device__\nunsigned int getGridHash(int4 gridPos, const simParams_t &params)\n{\n  //Wrap addressing, assume power-of-two grid dimensions\n  gridPos.x = gridPos.x & (params.gridSize.x - 1);\n  gridPos.y = gridPos.y & (params.gridSize.y - 1);\n  gridPos.z = gridPos.z & (params.gridSize.z - 1);\n  return UMAD( UMAD(gridPos.z, params.gridSize.y, gridPos.y), params.gridSize.x, gridPos.x );\n}\n\n__device__\nint4 getGridPos(const float4 p, const simParams_t &params)\n{\n  int4 gridPos;\n  gridPos.x = (int)floor((p.x - params.worldOrigin.x) / params.cellSize.x);\n  gridPos.y = (int)floor((p.y - params.worldOrigin.y) / params.cellSize.y);\n  gridPos.z = (int)floor((p.z - params.worldOrigin.z) / params.cellSize.z);\n  gridPos.w = 0;\n  return gridPos;\n}\n\n__global__ void collideK(\n    float4*__restrict__ d_Vel,          //output: new velocity\n    const float4*__restrict__ d_ReorderedPos, //input: reordered positions\n    const float4*__restrict__ d_ReorderedVel, //input: reordered velocities\n    const unsigned int*__restrict__ d_Index,        //input: reordered particle indices\n    const unsigned int*__restrict__ d_CellStart,    //input: cell boundaries\n    const unsigned int*__restrict__ d_CellEnd,\n    const simParams_t params,\n    const unsigned int numParticles)\n{\n  unsigned int index = blockIdx.x * blockDim.x + threadIdx.x;\n  if(index >= numParticles) return;\n\n  float4   pos = d_ReorderedPos[index];\n  float4   vel = d_ReorderedVel[index];\n  float4 force = {0, 0, 0, 0};\n\n  //Get address in grid\n  int4 gridPos = getGridPos(pos, params);\n\n  //Accumulate surrounding cells\n  for(int z = -1; z <= 1; z++)\n    for(int y = -1; y <= 1; y++)\n      for(int x = -1; x <= 1; x++){\n        //Get start particle index for this cell\n        int4 t = {x, y, z, 0};\n        unsigned int   hash = getGridHash(gridPos + t, params);\n        unsigned int startI = d_CellStart[hash];\n\n        //Skip empty cell\n        if(startI == 0xFFFFFFFFU) continue;\n\n        //Iterate over particles in this cell\n        unsigned int endI = d_CellEnd[hash];\n        for(unsigned int j = startI; j < endI; j++){\n          if(j == index) continue;\n\n          float4 pos2 = d_ReorderedPos[j];\n          float4 vel2 = d_ReorderedVel[j];\n\n          //Collide two spheres\n          force += collideSpheres(\n              pos, pos2,\n              vel, vel2,\n              params.particleRadius, params.particleRadius, \n              params.spring, params.damping, params.shear, params.attraction);\n        }\n      }\n\n  //Collide with cursor sphere\n  force += collideSpheres(\n      pos, {params.colliderPos.x, params.colliderPos.y, params.colliderPos.z, 0},\n      vel, {0, 0, 0, 0},\n      params.particleRadius, params.colliderRadius,\n      params.spring, params.damping, params.shear, params.attraction);\n\n  //Write new velocity back to original unsorted location\n  d_Vel[d_Index[index]] = vel + force;\n}"
        ]
    },
    "colorwheel-cuda": {
        "/Users/gbolet/hecbench-roofline/src/colorwheel-cuda/main.cu": [
            "inline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\ninline __host__ __device__ float4 sqrtf(float4 v)\n{\n    return make_float4(sqrtf(v.x), sqrtf(v.y), sqrtf(v.z), sqrtf(v.w));\n}\n\n__host__ __device__\nvoid setcols(int cw[MAXCOLS][3], int r, int g, int b, int k)\n{\n  cw[k][0] = r;\n  cw[k][1] = g;\n  cw[k][2] = b;\n}\n\n__host__ __device__\nvoid computeColor(float fx, float fy, uchar *pix)\n{\n  int cw[MAXCOLS][3];  // color wheel\n\n  // relative lengths of color transitions:\n  // these are chosen based on perceptual similarity\n  // (e.g. one can distinguish more shades between red and yellow \n  //  than between yellow and green)\n  int i;\n  int k = 0;\n  for (i = 0; i < RY; i++) setcols(cw, 255,     255*i/RY,   0,       k++);\n  for (i = 0; i < YG; i++) setcols(cw, 255-255*i/YG, 255,     0,     k++);\n  for (i = 0; i < GC; i++) setcols(cw, 0,       255,     255*i/GC,   k++);\n  for (i = 0; i < CB; i++) setcols(cw, 0,       255-255*i/CB, 255,   k++);\n  for (i = 0; i < BM; i++) setcols(cw, 255*i/BM,     0,     255,     k++);\n  for (i = 0; i < MR; i++) setcols(cw, 255,     0,     255-255*i/MR, k++);\n\n  float rad = sqrtf(fx * fx + fy * fy);\n  float a = atan2f(-fy, -fx) / (float)M_PI;\n  float fk = (a + 1.f) / 2.f * (MAXCOLS-1);\n  int k0 = (int)fk;\n  int k1 = (k0 + 1) % MAXCOLS;\n  float f = fk - k0;\n  for (int b = 0; b < 3; b++) {\n    float col0 = cw[k0][b] / 255.f;\n    float col1 = cw[k1][b] / 255.f;\n    float col = (1.f - f) * col0 + f * col1;\n    if (rad <= 1)\n      col = 1.f - rad * (1.f - col); // increase saturation with radius\n    else\n      col *= .75f; // out of range\n    pix[2 - b] = (int)(255.f * col);\n  }\n}\n\n__global__\nvoid color (uchar* pix, int size, int half_size, float range, float truerange)\n{\n  int y = blockDim.y * blockIdx.y + threadIdx.y;\n  int x = blockDim.x * blockIdx.x + threadIdx.x;\n\n  if (y < size && x < size) {\n    float fx = (float)x / (float)half_size * range - range;\n    float fy = (float)y / (float)half_size * range - range;\n    if (x == half_size || y == half_size) return; // make black coordinate axes\n    size_t idx = (y * size + x) * 3;\n    computeColor(fx/truerange, fy/truerange, pix+idx);\n  }\n}"
        ]
    },
    "urng-cuda": {
        "/Users/gbolet/hecbench-roofline/src/urng-cuda/kernel.cu": [
            "inline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\ninline __device__ float4 convert_float4(uchar4 data) \n{\n   float4 r = make_float4(data.x, data.y, data.z, data.w);\n   return r;\n}\n\n__device__ __forceinline__ \nuchar4 convert_uchar4_sat(float4 v) {\n  uchar4 res;\n  res.x = (unsigned char) ((v.x > 255.f) ? 255.f : (v.x < 0.f ? 0.f : v.x));\n  res.y = (unsigned char) ((v.y > 255.f) ? 255.f : (v.y < 0.f ? 0.f : v.y));\n  res.z = (unsigned char) ((v.z > 255.f) ? 255.f : (v.z < 0.f ? 0.f : v.z));\n  res.w = (unsigned char) ((v.w > 255.f) ? 255.f : (v.w < 0.f ? 0.f : v.w));\n  return res;\n}\n\n__device__\nfloat ran1(int idum, int *iv)\n{\n  int j;\n  int k;\n  int iy = 0;\n  int tid = threadIdx.x;\n\n  for(j = NTAB; j >=0; j--)      //Load the shuffle\n  {\n    k = idum / IQ;\n    idum = IA * (idum - k * IQ) - IR * k;\n\n    if(idum < 0)\n      idum += IM;\n\n    if(j < NTAB)\n      iv[NTAB* tid + j] = idum;\n  }\n  iy = iv[NTAB* tid];\n\n  k = idum / IQ;\n  idum = IA * (idum - k * IQ) - IR * k;\n\n  if(idum < 0)\n    idum += IM;\n\n  j = iy / NDIV;\n  iy = iv[NTAB * tid + j];\n  return (AM * iy);  //AM *iy will be between 0.0 and 1.0\n}\n\n__global__\nvoid noise_uniform(\n  const uchar4*__restrict__ inputImage, \n        uchar4*__restrict__ outputImage, \n  const int factor)\n{\n  int pos = blockIdx.x * blockDim.x + threadIdx.x;\n\n  float4 temp = convert_float4(inputImage[pos]);\n\n  /* compute average value of a pixel from its compoments */\n  float avg = (temp.x + temp.y + temp.z + temp.w) / 4.0f;\n\n  /* Each thread has NTAB private values */\n  __shared__ int iv[NTAB * GROUP_SIZE];\n\n  /* Calculate deviation from the avg value of a pixel */\n  float dev = ran1(-avg, iv);\n  dev = (dev - 0.55f) * (float)factor;\n\n  /* Saturate(clamp) the values */\n  outputImage[pos] = convert_uchar4_sat(temp + dev);\n}"
        ]
    },
    "geodesic-cuda": {
        "/Users/gbolet/hecbench-roofline/src/geodesic-cuda/main.cu": [
            "inline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\ninline __host__ __device__ float4 sqrtf(float4 v)\n{\n    return make_float4(sqrtf(v.x), sqrtf(v.y), sqrtf(v.z), sqrtf(v.w));\n}\n\n__global__ void \nkernel_distance (const float4 *__restrict__ d_A,\n                        float *__restrict__ d_C,\n                 const int N)\n{\n  const int wiID = blockIdx.x * blockDim.x + threadIdx.x;\n  if (wiID >= N) return;\n\n  const float GDC_DEG_TO_RAD = 3.141592654 / 180.0 ;  /* Degrees to radians */\n  const float GDC_FLATTENING = 1.0 - ( 6356752.31424518 / 6378137.0 ) ; \n  const float GDC_ECCENTRICITY = ( 6356752.31424518 / 6378137.0 ) ; \n  const float GDC_ELLIPSOIDAL =  1.0 / ( 6356752.31414 / 6378137.0 ) / ( 6356752.31414 / 6378137.0 ) - 1.0 ;\n  const float GC_SEMI_MINOR = 6356752.31424518f;\n  const float EPS                    = 0.5e-5f;\n  float  dist, BAZ , C , C2A , CU1 , CU2 , CX , CY , CZ ,\n         D , E , FAZ , SA , SU1 , SX  , SY , TU1 , TU2 , X , Y ; \n\n  const float rad_latitude_1  = d_A[wiID].x * GDC_DEG_TO_RAD ;\n  const float rad_longitude_1 = d_A[wiID].y * GDC_DEG_TO_RAD ;\n  const float rad_latitude_2  = d_A[wiID].z * GDC_DEG_TO_RAD ;\n  const float rad_longitude_2 = d_A[wiID].w * GDC_DEG_TO_RAD ;\n\n  TU1 = GDC_ECCENTRICITY * sinf ( rad_latitude_1 ) /\n    cosf ( rad_latitude_1 ) ;\n  TU2 = GDC_ECCENTRICITY * sinf ( rad_latitude_2 ) /\n    cosf ( rad_latitude_2 ) ;\n\n  CU1 = 1.0f / sqrtf ( TU1 * TU1 + 1.0f ) ;\n  SU1 = CU1 * TU1 ;\n  CU2 = 1.0f / sqrtf ( TU2 * TU2 + 1.0f ) ;\n  dist = CU1 * CU2 ;\n  BAZ = dist * TU2 ;\n  FAZ = BAZ * TU1 ;\n  X = rad_longitude_2 - rad_longitude_1 ;\n\n  do {\n    SX = sinf ( X ) ;\n    CX = cosf ( X ) ;\n    TU1 = CU2 * SX ;\n    TU2 = BAZ - SU1 * CU2 * CX ;\n    SY = sqrtf ( TU1 * TU1 + TU2 * TU2 ) ;\n    CY = dist * CX + FAZ ;\n    Y = atan2f ( SY, CY ) ;\n    SA = dist * SX / SY ;\n    C2A = - SA * SA + 1.0f;\n    CZ = FAZ + FAZ ;\n    if ( C2A > 0.0f ) CZ = -CZ / C2A + CY ;\n    E = CZ * CZ * 2.0f - 1.0f ;\n    C = ( ( -3.0f * C2A + 4.0f ) * GDC_FLATTENING + 4.0f ) * C2A *\n      GDC_FLATTENING / 16.0f ;\n    D = X ;\n    X = ( ( E * CY * C + CZ ) * SY * C + Y ) * SA ;\n    X = ( 1.0f - C ) * X * GDC_FLATTENING + rad_longitude_2 - rad_longitude_1 ;\n  } while ( fabsf ( D - X ) > EPS ) ;\n\n  X = sqrtf ( GDC_ELLIPSOIDAL * C2A + 1.0f ) + 1.0f ;\n  X = ( X - 2.0f ) / X ;\n  C = 1.0f - X ;\n  C = ( X * X / 4.0f + 1.0f ) / C ;\n  D = ( 0.375f * X * X - 1.0f ) * X ;\n  X = E * CY ;\n  dist = 1.0f - E - E ;\n  dist = ( ( ( ( SY * SY * 4.0f - 3.0f ) * dist * CZ * D / 6.0f -\n          X ) * D / 4.0f + CZ ) * SY * D + Y ) * C * GC_SEMI_MINOR ;\n  d_C[wiID] = dist;\n}"
        ]
    },
    "perlin-cuda": {
        "/Users/gbolet/hecbench-roofline/src/perlin-cuda/noise.cu": [
            "inline __host__ __device__ float2 make_float2(uint2 a)\n{\n    return make_float2(float(a.x), float(a.y));\n}\n\nstatic __forceinline__ __device__\nValueType dot(BasicVector<ValueType> a, BasicVector<ValueType> b)\n{\n    return a.dot(b);\n}\n\ninline __device__ __host__ float4 lerp(float4 a, float4 b, float t)\n{\n    return a + t*(b-a);\n}\n\n__inline__ __device__ float smooth(float t) {\n  return t * t * t * (t * (t * 6.f - 15.f) + 10.f);\n}\n\n__device__ float noiseAt(float x, float y, int seed) {\n\n  // Get top-left corner indices\n  const int ix = static_cast<int>(x),\n            iy = static_cast<int>(y);\n\n  // Weights\n  const float wx = x - ix,\n              wy = y - iy;\n\n  // Get gradients at cell corners\n  const int ix0 = ix & 255, iy0 = iy & 255;\n  const int ix1 = (ix0 + 1) & 255, iy1 = (iy0 + 1) & 255;\n  const int h0 = _hash[ix0], h1 = _hash[ix1];\n  const int iTL = (_hash[h0 + iy0] + seed) % N_GRADIENTS,\n            iTR = (_hash[h1 + iy0] + seed) % N_GRADIENTS,\n            iBL = (_hash[h0 + iy1] + seed) % N_GRADIENTS,\n            iBR = (_hash[h1 + iy1] + seed) % N_GRADIENTS;\n  const float2 gTopLeft  = make_float2(gradientX[iTL], gradientY[iTL]);\n  const float2 gTopRight = make_float2(gradientX[iTR], gradientY[iTR]);\n  const float2 gBotLeft  = make_float2(gradientX[iBL], gradientY[iBL]);\n  const float2 gBotRight = make_float2(gradientX[iBR], gradientY[iBR]);\n\n  // Calculate dots between distance and gradient vectors\n  const float dTopLeft  = dot(gTopLeft,  make_float2(wx,     wy));\n  const float dTopRight = dot(gTopRight, make_float2(wx - 1, wy));\n  const float dBotLeft  = dot(gBotLeft,  make_float2(wx,     wy - 1));\n  const float dBotRight = dot(gBotRight, make_float2(wx - 1, wy - 1));\n\n  // Calculate the smoothed distance between given point and the top-left corner\n  const float tx = smooth(wx), ty = smooth(wy);\n\n  // Interpolate with the other corners\n  const float leftInterp  = lerp(dTopLeft, dBotLeft, ty);\n  const float rightInterp = lerp(dTopRight, dBotRight, ty);\n\n  return (lerp(leftInterp, rightInterp, tx) + 1.0) * 0.5;\n}\n\n__device__ float sumOctaves(float x, float y, NoiseParams params) {\n  float frequency = 1;\n  float sum = noiseAt(x * frequency , y * frequency, params.seed);\n  float amplitude = 1;\n  float range = 1;\n  for (int i = 1; i < params.octaves; i++) {\n    frequency *= params.lacunarity;\n    amplitude *= params.persistence;\n    range += amplitude;\n    sum += amplitude * noiseAt(x * frequency, y * frequency, params.seed);\n  }\n  return sum / range;\n}\n\n__global__ void perlin(int yStart, int height, NoiseParams params, uint8_t *outPixels) {\n  // Pixel coordinates\n  const auto px = CUID(x);\n  const auto py = CUID(y) + yStart;\n\n  if (px >= WIN_WIDTH || py >= yStart + height) return;\n\n  auto noise = sumOctaves(px / params.ppu, py / params.ppu, params);\n\n  // Convert noise to pixel\n  const auto baseIdx = 4 * LIN(px, py, WIN_WIDTH);\n\n  const auto val = noise * 255;\n\n  outPixels[baseIdx + 0] = val;\n  outPixels[baseIdx + 1] = val;\n  outPixels[baseIdx + 2] = val;\n  outPixels[baseIdx + 3] = 255;\n}"
        ]
    },
    "epistasis-cuda": {
        "/Users/gbolet/hecbench-roofline/src/epistasis-cuda/main.cu": [
            "__device__\nfloat gammafunction(unsigned int n)\n{   \n  if(n == 0)\n    return 0.0f;\n  float x = ((float)n + 0.5f) * logf((float) n) - ((float)n - 1.0f);\n  return x;\n}\n\n__global__ void epi(const unsigned int* dev_data_zeros, \n                    const unsigned int* dev_data_ones, \n                    float* dev_scores, \n                    const int num_snp, \n                    const int PP_zeros, \n                    const int PP_ones,\n                    const int mask_zeros, \n                    const int mask_ones) \n{\n  int i, j, tid, p, k;\n\n  j = blockDim.x * blockIdx.x + threadIdx.x; \n  i = blockDim.y * blockIdx.y + threadIdx.y;\n  tid = i * num_snp + j;\n\n  if (j > i && i < num_snp && j < num_snp) {\n    unsigned int ft[2 * 9];\n    for(k = 0; k < 2 * 9; k++) ft[k] = 0;\n\n    unsigned int t00, t01, t02, t10, t11, t12, t20, t21, t22;\n    unsigned int di2, dj2;\n    unsigned int* SNPi;\n    unsigned int* SNPj;\n\n    // Phenotype 0\n    SNPi = (unsigned int*) &dev_data_zeros[i * 2];\n    SNPj = (unsigned int*) &dev_data_zeros[j * 2];\n\n    #pragma unroll 1\n    for (p = 0; p < 2 * PP_zeros * num_snp - 2 * num_snp; p += 2 * num_snp) {\n      di2 = ~(SNPi[p] | SNPi[p + 1]);\n      dj2 = ~(SNPj[p] | SNPj[p + 1]);\n\n      t00 = SNPi[p] & SNPj[p];\n      t01 = SNPi[p] & SNPj[p + 1];\n      t02 = SNPi[p] & dj2;\n      t10 = SNPi[p + 1] & SNPj[p];\n      t11 = SNPi[p + 1] & SNPj[p + 1];\n      t12 = SNPi[p + 1] & dj2;\n      t20 = di2 & SNPj[p];\n      t21 = di2 & SNPj[p + 1];\n      t22 = di2 & dj2;\n\n      ft[0] += __popc(t00);\n      ft[1] += __popc(t01);\n      ft[2] += __popc(t02);\n      ft[3] += __popc(t10);\n      ft[4] += __popc(t11);\n      ft[5] += __popc(t12);\n      ft[6] += __popc(t20);\n      ft[7] += __popc(t21);\n      ft[8] += __popc(t22);\n    }\n\n    // remainder\n    p = 2 * PP_zeros * num_snp - 2 * num_snp;\n    di2 = ~(SNPi[p] | SNPi[p + 1]);\n    dj2 = ~(SNPj[p] | SNPj[p + 1]);\n    di2 = di2 & mask_zeros;\n    dj2 = dj2 & mask_zeros;\n\n    t00 = SNPi[p] & SNPj[p];\n    t01 = SNPi[p] & SNPj[p + 1];\n    t02 = SNPi[p] & dj2;\n    t10 = SNPi[p + 1] & SNPj[p];\n    t11 = SNPi[p + 1] & SNPj[p + 1];\n    t12 = SNPi[p + 1] & dj2;\n    t20 = di2 & SNPj[p];\n    t21 = di2 & SNPj[p + 1];\n    t22 = di2 & dj2;\n\n    ft[0] += __popc(t00);\n    ft[1] += __popc(t01);\n    ft[2] += __popc(t02);\n    ft[3] += __popc(t10);\n    ft[4] += __popc(t11);\n    ft[5] += __popc(t12);\n    ft[6] += __popc(t20);\n    ft[7] += __popc(t21);\n    ft[8] += __popc(t22);\n\n    // Phenotype 1\n    SNPi = (unsigned int*) &dev_data_ones[i * 2];\n    SNPj = (unsigned int*) &dev_data_ones[j * 2];\n\n    #pragma unroll 1\n    for(p = 0; p < 2 * PP_ones * num_snp - 2 * num_snp; p += 2 * num_snp)\n    {\n      di2 = ~(SNPi[p] | SNPi[p + 1]);\n      dj2 = ~(SNPj[p] | SNPj[p + 1]);\n\n      t00 = SNPi[p] & SNPj[p];\n      t01 = SNPi[p] & SNPj[p + 1];\n      t02 = SNPi[p] & dj2;\n      t10 = SNPi[p + 1] & SNPj[p];\n      t11 = SNPi[p + 1] & SNPj[p + 1];\n      t12 = SNPi[p + 1] & dj2;\n      t20 = di2 & SNPj[p];\n      t21 = di2 & SNPj[p + 1];\n      t22 = di2 & dj2;\n\n      ft[9]  += __popc(t00);\n      ft[10] += __popc(t01);\n      ft[11] += __popc(t02);\n      ft[12] += __popc(t10);\n      ft[13] += __popc(t11);\n      ft[14] += __popc(t12);\n      ft[15] += __popc(t20);\n      ft[16] += __popc(t21);\n      ft[17] += __popc(t22);\n    }\n    p = 2 * PP_ones * num_snp - 2 * num_snp;\n    di2 = ~(SNPi[p] | SNPi[p + 1]);\n    dj2 = ~(SNPj[p] | SNPj[p + 1]);\n    di2 = di2 & mask_ones;\n    dj2 = dj2 & mask_ones;\n\n    t00 = SNPi[p] & SNPj[p];\n    t01 = SNPi[p] & SNPj[p + 1];\n    t02 = SNPi[p] & dj2;\n    t10 = SNPi[p + 1] & SNPj[p];\n    t11 = SNPi[p + 1] & SNPj[p + 1];\n    t12 = SNPi[p + 1] & dj2;\n    t20 = di2 & SNPj[p];\n    t21 = di2 & SNPj[p + 1];\n    t22 = di2 & dj2;\n\n    ft[9]  += __popc(t00);\n    ft[10] += __popc(t01);\n    ft[11] += __popc(t02);\n    ft[12] += __popc(t10);\n    ft[13] += __popc(t11);\n    ft[14] += __popc(t12);\n    ft[15] += __popc(t20);\n    ft[16] += __popc(t21);\n    ft[17] += __popc(t22);\n\n    // compute score\n    float score = 0.0f;\n\n    #pragma unroll\n    for(k = 0; k < 9; k++)\n      score += gammafunction(ft[k] + ft[9 + k] + 1) -\n               gammafunction(ft[k]) - gammafunction(ft[9 + k]);\n    score = fabsf(score);\n    if(score == 0.0f)\n      score = FLT_MAX;\n    dev_scores[tid] = score;\n  }\n}"
        ]
    },
    "tpacf-cuda": {
        "/Users/gbolet/hecbench-roofline/src/tpacf-cuda/histogram_kernel.cu": [
            "inline __host__ __device__ uint4 make_uint4(int4 a)\n{\n    return make_uint4(uint(a.x), uint(a.y), uint(a.z), uint(a.w));\n}\n\ninline __host__ __device__ uint4 min(uint4 a, uint4 b)\n{\n    return make_uint4(min(a.x,b.x), min(a.y,b.y), min(a.z,b.z), min(a.w,b.w));\n}\n\n__global__ void histoKernel(unsigned int*__restrict__ g_odata, unsigned int*__restrict__ g_idata, int size) {\n  // Map [31:6] bits to [31:6], [5:4] bits to [1:0], and [3:0] bits to [5:2]\n  // This ensures there are no bank conflicts when accessing s_Hist below.\n  // Basically, the location we write to is (threadPos + data*NUMTHREADS)/4.\n  // We take this mod 16 to find the bank we write to\n  // data*NUMTHREADS / 4 is congruent to 0 mod 16\n  // So we write to bank (threadPos / 4) % 16, or the [5:2] bits of threadPos.\n  const int threadPos = (threadIdx.x & (~63)) | ((threadIdx.x & 15) << 2) | ((threadIdx.x & 48) >> 4);\n\n  // Stores all per-thread sub-histograms\n  __shared__ unsigned char s_Hist[MEMPERBLOCK];\n\n  // Zero them out\n  for(int pos = threadIdx.x; pos < (MEMPERBLOCK >> 2); pos += blockDim.x)\n    ((unsigned int *)s_Hist)[pos] = 0;\n\n  __syncthreads();\n\n  // Location in g_idata in which this block starts reading\n  const int gStart = __mul24(blockIdx.x, DATAPERBLOCK);\n  // Amount of data to be processed by this block\n  const int blockData = min(size - gStart, DATAPERBLOCK);\n\n  unsigned int dataTemp;\n  for(int pos = threadIdx.x; pos < blockData; pos += blockDim.x){\n    // Read in integer from global memory, increment appropriate bins in shared memory.\n    dataTemp = g_idata[gStart + pos];\n    s_Hist[threadPos + __mul24( (dataTemp >>  2) & 63, NUMTHREADS)]++;\n    s_Hist[threadPos + __mul24( (dataTemp >> 10) & 63, NUMTHREADS)]++;\n    s_Hist[threadPos + __mul24( (dataTemp >> 18) & 63, NUMTHREADS)]++;\n    s_Hist[threadPos + __mul24( (dataTemp >> 26) & 63, NUMTHREADS)]++;\n  }\n\n  __syncthreads();\n\n  // Use NUMBINS threads to create a per-block sub-histogram from the data in shared memory\n  if(threadIdx.x < NUMBINS){\n    unsigned int sum = 0;\n    // Each thread calculates the total number of elements in bin tid.\n    const int tid = threadIdx.x;\n    // Starting point in the histogram\n    const int hStart = __mul24(tid, NUMTHREADS);\n    // Another trick to ensure no bank conflicts. See nVidia whitepaper for more details.\n    const int accumStart = (threadIdx.x & 15) * 4;\n\n    // Iterate through thread sub-histograms' tid bins to calculate the sum.\n    for(int i = 0, accum = accumStart; i < NUMTHREADS; i++){\n      sum += s_Hist[hStart + accum];\n      if(++accum == NUMTHREADS) accum = 0;\n    }\n    // Write to global memory.\n    g_odata[blockIdx.x * NUMBINS + tid] = sum;\n  }\n}",
            "__global__ void mergeKernel(unsigned int* d_iodata, int numBlocks) {\n  // Total number of histogram bins.\n  const int size = numBlocks * NUMBINS;\n  // (Starting) Position in global memory for this thread.\n  const int gPos = blockIdx.x * NUMBINS + threadIdx.x;\n  // Number of threads\n  const int numThreads = gridDim.x * blockDim.x;\n  unsigned int sum = 0;\n\n  // Compute bin counts for new sub-histograms\n  for(int pos = gPos; pos < size; pos += numThreads)\n    sum += d_iodata[pos];\n\n  // Write to memory, overwriting the (now useless) first portion of d_iodata.\n  d_iodata[gPos] = sum;\n}"
        ],
        "/Users/gbolet/hecbench-roofline/src/tpacf-cuda/ACF_kernel.cu": [
            "__device__ __constant__ double binbounds[NUMBINS-1];\n\n__global__ void ACFKernelSymm(cartesian g_idata1, unsigned int* g_odata)\n{\n  extern __shared__ double3 sdata[];\n  int tx = (blockIdx.x<<7) + threadIdx.x;\n  int by = (blockIdx.y<<7);\n  if(blockIdx.x < blockIdx.y) {    // All elements computed by block are above the main diagonal\n    by <<= (LOG2_GRID_SIZE - 2);\n    by += tx;\n#pragma unroll\n    for(int i=0; i<128; i+=4) {\n      g_odata[by+(i<<(LOG2_GRID_SIZE - 2))] = 2088533116; //  (124<<24) + (124<<16) + (124<<8) + (124);\n    }\n  }\n  else if(blockIdx.x > blockIdx.y) {  // All elements computed by block are below the main diagonal\n    double temp;\n    unsigned int temp2;\n    double3 vec1, vec2;\n\n    vec1.x = g_idata1.x[tx];\n    vec1.y = g_idata1.y[tx];\n    vec1.z = g_idata1.z[tx];\n    sdata[threadIdx.x].x = g_idata1.x[by+threadIdx.x];\n    sdata[threadIdx.x].y = g_idata1.y[by+threadIdx.x];\n    sdata[threadIdx.x].z = g_idata1.z[by+threadIdx.x];\n\n    __syncthreads();\n\n    by <<= (LOG2_GRID_SIZE - 2);\n    by += tx;\n\n#pragma unroll\n    for(int i=0; i<128; i+=4) {\n      temp2 = 0;\n#pragma unroll\n      for(int j=0; j<4; j++) {\n        vec2 = sdata[i+j];\n        temp = vec1.x*vec2.x + vec1.y*vec2.y + vec1.z*vec2.z;\n        if(temp < binbounds[30]) temp2 += (124<<(j<<3));\n        else if(temp < binbounds[29]) temp2 += (120<<(j<<3));\n        else if(temp < binbounds[28]) temp2 += (116<<(j<<3));\n        else if(temp < binbounds[27]) temp2 += (112<<(j<<3));\n        else if(temp < binbounds[26]) temp2 += (108<<(j<<3));\n        else if(temp < binbounds[25]) temp2 += (104<<(j<<3));\n        else if(temp < binbounds[24]) temp2 += (100<<(j<<3));\n        else if(temp < binbounds[23]) temp2 += (96<<(j<<3));\n        else if(temp < binbounds[22]) temp2 += (92<<(j<<3));\n        else if(temp < binbounds[21]) temp2 += (88<<(j<<3));\n        else if(temp < binbounds[20]) temp2 += (84<<(j<<3));\n        else if(temp < binbounds[19]) temp2 += (80<<(j<<3));\n        else if(temp < binbounds[18]) temp2 += (76<<(j<<3));\n        else if(temp < binbounds[17]) temp2 += (72<<(j<<3));\n        else if(temp < binbounds[16]) temp2 += (68<<(j<<3));\n        else if(temp < binbounds[15]) temp2 += (64<<(j<<3));\n        else if(temp < binbounds[14]) temp2 += (60<<(j<<3));\n        else if(temp < binbounds[13]) temp2 += (56<<(j<<3));\n        else if(temp < binbounds[12]) temp2 += (52<<(j<<3));\n        else if(temp < binbounds[11]) temp2 += (48<<(j<<3));\n        else if(temp < binbounds[10]) temp2 += (44<<(j<<3));\n        else if(temp < binbounds[9]) temp2 += (40<<(j<<3));\n        else if(temp < binbounds[8]) temp2 += (36<<(j<<3));\n        else if(temp < binbounds[7]) temp2 += (32<<(j<<3));\n        else if(temp < binbounds[6]) temp2 += (28<<(j<<3));\n        else if(temp < binbounds[5]) temp2 += (24<<(j<<3));\n        else if(temp < binbounds[4]) temp2 += (20<<(j<<3));\n        else if(temp < binbounds[3]) temp2 += (16<<(j<<3));\n        else if(temp < binbounds[2]) temp2 += (12<<(j<<3));\n        else if(temp < binbounds[1]) temp2 += (8<<(j<<3));\n        else if(temp < binbounds[0]) temp2 += (4<<(j<<3));\n        else temp2 += (0<<(j<<3));\n      }\n      g_odata[by+(i<<(LOG2_GRID_SIZE - 2))] = temp2;\n    }\n  }\n  else {  // blockIdx.x = blockIdx.y, so half the block will be ignorable..\n    double temp;\n    unsigned int temp2;\n    double3 vec1, vec2;\n\n    vec1.x = g_idata1.x[tx];\n    vec1.y = g_idata1.y[tx];\n    vec1.z = g_idata1.z[tx];\n    sdata[threadIdx.x].x = g_idata1.x[by+threadIdx.x];\n    sdata[threadIdx.x].y = g_idata1.y[by+threadIdx.x];\n    sdata[threadIdx.x].z = g_idata1.z[by+threadIdx.x];\n\n    __syncthreads();\n\n    by <<= (LOG2_GRID_SIZE - 2);\n    by += tx;\n\n#pragma unroll\n    for(int i=0; i<128; i+=4) {\n      temp2 = 0;\n#pragma unroll\n      for(int j=0; j<4; j++) {\n        if(threadIdx.x <= i+j) temp2 += (124<<(j<<3));\n        else { \n          vec2 = sdata[i+j];\n          temp = vec1.x*vec2.x + vec1.y*vec2.y + vec1.z*vec2.z;\n          if(temp < binbounds[30]) temp2 += (124<<(j<<3));\n          else if(temp < binbounds[29]) temp2 += (120<<(j<<3));\n          else if(temp < binbounds[28]) temp2 += (116<<(j<<3));\n          else if(temp < binbounds[27]) temp2 += (112<<(j<<3));\n          else if(temp < binbounds[26]) temp2 += (108<<(j<<3));\n          else if(temp < binbounds[25]) temp2 += (104<<(j<<3));\n          else if(temp < binbounds[24]) temp2 += (100<<(j<<3));\n          else if(temp < binbounds[23]) temp2 += (96<<(j<<3));\n          else if(temp < binbounds[22]) temp2 += (92<<(j<<3));\n          else if(temp < binbounds[21]) temp2 += (88<<(j<<3));\n          else if(temp < binbounds[20]) temp2 += (84<<(j<<3));\n          else if(temp < binbounds[19]) temp2 += (80<<(j<<3));\n          else if(temp < binbounds[18]) temp2 += (76<<(j<<3));\n          else if(temp < binbounds[17]) temp2 += (72<<(j<<3));\n          else if(temp < binbounds[16]) temp2 += (68<<(j<<3));\n          else if(temp < binbounds[15]) temp2 += (64<<(j<<3));\n          else if(temp < binbounds[14]) temp2 += (60<<(j<<3));\n          else if(temp < binbounds[13]) temp2 += (56<<(j<<3));\n          else if(temp < binbounds[12]) temp2 += (52<<(j<<3));\n          else if(temp < binbounds[11]) temp2 += (48<<(j<<3));\n          else if(temp < binbounds[10]) temp2 += (44<<(j<<3));\n          else if(temp < binbounds[9]) temp2 += (40<<(j<<3));\n          else if(temp < binbounds[8]) temp2 += (36<<(j<<3));\n          else if(temp < binbounds[7]) temp2 += (32<<(j<<3));\n          else if(temp < binbounds[6]) temp2 += (28<<(j<<3));\n          else if(temp < binbounds[5]) temp2 += (24<<(j<<3));\n          else if(temp < binbounds[4]) temp2 += (20<<(j<<3));\n          else if(temp < binbounds[3]) temp2 += (16<<(j<<3));\n          else if(temp < binbounds[2]) temp2 += (12<<(j<<3));\n          else if(temp < binbounds[1]) temp2 += (8<<(j<<3));\n          else if(temp < binbounds[0]) temp2 += (4<<(j<<3));\n          else temp2 += (0<<(j<<3));\n        }\n      }\n      g_odata[by+(i<<(LOG2_GRID_SIZE - 2))] = temp2;\n    }\n  }\n}",
            "__device__ __constant__ double binbounds[NUMBINS-1];\n\n__global__ void ACFKernel(cartesian g_idata1, cartesian g_idata2, unsigned int* g_odata) \n{\n  // Shared memory used to store vectors from g_idata2\n  extern __shared__ double3 sdata[];\n  double temp;\n  unsigned int temp2;\n  double3 vec1, vec2;\n  // tx is the \"x position\" in the grid\n  int tx = (blockIdx.x<<7) + threadIdx.x;\n  // \"y position\" depends on i (see below), this is just y block\n  int by = (blockIdx.y<<7);\n\n  // Is coalesced, as cartesians are aligned properly and there are no conflicts.\n  vec1.x = g_idata2.x[tx];\n  vec1.y = g_idata2.y[tx];\n  vec1.z = g_idata2.z[tx];\n  // Then reads one unique vector from global to shared per thread, the \"shared vectors\".\n  // Is coalesced for the same reason.\n  sdata[threadIdx.x].x = g_idata1.x[by+threadIdx.x];\n  sdata[threadIdx.x].y = g_idata1.y[by+threadIdx.x];\n  sdata[threadIdx.x].z = g_idata1.z[by+threadIdx.x];\n  // Each thread will compute the dot product of its assigned vector with every shared vector.\n\n  // Ensure all reads are finished before using them for any calculations\n  __syncthreads();\n\n  // Simplify some notation later on.\n  by <<= (LOG2_GRID_SIZE - 2);\n  by += tx;\n\n  // Unrolling offers significant speed-up\n#pragma unroll\n  for(int i=0; i<128; i+=4) {   // Iterate through 128 vectors in sdata\n    temp2 = 0;\n#pragma unroll\n    for(int j=0; j<4; j++) {    // 4 vectors per 1 int output\n      // sdata broadcasts sdata[i+j] to all threads in a block; so unnecessary bank conflicts are avoided.\n      vec2 = sdata[i+j];\n      temp = vec1.x*vec2.x + vec1.y*vec2.y + vec1.z*vec2.z;\n      // This follows the form (binNum << (elementNum << 3)).\n      // binNum is the bin we are assigning, elementNum is j, and by summing we pack four bin assignments to one int.\n      if(temp < binbounds[30]) temp2 += (124<<(j<<3));\n      else if(temp < binbounds[29]) temp2 += (120<<(j<<3));\n      else if(temp < binbounds[28]) temp2 += (116<<(j<<3));\n      else if(temp < binbounds[27]) temp2 += (112<<(j<<3));\n      else if(temp < binbounds[26]) temp2 += (108<<(j<<3));\n      else if(temp < binbounds[25]) temp2 += (104<<(j<<3));\n      else if(temp < binbounds[24]) temp2 += (100<<(j<<3));\n      else if(temp < binbounds[23]) temp2 += (96<<(j<<3));\n      else if(temp < binbounds[22]) temp2 += (92<<(j<<3));\n      else if(temp < binbounds[21]) temp2 += (88<<(j<<3));\n      else if(temp < binbounds[20]) temp2 += (84<<(j<<3));\n      else if(temp < binbounds[19]) temp2 += (80<<(j<<3));\n      else if(temp < binbounds[18]) temp2 += (76<<(j<<3));\n      else if(temp < binbounds[17]) temp2 += (72<<(j<<3));\n      else if(temp < binbounds[16]) temp2 += (68<<(j<<3));\n      else if(temp < binbounds[15]) temp2 += (64<<(j<<3));\n      else if(temp < binbounds[14]) temp2 += (60<<(j<<3));\n      else if(temp < binbounds[13]) temp2 += (56<<(j<<3));\n      else if(temp < binbounds[12]) temp2 += (52<<(j<<3));\n      else if(temp < binbounds[11]) temp2 += (48<<(j<<3));\n      else if(temp < binbounds[10]) temp2 += (44<<(j<<3));\n      else if(temp < binbounds[9]) temp2 += (40<<(j<<3));\n      else if(temp < binbounds[8]) temp2 += (36<<(j<<3));\n      else if(temp < binbounds[7]) temp2 += (32<<(j<<3));\n      else if(temp < binbounds[6]) temp2 += (28<<(j<<3));\n      else if(temp < binbounds[5]) temp2 += (24<<(j<<3));\n      else if(temp < binbounds[4]) temp2 += (20<<(j<<3));\n      else if(temp < binbounds[3]) temp2 += (16<<(j<<3));\n      else if(temp < binbounds[2]) temp2 += (12<<(j<<3));\n      else if(temp < binbounds[1]) temp2 += (8<<(j<<3));\n      else if(temp < binbounds[0]) temp2 += (4<<(j<<3));\n      else temp2 += (0<<(j<<3));\n    }\n    g_odata[by+(i<<(LOG2_GRID_SIZE - 2))] = temp2;\n  }\n}"
        ]
    },
    "wsm5-cuda": {
        "/Users/gbolet/hecbench-roofline/src/wsm5-cuda/kernel.h": [
            "#define rv      r_v\n\n\ninline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\ninline __host__ __device__ float4 fmaxf(float4 a, float4 b)\n{\n    return make_float4(fmaxf(a.x,b.x), fmaxf(a.y,b.y), fmaxf(a.z,b.z), fmaxf(a.w,b.w));\n}\n\ninline  __host__ __device__ float4 fminf(float4 a, float4 b)\n{\n    return make_float4(fminf(a.x,b.x), fminf(a.y,b.y), fminf(a.z,b.z), fminf(a.w,b.w));\n}\n\ninline __host__ __device__ int4 make_int4(float4 a)\n{\n    return make_int4(int(a.x), int(a.y), int(a.z), int(a.w));\n}\n\ninline __host__ __device__ int4 abs(int4 v)\n{\n    return make_int4(abs(v.x), abs(v.y), abs(v.z), abs(v.w));\n}\n\ninline __host__ __device__ float4 expf(float4 v)\n{\n    return make_float4(expf(v.x), expf(v.y), expf(v.z), expf(v.w));\n}\n\ninline __host__ __device__ float4 sqrtf(float4 v)\n{\n    return make_float4(sqrtf(v.x), sqrtf(v.y), sqrtf(v.z), sqrtf(v.w));\n}\n\n__device__ __forceinline__ float MAX<float>(float in, float in2)\n{\n    return fmaxf(in, in2);\n}\n\n__device__ __forceinline__ float MIN<float>(float in, float in2)\n{\n    return fminf(in, in2);\n}\n\n__global__\nvoid wsm ( \n        float *__restrict__ th,\n  const float *__restrict__ pii,\n        float *__restrict__ q,\n        float *__restrict__ qc,\n        float *__restrict__ qi,\n        float *__restrict__ qr,\n        float *__restrict__ qs,\n  const float *__restrict__ den,\n  const float *__restrict__ p,\n  const float *__restrict__ delz,\n        float *__restrict__ rain,\n        float *__restrict__ rainncv,\n        float *__restrict__ sr,\n        float *__restrict__ snow,\n        float *__restrict__ snowncv,\n  const float delt,\n  const int ids, const int ide,\n  const int jds, const int jde,\n  const int kds, const int kde,\n  const int ims, const int ime,\n  const int jms, const int jme,\n  const int kms, const int kme,\n  const int ips, const int ipe,\n  const int jps, const int jpe,\n  const int kps, const int kpe )\n{\n  float xlf, xmi, acrfac, vt2i, vt2s, supice, diameter ;\n  float roqi0, xni0, qimax, value, source, factor, xlwork2 ;\n  float t_k, q_k, qr_k, qc_k, qs_k, qi_k, qs1_k, qs2_k, cpm_k, xl_k, w1_k, w2_k, w3_k  ;\n\n#define hsub   xls\n#define hvap   xlv0\n#define cvap   cpv\n  float ttp ;\n  float dldt ;\n  float xa ;\n  float xb ;\n  float dldti ;\n  float xai ;\n  float xbi ;\n\n  float qs1[MKX] ; \n  float qs2[MKX] ; \n  float rh1[MKX] ; \n  float rh2[MKX] ; \n\n  if ( ig < ide-ids+1 && jg < jde-jds+1 ) {\n\n    int k ;\n\n#include \"constants.h\"\n\n    float t[MKX] ; \n    float cpm[MKX] ; \n    float xl[MKX] ; \n\n    for ( k = kps-1 ; k <= kpe-1 ; k++ ) {\n      t[k] = th[P3(ti,k,tj)] * pii[P3(ti,k,tj)] ;\n    }\n\n    for( k=kps-1 ;k<=kpe-1;k++) { \n      if ( qc[P3(ti,k,tj)] < 0.f ) { qc[P3(ti,k,tj)] = 0.f ; } \n      if ( qi[P3(ti,k,tj)] < 0.f ) { qi[P3(ti,k,tj)] = 0.f ; } \n      if ( qr[P3(ti,k,tj)] < 0.f ) { qr[P3(ti,k,tj)] = 0.f ; } \n      if ( qs[P3(ti,k,tj)] < 0.f ) { qs[P3(ti,k,tj)] = 0.f ; } \n    }\n\n    //      latent heat for phase changes and heat capacity. neglect the\n    //      changes during microphysical process calculation\n    //      emanuel(1994)\n\n#define CPMCAL(x) (cpd*(1.f-MAX(x,qmin))+MAX(x,qmin)*cpv)\n#define XLCAL(x)  (xlv0-xlv1*((x)-t0c))\n\n    for ( k = kps-1 ; k <= kpe-1 ; k++ ) {\n      cpm[k] = CPMCAL(q[P3(ti,k,tj)]) ;\n      xl[k] = XLCAL(t[k]) ;\n    }\n\n    //      compute the minor time steps.\n\n    float dtcldcr = 120.f ;\n    int loops = delt/dtcldcr+.5f ;\n\n    loops = MAX(loops,1) ;\n    float dtcld = delt/loops ;\n    if ( delt <= dtcldcr) dtcld = delt ;\n\n    int loop ;\n\n    for ( loop = 1 ; loop <= loops ; loop++ ) {\n      //      initialize the large scale variables\n      int mstep = 1 ;\n\n      ttp=t0c+0.01f ;\n      dldt=cvap-cliq ;\n      xa=-dldt/rv ;\n      xb=xa+hvap/(rv*ttp) ;\n      dldti=cvap-cice ;\n      xai=-dldti/rv ;\n      xbi=xai+hsub/(rv*ttp) ;\n\n      float tr, ltr, tt, pp, qq ;\n\n      for ( k = kps-1 ; k <= kpe-1 ; k++ ) {\n\n        pp = p[P3(ti,k,tj)] ;\n        tt = t[k] ;\n        tr = ttp/tt ;\n        ltr = logf(tr) ;\n\n        qq=psat*expf(ltr*(xa)+xb*(1.f-tr)) ;\n        qq=ep2*qq/(pp-qq) ;\n        qs1[k] = MAX(qq,qmin) ;\n        rh1[k] = MAX( q[P3(ti,k,tj)]/qs1[k],qmin) ;\n\n        if( tt < ttp ) {\n          qq=psat*expf(ltr*(xai)+xbi*(1.f-tr)) ;\n        } else {\n          qq=psat*expf(ltr*(xa)+xb*(1.f-tr)) ;\n        }\n        qq = ep2 * qq / (pp - qq) ;\n        qs2[k] = MAX(qq,qmin) ;\n        rh2[k] = MAX(q[P3(ti,k,tj)]/qs2[k],qmin) ;\n\n      }\n\n      float prevp_reg ;\n      float psdep_reg ;\n      float praut_reg ;\n      float psaut_reg ;\n      float pracw_reg ;\n      float psaci_reg ;\n      float psacw_reg ;\n      float pigen_reg ;\n      float pidep_reg ;\n      float pcond_reg ;\n      float psmlt_reg ;\n      float psevp_reg ;\n      float xni[MKX] ; \n\n      for ( k = kps-1 ; k <= kpe-1 ; k++ ) {\n        xni[k] = 1.e3f ;\n      }\n\n#define DIFFUS(x,y) (8.794e-5f * expf(logf(x)*(1.81f)) / (y))\n#define VISCOS(x,y) (1.496e-6f * ((x)*sqrtf(x)) /((x)+120.f)/(y))\n#define XKA(x,y) (1.414e3f*VISCOS((x),(y))*(y))\n#define DIFFAC(a,b,c,d,e) ((d)*(a)*(a)/(XKA((c),(d))*rv*(c)*(c))+1.f/((e)*DIFFUS((c),(b))))\n#define VENFAC(a,b,c) (expf(logf((VISCOS((b),(c))/DIFFUS((b),(a))))*((.3333333f)))*rsqrtf(VISCOS((b),(c)))*sqrtf(sqrtf(den0/(c))))\n\n#define LAMDAR(x,y) sqrtf(sqrtf(pidn0r/((x)*(y))))\n#define LAMDAS(x,y,z) sqrtf(sqrtf(pidn0s*(z)/((x)*(y))))\n\n      // calculate mstep for this column\n\n      float rsloper[MKX] ; \n      float rslopebr[MKX] ; \n      float rslope2r[MKX] ; \n      float rslope3r[MKX] ; \n      float rslopes[MKX] ; \n      float rslopebs[MKX] ; \n      float rslope2s[MKX] ; \n      float rslope3s[MKX] ; \n      float denfac[MKX] ; \n      float n0sfac[MKX] ; \n\n      float w1[MKX] ; \n      float w2[MKX] ; \n      float w3[MKX] ; \n\n      float w ; \n      float rmstep ;\n      int numdt ;\n      for ( k = kps-1 ; k <= kpe-1 ; k++ ) {\n        float supcol = t0c - t[k] ;\n        n0sfac[k] = MAX(MIN(expf(alpha*supcol),n0smax/n0s),1.f) ;\n        if ( qr[P3(ti,k,tj)] <= qcrmin ) {\n          rsloper[k]  = rslopermax ;\n          rslopebr[k] = rsloperbmax ;\n          rslope2r[k] = rsloper2max ;\n          rslope3r[k] = rsloper3max ;\n        } else {\n          rsloper[k]  = 1.f/LAMDAR(qr[P3(ti,k,tj)],den[P3(ti,k,tj)]) ;\n          rslopebr[k] = expf(logf(rsloper[k])*bvtr) ;\n          rslope2r[k] = rsloper[k] * rsloper[k] ; \n          rslope3r[k] = rslope2r[k] * rsloper[k] ; \n        }\n        if ( qs[P3(ti,k,tj)] <= qcrmin ) {\n          rslopes[k]  = rslopesmax ;\n          rslopebs[k] = rslopesbmax ;\n          rslope2s[k] = rslopes2max ;\n          rslope3s[k] = rslopes3max ;\n        } else {\n          rslopes[k] = 1.f/LAMDAS(qs[P3(ti,k,tj)],den[P3(ti,k,tj)],n0sfac[k]) ;\n          rslopebs[k] = expf(logf(rslopes[k])*bvts) ;\n          rslope2s[k] = rslopes[k] * rslopes[k] ; \n          rslope3s[k] = rslope2s[k] * rslopes[k] ; \n        }\n        denfac[k] = sqrtf(den0/den[P3(ti,k,tj)]) ;\n        w1[k] = pvtr*rslopebr[k]*denfac[k]/delz[P3(ti,k,tj)] ;\n        w2[k] = pvts*rslopebs[k]*denfac[k]/delz[P3(ti,k,tj)] ;\n\n        w = MAX(w1[k],w2[k]) ;\n        numdt = MAX((int)trunc(w*dtcld+.5f+.5f),1) ;\n        if ( numdt >= mstep ) mstep = numdt ;\n        //-------------------------------------------------------------\n        // Ni: ice crystal number concentration   [HDC 5c]\n        //-------------------------------------------------------------\n        float temp = (den[P3(ti,k,tj)]*MAX(qi[P3(ti,k,tj)],qmin)) ;\n        temp = sqrtf(sqrtf(temp*temp*temp)) ;\n        xni[k] = MIN(MAX(5.38e7f*temp,1.e3f),1.e6f) ;\n      }\n      rmstep = 1.f/mstep ;\n\n      int n ;\n      float dtcldden, coeres, rdelz ;\n\n\n      float den_k, falk1_k, falk1_kp1, fall1_k, delz_k, delz_kp1 ;\n      float        falk2_k, falk2_kp1, fall2_k;\n\n      for ( n = 1 ; n <= mstep ; n++ ) {\n        k = kpe - 1 ;\n        den_k = den[P3(ti,k,tj)] ;\n        falk1_kp1 = den_k*qr[P3(ti,k,tj)]*w1[k]*rmstep ;\n        falk2_kp1 = den_k*qs[P3(ti,k,tj)]*w2[k]*rmstep ;\n        dtcldden = dtcld/den_k ;\n        qr[P3(ti,k,tj)] = MAX(qr[P3(ti,k,tj)]-falk1_kp1*dtcldden,0.f) ;\n        qs[P3(ti,k,tj)] = MAX(qs[P3(ti,k,tj)]-falk2_kp1*dtcldden,0.f) ;\n        delz_kp1 = delz[P3(ti,k,tj)] ;\n        for ( k = kpe-2 ; k >= kps-1 ; k-- ) {\n          den_k = den[P3(ti,k,tj)] ;\n          falk1_k = den_k*qr[P3(ti,k,tj)]*w1[k]*rmstep ;\n          fall1_k = falk1_k ;\n          falk2_k = den_k*qs[P3(ti,k,tj)]*w2[k]*rmstep ;\n          fall2_k = falk2_k ;\n          dtcldden = dtcld/den_k ;\n          delz_k = delz[P3(ti,k,tj)] ;\n          rdelz = 1.f/delz_k ;\n          qr[P3(ti,k,tj)] = MAX(qr[P3(ti,k,tj)]- (falk1_k-falk1_kp1*delz_kp1*rdelz)* dtcldden,0.f) ;\n          qs[P3(ti,k,tj)] = MAX(qs[P3(ti,k,tj)]- (falk2_k-falk2_kp1*delz_kp1*rdelz)* dtcldden,0.f) ;\n          delz_kp1 = delz_k ;\n          falk1_kp1 = falk1_k ;\n          falk2_kp1 = falk2_k ;\n        }\n\n        for ( k = kpe-1 ; k >= kps-1 ; k-- ) {\n          if ( t[k] > t0c && qs[P3(ti,k,tj)] > 0.f) {\n            xlf = xlf0 ;\n            w3[k] = VENFAC(p[P3(ti,k,tj)],t[k],den[P3(ti,k,tj)]) ;\n            coeres = rslope2s[k]*sqrtf(rslopes[k]*rslopebs[2]) ;\n            psmlt_reg = XKA(t[k],den[P3(ti,k,tj)])/xlf*(t0c-t[k])*pi/2.f\n              *n0sfac[k]*(precs1*rslope2s[k]+precs2\n                  *w3[k]*coeres) ;\n            psmlt_reg = MIN(MAX(psmlt_reg*dtcld*rmstep,-qs[P3(ti,k,tj)]*rmstep),0.f) ;\n            qs[P3(ti,k,tj)] += psmlt_reg ;\n            qr[P3(ti,k,tj)] -= psmlt_reg ;\n            t[k] += xlf/CPMCAL(q[P3(ti,k,tj)])*psmlt_reg ;\n          }\n        }\n      }\n\n      //---------------------------------------------------------------\n      // Vice [ms-1] : fallout of ice crystal [HDC 5a]\n      //---------------------------------------------------------------\n      mstep = 1 ;\n      numdt = 1 ;\n      for ( k = kpe-1 ; k >= kps-1 ; k-- ) {\n        if (qi[P3(ti,k,tj)] <= 0.f) {\n          w2[k] = 0.f ;\n        } else {\n          xmi = den[P3(ti,k,tj)]*qi[P3(ti,k,tj)]/xni[k] ;\n          diameter  = MAX(MIN(dicon * sqrtf(xmi),dimax), 1.e-25f) ;\n          w1[k] = 1.49e4f*expf(logf(diameter)*(1.31f)) ;\n          w2[k] = w1[k]/delz[P3(ti,k,tj)] ;\n        }\n        numdt = MAX( (int) trunc(w2[k]*dtcld+.5f+.5f),1) ;\n        if(numdt > mstep) mstep = numdt ;\n      }\n      rmstep = 1.f/mstep ;\n\n      float falkc_k, falkc_kp1, fallc_k, fallc_kp1 ;\n      for ( n = 1 ; n <= mstep ; n++ ) {\n        k = kpe - 1 ;\n        den_k = den[P3(ti,k,tj)] ;\n        falkc_kp1 = den_k*qi[P3(ti,k,tj)]*w2[k]*rmstep ;\n        fallc_kp1 = fallc_kp1+falkc_kp1 ;\n        qi[P3(ti,k,tj)] = MAX(qi[P3(ti,k,tj)]-falkc_kp1*dtcld/den_k,0.f) ;\n        delz_kp1 = delz[P3(ti,k,tj)] ;\n        for ( k = kpe-2 ; k >= kps-1 ; k-- ) {\n          den_k = den[P3(ti,k,tj)] ;\n          falkc_k = den_k*qi[P3(ti,k,tj)]*w2[k]*rmstep ;\n          fallc_k = fallc_k+falkc_k ;\n          delz_k = delz[P3(ti,k,tj)] ;\n          qi[P3(ti,k,tj)] = MAX(qi[P3(ti,k,tj)]-(falkc_k-falkc_kp1\n                *delz_kp1/delz_k)*dtcld/den_k,0.f) ;\n          delz_kp1 = delz_k ;\n          falkc_kp1 = falkc_k ;\n          fallc_kp1 = fallc_k ;\n        }\n      }\n      float fallsum = fall1_k+fall2_k+fallc_k ;\n      float fallsum_qsi = fall2_k+fallc_k ;\n\n      rainncv[P2(ti,tj)] = 0.f ;\n      if(fallsum > 0.f) {\n        rainncv[P2(ti,tj)] = fallsum*delz[P3(ti,1,tj)]/denr*dtcld*1000.f ;\n        rain[P2(ti,tj)] = fallsum*delz[P3(ti,1,tj)]/denr*dtcld*1000.f + rain[P2(ti,tj)] ;\n      }\n      snowncv[P2(ti,tj)] = 0.f ;\n      if(fallsum_qsi > 0.f) {\n        snowncv[P2(ti,tj)] = fallsum_qsi*delz[P3(ti,0,tj)]/denr*dtcld*1000.f ;\n        snow[P2(ti,tj)] = fallsum_qsi*delz[P3(ti,0,tj)]/denr*dtcld*1000.f + snow[P2(ti,tj)] ;\n      }\n      sr[P2(ti,tj)] = 0.f ;\n      if ( fallsum > 0.f ) sr[P2(ti,tj)] = fallsum_qsi*delz[P3(ti,0,tj)]/denr*dtcld*1000.f/(rainncv[P2(ti,tj)]+1.e-12f) ;\n\n      //---------------------------------------------------------------\n      // pimlt: instantaneous melting of cloud ice [HL A47] [RH83 A28]\n      //       (T>T0: I->C)\n      //---------------------------------------------------------------\n\n\n      for ( k = kps-1 ; k <= kpe-1 ; k++ ) {\n\n        //  note -- many of these are turned into scalars of form name_reg by _def_ above\n        //  so that they will be stored in registers\n        prevp_reg = 0.f ;\n        psdep_reg = 0.f ;\n        praut_reg = 0.f ;\n        psaut_reg = 0.f ;\n        pracw_reg = 0.f ;\n        psaci_reg = 0.f ;\n        psacw_reg = 0.f ;\n        pigen_reg = 0.f ;\n        pidep_reg = 0.f ;\n        pcond_reg = 0.f ;\n        psevp_reg = 0.f ;\n\n        q_k =  q[P3(ti,k,tj)] ;\n        t_k = t[k] ;\n        qr_k =  qr[P3(ti,k,tj)] ;\n        qc_k = qc[P3(ti,k,tj)] ;\n        qs_k = qs[P3(ti,k,tj)] ;\n        qi_k = qi[P3(ti,k,tj)] ;\n        qs1_k = qs1[k] ;\n        qs2_k = qs2[k] ;\n        cpm_k = cpm[k] ;\n        xl_k = xl[k] ;\n\n        float supcol = t0c-t_k ;\n        xlf = xls-xl_k ;\n        if( supcol < 0.f ) xlf = xlf0 ;\n        if( supcol < 0.f && qi_k > 0.f ) {\n          qc_k = qc_k + qi_k ;\n          t_k = t_k - xlf/cpm_k*qi_k ;\n          qi_k = 0.f ;\n        }\n        //---------------------------------------------------------------\n        // pihmf: homogeneous freezing of cloud water below -40c [HL A45]\n        //        (T<-40C: C->I)\n        //---------------------------------------------------------------\n        if( supcol > 40.f && qc_k > 0.f ) {\n          qi_k = qi_k + qc_k ;\n          t_k = t_k + xlf/cpm_k*qc_k ;\n          qc_k = 0.f ;\n        }\n        //---------------------------------------------------------------\n        // pihtf: heterogeneous freezing of cloud water [HL A44]\n        //        (T0>T>-40C: C->I)\n        //---------------------------------------------------------------\n        if ( supcol > 0.f && qc_k > 0.f) {\n          float pfrzdtc = MIN(pfrz1*(expf(pfrz2*supcol)-1.f)\n              *den[P3(ti,k,tj)]/denr/xncr*qc_k*qc_k*dtcld,qc_k) ;\n          qi_k = qi_k + pfrzdtc ;\n          t_k = t_k + xlf/cpm_k*pfrzdtc ;\n          qc_k = qc_k-pfrzdtc ;\n        }\n        //---------------------------------------------------------------\n        // psfrz: freezing of rain water [HL A20] [LFO 45]\n        //        (T<T0, R->S)\n        //---------------------------------------------------------------\n        if( supcol > 0.f && qr_k > 0.f ) {\n          float temp = rsloper[k] ;\n          temp = temp*temp*temp*temp*temp*temp*temp ;\n          float pfrzdtr = MIN(20.f*(pi*pi)*pfrz1*n0r*denr/den[P3(ti,k,tj)]\n              *(expf(pfrz2*supcol)-1.f)*temp*dtcld,\n              qr_k) ;\n          qs_k = qs_k + pfrzdtr ;\n          t_k = t_k + xlf/cpm_k*pfrzdtr ;\n          qr_k = qr_k-pfrzdtr ;\n        }\n\n        //----------------------------------------------------------------\n        //     rsloper: reverse of the slope parameter of the rain(m)\n        //     xka:    thermal conductivity of air(jm-1s-1k-1)\n        //     work1:  the thermodynamic term in the denominator associated with\n        //             heat conduction and vapor diffusion\n        //             (ry88, y93, h85)\n        //     work2: parameter associated with the ventilation effects(y93)\n\n        n0sfac[k] = MAX(MIN(expf(alpha*supcol),n0smax/n0s),1.f) ;\n        if ( qr_k <= qcrmin ) {\n          rsloper[k]  = rslopermax ;\n          rslopebr[k] = rsloperbmax ;\n          rslope2r[k] = rsloper2max ;\n          rslope3r[k] = rsloper3max ;\n        } else {\n          rsloper[k] = 1.f/(sqrtf(sqrtf(pidn0r/((qr_k)*(den[P3(ti,k,tj)]))))) ;\n          rslopebr[k] = expf(logf(rsloper[k])*bvtr) ;\n          rslope2r[k] = rsloper[k] * rsloper[k] ;\n          rslope3r[k] = rslope2r[k] * rsloper[k] ;\n        }\n        if ( qs_k <= qcrmin ) {\n          rslopes[k]  = rslopesmax ;\n          rslopebs[k] = rslopesbmax ;\n          rslope2s[k] = rslopes2max ;\n          rslope3s[k] = rslopes3max ;\n        } else {\n          rslopes[k] = 1.f/(sqrtf(sqrtf(pidn0s*(n0sfac[k])/((qs_k)*(den[P3(ti,k,tj)]))))) ;\n          rslopebs[k] = expf(logf(rslopes[k])*bvts) ;\n          rslope2s[k] = rslopes[k] * rslopes[k] ;\n          rslope3s[k] = rslope2s[k] * rslopes[k] ;\n        }\n\n        w1_k = DIFFAC(xl_k,p[P3(ti,k,tj)],t_k,den[P3(ti,k,tj)],qs1_k) ;\n        w2_k = DIFFAC(xls,p[P3(ti,k,tj)],t_k,den[P3(ti,k,tj)],qs2_k) ;\n        w3_k = VENFAC(p[P3(ti,k,tj)],t_k,den[P3(ti,k,tj)]) ;\n\n        //\n        //===============================================================\n        //\n        // warm rain processes\n        //\n        // - follows the processes in RH83 and LFO except for autoconcersion\n        //\n        //===============================================================\n        //\n        float supsat = MAX(q_k,qmin)-qs1_k ;\n        float satdt = supsat/dtcld ;\n        //---------------------------------------------------------------\n        // praut: auto conversion rate from cloud to rain [HDC 16]\n        //        (C->R)\n        //---------------------------------------------------------------\n        if(qc_k > qc0) {\n          praut_reg = qck1*expf(logf(qc_k)*((7.f/3.f))) ;\n          praut_reg = MIN(praut_reg,qc_k/dtcld) ;\n        }\n        //---------------------------------------------------------------\n        // pracw: accretion of cloud water by rain [HL A40] [LFO 51]\n        //        (C->R)\n        //---------------------------------------------------------------\n        if(qr_k > qcrmin && qc_k > qmin) {\n          pracw_reg = MIN(pacrr*rslope3r[k]*rslopebr[k]\n              *qc_k*denfac[k],qc_k/dtcld) ;\n        }\n        //---------------------------------------------------------------\n        // prevp: evaporation/condensation rate of rain [HDC 14]\n        //        (V->R or R->V)\n        //---------------------------------------------------------------\n        if(qr_k > 0.f) {\n          coeres = rslope2r[k]*sqrtf(rsloper[k]*rslopebr[k]) ;\n          prevp_reg = (rh1[k]-1.f)*(precr1*rslope2r[k]\n              +precr2*w3_k*coeres)/w1_k ;\n          if(prevp_reg < 0.f) {\n            prevp_reg = MAX(prevp_reg,-qr_k/dtcld) ;\n            prevp_reg = MAX(prevp_reg,satdt/2.f) ;\n          } else {\n            prevp_reg = MIN(prevp_reg,satdt/2.f) ;\n          }\n        }\n\n        //\n        //===============================================================\n        //\n        // cold rain processes\n        //\n        // - follows the revised ice microphysics processes in HDC\n        // - the processes same as in RH83 and RH84  and LFO behave\n        //   following ice crystal hapits defined in HDC, inclduing\n        //   intercept parameter for snow (n0s), ice crystal number\n        //   concentration (ni), ice nuclei number concentration\n        //   (n0i), ice diameter (d)\n        //\n        //===============================================================\n        //\n        float rdtcld = 1.f/dtcld ;\n        supsat = MAX(q_k,qmin)-qs2_k ;\n        satdt = supsat/dtcld ;\n        int ifsat = 0 ;\n        //-------------------------------------------------------------\n        // Ni: ice crystal number concentraiton   [HDC 5c]\n        //-------------------------------------------------------------\n        float temp = (den[P3(ti,k,tj)]*MAX(qi_k,qmin)) ;\n        temp = sqrtf(sqrtf(temp*temp*temp)) ;\n        xni[k] = MIN(MAX(5.38e7f*temp,1.e3f),1.e6f) ;\n        float eacrs = expf(0.07f*(-supcol)) ;\n        //-------------------------------------------------------------\n        // psacw: Accretion of cloud water by snow  [HL A7] [LFO 24]\n        //        (T<T0: C->S, and T>=T0: C->R)\n        //-------------------------------------------------------------\n        if(qs_k > qcrmin && qc_k > qmin) {\n          psacw_reg = MIN(pacrc*n0sfac[k]*rslope3s[k] \n              *rslopebs[k]*qc_k*denfac[k]\n              ,qc_k*rdtcld) ;\n        }\n        //\n        if(supcol > 0) {\n          if(qs_k > qcrmin && qi_k > qmin) {\n            xmi = den[P3(ti,k,tj)]*qi_k/xni[k] ;\n            diameter  = MIN(dicon * sqrtf(xmi),dimax) ;\n            vt2i = 1.49e4f*powf(diameter,1.31f) ;\n            vt2s = pvts*rslopebs[k]*denfac[k] ;\n            //-------------------------------------------------------------\n            // psaci: Accretion of cloud ice by rain [HDC 10]\n            //        (T<T0: I->S)\n            //-------------------------------------------------------------\n            acrfac = 2.f*rslope3s[k]+2.f*diameter*rslope2s[k]\n              +diameter*diameter*rslopes[k] ;\n            psaci_reg = pi*qi_k*eacrs*n0s*n0sfac[k]\n              *abs(vt2s-vt2i)*acrfac*.25f ;\n          }\n          //-------------------------------------------------------------\n          // pidep: Deposition/Sublimation rate of ice [HDC 9]\n          //       (T<T0: V->I or I->V)\n          //-------------------------------------------------------------\n          if(qi_k > 0 && ifsat != 1) {\n            xmi = den[P3(ti,k,tj)]*qi_k/xni[k] ;\n            diameter = dicon * sqrtf(xmi) ;\n            pidep_reg = 4.f*diameter*xni[k]*(rh2[k]-1.f)/w2_k ;\n            supice = satdt-prevp_reg ;\n            if(pidep_reg < 0.f) {\n              pidep_reg = MAX(MAX(pidep_reg,satdt*.5f),supice) ;\n              pidep_reg = MAX(pidep_reg,-qi_k*rdtcld) ;\n            } else {\n              pidep_reg = MIN(MIN(pidep_reg,satdt*.5f),supice) ;\n            }\n            if(abs(prevp_reg+pidep_reg) >= abs(satdt)) ifsat = 1 ;\n          }\n          //-------------------------------------------------------------\n          // psdep: deposition/sublimation rate of snow [HDC 14]\n          //        (V->S or S->V)\n          //-------------------------------------------------------------\n          if( qs_k > 0.f && ifsat != 1) {\n            coeres = rslope2s[k]*sqrtf(rslopes[k]*rslopebs[k]) ;\n            psdep_reg = (rh2[k]-1.f)*n0sfac[k]\n              *(precs1*rslope2s[k]+precs2\n                  *w3_k*coeres)/w2_k ;\n            supice = satdt-prevp_reg-pidep_reg ;\n            if(psdep_reg < 0.f) {\n              psdep_reg = MAX(psdep_reg,-qs_k*rdtcld) ;\n              psdep_reg = MAX(MAX(psdep_reg,satdt*.5f),supice) ;\n            } else {\n              psdep_reg = MIN(MIN(psdep_reg,satdt*.5f),supice) ;\n            }\n            if(abs(prevp_reg+pidep_reg+psdep_reg) >= abs(satdt))\n              ifsat = 1 ;\n          }\n          //-------------------------------------------------------------\n          // pigen: generation(nucleation) of ice from vapor [HL A50] [HDC 7-8]\n          //       (T<T0: V->I)\n          //-------------------------------------------------------------\n          if(supsat > 0 && ifsat != 1) {\n            supice = satdt-prevp_reg-pidep_reg-psdep_reg ; \n            xni0 = 1.e3f*expf(0.1f*supcol) ;\n            roqi0 = 4.92e-11f*expf(logf(xni0)*(1.33f));\n            pigen_reg = MAX(0.f,(roqi0/den[P3(ti,k,tj)]-MAX(qi_k,0.f))\n                *rdtcld) ;\n            pigen_reg = MIN(MIN(pigen_reg,satdt),supice) ;\n          }\n          //\n          //-------------------------------------------------------------\n          // psaut: conversion(aggregation) of ice to snow [HDC 12]\n          //       (T<T0: I->S)\n          //-------------------------------------------------------------\n          if(qi_k > 0.f) {\n            qimax = roqimax/den[P3(ti,k,tj)] ;\n            psaut_reg = MAX(0.f,(qi_k-qimax)*rdtcld) ;\n          }\n        }\n        //-------------------------------------------------------------\n        // psevp: Evaporation of melting snow [HL A35] [RH83 A27]\n        //       (T>T0: S->V)\n        //-------------------------------------------------------------\n        if(supcol < 0.f) {\n          if(qs_k > 0.f && rh1[k] < 1.f) {\n            psevp_reg = psdep_reg*w2_k/w1_k ;\n          }  // asked Jimy about this, 11.6.07, JM\n          psevp_reg = MIN(MAX(psevp_reg,-qs_k*rdtcld),0.f) ;\n        }\n\n\n        //     check mass conservation of generation terms and feedback to the\n        //     large scale\n        if(t_k<=t0c) {\n          //\n          //     cloud water\n          //\n          value = MAX(qmin,qc_k) ;\n          source = (praut_reg+pracw_reg+psacw_reg)*dtcld ;\n          if (source > value) {\n            factor = value/source ;\n            praut_reg = praut_reg*factor ;\n            pracw_reg = pracw_reg*factor ;\n            psacw_reg = psacw_reg*factor ;\n          }\n          //\n          //     cloud ice\n          //\n          value = MAX(qmin,qi_k) ;\n          source = (psaut_reg+psaci_reg-pigen_reg-pidep_reg)*dtcld ;\n          if (source > value) {\n            factor = value/source ;\n            psaut_reg = psaut_reg*factor ;\n            psaci_reg = psaci_reg*factor ;\n            pigen_reg = pigen_reg*factor ;\n            pidep_reg = pidep_reg*factor ;\n          }\n\n          //\n          //     rain (added for WRFV3.0.1)\n          //\n          value = MAX(qmin,qr_k) ;\n          source = (-praut_reg+pracw_reg-prevp_reg)*dtcld ;\n          if (source > value) {\n            factor = value/source ;\n            praut_reg = praut_reg*factor ;\n            pracw_reg = pracw_reg*factor ;\n            prevp_reg = prevp_reg*factor ;\n          }\n          //\n          //     snow (added for WRFV3.0.1)\n          //\n          value = MAX(qmin,qs_k) ;\n          source = (-psdep_reg+psaut_reg-psaci_reg-psacw_reg)*dtcld ;\n          if (source > value) {\n            factor = value/source ;\n            psdep_reg = psdep_reg*factor ;\n            psaut_reg = psaut_reg*factor ;\n            psaci_reg = psaci_reg*factor ;\n            psacw_reg = psacw_reg*factor ;\n          }\n          //     (end added for WRFV3.0.1)\n\n          //\n          w3_k=-(prevp_reg+psdep_reg+pigen_reg+pidep_reg) ;\n          //     update\n          q_k = q_k+w3_k*dtcld ;\n          qc_k = MAX(qc_k-(praut_reg+pracw_reg+psacw_reg)*dtcld,0.f) ;\n          qr_k = MAX(qr_k+(praut_reg+pracw_reg+prevp_reg)*dtcld,0.f) ;\n          qi_k = MAX(qi_k-(psaut_reg+psaci_reg-pigen_reg-pidep_reg)*dtcld,0.f) ;\n          qs_k = MAX(qs_k+(psdep_reg+psaut_reg+psaci_reg+psacw_reg)*dtcld,0.f) ;\n          xlf = xls-xl_k ;\n          xlwork2 = -xls*(psdep_reg+pidep_reg+pigen_reg)-xl_k*prevp_reg-xlf*psacw_reg ;\n          t_k = t_k-xlwork2/cpm_k*dtcld ;\n        } else {\n          //\n          //     cloud water\n          //\n          value = MAX(qmin,qc_k) ;\n          source=(praut_reg+pracw_reg+psacw_reg)*dtcld ;\n          if (source > value) {\n            factor = value/source ;\n            praut_reg = praut_reg*factor ;\n            pracw_reg = pracw_reg*factor ;\n            psacw_reg = psacw_reg*factor ;\n          }\n          //\n          //     rain (added for WRFV3.0.1)\n          //\n          value = MAX(qmin,qr_k) ;\n          source = (-praut_reg-pracw_reg-prevp_reg-psacw_reg)*dtcld ;\n          if (source > value) {\n            factor = value/source ;\n            praut_reg = praut_reg*factor ;\n            pracw_reg = pracw_reg*factor ;\n            prevp_reg = prevp_reg*factor ;\n            psacw_reg = psacw_reg*factor ;\n          }\n          //     (end added for WRFV3.0.1)\n          //\n          //     snow\n          //\n          value = MAX(qcrmin,qs_k) ;\n          source=(-psevp_reg)*dtcld ;\n          if (source > value) {\n            factor = value/source ;\n            psevp_reg = psevp_reg*factor ;\n          }\n          w3_k=-(prevp_reg+psevp_reg) ;\n          //     update\n          q_k = q_k+w3_k*dtcld ;\n          qc_k = MAX(qc_k-(praut_reg+pracw_reg+psacw_reg)*dtcld,0.f) ;\n          qr_k = MAX(qr_k+(praut_reg+pracw_reg+prevp_reg +psacw_reg)*dtcld,0.f) ;\n          qs_k = MAX(qs_k+psevp_reg*dtcld,0.f) ;\n          xlf = xls-xl_k ;\n          xlwork2 = -xl_k*(prevp_reg+psevp_reg) ;\n          t_k = t_k-xlwork2/cpm_k*dtcld ;\n        }\n        //\n        // Inline expansion for fpvs\n        cvap = cpv ;\n        ttp=t0c+0.01f ;\n        dldt=cvap-cliq ;\n        xa=-dldt/rv ;\n        xb=xa+hvap/(rv*ttp) ;\n        dldti=cvap-cice ;\n        xai=-dldti/rv ;\n        xbi=xai+hsub/(rv*ttp) ;\n        tr=ttp/t_k ;\n        qs1_k=psat*expf(logf(tr)*(xa))*expf(xb*(1.f-tr)) ;\n        qs1_k = ep2 * qs1_k / (p[P3(ti,k,tj)] - qs1_k) ;\n        qs1_k = MAX(qs1_k,qmin) ;\n\n        //  pcond: condensational/evaporational rate of cloud water [HL A46] [RH83 A6]\n        //     if there exists additional water vapor condensated/if\n        //     evaporation of cloud water is not enough to remove subsaturation\n        \n        w1_k = ((MAX(q_k,qmin)-(qs1_k)) /\n            (1.f+(xl_k)*(xl_k)/(rv*(cpm_k))*(qs1_k)/((t_k)*(t_k)))) ;\n        // w3_k = qc_k+w1_k ;   NOT USED\n        pcond_reg = MIN(MAX(w1_k/dtcld,0.f),MAX(q_k,0.f)/dtcld) ;\n        if(qc_k > 0.f && w1_k < 0.f) {\n          pcond_reg = MAX(w1_k,-qc_k)/dtcld ;\n        }\n        q_k = q_k-pcond_reg*dtcld ;\n        qc_k = MAX(qc_k+pcond_reg*dtcld,0.f) ;\n        t_k = t_k+pcond_reg*xl_k/cpm_k*dtcld ;\n\n        //     padding for small values\n        if(qc_k <= qmin) qc_k = 0.f ;\n        if(qi_k <= qmin) qi_k = 0.f ;\n\n        q[P3(ti,k,tj)] = q_k ;\n        t[k] = t_k ;\n        qr[P3(ti,k,tj)] = qr_k ;\n        qc[P3(ti,k,tj)] = qc_k ;\n        qs[P3(ti,k,tj)] = qs_k ;\n        qi[P3(ti,k,tj)] = qi_k ;\n        qs1[k] = qs1_k ;\n\n      }\n    }\n\n    for ( k = kps-1 ; k <= kpe-1 ; k++ )\n      th[P3(ti,k,tj)] = t[k] / pii[P3(ti,k,tj)] ;\n  }\n}"
        ]
    },
    "face-cuda": {
        "/Users/gbolet/hecbench-roofline/src/face-cuda/haar.cu": [
            "__global__ \nvoid filter_kernel (const int*__restrict__ d_rectangles_array, \n                    int**__restrict__ d_scaled_rectangles_array, \n                    int*__restrict__ data, int width, int total_nodes)\n{\n  int gid = blockDim.x * blockIdx.x + threadIdx.x;\n  if (gid >= total_nodes) return;\n\n  int idx = gid * 12;\n  for (int k = 0; k < 3; k++)\n  {\n    int tr_x = d_rectangles_array[idx + k * 4];\n    int tr_y = d_rectangles_array[idx + 1 + k * 4];\n    int tr_width = d_rectangles_array[idx + 2 + k * 4];\n    int tr_height = d_rectangles_array[idx + 3 + k * 4];\n    int *p0 = data + width * (tr_y) + (tr_x);\n    int *p1 = data + width * (tr_y) + (tr_x + tr_width);\n    int *p2 = data + width * (tr_y + tr_height) + (tr_x);\n    int *p3 = data + width * (tr_y + tr_height) + (tr_x + tr_width);\n    if (k < 2)\n    {\n      d_scaled_rectangles_array[idx + k * 4]     = p0;\n      d_scaled_rectangles_array[idx + k * 4 + 1] = p1; \n      d_scaled_rectangles_array[idx + k * 4 + 2] = p2; \n      d_scaled_rectangles_array[idx + k * 4 + 3] = p3; \n    }\n    else\n    {\n      bool z = ((tr_x == 0) && (tr_y == 0) && (tr_width == 0) && (tr_height == 0));\n      d_scaled_rectangles_array[idx + k * 4]     = z ? NULL : p0;\n      d_scaled_rectangles_array[idx + k * 4 + 1] = z ? NULL : p1;\n      d_scaled_rectangles_array[idx + k * 4 + 2] = z ? NULL : p2;\n      d_scaled_rectangles_array[idx + k * 4 + 3] = z ? NULL : p3;\n    } /* end of branch if(k<2) */\n  }   /* end of k loop */\n}"
        ]
    },
    "cooling-cuda": {
        "/Users/gbolet/hecbench-roofline/src/cooling-cuda/main.cu": [
            "#define Real float\n\n\ninline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\ninline __host__ __device__ float4 fabs(float4 v)\n{\n    return make_float4(fabs(v.x), fabs(v.y), fabs(v.z), fabs(v.w));\n}\n\n__host__ __device__\nReal primordial_cool(Real n, Real T, int heat_flag)\n{\n  Real n_h, Y, y, g_ff, cool;\n  Real n_h0, n_hp, n_he0, n_hep, n_hepp, n_e, n_e_old;\n  Real alpha_hp, alpha_hep, alpha_d, alpha_hepp, gamma_eh0, gamma_ehe0, gamma_ehep;\n  Real le_h0, le_hep, li_h0, li_he0, li_hep, lr_hp, lr_hep, lr_hepp, ld_hep, l_ff;\n  Real gamma_lh0, gamma_lhe0, gamma_lhep, e_h0, e_he0, e_hep, H;\n  int n_iter;\n  Real diff, tol;\n\n  Y = 0.24; //helium abundance by mass\n  y = Y/(4 - 4*Y);\n\n  // set the hydrogen number density\n  n_h = n;\n\n  // calculate the recombination and collisional ionization rates\n  // (Table 2 from Katz 1996)\n  alpha_hp   = (8.4e-11) * (1.0/sqrt(T)) * pow((T/1e3),(-0.2)) * (1.0 / (1.0 + pow((T/1e6),(0.7))));\n  alpha_hep  = (1.5e-10) * (pow(T,(-0.6353)));\n  alpha_d    = (1.9e-3)  * (pow(T,(-1.5))) * exp(-470000.0/T) * (1.0 + 0.3*exp(-94000.0/T));\n  alpha_hepp = (3.36e-10)* (1.0/sqrt(T)) * pow((T/1e3),(-0.2)) * (1.0 / (1.0 + pow((T/1e6),(0.7))));\n  gamma_eh0  = (5.85e-11)* sqrt(T) * exp(-157809.1/T) * (1.0 / (1.0 + sqrt(T/1e5)));\n  gamma_ehe0 = (2.38e-11)* sqrt(T) * exp(-285335.4/T) * (1.0 / (1.0 + sqrt(T/1e5)));\n  gamma_ehep = (5.68e-12)* sqrt(T) * exp(-631515.0/T) * (1.0 / (1.0 + sqrt(T/1e5)));\n  // externally evaluated integrals for photoionization rates\n  // assumed J(nu) = 10^-22 (nu_L/nu)\n  gamma_lh0 = 3.19851e-13;\n  gamma_lhe0 = 3.13029e-13;\n  gamma_lhep = 2.00541e-14;\n  // externally evaluated integrals for heating rates\n  e_h0 = 2.4796e-24;\n  e_he0 = 6.86167e-24;\n  e_hep = 6.21868e-25;\n\n  // assuming no photoionization, solve equations for number density of\n  // each species\n  n_e = n_h; //as a first guess, use the hydrogen number density\n  n_iter = 20;\n  diff = 1.0;\n  tol = 1.0e-6;\n  if (heat_flag) {\n    for (int i=0; i<n_iter; i++) {\n      n_e_old = n_e;\n      n_h0   = n_h*alpha_hp / (alpha_hp + gamma_eh0 + gamma_lh0/n_e);\n      n_hp   = n_h - n_h0;\n      n_hep  = y*n_h / (1.0 + (alpha_hep + alpha_d)/(gamma_ehe0 + gamma_lhe0/n_e) + (gamma_ehep + gamma_lhep/n_e)/alpha_hepp);\n      n_he0  = n_hep*(alpha_hep + alpha_d) / (gamma_ehe0 + gamma_lhe0/n_e);\n      n_hepp = n_hep*(gamma_ehep + gamma_lhep/n_e)/alpha_hepp;\n      n_e    = n_hp + n_hep + 2*n_hepp;\n      diff = fabs(n_e_old - n_e);\n      if (diff < tol) break;\n    }\n  }\n  else {\n    n_h0   = n_h*alpha_hp / (alpha_hp + gamma_eh0);\n    n_hp   = n_h - n_h0;\n    n_hep  = y*n_h / (1.0 + (alpha_hep + alpha_d)/(gamma_ehe0) + (gamma_ehep)/alpha_hepp);\n    n_he0  = n_hep*(alpha_hep + alpha_d) / (gamma_ehe0);\n    n_hepp = n_hep*(gamma_ehep)/alpha_hepp;\n    n_e    = n_hp + n_hep + 2*n_hepp;\n  }\n\n  // using number densities, calculate cooling rates for\n  // various processes (Table 1 from Katz 1996)\n  le_h0 = (7.50e-19) * exp(-118348.0/T) * (1.0 / (1.0 + sqrt(T/1e5))) * n_e * n_h0;\n  le_hep = (5.54e-17) * pow(T,(-0.397)) * exp(-473638.0/T) * (1.0 / (1.0 + sqrt(T/1e5))) * n_e * n_hep;\n  li_h0 = (1.27e-21) * sqrt(T) * exp(-157809.1/T) * (1.0 / (1.0 + sqrt(T/1e5))) * n_e * n_h0;\n  li_he0 = (9.38e-22) * sqrt(T) * exp(-285335.4/T) * (1.0 / (1.0 + sqrt(T/1e5))) * n_e * n_he0;\n  li_hep = (4.95e-22) * sqrt(T) * exp(-631515.0/T) * (1.0 / (1.0 + sqrt(T/1e5))) * n_e * n_hep;\n  lr_hp = (8.70e-27) * sqrt(T) * pow((T/1e3),(-0.2)) * (1.0 / (1.0 + pow((T/1e6),(0.7)))) * n_e * n_hp;\n  lr_hep = (1.55e-26) * pow(T,(0.3647)) * n_e * n_hep;\n  lr_hepp = (3.48e-26) * sqrt(T) * pow((T/1e3),(-0.2)) * (1.0 / (1.0 + pow((T/1e6),(0.7)))) * n_e * n_hepp;\n  ld_hep = (1.24e-13) * pow(T,(-1.5)) * exp(-470000.0/T) * (1.0 + 0.3*exp(-94000.0/T)) * n_e * n_hep;\n  g_ff = 1.1 + 0.34*exp(-(5.5-log(T))*(5.5-log(T))/3.0); // Gaunt factor\n  l_ff = (1.42e-27) * g_ff * sqrt(T) * (n_hp + n_hep + 4*n_hepp) * n_e;\n\n  // calculate total cooling rate (erg s^-1 cm^-3)\n  cool = le_h0 + le_hep + li_h0 + li_he0 + li_hep + lr_hp + lr_hep + lr_hepp + ld_hep + l_ff;\n\n  // calculate total photoionization heating rate\n  H = 0.0;\n  if (heat_flag) {\n    H = n_h0*e_h0 + n_he0*e_he0 + n_hep*e_hep;\n  }\n\n  cool -= H;\n\n  return cool;\n}\n\n__global__\nvoid cool_kernel (\n  const int  num,\n  const Real n,\n  const Real *__restrict__ T,\n        Real *__restrict__ r,\n  const int  heat_flag)\n{\n  int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < num)\n    r[i] = primordial_cool(n, T[i], heat_flag);\n}"
        ]
    },
    "boxfilter-cuda": {
        "/Users/gbolet/hecbench-roofline/src/boxfilter-cuda/main.cu": [
            "__device__\nunsigned int rgbaFloat4ToUint(const float4 rgba)\n{\n    unsigned int uiPackedRGBA = 0U;\n    uiPackedRGBA |= 0x000000FF & (unsigned int)rgba.x;\n    uiPackedRGBA |= 0x0000FF00 & (((unsigned int)rgba.y) << 8);\n    uiPackedRGBA |= 0x00FF0000 & (((unsigned int)rgba.z) << 16);\n    uiPackedRGBA |= 0xFF000000 & (((unsigned int)rgba.w) << 24);\n    return uiPackedRGBA;\n}\n\n__global__ void row_kernel (\n    const uchar4* __restrict__ ucSource, \n            uint* __restrict__ uiDest,\n    const unsigned int uiWidth,\n    const unsigned int uiHeight,\n    const int iRadius,\n    const int iRadiusAligned, \n    const float fScale, \n    const unsigned int uiNumOutputPix)\n{\n  extern __shared__ uchar4 uc4LocalData[];\n\n  int lid = threadIdx.x;\n  int gidx = blockIdx.x;\n  int gidy = blockIdx.y; \n\n  int globalPosX = gidx * uiNumOutputPix + lid - iRadiusAligned;\n  int globalPosY = gidy;\n  int iGlobalOffset = globalPosY * uiWidth + globalPosX;\n\n  // Read global data into LMEM\n  if (globalPosX >= 0 && globalPosX < uiWidth)\n  {\n    uc4LocalData[lid] = ucSource[iGlobalOffset];\n  }\n  else\n    uc4LocalData[lid] = {0, 0, 0, 0}; \n\n  __syncthreads();\n\n  if((globalPosX >= 0) && (globalPosX < uiWidth) && (lid >= iRadiusAligned) && \n      (lid < (iRadiusAligned + (int)uiNumOutputPix)))\n  {\n    // Init summation registers to zero\n    float4 f4Sum = {0.0f, 0.0f, 0.0f, 0.0f};\n\n    // Do summation, using inline function to break up uint value from LMEM into independent RGBA values\n    int iOffsetX = lid - iRadius;\n    int iLimit = iOffsetX + (2 * iRadius) + 1;\n    for(; iOffsetX < iLimit; iOffsetX++)\n    {\n      f4Sum.x += uc4LocalData[iOffsetX].x;\n      f4Sum.y += uc4LocalData[iOffsetX].y;\n      f4Sum.z += uc4LocalData[iOffsetX].z;\n      f4Sum.w += uc4LocalData[iOffsetX].w; \n    }\n\n    // Use inline function to scale and convert registers to packed RGBA values in a uchar4, \n    // and write back out to GMEM\n    uiDest[iGlobalOffset] = rgbaFloat4ToUint(f4Sum, fScale);\n  }\n}",
            "inline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\n__device__\nunsigned int rgbaFloat4ToUint(const float4 rgba)\n{\n    unsigned int uiPackedRGBA = 0U;\n    uiPackedRGBA |= 0x000000FF & (unsigned int)rgba.x;\n    uiPackedRGBA |= 0x0000FF00 & (((unsigned int)rgba.y) << 8);\n    uiPackedRGBA |= 0x00FF0000 & (((unsigned int)rgba.z) << 16);\n    uiPackedRGBA |= 0xFF000000 & (((unsigned int)rgba.w) << 24);\n    return uiPackedRGBA;\n}\n\n__device__\nfloat4 rgbaUintToFloat4(const unsigned int uiPackedRGBA)\n{\n    float4 rgba;\n    rgba.x = uiPackedRGBA & 0xff;\n    rgba.y = (uiPackedRGBA >> 8) & 0xff;\n    rgba.z = (uiPackedRGBA >> 16) & 0xff;\n    rgba.w = (uiPackedRGBA >> 24) & 0xff;\n    return rgba;\n}\n\n__global__ void col_kernel (\n    const uint* __restrict__ uiSource,\n          uint* __restrict__ uiDest,\n    const unsigned int uiWidth, \n    const unsigned int uiHeight, \n    const int iRadius, \n    const float fScale)\n{\n  size_t globalPosX = blockIdx.x * blockDim.x + threadIdx.x;\n  const uint* uiInputImage = &uiSource[globalPosX];\n  uint* uiOutputImage = &uiDest[globalPosX];\n\n  float4 f4Sum;\n\n  float4 top_color = rgbaUintToFloat4(uiInputImage[0]);\n  float4 bot_color = rgbaUintToFloat4(uiInputImage[(uiHeight - 1) * uiWidth]);\n\n  f4Sum = top_color *\n          make_float4((float)iRadius, (float)iRadius, (float)iRadius, (float)iRadius); \n  for (int y = 0; y < iRadius + 1; y++) \n  {\n    f4Sum += rgbaUintToFloat4(uiInputImage[y * uiWidth]);\n  }\n  uiOutputImage[0] = rgbaFloat4ToUint(f4Sum, fScale);\n\n  for(int y = 1; y < iRadius + 1; y++) \n  {\n    f4Sum += rgbaUintToFloat4(uiInputImage[(y + iRadius) * uiWidth]);\n    f4Sum -= top_color;\n    uiOutputImage[y * uiWidth] = rgbaFloat4ToUint(f4Sum, fScale);\n  }\n\n  for(int y = iRadius + 1; y < uiHeight - iRadius; y++) \n  {\n    f4Sum += rgbaUintToFloat4(uiInputImage[(y + iRadius) * uiWidth]);\n    f4Sum -= rgbaUintToFloat4(uiInputImage[((y - iRadius) * uiWidth) - uiWidth]);\n    uiOutputImage[y * uiWidth] = rgbaFloat4ToUint(f4Sum, fScale);\n  }\n\n  for (int y = uiHeight - iRadius; y < uiHeight; y++) \n  {\n    f4Sum += bot_color;\n    f4Sum -= rgbaUintToFloat4(uiInputImage[((y - iRadius) * uiWidth) - uiWidth]);\n    uiOutputImage[y * uiWidth] = rgbaFloat4ToUint(f4Sum, fScale);\n  }\n}"
        ]
    },
    "ssim-cuda": {
        "/Users/gbolet/hecbench-roofline/src/ssim-cuda/main.cu": [
            "__global__ void \ncompute_ssim(const uint32_t dimx, const uint32_t dimy, const uint32_t dimz,\n             float* __restrict__ _fx, float* __restrict__ _fy, vec3i gdims,              \n             float data_range, float cov_norm, float K1, float K2,\n             float* __restrict__ out)\n{\n  const int32_t x = blockIdx.x * blockDim.x + threadIdx.x; if (x >= dimx) return;\n  const int32_t y = blockIdx.y * blockDim.y + threadIdx.y; if (y >= dimy) return;\n  const int32_t z = blockIdx.z * blockDim.z + threadIdx.z; if (z >= dimz) return;\n\n  float ux = 0.f, uy = 0.f, uxx = 0.f, uyy = 0.f, uxy = 0.f;\n\n  for (int kz = 0; kz < WIN_SIZE; ++kz) {\n  for (int ky = 0; ky < WIN_SIZE; ++ky) {\n  for (int kx = 0; kx < WIN_SIZE; ++kx) {\n\n    const vec3i g = vec3i(x + kx, y + ky, z + kz);\n    const uint32_t gidx = g.x + g.y * gdims.x + g.z * gdims.x * gdims.y;\n    const float fx = _fx[gidx];\n    const float fy = _fy[gidx];\n\n    ux  += fx;\n    uy  += fy;    \n    uxx += fx * fx;\n    uyy += fy * fy;\n    uxy += fx * fy;\n\n  } } }\n    \n  const float w = 1.f / (WIN_SIZE*WIN_SIZE*WIN_SIZE); // uniform filter\n  ux  *= w;\n  uy  *= w;\n  uxx *= w;\n  uyy *= w;\n  uxy *= w;\n\n  const float vx = cov_norm * (uxx - ux * ux);\n  const float vy = cov_norm * (uyy - uy * uy);\n  const float vxy = cov_norm * (uxy - ux * uy);\n\n  const float R = data_range;\n  const float C1 = (K1 * R) * (K1 * R);\n  const float C2 = (K2 * R) * (K2 * R);\n\n  const float A1 = 2.f * ux * uy + C1;\n  const float A2 = 2.f * vxy + C2;\n  const float B1 = ux * ux + uy * uy + C1;\n  const float B2 = vx + vy + C2;\n  const float D = B1 * B2;\n  const float S = (A1 * A2) / D;\n\n  out[x + y * dimx + z * dimx * dimy] = S;\n}"
        ]
    },
    "resnet-kernels-cuda": {
        "/Users/gbolet/hecbench-roofline/src/resnet-kernels-cuda/Kernel128_winograd.cu": [
            "__global__ void kernel_128_winograd_BtdB(\n  const float *__restrict__ pInputs,\n        float *__restrict__ pOutputs)\n{\n  int Inx = blockIdx.x<<2, Iny0 = blockIdx.y<<2, Iny1 = threadIdx.y, Inz = threadIdx.x;\n  int Iny = Iny0+Iny1, stride_r = 2048, stride_c = 128; // 2048 = 16*128\n  int c_glb_start = Inx*stride_r + Iny*stride_c + Inz, c_input = Iny1*stride_c + Inz;\n\n  extern __shared__ float input[];\n\n  int tmp[6] = {0, 768, 1536, 2304, 3072, 3840}; // 768 = 6*128\n  for (int i = 0; i < 6; i++) {\n    input[c_input + tmp[i]] = pInputs[c_glb_start + i*stride_r];\n  }\n  __syncthreads();\n\n  float BTd[6];\n  switch(Iny1) {\n    case 0:\n      for (int j = 0; j < 6; j++) {\n        BTd[j] = d(input, 0, j, Inz)*4 - d(input, 2, j, Inz)*5 + d(input, 4, j, Inz);\n      }\n      break;\n    case 1:\n      for (int j = 0; j < 6; j++) {\n        BTd[j] = -d(input, 1, j, Inz)*4 - d(input, 2, j, Inz)*4 + d(input, 3, j, Inz) + d(input, 4, j, Inz);\n      }\n      break;\n    case 2:\n      for (int j = 0; j < 6; j++) {\n        BTd[j] = d(input, 1, j, Inz)*4 - d(input, 2, j, Inz)*4 - d(input, 3, j, Inz) + d(input, 4, j, Inz);\n      }\n      break;\n    case 3:\n      for (int j = 0; j < 6; j++) {\n        BTd[j] = -d(input, 1, j, Inz)*2 - d(input, 2, j, Inz) + d(input, 3, j, Inz)*2 + d(input, 4, j, Inz);\n      }\n      break;\n    case 4:\n      for (int j = 0; j < 6; j++) {\n        BTd[j] = d(input, 1, j, Inz)*2 - d(input, 2, j, Inz) - d(input, 3, j, Inz)*2 + d(input, 4, j, Inz);\n      }\n      break;\n    case 5:\n      for (int j = 0; j < 6; j++) {\n        BTd[j] = d(input, 1, j, Inz)*4 - d(input, 3, j, Inz)*5 + d(input, 5, j, Inz);\n      }\n      break;\n  }\n  __syncthreads();\n\n  int tmp_offset = Iny1*768+Inz;\n  for (int i = 0; i < 6; i++) {\n    input[tmp_offset + i*stride_c] = BTd[i];\n  }\n  __syncthreads();\n\n  float BTdB[6];\n  switch(Iny1) {\n    case 0:\n      for (int i = 0; i < 6; i++) {\n        BTdB[i] = 4*d(input, i, 0, Inz) - 5*d(input, i, 2, Inz) + d(input, i, 4, Inz);\n      }\n      break;\n    case 1:\n      for (int i = 0; i < 6; i++) {\n        BTdB[i] = -4*d(input, i, 1, Inz) - 4*d(input, i, 2, Inz) + d(input, i, 3, Inz) + d(input, i, 4, Inz);\n      }\n      break;\n    case 2:\n      for (int i = 0; i < 6; i++) {\n        BTdB[i] = 4*d(input, i, 1, Inz) - 4*d(input, i, 2, Inz) - d(input, i, 3, Inz) + d(input, i, 4, Inz);\n      }\n      break;\n    case 3:\n      for (int i = 0; i < 6; i++) {\n        BTdB[i] = -2*d(input, i, 1, Inz) - d(input, i, 2, Inz) + 2*d(input, i, 3, Inz) + d(input, i, 4, Inz);\n      }\n      break;\n    case 4:\n      for (int i = 0; i < 6; i++) {\n        BTdB[i] = 2*d(input, i, 1, Inz) - d(input, i, 2, Inz) - 2*d(input, i, 3, Inz) + d(input, i, 4, Inz);\n      }\n      break;\n    case 5:\n      for (int i = 0; i < 6; i++) {\n        BTdB[i] = 4*d(input, i, 1, Inz) - 5*d(input, i, 3, Inz) + d(input, i, 5, Inz);\n      }\n      break;\n  }\n  __syncthreads();\n\n  for (int i = 0; i < 6; i++) {\n    pOutputs[(Iny1 + i*6)*2048 + (blockIdx.x*4+blockIdx.y)*128 + Inz] = BTdB[i];\n  }\n}",
            "__global__ void kernel_128_winograd_AtIA(\n  const float *__restrict__ pInputs,\n  const float *__restrict__ pBiases,\n  const float *__restrict__ pScales,\n        float *__restrict__ pOutputs)\n{\n  int Tilex = blockIdx.x, Tiley = blockIdx.y, Iny = threadIdx.y, kz = blockIdx.z, Inx = threadIdx.x;\n  int c_input = Inx*6 + Iny;\n\n  __shared__ float bias, scale;\n  extern __shared__ float input[];\n\n  input[c_input] = pInputs[c_input*16*128 + (Tilex*4+Tiley)*128 + kz];\n  bias = pBiases[kz];\n  scale = pScales[kz];\n  __syncthreads();\n\n  float tmp = 0;\n  switch(Inx) {\n    case 0:\n      tmp = input[Iny] + input[6+Iny] + input[12+Iny] + input[18+Iny] + input[24+Iny];\n      break;\n    case 1:\n      tmp = input[6+Iny] - input[12+Iny] + 2*input[18+Iny] - 2*input[24+Iny];\n      break;\n    case 2:\n      tmp = input[6+Iny] + input[12+Iny] + 4*input[18+Iny] + 4*input[24+Iny];\n      break;\n    case 3:\n      tmp = input[6+Iny] - input[12+Iny] + 8*input[18+Iny] - 8*input[24+Iny] + input[30+Iny];\n      break;\n  }\n  __syncthreads();\n\n  input[c_input] = tmp;\n  __syncthreads();\n\n  if (Inx > 3 || (Tilex == 3 && Inx > 1)) return;\n\n  int x;\n  float o;\n  switch(Iny) {\n    case 0:\n      x = Inx*6;\n      o = scale*(input[x]+input[x+1]+input[x+2]+input[x+3]+input[x+4])+ bias;\n      pOutputs[(((Tilex<<2)+1+Inx)*16 + (Tiley<<2)+1)*128 + kz] = o > 0 ? o : 0;\n      break;\n    case 1:\n      x = Inx*6;\n      o = scale*(input[x+1] - input[x+2] + 2*input[x+3] - 2*input[x+4]) + bias;\n      pOutputs[(((Tilex<<2)+1+Inx)*16 + (Tiley<<2)+2)*128 + kz] = o > 0 ? o : 0;\n      break;\n    case 2:\n      if (Tiley == 3) break;\n      x = Inx*6;\n      o = scale*(input[x+1] + input[x+2] + 4*input[x+3] + 4*input[x+4]) + bias;\n      pOutputs[(((Tilex<<2)+1+Inx)*16 + (Tiley<<2)+3)*128 + kz] = o > 0 ? o : 0;\n      break;\n    case 3:\n      if (Tiley == 3) break;\n      x = Inx*6;\n      o = scale*(input[x+1] - input[x+2] + 8*input[x+3] - 8*input[x+4] + input[x+5]) + bias;\n      pOutputs[(((Tilex<<2)+1+Inx)*16 + (Tiley<<2)+4)*128 + kz] = o > 0 ? o : 0;\n      break;\n  }\n}",
            "__global__ void kernel_128_OuterProduct_128(\n  const float *__restrict__ A,\n  const float *__restrict__ B,\n        float *__restrict__ C)\n{\n  int Tile = blockIdx.x, Part = blockIdx.y, tX = threadIdx.x, tY = threadIdx.y;\n  int c_input = tY*128 + tX, c_kernel = c_input;\n  int T_offset = (Tile<<11) + (Part<<10) + c_input;\n  int B_offset = (Tile<<14) + c_kernel;\n\n  extern __shared__ float input[];\n  float *kernel = input + 1024, *out = kernel + 8192;\n  int B_stride[32] = {0, 128, 256, 384, 512, 640, 768, 896,\n                      1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920,\n                      2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944,\n                      3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968};\n  out[c_input] = 0.0f;\n  \n  input[c_input] = A[T_offset];\n  \n  for (int k = 0; k < 4; k++) {\n    int B_start = B_offset + (k<<12); // 32*64\n    kernel[c_kernel] = B[B_start], kernel[c_kernel+1024] = B[B_start+1024];\n    kernel[c_kernel+2048] = B[B_start+2048], kernel[c_kernel+3072] = B[B_start+3072];\n    __syncthreads();\n  \n    float sum = 0;\n    int y_tmp = (tY<<7)+(k<<5);\n    for (int j = 0; j < 32; j++) {\n      sum += input[y_tmp + j] * kernel[tX + B_stride[j]];\n    }\n    out[tY*128 + tX] += sum;\n    __syncthreads();\n  }\n  \n  C[T_offset] = out[c_input];\n}"
        ],
        "/Users/gbolet/hecbench-roofline/src/resnet-kernels-cuda/Kernel256_one.cu": [
            "__global__ void kernel_1024_one_256(\n  const float *__restrict__ A,\n  const float *__restrict__ B,\n  const float *__restrict__ bnBias,\n  const float *__restrict__ bnScale,\n        float *__restrict__ C) \n{\n  int tile = blockIdx.x, in_channel = threadIdx.x, line = threadIdx.y;\n  int ind = line*256 + in_channel;\n\n  extern __shared__ float shared_[];\n  float *__restrict__ weights = shared_ + 1024*4,\n        *__restrict__ output = weights + 256*16,\n        *__restrict__ input = shared_;\n  float *__restrict__ bias = output + 4*256,\n        *__restrict__ scale = bias + 256;\n\n  for (int i = 0; i < 4; i++)\n    input[ind + i*1024] = A[tile*4096 + i*1024 + ind];\n  bias[in_channel] = bnBias[in_channel];\n  scale[in_channel] = bnScale[in_channel];\n  output[ind] = 0.0f;\n  __syncthreads();\n\n  for (int k = 0; k < 1024; k += 16) {\n    const float *B_start = B + k*256;\n    for (int i = 0; i < 4; i++)\n      weights[ind + i*1024] = B_start[i*1024 + ind];\n    __syncthreads();\n\n    const float *A_start = input + k;\n    for (int p = 0; p < 16; p++) {\n      output[ind] += A_start[line*1024 + p] * weights[in_channel + p*256];\n    }\n    __syncthreads();\n  }\n\n  float *C_start = C + tile*1024, res = scale[in_channel] * output[ind] + bias[in_channel];\n  C_start[ind] = res > 0 ? res : 0;\n}",
            "__global__ void kernel_256_one_1024(\n  const float *__restrict__ A,\n  const float *__restrict__ B,\n  const float *__restrict__ bnBias,\n  const float *__restrict__ bnScale,\n        float *__restrict__ C) \n{\n  int tile = blockIdx.x, part = blockIdx.y, in_channel = threadIdx.x, line = threadIdx.y;\n  int ind = line*256 + in_channel;\n\n  extern __shared__ float shared_[];\n  float *weights = shared_ + 256*4, *output = weights + 256*32, *input = shared_;\n  float *bias = output + 4*256, *scale = bias + 256;\n\n  input[ind] = A[tile * 1024 + ind];\n  bias[in_channel] = bnBias[part*256 + in_channel];\n  scale[in_channel] = bnScale[part*256+ in_channel];\n  output[ind] = 0.0f;\n  __syncthreads();\n\n  for (int k = 0; k < 256; k += 32) {\n    for (int i = 0; i < 8; i++)\n      weights[ind + 1024*i] = B[(k + i*4 + line)*1024 + part*256 + in_channel];\n    __syncthreads();\n\n    float *A_start = input + k;\n    for (int p = 0; p < 32; p++) {\n      output[ind] += A_start[line*256 + p] * weights[in_channel + p*256];\n    }\n    __syncthreads();\n  }\n\n  float *C_start = C + tile*4096 + part*256;\n  C_start[line * 1024 + in_channel] = scale[in_channel] * output[ind] + bias[in_channel];\n}"
        ],
        "/Users/gbolet/hecbench-roofline/src/resnet-kernels-cuda/Kernel128_one.cu": [
            "__global__ void kernel_512_one_128(\n  const float *__restrict__ A,\n  const float *__restrict__ B,\n  const float *__restrict__ bnBias,\n  const float *__restrict__ bnScale,\n        float *__restrict__ C) \n{\n  int tile = blockIdx.x, in_channel = threadIdx.x, line = threadIdx.y;\n  int ind = line*128 + in_channel;\n\n  extern __shared__ float shared_[];\n  float *__restrict__ weights = shared_ + 512*4,\n        *__restrict__ output = weights + 128*64,\n        *__restrict__ input = shared_;\n  float *__restrict__ bias = output + 4*128,\n        *__restrict__ scale = bias + 128;\n\n  for (int i = 0; i < 4; i++)\n    input[ind + i*512] = A[tile*2048 + i*512 + ind];\n  bias[in_channel] = bnBias[in_channel];\n  scale[in_channel] = bnScale[in_channel];\n  output[ind] = 0.0f;\n  __syncthreads();\n\n  for (int k = 0; k < 512; k += 64) {\n    const float *B_start = B + k*128;\n    for (int i = 0; i < 16; i++)\n      weights[ind + i*512] = B_start[i*512 + ind];\n    __syncthreads();\n\n    const float *A_start = input + k;\n    for (int p = 0; p < 64; p++) {\n      output[ind] += A_start[line*512 + p] * weights[in_channel + p*128];\n    }\n    __syncthreads();\n  }\n\n  float *C_start = C + tile*512, res = scale[in_channel] * output[ind] + bias[in_channel];\n  C_start[ind] = res > 0 ? res : 0;\n}",
            "__global__ void kernel_128_one_512(\n  const float *__restrict__ A,\n  const float *__restrict__ B,\n  const float *__restrict__ bnBias,\n  const float *__restrict__ bnScale,\n        float *__restrict__ C) \n{\n  int tile = blockIdx.x, part = blockIdx.y, in_channel = threadIdx.x, line = threadIdx.y;\n  int ind = line*128 + in_channel;\n\n  extern __shared__ float shared_[];\n  float *weights = shared_ + 128*4, *output = weights + 128*64, *input = shared_;\n  float *bias = output + 4*128, *scale = bias + 128;\n\n  input[ind] = A[tile * 512 + ind];\n  bias[in_channel] = bnBias[part*128 + in_channel];\n  scale[in_channel] = bnScale[part*128+ in_channel];\n  output[ind] = 0.0f;\n  __syncthreads();\n\n  for (int k = 0; k < 128; k += 64) {\n    for (int i = 0; i < 16; i++)\n      weights[ind + 512*i] = B[(k + i*4 + line)*512 + part*128 + in_channel];\n    __syncthreads();\n\n    float *A_start = input + k;\n    for (int p = 0; p < 64; p++) {\n      output[ind] += A_start[line*128 + p] * weights[in_channel + p*128];\n    }\n    __syncthreads();\n  }\n\n  float *C_start = C + tile*2048 + part*128;\n  float res = scale[in_channel] * output[ind] + bias[in_channel];\n  C_start[line * 512 + in_channel] = res;\n}"
        ],
        "/Users/gbolet/hecbench-roofline/src/resnet-kernels-cuda/Kernel256_winograd.cu": [
            "__global__ void kernel_256_winograd_BtdB(\n  const float *__restrict__ pInputs,\n        float *__restrict__ pOutputs)\n{\n  int Inx = blockIdx.x<<2, Iny0 = blockIdx.y<<2, Part = blockIdx.z,\n      Iny1 = threadIdx.y, Inz = threadIdx.x;\n  int Iny = Iny0+Iny1, stride_r = 4096, stride_c = 256; // 4096 = 16*256\n  int c_glb_start = Inx*stride_r + Iny*stride_c + Inz + (Part<<7), c_input = Iny1*128 + Inz;\n\n  extern __shared__ float input[];\n\n  int stride_768[6] = {0, 768, 1536, 2304, 3072, 3840}; // 768 = 6*128\n  for (int i = 0; i < 6; i++) {\n    input[c_input + stride_768[i]] = pInputs[c_glb_start + i*stride_r];\n  }\n  __syncthreads();\n\n  float BTd[6];\n  switch(Iny1) {\n    case 0:\n      for (int j = 0; j < 6; j++) {\n        BTd[j] = d(input, 0, j, Inz)*4 - d(input, 2, j, Inz)*5 + d(input, 4, j, Inz);\n      }\n      break;\n    case 1:\n      for (int j = 0; j < 6; j++) {\n        BTd[j] = -d(input, 1, j, Inz)*4 - d(input, 2, j, Inz)*4 + d(input, 3, j, Inz) + d(input, 4, j, Inz);\n      }\n      break;\n    case 2:\n      for (int j = 0; j < 6; j++) {\n        BTd[j] = d(input, 1, j, Inz)*4 - d(input, 2, j, Inz)*4 - d(input, 3, j, Inz) + d(input, 4, j, Inz);\n      }\n      break;\n    case 3:\n      for (int j = 0; j < 6; j++) {\n        BTd[j] = -d(input, 1, j, Inz)*2 - d(input, 2, j, Inz) + d(input, 3, j, Inz)*2 + d(input, 4, j, Inz);\n      }\n      break;\n    case 4:\n      for (int j = 0; j < 6; j++) {\n        BTd[j] = d(input, 1, j, Inz)*2 - d(input, 2, j, Inz) - d(input, 3, j, Inz)*2 + d(input, 4, j, Inz);\n      }\n      break;\n    case 5:\n      for (int j = 0; j < 6; j++) {\n        BTd[j] = d(input, 1, j, Inz)*4 - d(input, 3, j, Inz)*5 + d(input, 5, j, Inz);\n      }\n      break;\n  }\n  __syncthreads();\n\n  int tmp_offset = Iny1*768+Inz;\n  for (int i = 0; i < 6; i++) {\n    input[tmp_offset + i*128] = BTd[i];\n  }\n  __syncthreads();\n\n  float BTdB[6];\n  switch(Iny1) {\n    case 0:\n      for (int i = 0; i < 6; i++) {\n        BTdB[i] = 4*d(input, i, 0, Inz) - 5*d(input, i, 2, Inz) + d(input, i, 4, Inz);\n      }\n      break;\n    case 1:\n      for (int i = 0; i < 6; i++) {\n        BTdB[i] = -4*d(input, i, 1, Inz) - 4*d(input, i, 2, Inz) + d(input, i, 3, Inz) + d(input, i, 4, Inz);\n      }\n      break;\n    case 2:\n      for (int i = 0; i < 6; i++) {\n        BTdB[i] = 4*d(input, i, 1, Inz) - 4*d(input, i, 2, Inz) - d(input, i, 3, Inz) + d(input, i, 4, Inz);\n      }\n      break;\n    case 3:\n      for (int i = 0; i < 6; i++) {\n        BTdB[i] = -2*d(input, i, 1, Inz) - d(input, i, 2, Inz) + 2*d(input, i, 3, Inz) + d(input, i, 4, Inz);\n      }\n      break;\n    case 4:\n      for (int i = 0; i < 6; i++) {\n        BTdB[i] = 2*d(input, i, 1, Inz) - d(input, i, 2, Inz) - 2*d(input, i, 3, Inz) + d(input, i, 4, Inz);\n      }\n      break;\n    case 5:\n      for (int i = 0; i < 6; i++) {\n        BTdB[i] = 4*d(input, i, 1, Inz) - 5*d(input, i, 3, Inz) + d(input, i, 5, Inz);\n      }\n      break;\n  }\n  __syncthreads();\n\n  for (int i = 0; i < 6; i++) {\n    pOutputs[(Iny1 + i*6)*4096 + (blockIdx.x*4+blockIdx.y)*256 + Inz + (Part<<7)] = BTdB[i];\n  }\n}",
            "__global__ void kernel_256_winograd_AtIA(\n  const float *__restrict__ pInputs,\n  const float *__restrict__ pBiases,\n  const float *__restrict__ pScales,\n        float *__restrict__ pOutputs)\n{\n  int Tilex = blockIdx.x, Inx = threadIdx.x;\n  int Tiley = blockIdx.y, Iny = threadIdx.y;\n  int kz = blockIdx.z; \n  int c_input = Inx*6 + Iny;\n\n  __shared__ float bias, scale;\n  extern __shared__ float input[];\n\n  input[c_input] = pInputs[c_input*16*256 + (Tilex*4+Tiley)*256 + kz];\n  bias = pBiases[kz];\n  scale = pScales[kz];\n  __syncthreads();\n\n  float tmp = 0;\n  switch(Inx) {\n    case 0:\n      tmp = input[Iny] + input[6+Iny] + input[12+Iny] + input[18+Iny] + input[24+Iny];\n      break;\n    case 1:\n      tmp = input[6+Iny] - input[12+Iny] + 2*input[18+Iny] - 2*input[24+Iny];\n      break;\n    case 2:\n      tmp = input[6+Iny] + input[12+Iny] + 4*input[18+Iny] + 4*input[24+Iny];\n      break;\n    case 3:\n      tmp = input[6+Iny] - input[12+Iny] + 8*input[18+Iny] - 8*input[24+Iny] + input[30+Iny];\n      break;\n  }\n  __syncthreads();\n\n  input[c_input] = tmp;\n  __syncthreads();\n\n  if (Inx > 3 || (Tilex == 3 && Inx > 1)) return;\n\n  int x;\n  float o;\n  switch(Iny) {\n    case 0:\n      x = Inx*6;\n      o = scale*(input[x]+input[x+1]+input[x+2]+input[x+3]+input[x+4]) + bias;\n      pOutputs[(((Tilex<<2)+1+Inx)*16 + (Tiley<<2)+1)*256 + kz] = o > 0 ? o : 0;\n      break;\n    case 1:\n      x = Inx*6;\n      o = scale*(input[x+1] - input[x+2] + 2*input[x+3] - 2*input[x+4]) + bias;\n      pOutputs[(((Tilex<<2)+1+Inx)*16 + (Tiley<<2)+2)*256 + kz] = o > 0 ? o : 0;\n      break;\n    case 2:\n      if (Tiley == 3) break;\n      x = Inx*6;\n      o = scale*(input[x+1] + input[x+2] + 4*input[x+3] + 4*input[x+4]) + bias;\n      pOutputs[(((Tilex<<2)+1+Inx)*16 + (Tiley<<2)+3)*256 + kz] = o > 0 ? o : 0;\n      break;\n    case 3:\n      if (Tiley == 3) break;\n      x = Inx*6;\n      o = scale*(input[x+1] - input[x+2] + 8*input[x+3] - 8*input[x+4] + input[x+5]) + bias;\n      pOutputs[(((Tilex<<2)+1+Inx)*16 + (Tiley<<2)+4)*256 + kz] = o > 0 ? o : 0;\n      break;\n  }\n}",
            "__global__ void kernel_256_OuterProduct_256(\n  const float *__restrict__ A,\n  const float *__restrict__ B,\n        float *__restrict__ C)\n{\n  int Tile = blockIdx.x, Part = blockIdx.y, \n      tX = threadIdx.x, tY = threadIdx.y;\n  int c_input = tY*256 + tX,\n      c_kernel = c_input,\n      T_offset = (Tile<<12) + (Part<<11) + c_input, B_offset = (Tile<<16) + c_kernel;\n\n  extern __shared__ float input[];\n  float *kernel = input + 2048, *out = kernel + 8192;\n  int B_stride[32] = {0, 256, 512, 768, 1024, 1280, 1536, 1792, \n                      2048, 2304, 2560, 2816, 3072, 3328, 3584, 3840,\n                      4096, 4352, 4608, 4864, 5120, 5376, 5632, 5888,\n                      6144, 6400, 6656, 6912, 7168, 7424, 7680, 7936};\n  out[c_input] = 0.0f;\n  out[c_input+1024] = 0;\n\n  input[c_input] = A[T_offset];\n  input[c_input+1024] = A[T_offset+1024];\n\n  for (int k = 0; k < 8; k++) {\n    int B_start = B_offset + (k<<13); // 32*64\n    kernel[c_kernel] = B[B_start], kernel[c_kernel+1024] = B[B_start+1024];\n    kernel[c_kernel+2048] = B[B_start+2048], kernel[c_kernel+3072] = B[B_start+3072];\n    kernel[c_kernel+4096] = B[B_start+4096], kernel[c_kernel+5120] = B[B_start+5120];\n    kernel[c_kernel+6144] = B[B_start+6144], kernel[c_kernel+7168] = B[B_start+7168];\n\n    __syncthreads();\n\n    float sum = 0, sum1 = 0;\n    int y_tmp = (tY<<8)+(k<<5), y_tmp1 = y_tmp+1024;\n    for (int j = 0; j < 32; j++) {\n      sum += input[y_tmp + j] * kernel[tX + B_stride[j]];\n      sum1 += input[y_tmp1 + j] * kernel[tX + B_stride[j]];\n    }\n    out[c_input] += sum;\n    out[c_input+1024] += sum1;\n    __syncthreads();\n  }\n\n  C[T_offset] = out[c_input];\n  C[T_offset+1024] = out[c_input+1024];\n}"
        ]
    },
    "matrix-rotate-cuda": {
        "/Users/gbolet/hecbench-roofline/src/matrix-rotate-cuda/main.cu": [
            "__global__ void rotate_matrix_parallel (float *matrix, const int n) {\n  int layer = blockIdx.x * blockDim.x + threadIdx.x;\n  if (layer < n/2) {\n    int first = layer;\n    int last = n - 1 - layer;\n    for(int i = first; i < last; ++i) {\n      int offset = i - first;\n\n      float top = matrix[first*n+i]; // save top\n      // left -> top\n      matrix[first*n+i] = matrix[(last-offset)*n+first];\n\n      // bottom -> left\n      matrix[(last-offset)*n+first] = matrix[last*n+(last-offset)];\n\n      // right -> bottom\n      matrix[last*n+(last-offset)] = matrix[i*n+last];\n\n      // top -> right\n      matrix[i*n+last] = top; // right <- saved top\n    }\n  }\n}"
        ]
    },
    "tonemapping-cuda": {
        "/Users/gbolet/hecbench-roofline/src/tonemapping-cuda/kernels.cu": [
            "inline __device__\nfloat luminance(float r, float g, float b)\n{\n  return ( 0.2126f * r ) + ( 0.7152f * g ) + ( 0.0722f * b );\n}\n\n__global__\nvoid toneMapping(\n    const float *__restrict__ const input, \n          float *__restrict__ const output, \n    const float averageLuminance, \n    const float gamma, \n    const float c, \n    const float delta,\n    const uint width,\n    const uint numChannels,\n    const uint height)\n{\n  uint x = blockIdx.x * blockDim.x + threadIdx.x;\n  uint y = blockIdx.y * blockDim.y + threadIdx.y;\n  float r, g, b;\n  float cLPattanaik;\n  float yLPattanaik;\n\n  float r1 = input[width * numChannels * y + (x * numChannels + 0)];\n  float g1 = input[width * numChannels * y + (x * numChannels + 1)];\n  float b1 = input[width * numChannels * y + (x * numChannels + 2)];\n\n  float yLuminance = luminance(r1, g1, b1);\n  float gcPattanaik = c * averageLuminance;\n\n  if (x != 0 && y != 0 && x != width-1 && y != height-1)\n  {\n    //Calculating mean\n    float leftUp = 0.0f;\n    float up = 0.0f;\n    float rightUp = 0.0f;\n    float left = 0.0f;\n    float right = 0.0f;\n    float leftDown = 0.0f;\n    float down = 0.0f;\n    float rightDown = 0.0f;\n\n    r = input[width * numChannels * (y - 1) + ((x - 1) * numChannels) + 0];\n    g = input[width * numChannels * (y - 1) + ((x - 1) * numChannels) + 1];\n    b = input[width * numChannels * (y - 1) + ((x - 1) * numChannels) + 2];\n\n    leftUp = luminance( r, g, b );\n\n    r = input[width * numChannels * (y - 1) + ((x) * numChannels) + 0];\n    g = input[width * numChannels * (y - 1) + ((x) * numChannels) + 1];\n    b = input[width * numChannels * (y - 1) + ((x) * numChannels) + 2];\n\n    up = luminance( r, g, b );\n\n    r = input[width * numChannels * (y - 1) + ((x + 1) * numChannels) + 0];\n    g = input[width * numChannels * (y - 1) + ((x + 1) * numChannels) + 1];\n    b = input[width * numChannels * (y - 1) + ((x + 1) * numChannels) + 2];\n\n    rightUp = luminance( r, g, b );\n\n    r = input[width * numChannels * (y) + ((x - 1) * numChannels) + 0];\n    g = input[width * numChannels * (y) + ((x - 1) * numChannels) + 1];\n    b = input[width * numChannels * (y) + ((x - 1) * numChannels) + 2];\n\n    left = luminance( r, g, b );  \n\n    r = input[width * numChannels * (y) + ((x + 1) * numChannels) + 0];\n    g = input[width * numChannels * (y) + ((x + 1) * numChannels) + 1];\n    b = input[width * numChannels * (y) + ((x + 1) * numChannels) + 2];\n\n    right = luminance( r, g, b );  \n\n    r = input[width * numChannels * (y + 1) + ((x - 1) * numChannels) + 0];\n    g = input[width * numChannels * (y + 1) + ((x - 1) * numChannels) + 1];\n    b = input[width * numChannels * (y + 1) + ((x - 1) * numChannels) + 2];\n\n    leftDown = luminance( r, g, b );\n\n    r = input[width * numChannels * (y + 1) + ((x) * numChannels) + 0];\n    g = input[width * numChannels * (y + 1) + ((x) * numChannels) + 1];\n    b = input[width * numChannels * (y + 1) + ((x) * numChannels) + 2];\n\n    down = luminance( r, g, b );\n\n    r = input[width * numChannels * (y + 1) + ((x + 1) * numChannels) + 0];\n    g = input[width * numChannels * (y + 1) + ((x + 1) * numChannels) + 1];\n    b = input[width * numChannels * (y + 1) + ((x + 1) * numChannels) + 2];\n\n    rightDown = luminance( r, g, b );\n\n    //Calculate median    \n    yLPattanaik = (leftUp + up + rightUp + left + right + leftDown + down + rightDown) / 8;    \n  }\n  else\n  {\n    yLPattanaik = yLuminance;\n  }\n\n  cLPattanaik =  yLPattanaik * logf(delta + yLPattanaik / yLuminance) + gcPattanaik;\n\n\n  float yDPattanaik = yLuminance / (yLuminance + cLPattanaik);\n\n  r = powf((r1 / yLuminance), gamma) * yDPattanaik;\n  g = powf((g1 / yLuminance), gamma) * yDPattanaik;  \n  b = powf((b1 / yLuminance), gamma) * yDPattanaik;\n\n  output[width * numChannels * y + (x * numChannels + 0)] = r;\n  output[width * numChannels * y + (x * numChannels + 1)] = g;\n  output[width * numChannels * y + (x * numChannels + 2)] = b;\n  output[width * numChannels * y + (x * numChannels + 3)] = \n    input[width * numChannels * y + (x * numChannels + 3)];\n}"
        ]
    },
    "chacha20-cuda": {
        "/Users/gbolet/hecbench-roofline/src/chacha20-cuda/main.cu": [
            "__device__\nvoid hex_to_raw(const char* src, const int n /*src size*/, uint8_t* dst, const uint8_t* char_to_uint){\n  for (int i = threadIdx.x; i < n/2; i = i + blockDim.x) {\n    uint8_t hi = char_to_uint[src[i*2 + 0]];\n    uint8_t lo = char_to_uint[src[i*2 + 1]];\n    dst[i] = (hi << 4) | lo;\n  }\n}\n\n__global__\nvoid test_keystreams (\n    const char *__restrict__ text_key,\n    const char *__restrict__ text_nonce,\n    const char *__restrict__ text_keystream,\n    const uint8_t *__restrict__ char_to_uint,\n    uint8_t *__restrict__ raw_key,\n    uint8_t *__restrict__ raw_nonce,\n    uint8_t *__restrict__ raw_keystream,\n    uint8_t *__restrict__ result,\n    const int text_key_size,\n    const int text_nonce_size,\n    const int text_keystream_size)\n\n{\n  hex_to_raw(text_key, text_key_size, raw_key, char_to_uint);\n  hex_to_raw(text_nonce, text_nonce_size, raw_nonce, char_to_uint);\n  hex_to_raw(text_keystream, text_keystream_size, raw_keystream, char_to_uint);\n   \n  if (threadIdx.x == 0) {\n    Chacha20 chacha(raw_key, raw_nonce);\n    chacha.crypt(result, text_keystream_size / 2);\n  }\n}"
        ]
    },
    "streamOrderedAllocation-cuda": {
        "/Users/gbolet/hecbench-roofline/src/streamOrderedAllocation-cuda/main.cu": [
            "__global__ void vectorAddGPU(const float *a, const float *b, float *c, int N) {\n  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n\n  if (idx < N) {\n    c[idx] = a[idx] + b[idx];\n  }\n}"
        ]
    },
    "segsort-cuda": {
        "/Users/gbolet/hecbench-roofline/src/segsort-cuda/src/bb_bin.cuh": [
            "#define T ((int)32)\n\n\n__device__ __forceinline__\ndouble atomicMax(double *address, double val)\n{\n  unsigned long long ret = __double_as_longlong(*address);\n  while(val > __longlong_as_double(ret))\n  {\n    unsigned long long old = ret;\n    if((ret = atomicCAS((unsigned long long *)address, old, __double_as_longlong(val))) == old)\n      break;\n  }\n  return __longlong_as_double(ret);\n}\n\n__device__\nvoid warp_exclusive_sum(const T * in, T * out, const int n)\n{\n    const int lane = threadIdx.x & 31;\n\n    T data = (lane > 0 && lane < n) ? in[lane-1] : 0;\n\n    for(int i = 1; i < 32; i *= 2) {\n        T other = __shfl_up_sync(0xFFFFFFFF, data, i);\n        if(lane > i)\n            data += other;\n    }\n\n    if(lane < n) {\n        out[lane] = data;\n    }\n}\n\n__global__\nvoid bb_bin_histo(\n    int *d_bin_counter,\n    const Offset *d_seg_begins, const Offset *d_seg_ends, const int num_segs)\n{\n    const int tid = threadIdx.x;\n    const int gid = blockIdx.x * blockDim.x + threadIdx.x;\n\n    __shared__ int local_histo[SEGBIN_NUM + 1];\n    if (tid < SEGBIN_NUM + 1)\n        local_histo[tid] = 0;\n    __syncthreads();\n\n    if (gid < num_segs)\n    {\n        const int size = d_seg_ends[gid] - d_seg_begins[gid];\n\n        if (size <= 1)\n            atomicAdd(&local_histo[0 ], 1);\n        if (1  < size && size <= 2 )\n            atomicAdd(&local_histo[1 ], 1);\n        if (2  < size && size <= 4 )\n            atomicAdd(&local_histo[2 ], 1);\n        if (4  < size && size <= 8 )\n            atomicAdd(&local_histo[3 ], 1);\n        if (8  < size && size <= 16)\n            atomicAdd(&local_histo[4 ], 1);\n        if (16 < size && size <= 32)\n            atomicAdd(&local_histo[5 ], 1);\n        if (32 < size && size <= 64)\n            atomicAdd(&local_histo[6 ], 1);\n        if (64 < size && size <= 128)\n            atomicAdd(&local_histo[7 ], 1);\n        if (128 < size && size <= 256)\n            atomicAdd(&local_histo[8 ], 1);\n        if (256 < size && size <= 512)\n            atomicAdd(&local_histo[9 ], 1);\n        if (512 < size && size <= 1024)\n            atomicAdd(&local_histo[10], 1);\n        if (1024 < size && size <= 2048)\n            atomicAdd(&local_histo[11], 1);\n        if (2048 < size) {\n            // atomicAdd(&local_histo[12], 1);\n            atomicMax(&local_histo[13], size);\n        }\n    }\n    __syncthreads();\n\n    if(tid < 32) {\n        warp_exclusive_sum(local_histo, local_histo, SEGBIN_NUM);\n\n        if (tid < SEGBIN_NUM)\n            atomicAdd(&d_bin_counter[tid], local_histo[tid]);\n        if (tid == SEGBIN_NUM)\n            atomicMax(&d_bin_counter[tid], local_histo[tid]);\n    }\n}",
            "__global__\nvoid bb_bin_group(\n    int *d_bin_segs_id, int *d_bin_counter,\n    const Offset *d_seg_begins, const Offset *d_seg_ends, const int num_segs)\n{\n    const int gid = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (gid < num_segs)\n    {\n        const int size = d_seg_ends[gid] - d_seg_begins[gid];\n        int position;\n        if (size <= 1)\n            position = atomicAdd(&d_bin_counter[0 ], 1);\n        else if (size <= 2)\n            position = atomicAdd(&d_bin_counter[1 ], 1);\n        else if (size <= 4)\n            position = atomicAdd(&d_bin_counter[2 ], 1);\n        else if (size <= 8)\n            position = atomicAdd(&d_bin_counter[3 ], 1);\n        else if (size <= 16)\n            position = atomicAdd(&d_bin_counter[4 ], 1);\n        else if (size <= 32)\n            position = atomicAdd(&d_bin_counter[5 ], 1);\n        else if (size <= 64)\n            position = atomicAdd(&d_bin_counter[6 ], 1);\n        else if (size <= 128)\n            position = atomicAdd(&d_bin_counter[7 ], 1);\n        else if (size <= 256)\n            position = atomicAdd(&d_bin_counter[8 ], 1);\n        else if (size <= 512)\n            position = atomicAdd(&d_bin_counter[9 ], 1);\n        else if (size <= 1024)\n            position = atomicAdd(&d_bin_counter[10], 1);\n        else if (size <= 2048)\n            position = atomicAdd(&d_bin_counter[11], 1);\n        else\n            position = atomicAdd(&d_bin_counter[12], 1);\n        d_bin_segs_id[position] = gid;\n    }\n}"
        ],
        "/Users/gbolet/hecbench-roofline/src/segsort-cuda/src/bb_comput_s_keys.cuh": [
            "#define K 1.38065e-23        // J/K, Boltzmann constant\n\n\n__global__\nvoid gen_copy(\n    K *key, K *keyB,\n    const Offset *seg_begins, const Offset *seg_ends,\n    const int *bin, const int bin_size)\n{\n    const int gid = threadIdx.x + blockIdx.x * blockDim.x;\n    const int bin_it = gid;\n    int k;\n    int seg_size;\n    if(bin_it < bin_size) {\n        k = seg_begins[bin[bin_it]];\n        seg_size = seg_ends[bin[bin_it]]-seg_begins[bin[bin_it]];\n        if(seg_size == 1)\n        {\n            keyB[k] = key[k];\n        }\n    }\n}",
            "#define K 1.38065e-23        // J/K, Boltzmann constant\n\n\n__device__ inline void exch_intxn_keys(K &k0, K &k1, K &k2, K &k3, K &k4, K &k5, K &k6, K &k7, K &k8, K &k9, K &k10, K &k11, K &k12, K &k13, K &k14, K &k15, K &k16, K &k17, K &k18, K &k19, K &k20, K &k21, K &k22, K &k23, K &k24, K &k25, K &k26, K &k27, K &k28, K &k29, K &k30, K &k31, int mask, const int bit) {\n    K ex_k0, ex_k1;\n    if(bit) SWP_KEY(K, k0, k30);\n    if(bit) SWP_KEY(K, k1, k31);\n    if(bit) SWP_KEY(K, k2, k28);\n    if(bit) SWP_KEY(K, k3, k29);\n    if(bit) SWP_KEY(K, k4, k26);\n    if(bit) SWP_KEY(K, k5, k27);\n    if(bit) SWP_KEY(K, k6, k24);\n    if(bit) SWP_KEY(K, k7, k25);\n    if(bit) SWP_KEY(K, k8, k22);\n    if(bit) SWP_KEY(K, k9, k23);\n    if(bit) SWP_KEY(K, k10, k20);\n    if(bit) SWP_KEY(K, k11, k21);\n    if(bit) SWP_KEY(K, k12, k18);\n    if(bit) SWP_KEY(K, k13, k19);\n    if(bit) SWP_KEY(K, k14, k16);\n    if(bit) SWP_KEY(K, k15, k17);\n    ex_k0 = k0;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k1, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k0 = ex_k0;\n    k1 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k2;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k3, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k2 = ex_k0;\n    k3 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k4;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k5, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k4 = ex_k0;\n    k5 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k6;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k7, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k6 = ex_k0;\n    k7 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k8;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k9, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k8 = ex_k0;\n    k9 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k10;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k11, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k10 = ex_k0;\n    k11 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k12;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k13, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k12 = ex_k0;\n    k13 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k14;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k15, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k14 = ex_k0;\n    k15 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k16;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k17, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k16 = ex_k0;\n    k17 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k18;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k19, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k18 = ex_k0;\n    k19 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k20;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k21, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k20 = ex_k0;\n    k21 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k22;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k23, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k22 = ex_k0;\n    k23 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k24;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k25, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k24 = ex_k0;\n    k25 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k26;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k27, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k26 = ex_k0;\n    k27 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k28;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k29, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k28 = ex_k0;\n    k29 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k30;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k31, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k30 = ex_k0;\n    k31 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    if(bit) SWP_KEY(K, k0, k30);\n    if(bit) SWP_KEY(K, k1, k31);\n    if(bit) SWP_KEY(K, k2, k28);\n    if(bit) SWP_KEY(K, k3, k29);\n    if(bit) SWP_KEY(K, k4, k26);\n    if(bit) SWP_KEY(K, k5, k27);\n    if(bit) SWP_KEY(K, k6, k24);\n    if(bit) SWP_KEY(K, k7, k25);\n    if(bit) SWP_KEY(K, k8, k22);\n    if(bit) SWP_KEY(K, k9, k23);\n    if(bit) SWP_KEY(K, k10, k20);\n    if(bit) SWP_KEY(K, k11, k21);\n    if(bit) SWP_KEY(K, k12, k18);\n    if(bit) SWP_KEY(K, k13, k19);\n    if(bit) SWP_KEY(K, k14, k16);\n    if(bit) SWP_KEY(K, k15, k17);\n}\n\n__global__\nvoid gen_bk256_wp2_tc1_r2_r2_orig(\n    K *key, K *keyB,\n    const Offset *seg_begins, const Offset *seg_ends,\n    const int *bin, const int bin_size)\n{\n    const int gid = threadIdx.x + blockIdx.x * blockDim.x;\n    const int bin_it = (gid>>1);\n    const int tid = (threadIdx.x & 1);\n    const int bit1 = (tid>>0)&0x1;\n    K rg_k0 ;\n    int k;\n    int seg_size;\n    if(bin_it < bin_size) {\n        k = seg_begins[bin[bin_it]];\n        seg_size = seg_ends[bin[bin_it]]-seg_begins[bin[bin_it]];\n        rg_k0  = (tid+0   <seg_size)?key[k+tid+0   ]:std::numeric_limits<K>::max();\n        // sort 2 elements\n        // exch_intxn: generate exch_intxn_keys()\n        exch_intxn_keys(rg_k0 ,\n                        0x1,bit1);\n        if((tid<<0)+0 <seg_size) keyB[k+(tid<<0)+0 ] = rg_k0 ;\n    }\n}",
            "#define K 1.38065e-23        // J/K, Boltzmann constant\n\n\n__device__ inline void exch_intxn_keys(K &k0, K &k1, K &k2, K &k3, K &k4, K &k5, K &k6, K &k7, K &k8, K &k9, K &k10, K &k11, K &k12, K &k13, K &k14, K &k15, K &k16, K &k17, K &k18, K &k19, K &k20, K &k21, K &k22, K &k23, K &k24, K &k25, K &k26, K &k27, K &k28, K &k29, K &k30, K &k31, int mask, const int bit) {\n    K ex_k0, ex_k1;\n    if(bit) SWP_KEY(K, k0, k30);\n    if(bit) SWP_KEY(K, k1, k31);\n    if(bit) SWP_KEY(K, k2, k28);\n    if(bit) SWP_KEY(K, k3, k29);\n    if(bit) SWP_KEY(K, k4, k26);\n    if(bit) SWP_KEY(K, k5, k27);\n    if(bit) SWP_KEY(K, k6, k24);\n    if(bit) SWP_KEY(K, k7, k25);\n    if(bit) SWP_KEY(K, k8, k22);\n    if(bit) SWP_KEY(K, k9, k23);\n    if(bit) SWP_KEY(K, k10, k20);\n    if(bit) SWP_KEY(K, k11, k21);\n    if(bit) SWP_KEY(K, k12, k18);\n    if(bit) SWP_KEY(K, k13, k19);\n    if(bit) SWP_KEY(K, k14, k16);\n    if(bit) SWP_KEY(K, k15, k17);\n    ex_k0 = k0;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k1, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k0 = ex_k0;\n    k1 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k2;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k3, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k2 = ex_k0;\n    k3 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k4;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k5, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k4 = ex_k0;\n    k5 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k6;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k7, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k6 = ex_k0;\n    k7 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k8;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k9, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k8 = ex_k0;\n    k9 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k10;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k11, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k10 = ex_k0;\n    k11 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k12;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k13, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k12 = ex_k0;\n    k13 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k14;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k15, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k14 = ex_k0;\n    k15 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k16;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k17, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k16 = ex_k0;\n    k17 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k18;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k19, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k18 = ex_k0;\n    k19 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k20;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k21, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k20 = ex_k0;\n    k21 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k22;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k23, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k22 = ex_k0;\n    k23 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k24;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k25, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k24 = ex_k0;\n    k25 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k26;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k27, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k26 = ex_k0;\n    k27 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k28;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k29, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k28 = ex_k0;\n    k29 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k30;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k31, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k30 = ex_k0;\n    k31 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    if(bit) SWP_KEY(K, k0, k30);\n    if(bit) SWP_KEY(K, k1, k31);\n    if(bit) SWP_KEY(K, k2, k28);\n    if(bit) SWP_KEY(K, k3, k29);\n    if(bit) SWP_KEY(K, k4, k26);\n    if(bit) SWP_KEY(K, k5, k27);\n    if(bit) SWP_KEY(K, k6, k24);\n    if(bit) SWP_KEY(K, k7, k25);\n    if(bit) SWP_KEY(K, k8, k22);\n    if(bit) SWP_KEY(K, k9, k23);\n    if(bit) SWP_KEY(K, k10, k20);\n    if(bit) SWP_KEY(K, k11, k21);\n    if(bit) SWP_KEY(K, k12, k18);\n    if(bit) SWP_KEY(K, k13, k19);\n    if(bit) SWP_KEY(K, k14, k16);\n    if(bit) SWP_KEY(K, k15, k17);\n}\n\n__global__\nvoid gen_bk128_wp2_tc2_r3_r4_orig(\n    K *key, K *keyB,\n    const Offset *seg_begins, const Offset *seg_ends,\n    const int *bin, const int bin_size)\n{\n    const int gid = threadIdx.x + blockIdx.x * blockDim.x;\n    const int bin_it = (gid>>1);\n    const int tid = (threadIdx.x & 1);\n    const int bit1 = (tid>>0)&0x1;\n    K rg_k0 ;\n    K rg_k1 ;\n    int k;\n    int seg_size;\n    if(bin_it < bin_size) {\n        k = seg_begins[bin[bin_it]];\n        seg_size = seg_ends[bin[bin_it]]-seg_begins[bin[bin_it]];\n        rg_k0  = (tid+0   <seg_size)?key[k+tid+0   ]:std::numeric_limits<K>::max();\n        rg_k1  = (tid+2   <seg_size)?key[k+tid+2   ]:std::numeric_limits<K>::max();\n        // sort 4 elements\n        // exch_intxn: switch to exch_local()\n        CMP_SWP_KEY(K,rg_k0 ,rg_k1 );\n        // exch_intxn: generate exch_intxn_keys()\n        exch_intxn_keys(rg_k0 ,rg_k1 ,\n                        0x1,bit1);\n        // exch_paral: switch to exch_local()\n        CMP_SWP_KEY(K,rg_k0 ,rg_k1 );\n        if((tid<<1)+0 <seg_size) keyB[k+(tid<<1)+0 ] = rg_k0 ;\n        if((tid<<1)+1 <seg_size) keyB[k+(tid<<1)+1 ] = rg_k1 ;\n    }\n}",
            "#define K 1.38065e-23        // J/K, Boltzmann constant\n\n\n__device__ inline void exch_intxn_keys(K &k0, K &k1, K &k2, K &k3, K &k4, K &k5, K &k6, K &k7, K &k8, K &k9, K &k10, K &k11, K &k12, K &k13, K &k14, K &k15, K &k16, K &k17, K &k18, K &k19, K &k20, K &k21, K &k22, K &k23, K &k24, K &k25, K &k26, K &k27, K &k28, K &k29, K &k30, K &k31, int mask, const int bit) {\n    K ex_k0, ex_k1;\n    if(bit) SWP_KEY(K, k0, k30);\n    if(bit) SWP_KEY(K, k1, k31);\n    if(bit) SWP_KEY(K, k2, k28);\n    if(bit) SWP_KEY(K, k3, k29);\n    if(bit) SWP_KEY(K, k4, k26);\n    if(bit) SWP_KEY(K, k5, k27);\n    if(bit) SWP_KEY(K, k6, k24);\n    if(bit) SWP_KEY(K, k7, k25);\n    if(bit) SWP_KEY(K, k8, k22);\n    if(bit) SWP_KEY(K, k9, k23);\n    if(bit) SWP_KEY(K, k10, k20);\n    if(bit) SWP_KEY(K, k11, k21);\n    if(bit) SWP_KEY(K, k12, k18);\n    if(bit) SWP_KEY(K, k13, k19);\n    if(bit) SWP_KEY(K, k14, k16);\n    if(bit) SWP_KEY(K, k15, k17);\n    ex_k0 = k0;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k1, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k0 = ex_k0;\n    k1 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k2;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k3, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k2 = ex_k0;\n    k3 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k4;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k5, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k4 = ex_k0;\n    k5 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k6;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k7, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k6 = ex_k0;\n    k7 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k8;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k9, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k8 = ex_k0;\n    k9 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k10;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k11, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k10 = ex_k0;\n    k11 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k12;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k13, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k12 = ex_k0;\n    k13 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k14;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k15, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k14 = ex_k0;\n    k15 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k16;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k17, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k16 = ex_k0;\n    k17 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k18;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k19, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k18 = ex_k0;\n    k19 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k20;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k21, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k20 = ex_k0;\n    k21 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k22;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k23, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k22 = ex_k0;\n    k23 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k24;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k25, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k24 = ex_k0;\n    k25 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k26;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k27, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k26 = ex_k0;\n    k27 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k28;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k29, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k28 = ex_k0;\n    k29 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k30;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k31, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k30 = ex_k0;\n    k31 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    if(bit) SWP_KEY(K, k0, k30);\n    if(bit) SWP_KEY(K, k1, k31);\n    if(bit) SWP_KEY(K, k2, k28);\n    if(bit) SWP_KEY(K, k3, k29);\n    if(bit) SWP_KEY(K, k4, k26);\n    if(bit) SWP_KEY(K, k5, k27);\n    if(bit) SWP_KEY(K, k6, k24);\n    if(bit) SWP_KEY(K, k7, k25);\n    if(bit) SWP_KEY(K, k8, k22);\n    if(bit) SWP_KEY(K, k9, k23);\n    if(bit) SWP_KEY(K, k10, k20);\n    if(bit) SWP_KEY(K, k11, k21);\n    if(bit) SWP_KEY(K, k12, k18);\n    if(bit) SWP_KEY(K, k13, k19);\n    if(bit) SWP_KEY(K, k14, k16);\n    if(bit) SWP_KEY(K, k15, k17);\n}\n\n__global__\nvoid gen_bk128_wp2_tc4_r5_r8_orig(\n    K *key, K *keyB,\n    const Offset *seg_begins, const Offset *seg_ends,\n    const int *bin, const int bin_size)\n{\n    const int gid = threadIdx.x + blockIdx.x * blockDim.x;\n    const int bin_it = (gid>>1);\n    const int tid = (threadIdx.x & 1);\n    const int bit1 = (tid>>0)&0x1;\n    K rg_k0 ;\n    K rg_k1 ;\n    K rg_k2 ;\n    K rg_k3 ;\n    int k;\n    int seg_size;\n    if(bin_it < bin_size) {\n        k = seg_begins[bin[bin_it]];\n        seg_size = seg_ends[bin[bin_it]]-seg_begins[bin[bin_it]];\n        rg_k0  = (tid+0   <seg_size)?key[k+tid+0   ]:std::numeric_limits<K>::max();\n        rg_k1  = (tid+2   <seg_size)?key[k+tid+2   ]:std::numeric_limits<K>::max();\n        rg_k2  = (tid+4   <seg_size)?key[k+tid+4   ]:std::numeric_limits<K>::max();\n        rg_k3  = (tid+6   <seg_size)?key[k+tid+6   ]:std::numeric_limits<K>::max();\n        // sort 8 elements\n        // exch_intxn: switch to exch_local()\n        CMP_SWP_KEY(K,rg_k0 ,rg_k1 );\n        CMP_SWP_KEY(K,rg_k2 ,rg_k3 );\n        // exch_intxn: switch to exch_local()\n        CMP_SWP_KEY(K,rg_k0 ,rg_k3 );\n        CMP_SWP_KEY(K,rg_k1 ,rg_k2 );\n        // exch_paral: switch to exch_local()\n        CMP_SWP_KEY(K,rg_k0 ,rg_k1 );\n        CMP_SWP_KEY(K,rg_k2 ,rg_k3 );\n        // exch_intxn: generate exch_intxn_keys()\n        exch_intxn_keys(rg_k0 ,rg_k1 ,rg_k2 ,rg_k3 ,\n                        0x1,bit1);\n        // exch_paral: switch to exch_local()\n        CMP_SWP_KEY(K,rg_k0 ,rg_k2 );\n        CMP_SWP_KEY(K,rg_k1 ,rg_k3 );\n        // exch_paral: switch to exch_local()\n        CMP_SWP_KEY(K,rg_k0 ,rg_k1 );\n        CMP_SWP_KEY(K,rg_k2 ,rg_k3 );\n        if((tid<<2)+0 <seg_size) keyB[k+(tid<<2)+0 ] = rg_k0 ;\n        if((tid<<2)+1 <seg_size) keyB[k+(tid<<2)+1 ] = rg_k1 ;\n        if((tid<<2)+2 <seg_size) keyB[k+(tid<<2)+2 ] = rg_k2 ;\n        if((tid<<2)+3 <seg_size) keyB[k+(tid<<2)+3 ] = rg_k3 ;\n    }\n}",
            "#define K 1.38065e-23        // J/K, Boltzmann constant\n\n\n__device__ inline void exch_intxn_keys(K &k0, K &k1, K &k2, K &k3, K &k4, K &k5, K &k6, K &k7, K &k8, K &k9, K &k10, K &k11, K &k12, K &k13, K &k14, K &k15, K &k16, K &k17, K &k18, K &k19, K &k20, K &k21, K &k22, K &k23, K &k24, K &k25, K &k26, K &k27, K &k28, K &k29, K &k30, K &k31, int mask, const int bit) {\n    K ex_k0, ex_k1;\n    if(bit) SWP_KEY(K, k0, k30);\n    if(bit) SWP_KEY(K, k1, k31);\n    if(bit) SWP_KEY(K, k2, k28);\n    if(bit) SWP_KEY(K, k3, k29);\n    if(bit) SWP_KEY(K, k4, k26);\n    if(bit) SWP_KEY(K, k5, k27);\n    if(bit) SWP_KEY(K, k6, k24);\n    if(bit) SWP_KEY(K, k7, k25);\n    if(bit) SWP_KEY(K, k8, k22);\n    if(bit) SWP_KEY(K, k9, k23);\n    if(bit) SWP_KEY(K, k10, k20);\n    if(bit) SWP_KEY(K, k11, k21);\n    if(bit) SWP_KEY(K, k12, k18);\n    if(bit) SWP_KEY(K, k13, k19);\n    if(bit) SWP_KEY(K, k14, k16);\n    if(bit) SWP_KEY(K, k15, k17);\n    ex_k0 = k0;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k1, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k0 = ex_k0;\n    k1 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k2;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k3, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k2 = ex_k0;\n    k3 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k4;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k5, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k4 = ex_k0;\n    k5 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k6;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k7, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k6 = ex_k0;\n    k7 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k8;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k9, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k8 = ex_k0;\n    k9 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k10;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k11, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k10 = ex_k0;\n    k11 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k12;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k13, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k12 = ex_k0;\n    k13 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k14;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k15, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k14 = ex_k0;\n    k15 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k16;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k17, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k16 = ex_k0;\n    k17 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k18;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k19, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k18 = ex_k0;\n    k19 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k20;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k21, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k20 = ex_k0;\n    k21 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k22;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k23, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k22 = ex_k0;\n    k23 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k24;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k25, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k24 = ex_k0;\n    k25 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k26;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k27, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k26 = ex_k0;\n    k27 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k28;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k29, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k28 = ex_k0;\n    k29 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k30;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k31, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k30 = ex_k0;\n    k31 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    if(bit) SWP_KEY(K, k0, k30);\n    if(bit) SWP_KEY(K, k1, k31);\n    if(bit) SWP_KEY(K, k2, k28);\n    if(bit) SWP_KEY(K, k3, k29);\n    if(bit) SWP_KEY(K, k4, k26);\n    if(bit) SWP_KEY(K, k5, k27);\n    if(bit) SWP_KEY(K, k6, k24);\n    if(bit) SWP_KEY(K, k7, k25);\n    if(bit) SWP_KEY(K, k8, k22);\n    if(bit) SWP_KEY(K, k9, k23);\n    if(bit) SWP_KEY(K, k10, k20);\n    if(bit) SWP_KEY(K, k11, k21);\n    if(bit) SWP_KEY(K, k12, k18);\n    if(bit) SWP_KEY(K, k13, k19);\n    if(bit) SWP_KEY(K, k14, k16);\n    if(bit) SWP_KEY(K, k15, k17);\n}\n\n__device__ inline void exch_paral_keys(K &k0, K &k1, K &k2, K &k3, K &k4, K &k5, K &k6, K &k7, K &k8, K &k9, K &k10, K &k11, K &k12, K &k13, K &k14, K &k15, K &k16, K &k17, K &k18, K &k19, K &k20, K &k21, K &k22, K &k23, K &k24, K &k25, K &k26, K &k27, K &k28, K &k29, K &k30, K &k31, int mask, const int bit) {\n    K ex_k0, ex_k1;\n    if(bit) SWP_KEY(K, k0, k1);\n    if(bit) SWP_KEY(K, k2, k3);\n    if(bit) SWP_KEY(K, k4, k5);\n    if(bit) SWP_KEY(K, k6, k7);\n    if(bit) SWP_KEY(K, k8, k9);\n    if(bit) SWP_KEY(K, k10, k11);\n    if(bit) SWP_KEY(K, k12, k13);\n    if(bit) SWP_KEY(K, k14, k15);\n    if(bit) SWP_KEY(K, k16, k17);\n    if(bit) SWP_KEY(K, k18, k19);\n    if(bit) SWP_KEY(K, k20, k21);\n    if(bit) SWP_KEY(K, k22, k23);\n    if(bit) SWP_KEY(K, k24, k25);\n    if(bit) SWP_KEY(K, k26, k27);\n    if(bit) SWP_KEY(K, k28, k29);\n    if(bit) SWP_KEY(K, k30, k31);\n    ex_k0 = k0;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k1, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k0 = ex_k0;\n    k1 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k2;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k3, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k2 = ex_k0;\n    k3 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k4;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k5, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k4 = ex_k0;\n    k5 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k6;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k7, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k6 = ex_k0;\n    k7 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k8;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k9, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k8 = ex_k0;\n    k9 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k10;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k11, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k10 = ex_k0;\n    k11 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k12;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k13, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k12 = ex_k0;\n    k13 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k14;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k15, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k14 = ex_k0;\n    k15 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k16;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k17, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k16 = ex_k0;\n    k17 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k18;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k19, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k18 = ex_k0;\n    k19 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k20;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k21, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k20 = ex_k0;\n    k21 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k22;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k23, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k22 = ex_k0;\n    k23 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k24;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k25, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k24 = ex_k0;\n    k25 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k26;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k27, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k26 = ex_k0;\n    k27 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k28;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k29, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k28 = ex_k0;\n    k29 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k30;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k31, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k30 = ex_k0;\n    k31 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    if(bit) SWP_KEY(K, k0, k1);\n    if(bit) SWP_KEY(K, k2, k3);\n    if(bit) SWP_KEY(K, k4, k5);\n    if(bit) SWP_KEY(K, k6, k7);\n    if(bit) SWP_KEY(K, k8, k9);\n    if(bit) SWP_KEY(K, k10, k11);\n    if(bit) SWP_KEY(K, k12, k13);\n    if(bit) SWP_KEY(K, k14, k15);\n    if(bit) SWP_KEY(K, k16, k17);\n    if(bit) SWP_KEY(K, k18, k19);\n    if(bit) SWP_KEY(K, k20, k21);\n    if(bit) SWP_KEY(K, k22, k23);\n    if(bit) SWP_KEY(K, k24, k25);\n    if(bit) SWP_KEY(K, k26, k27);\n    if(bit) SWP_KEY(K, k28, k29);\n    if(bit) SWP_KEY(K, k30, k31);\n}\n\n__global__\nvoid gen_bk128_wp4_tc4_r9_r16_strd(\n    K *key, K *keyB,\n    const Offset *seg_begins, const Offset *seg_ends,\n    const int *bin, const int bin_size)\n{\n    const int gid = threadIdx.x + blockIdx.x * blockDim.x;\n    const int bin_it = (gid>>2);\n    const int tid = (threadIdx.x & 3);\n    const int bit1 = (tid>>0)&0x1;\n    const int bit2 = (tid>>1)&0x1;\n    K rg_k0 ;\n    K rg_k1 ;\n    K rg_k2 ;\n    K rg_k3 ;\n    int normalized_bin_size = (bin_size/8)*8;\n    int k;\n    int seg_size;\n    if(bin_it < bin_size) {\n        k = seg_begins[bin[bin_it]];\n        seg_size = seg_ends[bin[bin_it]]-seg_begins[bin[bin_it]];\n        rg_k0  = (tid+0   <seg_size)?key[k+tid+0   ]:std::numeric_limits<K>::max();\n        rg_k1  = (tid+4   <seg_size)?key[k+tid+4   ]:std::numeric_limits<K>::max();\n        rg_k2  = (tid+8   <seg_size)?key[k+tid+8   ]:std::numeric_limits<K>::max();\n        rg_k3  = (tid+12  <seg_size)?key[k+tid+12  ]:std::numeric_limits<K>::max();\n        // sort 16 elements\n        // exch_intxn: switch to exch_local()\n        CMP_SWP_KEY(K,rg_k0 ,rg_k1 );\n        CMP_SWP_KEY(K,rg_k2 ,rg_k3 );\n        // exch_intxn: switch to exch_local()\n        CMP_SWP_KEY(K,rg_k0 ,rg_k3 );\n        CMP_SWP_KEY(K,rg_k1 ,rg_k2 );\n        // exch_paral: switch to exch_local()\n        CMP_SWP_KEY(K,rg_k0 ,rg_k1 );\n        CMP_SWP_KEY(K,rg_k2 ,rg_k3 );\n        // exch_intxn: generate exch_intxn_keys()\n        exch_intxn_keys(rg_k0 ,rg_k1 ,rg_k2 ,rg_k3 ,\n                        0x1,bit1);\n        // exch_paral: switch to exch_local()\n        CMP_SWP_KEY(K,rg_k0 ,rg_k2 );\n        CMP_SWP_KEY(K,rg_k1 ,rg_k3 );\n        // exch_paral: switch to exch_local()\n        CMP_SWP_KEY(K,rg_k0 ,rg_k1 );\n        CMP_SWP_KEY(K,rg_k2 ,rg_k3 );\n        // exch_intxn: generate exch_intxn_keys()\n        exch_intxn_keys(rg_k0 ,rg_k1 ,rg_k2 ,rg_k3 ,\n                        0x3,bit2);\n        // exch_paral: generate exch_paral_keys()\n        exch_paral_keys(rg_k0 ,rg_k1 ,rg_k2 ,rg_k3 ,\n                        0x1,bit1);\n        // exch_paral: switch to exch_local()\n        CMP_SWP_KEY(K,rg_k0 ,rg_k2 );\n        CMP_SWP_KEY(K,rg_k1 ,rg_k3 );\n        // exch_paral: switch to exch_local()\n        CMP_SWP_KEY(K,rg_k0 ,rg_k1 );\n        CMP_SWP_KEY(K,rg_k2 ,rg_k3 );\n    }\n\n    if(bin_it < normalized_bin_size) {\n        // store back the results\n        int lane_id = threadIdx.x & 31;\n        rg_k1  = __shfl_xor_sync(0xffffffff,rg_k1 , 0x1 );\n        rg_k3  = __shfl_xor_sync(0xffffffff,rg_k3 , 0x1 );\n        if(lane_id&0x1 ) SWP_KEY(K, rg_k0 , rg_k1 );\n        if(lane_id&0x1 ) SWP_KEY(K, rg_k2 , rg_k3 );\n        rg_k1  = __shfl_xor_sync(0xffffffff,rg_k1 , 0x1 );\n        rg_k3  = __shfl_xor_sync(0xffffffff,rg_k3 , 0x1 );\n        rg_k2  = __shfl_xor_sync(0xffffffff,rg_k2 , 0x2 );\n        rg_k3  = __shfl_xor_sync(0xffffffff,rg_k3 , 0x2 );\n        if(lane_id&0x2 ) SWP_KEY(K, rg_k0 , rg_k2 );\n        if(lane_id&0x2 ) SWP_KEY(K, rg_k1 , rg_k3 );\n        rg_k2  = __shfl_xor_sync(0xffffffff,rg_k2 , 0x2 );\n        rg_k3  = __shfl_xor_sync(0xffffffff,rg_k3 , 0x2 );\n        rg_k1  = __shfl_xor_sync(0xffffffff,rg_k1 , 0x4 );\n        rg_k3  = __shfl_xor_sync(0xffffffff,rg_k3 , 0x4 );\n        if(lane_id&0x4 ) SWP_KEY(K, rg_k0 , rg_k1 );\n        if(lane_id&0x4 ) SWP_KEY(K, rg_k2 , rg_k3 );\n        rg_k1  = __shfl_xor_sync(0xffffffff,rg_k1 , 0x4 );\n        rg_k3  = __shfl_xor_sync(0xffffffff,rg_k3 , 0x4 );\n        rg_k2  = __shfl_xor_sync(0xffffffff,rg_k2 , 0x8 );\n        rg_k3  = __shfl_xor_sync(0xffffffff,rg_k3 , 0x8 );\n        if(lane_id&0x8 ) SWP_KEY(K, rg_k0 , rg_k2 );\n        if(lane_id&0x8 ) SWP_KEY(K, rg_k1 , rg_k3 );\n        rg_k2  = __shfl_xor_sync(0xffffffff,rg_k2 , 0x8 );\n        rg_k3  = __shfl_xor_sync(0xffffffff,rg_k3 , 0x8 );\n        rg_k1  = __shfl_xor_sync(0xffffffff,rg_k1 , 0x10);\n        rg_k3  = __shfl_xor_sync(0xffffffff,rg_k3 , 0x10);\n        if(lane_id&0x10) SWP_KEY(K, rg_k0 , rg_k1 );\n        if(lane_id&0x10) SWP_KEY(K, rg_k2 , rg_k3 );\n        rg_k1  = __shfl_xor_sync(0xffffffff,rg_k1 , 0x10);\n        rg_k3  = __shfl_xor_sync(0xffffffff,rg_k3 , 0x10);\n        int kk;\n        int ss;\n        int base = (lane_id/16)*16;\n        kk = __shfl_sync(0xffffffff,k, 0 );\n        ss = __shfl_sync(0xffffffff,seg_size, 0 );\n        if((lane_id>>4)==0&&lane_id-base<ss) keyB[kk+lane_id-base] = rg_k0 ;\n        kk = __shfl_sync(0xffffffff,k, 4 );\n        ss = __shfl_sync(0xffffffff,seg_size, 4 );\n        if((lane_id>>4)==1&&lane_id-base<ss) keyB[kk+lane_id-base] = rg_k0 ;\n        kk = __shfl_sync(0xffffffff,k, 8 );\n        ss = __shfl_sync(0xffffffff,seg_size, 8 );\n        if((lane_id>>4)==0&&lane_id-base<ss) keyB[kk+lane_id-base] = rg_k2 ;\n        kk = __shfl_sync(0xffffffff,k, 12);\n        ss = __shfl_sync(0xffffffff,seg_size, 12);\n        if((lane_id>>4)==1&&lane_id-base<ss) keyB[kk+lane_id-base] = rg_k2 ;\n        kk = __shfl_sync(0xffffffff,k, 16);\n        ss = __shfl_sync(0xffffffff,seg_size, 16);\n        if((lane_id>>4)==0&&lane_id-base<ss) keyB[kk+lane_id-base] = rg_k1 ;\n        kk = __shfl_sync(0xffffffff,k, 20);\n        ss = __shfl_sync(0xffffffff,seg_size, 20);\n        if((lane_id>>4)==1&&lane_id-base<ss) keyB[kk+lane_id-base] = rg_k1 ;\n        kk = __shfl_sync(0xffffffff,k, 24);\n        ss = __shfl_sync(0xffffffff,seg_size, 24);\n        if((lane_id>>4)==0&&lane_id-base<ss) keyB[kk+lane_id-base] = rg_k3 ;\n        kk = __shfl_sync(0xffffffff,k, 28);\n        ss = __shfl_sync(0xffffffff,seg_size, 28);\n        if((lane_id>>4)==1&&lane_id-base<ss) keyB[kk+lane_id-base] = rg_k3 ;\n    } else if(bin_it < bin_size) {\n        if((tid<<2)+0 <seg_size) keyB[k+(tid<<2)+0 ] = rg_k0 ;\n        if((tid<<2)+1 <seg_size) keyB[k+(tid<<2)+1 ] = rg_k1 ;\n        if((tid<<2)+2 <seg_size) keyB[k+(tid<<2)+2 ] = rg_k2 ;\n        if((tid<<2)+3 <seg_size) keyB[k+(tid<<2)+3 ] = rg_k3 ;\n    }\n}",
            "#define K 1.38065e-23        // J/K, Boltzmann constant\n\n\n__device__ inline void exch_intxn_keys(K &k0, K &k1, K &k2, K &k3, K &k4, K &k5, K &k6, K &k7, K &k8, K &k9, K &k10, K &k11, K &k12, K &k13, K &k14, K &k15, K &k16, K &k17, K &k18, K &k19, K &k20, K &k21, K &k22, K &k23, K &k24, K &k25, K &k26, K &k27, K &k28, K &k29, K &k30, K &k31, int mask, const int bit) {\n    K ex_k0, ex_k1;\n    if(bit) SWP_KEY(K, k0, k30);\n    if(bit) SWP_KEY(K, k1, k31);\n    if(bit) SWP_KEY(K, k2, k28);\n    if(bit) SWP_KEY(K, k3, k29);\n    if(bit) SWP_KEY(K, k4, k26);\n    if(bit) SWP_KEY(K, k5, k27);\n    if(bit) SWP_KEY(K, k6, k24);\n    if(bit) SWP_KEY(K, k7, k25);\n    if(bit) SWP_KEY(K, k8, k22);\n    if(bit) SWP_KEY(K, k9, k23);\n    if(bit) SWP_KEY(K, k10, k20);\n    if(bit) SWP_KEY(K, k11, k21);\n    if(bit) SWP_KEY(K, k12, k18);\n    if(bit) SWP_KEY(K, k13, k19);\n    if(bit) SWP_KEY(K, k14, k16);\n    if(bit) SWP_KEY(K, k15, k17);\n    ex_k0 = k0;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k1, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k0 = ex_k0;\n    k1 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k2;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k3, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k2 = ex_k0;\n    k3 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k4;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k5, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k4 = ex_k0;\n    k5 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k6;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k7, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k6 = ex_k0;\n    k7 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k8;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k9, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k8 = ex_k0;\n    k9 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k10;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k11, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k10 = ex_k0;\n    k11 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k12;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k13, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k12 = ex_k0;\n    k13 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k14;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k15, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k14 = ex_k0;\n    k15 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k16;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k17, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k16 = ex_k0;\n    k17 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k18;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k19, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k18 = ex_k0;\n    k19 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k20;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k21, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k20 = ex_k0;\n    k21 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k22;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k23, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k22 = ex_k0;\n    k23 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k24;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k25, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k24 = ex_k0;\n    k25 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k26;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k27, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k26 = ex_k0;\n    k27 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k28;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k29, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k28 = ex_k0;\n    k29 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k30;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k31, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k30 = ex_k0;\n    k31 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    if(bit) SWP_KEY(K, k0, k30);\n    if(bit) SWP_KEY(K, k1, k31);\n    if(bit) SWP_KEY(K, k2, k28);\n    if(bit) SWP_KEY(K, k3, k29);\n    if(bit) SWP_KEY(K, k4, k26);\n    if(bit) SWP_KEY(K, k5, k27);\n    if(bit) SWP_KEY(K, k6, k24);\n    if(bit) SWP_KEY(K, k7, k25);\n    if(bit) SWP_KEY(K, k8, k22);\n    if(bit) SWP_KEY(K, k9, k23);\n    if(bit) SWP_KEY(K, k10, k20);\n    if(bit) SWP_KEY(K, k11, k21);\n    if(bit) SWP_KEY(K, k12, k18);\n    if(bit) SWP_KEY(K, k13, k19);\n    if(bit) SWP_KEY(K, k14, k16);\n    if(bit) SWP_KEY(K, k15, k17);\n}\n\n__device__ inline void exch_paral_keys(K &k0, K &k1, K &k2, K &k3, K &k4, K &k5, K &k6, K &k7, K &k8, K &k9, K &k10, K &k11, K &k12, K &k13, K &k14, K &k15, K &k16, K &k17, K &k18, K &k19, K &k20, K &k21, K &k22, K &k23, K &k24, K &k25, K &k26, K &k27, K &k28, K &k29, K &k30, K &k31, int mask, const int bit) {\n    K ex_k0, ex_k1;\n    if(bit) SWP_KEY(K, k0, k1);\n    if(bit) SWP_KEY(K, k2, k3);\n    if(bit) SWP_KEY(K, k4, k5);\n    if(bit) SWP_KEY(K, k6, k7);\n    if(bit) SWP_KEY(K, k8, k9);\n    if(bit) SWP_KEY(K, k10, k11);\n    if(bit) SWP_KEY(K, k12, k13);\n    if(bit) SWP_KEY(K, k14, k15);\n    if(bit) SWP_KEY(K, k16, k17);\n    if(bit) SWP_KEY(K, k18, k19);\n    if(bit) SWP_KEY(K, k20, k21);\n    if(bit) SWP_KEY(K, k22, k23);\n    if(bit) SWP_KEY(K, k24, k25);\n    if(bit) SWP_KEY(K, k26, k27);\n    if(bit) SWP_KEY(K, k28, k29);\n    if(bit) SWP_KEY(K, k30, k31);\n    ex_k0 = k0;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k1, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k0 = ex_k0;\n    k1 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k2;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k3, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k2 = ex_k0;\n    k3 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k4;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k5, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k4 = ex_k0;\n    k5 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k6;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k7, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k6 = ex_k0;\n    k7 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k8;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k9, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k8 = ex_k0;\n    k9 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k10;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k11, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k10 = ex_k0;\n    k11 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k12;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k13, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k12 = ex_k0;\n    k13 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k14;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k15, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k14 = ex_k0;\n    k15 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k16;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k17, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k16 = ex_k0;\n    k17 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k18;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k19, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k18 = ex_k0;\n    k19 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k20;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k21, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k20 = ex_k0;\n    k21 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k22;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k23, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k22 = ex_k0;\n    k23 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k24;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k25, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k24 = ex_k0;\n    k25 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k26;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k27, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k26 = ex_k0;\n    k27 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k28;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k29, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k28 = ex_k0;\n    k29 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k30;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k31, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k30 = ex_k0;\n    k31 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    if(bit) SWP_KEY(K, k0, k1);\n    if(bit) SWP_KEY(K, k2, k3);\n    if(bit) SWP_KEY(K, k4, k5);\n    if(bit) SWP_KEY(K, k6, k7);\n    if(bit) SWP_KEY(K, k8, k9);\n    if(bit) SWP_KEY(K, k10, k11);\n    if(bit) SWP_KEY(K, k12, k13);\n    if(bit) SWP_KEY(K, k14, k15);\n    if(bit) SWP_KEY(K, k16, k17);\n    if(bit) SWP_KEY(K, k18, k19);\n    if(bit) SWP_KEY(K, k20, k21);\n    if(bit) SWP_KEY(K, k22, k23);\n    if(bit) SWP_KEY(K, k24, k25);\n    if(bit) SWP_KEY(K, k26, k27);\n    if(bit) SWP_KEY(K, k28, k29);\n    if(bit) SWP_KEY(K, k30, k31);\n}\n\n__global__\nvoid gen_bk128_wp8_tc4_r17_r32_strd(\n    K *key, K *keyB,\n    const Offset *seg_begins, const Offset *seg_ends,\n    const int *bin, const int bin_size)\n{\n    const int gid = threadIdx.x + blockIdx.x * blockDim.x;\n    const int bin_it = (gid>>3);\n    const int tid = (threadIdx.x & 7);\n    const int bit1 = (tid>>0)&0x1;\n    const int bit2 = (tid>>1)&0x1;\n    const int bit3 = (tid>>2)&0x1;\n    K rg_k0 ;\n    K rg_k1 ;\n    K rg_k2 ;\n    K rg_k3 ;\n    int normalized_bin_size = (bin_size/4)*4;\n    int k;\n    int seg_size;\n    if(bin_it < bin_size) {\n        k = seg_begins[bin[bin_it]];\n        seg_size = seg_ends[bin[bin_it]]-seg_begins[bin[bin_it]];\n        rg_k0  = (tid+0   <seg_size)?key[k+tid+0   ]:std::numeric_limits<K>::max();\n        rg_k1  = (tid+8   <seg_size)?key[k+tid+8   ]:std::numeric_limits<K>::max();\n        rg_k2  = (tid+16  <seg_size)?key[k+tid+16  ]:std::numeric_limits<K>::max();\n        rg_k3  = (tid+24  <seg_size)?key[k+tid+24  ]:std::numeric_limits<K>::max();\n        // sort 32 elements\n        // exch_intxn: switch to exch_local()\n        CMP_SWP_KEY(K,rg_k0 ,rg_k1 );\n        CMP_SWP_KEY(K,rg_k2 ,rg_k3 );\n        // exch_intxn: switch to exch_local()\n        CMP_SWP_KEY(K,rg_k0 ,rg_k3 );\n        CMP_SWP_KEY(K,rg_k1 ,rg_k2 );\n        // exch_paral: switch to exch_local()\n        CMP_SWP_KEY(K,rg_k0 ,rg_k1 );\n        CMP_SWP_KEY(K,rg_k2 ,rg_k3 );\n        // exch_intxn: generate exch_intxn_keys()\n        exch_intxn_keys(rg_k0 ,rg_k1 ,rg_k2 ,rg_k3 ,\n                        0x1,bit1);\n        // exch_paral: switch to exch_local()\n        CMP_SWP_KEY(K,rg_k0 ,rg_k2 );\n        CMP_SWP_KEY(K,rg_k1 ,rg_k3 );\n        // exch_paral: switch to exch_local()\n        CMP_SWP_KEY(K,rg_k0 ,rg_k1 );\n        CMP_SWP_KEY(K,rg_k2 ,rg_k3 );\n        // exch_intxn: generate exch_intxn_keys()\n        exch_intxn_keys(rg_k0 ,rg_k1 ,rg_k2 ,rg_k3 ,\n                        0x3,bit2);\n        // exch_paral: generate exch_paral_keys()\n        exch_paral_keys(rg_k0 ,rg_k1 ,rg_k2 ,rg_k3 ,\n                        0x1,bit1);\n        // exch_paral: switch to exch_local()\n        CMP_SWP_KEY(K,rg_k0 ,rg_k2 );\n        CMP_SWP_KEY(K,rg_k1 ,rg_k3 );\n        // exch_paral: switch to exch_local()\n        CMP_SWP_KEY(K,rg_k0 ,rg_k1 );\n        CMP_SWP_KEY(K,rg_k2 ,rg_k3 );\n        // exch_intxn: generate exch_intxn_keys()\n        exch_intxn_keys(rg_k0 ,rg_k1 ,rg_k2 ,rg_k3 ,\n                        0x7,bit3);\n        // exch_paral: generate exch_paral_keys()\n        exch_paral_keys(rg_k0 ,rg_k1 ,rg_k2 ,rg_k3 ,\n                        0x2,bit2);\n        // exch_paral: generate exch_paral_keys()\n        exch_paral_keys(rg_k0 ,rg_k1 ,rg_k2 ,rg_k3 ,\n                        0x1,bit1);\n        // exch_paral: switch to exch_local()\n        CMP_SWP_KEY(K,rg_k0 ,rg_k2 );\n        CMP_SWP_KEY(K,rg_k1 ,rg_k3 );\n        // exch_paral: switch to exch_local()\n        CMP_SWP_KEY(K,rg_k0 ,rg_k1 );\n        CMP_SWP_KEY(K,rg_k2 ,rg_k3 );\n    }\n\n    if(bin_it < normalized_bin_size) {\n        // store back the results\n        int lane_id = threadIdx.x & 31;\n        rg_k1  = __shfl_xor_sync(0xffffffff,rg_k1 , 0x1 );\n        rg_k3  = __shfl_xor_sync(0xffffffff,rg_k3 , 0x1 );\n        if(lane_id&0x1 ) SWP_KEY(K, rg_k0 , rg_k1 );\n        if(lane_id&0x1 ) SWP_KEY(K, rg_k2 , rg_k3 );\n        rg_k1  = __shfl_xor_sync(0xffffffff,rg_k1 , 0x1 );\n        rg_k3  = __shfl_xor_sync(0xffffffff,rg_k3 , 0x1 );\n        rg_k2  = __shfl_xor_sync(0xffffffff,rg_k2 , 0x2 );\n        rg_k3  = __shfl_xor_sync(0xffffffff,rg_k3 , 0x2 );\n        if(lane_id&0x2 ) SWP_KEY(K, rg_k0 , rg_k2 );\n        if(lane_id&0x2 ) SWP_KEY(K, rg_k1 , rg_k3 );\n        rg_k2  = __shfl_xor_sync(0xffffffff,rg_k2 , 0x2 );\n        rg_k3  = __shfl_xor_sync(0xffffffff,rg_k3 , 0x2 );\n        rg_k1  = __shfl_xor_sync(0xffffffff,rg_k1 , 0x4 );\n        rg_k3  = __shfl_xor_sync(0xffffffff,rg_k3 , 0x4 );\n        if(lane_id&0x4 ) SWP_KEY(K, rg_k0 , rg_k1 );\n        if(lane_id&0x4 ) SWP_KEY(K, rg_k2 , rg_k3 );\n        rg_k1  = __shfl_xor_sync(0xffffffff,rg_k1 , 0x4 );\n        rg_k3  = __shfl_xor_sync(0xffffffff,rg_k3 , 0x4 );\n        rg_k2  = __shfl_xor_sync(0xffffffff,rg_k2 , 0x8 );\n        rg_k3  = __shfl_xor_sync(0xffffffff,rg_k3 , 0x8 );\n        if(lane_id&0x8 ) SWP_KEY(K, rg_k0 , rg_k2 );\n        if(lane_id&0x8 ) SWP_KEY(K, rg_k1 , rg_k3 );\n        rg_k2  = __shfl_xor_sync(0xffffffff,rg_k2 , 0x8 );\n        rg_k3  = __shfl_xor_sync(0xffffffff,rg_k3 , 0x8 );\n        rg_k1  = __shfl_xor_sync(0xffffffff,rg_k1 , 0x10);\n        rg_k3  = __shfl_xor_sync(0xffffffff,rg_k3 , 0x10);\n        if(lane_id&0x10) SWP_KEY(K, rg_k0 , rg_k1 );\n        if(lane_id&0x10) SWP_KEY(K, rg_k2 , rg_k3 );\n        rg_k1  = __shfl_xor_sync(0xffffffff,rg_k1 , 0x10);\n        rg_k3  = __shfl_xor_sync(0xffffffff,rg_k3 , 0x10);\n        int kk;\n        int ss;\n        kk = __shfl_sync(0xffffffff,k, 0 );\n        ss = __shfl_sync(0xffffffff,seg_size, 0 );\n        if(lane_id+0  <ss) keyB[kk+lane_id+0  ] = rg_k0 ;\n        kk = __shfl_sync(0xffffffff,k, 8 );\n        ss = __shfl_sync(0xffffffff,seg_size, 8 );\n        if(lane_id+0  <ss) keyB[kk+lane_id+0  ] = rg_k2 ;\n        kk = __shfl_sync(0xffffffff,k, 16);\n        ss = __shfl_sync(0xffffffff,seg_size, 16);\n        if(lane_id+0  <ss) keyB[kk+lane_id+0  ] = rg_k1 ;\n        kk = __shfl_sync(0xffffffff,k, 24);\n        ss = __shfl_sync(0xffffffff,seg_size, 24);\n        if(lane_id+0  <ss) keyB[kk+lane_id+0  ] = rg_k3 ;\n    } else if(bin_it < bin_size) {\n        if((tid<<2)+0 <seg_size) keyB[k+(tid<<2)+0 ] = rg_k0 ;\n        if((tid<<2)+1 <seg_size) keyB[k+(tid<<2)+1 ] = rg_k1 ;\n        if((tid<<2)+2 <seg_size) keyB[k+(tid<<2)+2 ] = rg_k2 ;\n        if((tid<<2)+3 <seg_size) keyB[k+(tid<<2)+3 ] = rg_k3 ;\n    }\n}",
            "#define K 1.38065e-23        // J/K, Boltzmann constant\n\n\n__device__ inline void exch_intxn_keys(K &k0, K &k1, K &k2, K &k3, K &k4, K &k5, K &k6, K &k7, K &k8, K &k9, K &k10, K &k11, K &k12, K &k13, K &k14, K &k15, K &k16, K &k17, K &k18, K &k19, K &k20, K &k21, K &k22, K &k23, K &k24, K &k25, K &k26, K &k27, K &k28, K &k29, K &k30, K &k31, int mask, const int bit) {\n    K ex_k0, ex_k1;\n    if(bit) SWP_KEY(K, k0, k30);\n    if(bit) SWP_KEY(K, k1, k31);\n    if(bit) SWP_KEY(K, k2, k28);\n    if(bit) SWP_KEY(K, k3, k29);\n    if(bit) SWP_KEY(K, k4, k26);\n    if(bit) SWP_KEY(K, k5, k27);\n    if(bit) SWP_KEY(K, k6, k24);\n    if(bit) SWP_KEY(K, k7, k25);\n    if(bit) SWP_KEY(K, k8, k22);\n    if(bit) SWP_KEY(K, k9, k23);\n    if(bit) SWP_KEY(K, k10, k20);\n    if(bit) SWP_KEY(K, k11, k21);\n    if(bit) SWP_KEY(K, k12, k18);\n    if(bit) SWP_KEY(K, k13, k19);\n    if(bit) SWP_KEY(K, k14, k16);\n    if(bit) SWP_KEY(K, k15, k17);\n    ex_k0 = k0;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k1, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k0 = ex_k0;\n    k1 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k2;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k3, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k2 = ex_k0;\n    k3 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k4;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k5, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k4 = ex_k0;\n    k5 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k6;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k7, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k6 = ex_k0;\n    k7 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k8;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k9, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k8 = ex_k0;\n    k9 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k10;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k11, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k10 = ex_k0;\n    k11 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k12;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k13, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k12 = ex_k0;\n    k13 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k14;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k15, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k14 = ex_k0;\n    k15 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k16;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k17, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k16 = ex_k0;\n    k17 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k18;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k19, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k18 = ex_k0;\n    k19 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k20;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k21, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k20 = ex_k0;\n    k21 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k22;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k23, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k22 = ex_k0;\n    k23 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k24;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k25, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k24 = ex_k0;\n    k25 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k26;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k27, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k26 = ex_k0;\n    k27 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k28;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k29, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k28 = ex_k0;\n    k29 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k30;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k31, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k30 = ex_k0;\n    k31 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    if(bit) SWP_KEY(K, k0, k30);\n    if(bit) SWP_KEY(K, k1, k31);\n    if(bit) SWP_KEY(K, k2, k28);\n    if(bit) SWP_KEY(K, k3, k29);\n    if(bit) SWP_KEY(K, k4, k26);\n    if(bit) SWP_KEY(K, k5, k27);\n    if(bit) SWP_KEY(K, k6, k24);\n    if(bit) SWP_KEY(K, k7, k25);\n    if(bit) SWP_KEY(K, k8, k22);\n    if(bit) SWP_KEY(K, k9, k23);\n    if(bit) SWP_KEY(K, k10, k20);\n    if(bit) SWP_KEY(K, k11, k21);\n    if(bit) SWP_KEY(K, k12, k18);\n    if(bit) SWP_KEY(K, k13, k19);\n    if(bit) SWP_KEY(K, k14, k16);\n    if(bit) SWP_KEY(K, k15, k17);\n}\n\n__device__ inline void exch_paral_keys(K &k0, K &k1, K &k2, K &k3, K &k4, K &k5, K &k6, K &k7, K &k8, K &k9, K &k10, K &k11, K &k12, K &k13, K &k14, K &k15, K &k16, K &k17, K &k18, K &k19, K &k20, K &k21, K &k22, K &k23, K &k24, K &k25, K &k26, K &k27, K &k28, K &k29, K &k30, K &k31, int mask, const int bit) {\n    K ex_k0, ex_k1;\n    if(bit) SWP_KEY(K, k0, k1);\n    if(bit) SWP_KEY(K, k2, k3);\n    if(bit) SWP_KEY(K, k4, k5);\n    if(bit) SWP_KEY(K, k6, k7);\n    if(bit) SWP_KEY(K, k8, k9);\n    if(bit) SWP_KEY(K, k10, k11);\n    if(bit) SWP_KEY(K, k12, k13);\n    if(bit) SWP_KEY(K, k14, k15);\n    if(bit) SWP_KEY(K, k16, k17);\n    if(bit) SWP_KEY(K, k18, k19);\n    if(bit) SWP_KEY(K, k20, k21);\n    if(bit) SWP_KEY(K, k22, k23);\n    if(bit) SWP_KEY(K, k24, k25);\n    if(bit) SWP_KEY(K, k26, k27);\n    if(bit) SWP_KEY(K, k28, k29);\n    if(bit) SWP_KEY(K, k30, k31);\n    ex_k0 = k0;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k1, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k0 = ex_k0;\n    k1 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k2;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k3, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k2 = ex_k0;\n    k3 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k4;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k5, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k4 = ex_k0;\n    k5 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k6;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k7, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k6 = ex_k0;\n    k7 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k8;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k9, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k8 = ex_k0;\n    k9 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k10;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k11, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k10 = ex_k0;\n    k11 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k12;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k13, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k12 = ex_k0;\n    k13 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k14;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k15, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k14 = ex_k0;\n    k15 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k16;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k17, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k16 = ex_k0;\n    k17 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k18;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k19, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k18 = ex_k0;\n    k19 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k20;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k21, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k20 = ex_k0;\n    k21 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k22;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k23, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k22 = ex_k0;\n    k23 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k24;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k25, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k24 = ex_k0;\n    k25 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k26;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k27, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k26 = ex_k0;\n    k27 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k28;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k29, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k28 = ex_k0;\n    k29 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k30;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k31, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k30 = ex_k0;\n    k31 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    if(bit) SWP_KEY(K, k0, k1);\n    if(bit) SWP_KEY(K, k2, k3);\n    if(bit) SWP_KEY(K, k4, k5);\n    if(bit) SWP_KEY(K, k6, k7);\n    if(bit) SWP_KEY(K, k8, k9);\n    if(bit) SWP_KEY(K, k10, k11);\n    if(bit) SWP_KEY(K, k12, k13);\n    if(bit) SWP_KEY(K, k14, k15);\n    if(bit) SWP_KEY(K, k16, k17);\n    if(bit) SWP_KEY(K, k18, k19);\n    if(bit) SWP_KEY(K, k20, k21);\n    if(bit) SWP_KEY(K, k22, k23);\n    if(bit) SWP_KEY(K, k24, k25);\n    if(bit) SWP_KEY(K, k26, k27);\n    if(bit) SWP_KEY(K, k28, k29);\n    if(bit) SWP_KEY(K, k30, k31);\n}\n\n__global__\nvoid gen_bk128_wp16_tc4_r33_r64_strd(\n    K *key, K *keyB,\n    const Offset *seg_begins, const Offset *seg_ends,\n    const int *bin, const int bin_size)\n{\n    const int gid = threadIdx.x + blockIdx.x * blockDim.x;\n    const int bin_it = (gid>>4);\n    const int tid = (threadIdx.x & 15);\n    const int bit1 = (tid>>0)&0x1;\n    const int bit2 = (tid>>1)&0x1;\n    const int bit3 = (tid>>2)&0x1;\n    const int bit4 = (tid>>3)&0x1;\n    K rg_k0 ;\n    K rg_k1 ;\n    K rg_k2 ;\n    K rg_k3 ;\n    int normalized_bin_size = (bin_size/2)*2;\n    int k;\n    int seg_size;\n    if(bin_it < bin_size) {\n        k = seg_begins[bin[bin_it]];\n        seg_size = seg_ends[bin[bin_it]]-seg_begins[bin[bin_it]];\n        rg_k0  = (tid+0   <seg_size)?key[k+tid+0   ]:std::numeric_limits<K>::max();\n        rg_k1  = (tid+16  <seg_size)?key[k+tid+16  ]:std::numeric_limits<K>::max();\n        rg_k2  = (tid+32  <seg_size)?key[k+tid+32  ]:std::numeric_limits<K>::max();\n        rg_k3  = (tid+48  <seg_size)?key[k+tid+48  ]:std::numeric_limits<K>::max();\n        // sort 64 elements\n        // exch_intxn: switch to exch_local()\n        CMP_SWP_KEY(K,rg_k0 ,rg_k1 );\n        CMP_SWP_KEY(K,rg_k2 ,rg_k3 );\n        // exch_intxn: switch to exch_local()\n        CMP_SWP_KEY(K,rg_k0 ,rg_k3 );\n        CMP_SWP_KEY(K,rg_k1 ,rg_k2 );\n        // exch_paral: switch to exch_local()\n        CMP_SWP_KEY(K,rg_k0 ,rg_k1 );\n        CMP_SWP_KEY(K,rg_k2 ,rg_k3 );\n        // exch_intxn: generate exch_intxn_keys()\n        exch_intxn_keys(rg_k0 ,rg_k1 ,rg_k2 ,rg_k3 ,\n                        0x1,bit1);\n        // exch_paral: switch to exch_local()\n        CMP_SWP_KEY(K,rg_k0 ,rg_k2 );\n        CMP_SWP_KEY(K,rg_k1 ,rg_k3 );\n        // exch_paral: switch to exch_local()\n        CMP_SWP_KEY(K,rg_k0 ,rg_k1 );\n        CMP_SWP_KEY(K,rg_k2 ,rg_k3 );\n        // exch_intxn: generate exch_intxn_keys()\n        exch_intxn_keys(rg_k0 ,rg_k1 ,rg_k2 ,rg_k3 ,\n                        0x3,bit2);\n        // exch_paral: generate exch_paral_keys()\n        exch_paral_keys(rg_k0 ,rg_k1 ,rg_k2 ,rg_k3 ,\n                        0x1,bit1);\n        // exch_paral: switch to exch_local()\n        CMP_SWP_KEY(K,rg_k0 ,rg_k2 );\n        CMP_SWP_KEY(K,rg_k1 ,rg_k3 );\n        // exch_paral: switch to exch_local()\n        CMP_SWP_KEY(K,rg_k0 ,rg_k1 );\n        CMP_SWP_KEY(K,rg_k2 ,rg_k3 );\n        // exch_intxn: generate exch_intxn_keys()\n        exch_intxn_keys(rg_k0 ,rg_k1 ,rg_k2 ,rg_k3 ,\n                        0x7,bit3);\n        // exch_paral: generate exch_paral_keys()\n        exch_paral_keys(rg_k0 ,rg_k1 ,rg_k2 ,rg_k3 ,\n                        0x2,bit2);\n        // exch_paral: generate exch_paral_keys()\n        exch_paral_keys(rg_k0 ,rg_k1 ,rg_k2 ,rg_k3 ,\n                        0x1,bit1);\n        // exch_paral: switch to exch_local()\n        CMP_SWP_KEY(K,rg_k0 ,rg_k2 );\n        CMP_SWP_KEY(K,rg_k1 ,rg_k3 );\n        // exch_paral: switch to exch_local()\n        CMP_SWP_KEY(K,rg_k0 ,rg_k1 );\n        CMP_SWP_KEY(K,rg_k2 ,rg_k3 );\n        // exch_intxn: generate exch_intxn_keys()\n        exch_intxn_keys(rg_k0 ,rg_k1 ,rg_k2 ,rg_k3 ,\n                        0xf,bit4);\n        // exch_paral: generate exch_paral_keys()\n        exch_paral_keys(rg_k0 ,rg_k1 ,rg_k2 ,rg_k3 ,\n                        0x4,bit3);\n        // exch_paral: generate exch_paral_keys()\n        exch_paral_keys(rg_k0 ,rg_k1 ,rg_k2 ,rg_k3 ,\n                        0x2,bit2);\n        // exch_paral: generate exch_paral_keys()\n        exch_paral_keys(rg_k0 ,rg_k1 ,rg_k2 ,rg_k3 ,\n                        0x1,bit1);\n        // exch_paral: switch to exch_local()\n        CMP_SWP_KEY(K,rg_k0 ,rg_k2 );\n        CMP_SWP_KEY(K,rg_k1 ,rg_k3 );\n        // exch_paral: switch to exch_local()\n        CMP_SWP_KEY(K,rg_k0 ,rg_k1 );\n        CMP_SWP_KEY(K,rg_k2 ,rg_k3 );\n    }\n\n    if(bin_it < normalized_bin_size) {\n        // store back the results\n        int lane_id = threadIdx.x & 31;\n        rg_k1  = __shfl_xor_sync(0xffffffff,rg_k1 , 0x1 );\n        rg_k3  = __shfl_xor_sync(0xffffffff,rg_k3 , 0x1 );\n        if(lane_id&0x1 ) SWP_KEY(K, rg_k0 , rg_k1 );\n        if(lane_id&0x1 ) SWP_KEY(K, rg_k2 , rg_k3 );\n        rg_k1  = __shfl_xor_sync(0xffffffff,rg_k1 , 0x1 );\n        rg_k3  = __shfl_xor_sync(0xffffffff,rg_k3 , 0x1 );\n        rg_k2  = __shfl_xor_sync(0xffffffff,rg_k2 , 0x2 );\n        rg_k3  = __shfl_xor_sync(0xffffffff,rg_k3 , 0x2 );\n        if(lane_id&0x2 ) SWP_KEY(K, rg_k0 , rg_k2 );\n        if(lane_id&0x2 ) SWP_KEY(K, rg_k1 , rg_k3 );\n        rg_k2  = __shfl_xor_sync(0xffffffff,rg_k2 , 0x2 );\n        rg_k3  = __shfl_xor_sync(0xffffffff,rg_k3 , 0x2 );\n        rg_k1  = __shfl_xor_sync(0xffffffff,rg_k1 , 0x4 );\n        rg_k3  = __shfl_xor_sync(0xffffffff,rg_k3 , 0x4 );\n        if(lane_id&0x4 ) SWP_KEY(K, rg_k0 , rg_k1 );\n        if(lane_id&0x4 ) SWP_KEY(K, rg_k2 , rg_k3 );\n        rg_k1  = __shfl_xor_sync(0xffffffff,rg_k1 , 0x4 );\n        rg_k3  = __shfl_xor_sync(0xffffffff,rg_k3 , 0x4 );\n        rg_k2  = __shfl_xor_sync(0xffffffff,rg_k2 , 0x8 );\n        rg_k3  = __shfl_xor_sync(0xffffffff,rg_k3 , 0x8 );\n        if(lane_id&0x8 ) SWP_KEY(K, rg_k0 , rg_k2 );\n        if(lane_id&0x8 ) SWP_KEY(K, rg_k1 , rg_k3 );\n        rg_k2  = __shfl_xor_sync(0xffffffff,rg_k2 , 0x8 );\n        rg_k3  = __shfl_xor_sync(0xffffffff,rg_k3 , 0x8 );\n        rg_k1  = __shfl_xor_sync(0xffffffff,rg_k1 , 0x10);\n        rg_k3  = __shfl_xor_sync(0xffffffff,rg_k3 , 0x10);\n        if(lane_id&0x10) SWP_KEY(K, rg_k0 , rg_k1 );\n        if(lane_id&0x10) SWP_KEY(K, rg_k2 , rg_k3 );\n        rg_k1  = __shfl_xor_sync(0xffffffff,rg_k1 , 0x10);\n        rg_k3  = __shfl_xor_sync(0xffffffff,rg_k3 , 0x10);\n        int kk;\n        int ss;\n        kk = __shfl_sync(0xffffffff,k, 0 );\n        ss = __shfl_sync(0xffffffff,seg_size, 0 );\n        if(lane_id+0  <ss) keyB[kk+lane_id+0  ] = rg_k0 ;\n        if(lane_id+32 <ss) keyB[kk+lane_id+32 ] = rg_k2 ;\n        kk = __shfl_sync(0xffffffff,k, 16);\n        ss = __shfl_sync(0xffffffff,seg_size, 16);\n        if(lane_id+0  <ss) keyB[kk+lane_id+0  ] = rg_k1 ;\n        if(lane_id+32 <ss) keyB[kk+lane_id+32 ] = rg_k3 ;\n    } else if(bin_it < bin_size) {\n        if((tid<<2)+0 <seg_size) keyB[k+(tid<<2)+0 ] = rg_k0 ;\n        if((tid<<2)+1 <seg_size) keyB[k+(tid<<2)+1 ] = rg_k1 ;\n        if((tid<<2)+2 <seg_size) keyB[k+(tid<<2)+2 ] = rg_k2 ;\n        if((tid<<2)+3 <seg_size) keyB[k+(tid<<2)+3 ] = rg_k3 ;\n    }\n}",
            "#define K 1.38065e-23        // J/K, Boltzmann constant\n\n\n__device__ inline void exch_intxn_keys(K &k0, K &k1, K &k2, K &k3, K &k4, K &k5, K &k6, K &k7, K &k8, K &k9, K &k10, K &k11, K &k12, K &k13, K &k14, K &k15, K &k16, K &k17, K &k18, K &k19, K &k20, K &k21, K &k22, K &k23, K &k24, K &k25, K &k26, K &k27, K &k28, K &k29, K &k30, K &k31, int mask, const int bit) {\n    K ex_k0, ex_k1;\n    if(bit) SWP_KEY(K, k0, k30);\n    if(bit) SWP_KEY(K, k1, k31);\n    if(bit) SWP_KEY(K, k2, k28);\n    if(bit) SWP_KEY(K, k3, k29);\n    if(bit) SWP_KEY(K, k4, k26);\n    if(bit) SWP_KEY(K, k5, k27);\n    if(bit) SWP_KEY(K, k6, k24);\n    if(bit) SWP_KEY(K, k7, k25);\n    if(bit) SWP_KEY(K, k8, k22);\n    if(bit) SWP_KEY(K, k9, k23);\n    if(bit) SWP_KEY(K, k10, k20);\n    if(bit) SWP_KEY(K, k11, k21);\n    if(bit) SWP_KEY(K, k12, k18);\n    if(bit) SWP_KEY(K, k13, k19);\n    if(bit) SWP_KEY(K, k14, k16);\n    if(bit) SWP_KEY(K, k15, k17);\n    ex_k0 = k0;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k1, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k0 = ex_k0;\n    k1 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k2;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k3, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k2 = ex_k0;\n    k3 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k4;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k5, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k4 = ex_k0;\n    k5 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k6;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k7, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k6 = ex_k0;\n    k7 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k8;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k9, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k8 = ex_k0;\n    k9 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k10;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k11, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k10 = ex_k0;\n    k11 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k12;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k13, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k12 = ex_k0;\n    k13 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k14;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k15, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k14 = ex_k0;\n    k15 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k16;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k17, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k16 = ex_k0;\n    k17 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k18;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k19, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k18 = ex_k0;\n    k19 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k20;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k21, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k20 = ex_k0;\n    k21 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k22;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k23, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k22 = ex_k0;\n    k23 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k24;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k25, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k24 = ex_k0;\n    k25 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k26;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k27, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k26 = ex_k0;\n    k27 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k28;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k29, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k28 = ex_k0;\n    k29 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k30;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k31, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k30 = ex_k0;\n    k31 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    if(bit) SWP_KEY(K, k0, k30);\n    if(bit) SWP_KEY(K, k1, k31);\n    if(bit) SWP_KEY(K, k2, k28);\n    if(bit) SWP_KEY(K, k3, k29);\n    if(bit) SWP_KEY(K, k4, k26);\n    if(bit) SWP_KEY(K, k5, k27);\n    if(bit) SWP_KEY(K, k6, k24);\n    if(bit) SWP_KEY(K, k7, k25);\n    if(bit) SWP_KEY(K, k8, k22);\n    if(bit) SWP_KEY(K, k9, k23);\n    if(bit) SWP_KEY(K, k10, k20);\n    if(bit) SWP_KEY(K, k11, k21);\n    if(bit) SWP_KEY(K, k12, k18);\n    if(bit) SWP_KEY(K, k13, k19);\n    if(bit) SWP_KEY(K, k14, k16);\n    if(bit) SWP_KEY(K, k15, k17);\n}\n\n__device__ inline void exch_paral_keys(K &k0, K &k1, K &k2, K &k3, K &k4, K &k5, K &k6, K &k7, K &k8, K &k9, K &k10, K &k11, K &k12, K &k13, K &k14, K &k15, K &k16, K &k17, K &k18, K &k19, K &k20, K &k21, K &k22, K &k23, K &k24, K &k25, K &k26, K &k27, K &k28, K &k29, K &k30, K &k31, int mask, const int bit) {\n    K ex_k0, ex_k1;\n    if(bit) SWP_KEY(K, k0, k1);\n    if(bit) SWP_KEY(K, k2, k3);\n    if(bit) SWP_KEY(K, k4, k5);\n    if(bit) SWP_KEY(K, k6, k7);\n    if(bit) SWP_KEY(K, k8, k9);\n    if(bit) SWP_KEY(K, k10, k11);\n    if(bit) SWP_KEY(K, k12, k13);\n    if(bit) SWP_KEY(K, k14, k15);\n    if(bit) SWP_KEY(K, k16, k17);\n    if(bit) SWP_KEY(K, k18, k19);\n    if(bit) SWP_KEY(K, k20, k21);\n    if(bit) SWP_KEY(K, k22, k23);\n    if(bit) SWP_KEY(K, k24, k25);\n    if(bit) SWP_KEY(K, k26, k27);\n    if(bit) SWP_KEY(K, k28, k29);\n    if(bit) SWP_KEY(K, k30, k31);\n    ex_k0 = k0;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k1, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k0 = ex_k0;\n    k1 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k2;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k3, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k2 = ex_k0;\n    k3 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k4;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k5, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k4 = ex_k0;\n    k5 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k6;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k7, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k6 = ex_k0;\n    k7 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k8;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k9, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k8 = ex_k0;\n    k9 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k10;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k11, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k10 = ex_k0;\n    k11 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k12;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k13, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k12 = ex_k0;\n    k13 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k14;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k15, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k14 = ex_k0;\n    k15 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k16;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k17, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k16 = ex_k0;\n    k17 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k18;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k19, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k18 = ex_k0;\n    k19 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k20;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k21, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k20 = ex_k0;\n    k21 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k22;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k23, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k22 = ex_k0;\n    k23 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k24;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k25, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k24 = ex_k0;\n    k25 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k26;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k27, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k26 = ex_k0;\n    k27 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k28;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k29, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k28 = ex_k0;\n    k29 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k30;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k31, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k30 = ex_k0;\n    k31 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    if(bit) SWP_KEY(K, k0, k1);\n    if(bit) SWP_KEY(K, k2, k3);\n    if(bit) SWP_KEY(K, k4, k5);\n    if(bit) SWP_KEY(K, k6, k7);\n    if(bit) SWP_KEY(K, k8, k9);\n    if(bit) SWP_KEY(K, k10, k11);\n    if(bit) SWP_KEY(K, k12, k13);\n    if(bit) SWP_KEY(K, k14, k15);\n    if(bit) SWP_KEY(K, k16, k17);\n    if(bit) SWP_KEY(K, k18, k19);\n    if(bit) SWP_KEY(K, k20, k21);\n    if(bit) SWP_KEY(K, k22, k23);\n    if(bit) SWP_KEY(K, k24, k25);\n    if(bit) SWP_KEY(K, k26, k27);\n    if(bit) SWP_KEY(K, k28, k29);\n    if(bit) SWP_KEY(K, k30, k31);\n}\n\n__global__\nvoid gen_bk256_wp8_tc16_r65_r128_strd(\n    K *key, K *keyB,\n    const Offset *seg_begins, const Offset *seg_ends,\n    const int *bin, const int bin_size)\n{\n    const int gid = threadIdx.x + blockIdx.x * blockDim.x;\n    const int bin_it = (gid>>3);\n    const int tid = (threadIdx.x & 7);\n    const int bit1 = (tid>>0)&0x1;\n    const int bit2 = (tid>>1)&0x1;\n    const int bit3 = (tid>>2)&0x1;\n    K rg_k0 ;\n    K rg_k1 ;\n    K rg_k2 ;\n    K rg_k3 ;\n    K rg_k4 ;\n    K rg_k5 ;\n    K rg_k6 ;\n    K rg_k7 ;\n    K rg_k8 ;\n    K rg_k9 ;\n    K rg_k10;\n    K rg_k11;\n    K rg_k12;\n    K rg_k13;\n    K rg_k14;\n    K rg_k15;\n    int normalized_bin_size = (bin_size/4)*4;\n    int k;\n    int seg_size;\n    if(bin_it < bin_size) {\n        k = seg_begins[bin[bin_it]];\n        seg_size = seg_ends[bin[bin_it]]-seg_begins[bin[bin_it]];\n        rg_k0  = (tid+0   <seg_size)?key[k+tid+0   ]:std::numeric_limits<K>::max();\n        rg_k1  = (tid+8   <seg_size)?key[k+tid+8   ]:std::numeric_limits<K>::max();\n        rg_k2  = (tid+16  <seg_size)?key[k+tid+16  ]:std::numeric_limits<K>::max();\n        rg_k3  = (tid+24  <seg_size)?key[k+tid+24  ]:std::numeric_limits<K>::max();\n        rg_k4  = (tid+32  <seg_size)?key[k+tid+32  ]:std::numeric_limits<K>::max();\n        rg_k5  = (tid+40  <seg_size)?key[k+tid+40  ]:std::numeric_limits<K>::max();\n        rg_k6  = (tid+48  <seg_size)?key[k+tid+48  ]:std::numeric_limits<K>::max();\n        rg_k7  = (tid+56  <seg_size)?key[k+tid+56  ]:std::numeric_limits<K>::max();\n        rg_k8  = (tid+64  <seg_size)?key[k+tid+64  ]:std::numeric_limits<K>::max();\n        rg_k9  = (tid+72  <seg_size)?key[k+tid+72  ]:std::numeric_limits<K>::max();\n        rg_k10 = (tid+80  <seg_size)?key[k+tid+80  ]:std::numeric_limits<K>::max();\n        rg_k11 = (tid+88  <seg_size)?key[k+tid+88  ]:std::numeric_limits<K>::max();\n        rg_k12 = (tid+96  <seg_size)?key[k+tid+96  ]:std::numeric_limits<K>::max();\n        rg_k13 = (tid+104 <seg_size)?key[k+tid+104 ]:std::numeric_limits<K>::max();\n        rg_k14 = (tid+112 <seg_size)?key[k+tid+112 ]:std::numeric_limits<K>::max();\n        rg_k15 = (tid+120 <seg_size)?key[k+tid+120 ]:std::numeric_limits<K>::max();\n        // sort 128 elements\n        // exch_intxn: switch to exch_local()\n        CMP_SWP_KEY(K,rg_k0 ,rg_k1 );\n        CMP_SWP_KEY(K,rg_k2 ,rg_k3 );\n        CMP_SWP_KEY(K,rg_k4 ,rg_k5 );\n        CMP_SWP_KEY(K,rg_k6 ,rg_k7 );\n        CMP_SWP_KEY(K,rg_k8 ,rg_k9 );\n        CMP_SWP_KEY(K,rg_k10,rg_k11);\n        CMP_SWP_KEY(K,rg_k12,rg_k13);\n        CMP_SWP_KEY(K,rg_k14,rg_k15);\n        // exch_intxn: switch to exch_local()\n        CMP_SWP_KEY(K,rg_k0 ,rg_k3 );\n        CMP_SWP_KEY(K,rg_k1 ,rg_k2 );\n        CMP_SWP_KEY(K,rg_k4 ,rg_k7 );\n        CMP_SWP_KEY(K,rg_k5 ,rg_k6 );\n        CMP_SWP_KEY(K,rg_k8 ,rg_k11);\n        CMP_SWP_KEY(K,rg_k9 ,rg_k10);\n        CMP_SWP_KEY(K,rg_k12,rg_k15);\n        CMP_SWP_KEY(K,rg_k13,rg_k14);\n        // exch_paral: switch to exch_local()\n        CMP_SWP_KEY(K,rg_k0 ,rg_k1 );\n        CMP_SWP_KEY(K,rg_k2 ,rg_k3 );\n        CMP_SWP_KEY(K,rg_k4 ,rg_k5 );\n        CMP_SWP_KEY(K,rg_k6 ,rg_k7 );\n        CMP_SWP_KEY(K,rg_k8 ,rg_k9 );\n        CMP_SWP_KEY(K,rg_k10,rg_k11);\n        CMP_SWP_KEY(K,rg_k12,rg_k13);\n        CMP_SWP_KEY(K,rg_k14,rg_k15);\n        // exch_intxn: switch to exch_local()\n        CMP_SWP_KEY(K,rg_k0 ,rg_k7 );\n        CMP_SWP_KEY(K,rg_k1 ,rg_k6 );\n        CMP_SWP_KEY(K,rg_k2 ,rg_k5 );\n        CMP_SWP_KEY(K,rg_k3 ,rg_k4 );\n        CMP_SWP_KEY(K,rg_k8 ,rg_k15);\n        CMP_SWP_KEY(K,rg_k9 ,rg_k14);\n        CMP_SWP_KEY(K,rg_k10,rg_k13);\n        CMP_SWP_KEY(K,rg_k11,rg_k12);\n        // exch_paral: switch to exch_local()\n        CMP_SWP_KEY(K,rg_k0 ,rg_k2 );\n        CMP_SWP_KEY(K,rg_k1 ,rg_k3 );\n        CMP_SWP_KEY(K,rg_k4 ,rg_k6 );\n        CMP_SWP_KEY(K,rg_k5 ,rg_k7 );\n        CMP_SWP_KEY(K,rg_k8 ,rg_k10);\n        CMP_SWP_KEY(K,rg_k9 ,rg_k11);\n        CMP_SWP_KEY(K,rg_k12,rg_k14);\n        CMP_SWP_KEY(K,rg_k13,rg_k15);\n        // exch_paral: switch to exch_local()\n        CMP_SWP_KEY(K,rg_k0 ,rg_k1 );\n        CMP_SWP_KEY(K,rg_k2 ,rg_k3 );\n        CMP_SWP_KEY(K,rg_k4 ,rg_k5 );\n        CMP_SWP_KEY(K,rg_k6 ,rg_k7 );\n        CMP_SWP_KEY(K,rg_k8 ,rg_k9 );\n        CMP_SWP_KEY(K,rg_k10,rg_k11);\n        CMP_SWP_KEY(K,rg_k12,rg_k13);\n        CMP_SWP_KEY(K,rg_k14,rg_k15);\n        // exch_intxn: switch to exch_local()\n        CMP_SWP_KEY(K,rg_k0 ,rg_k15);\n        CMP_SWP_KEY(K,rg_k1 ,rg_k14);\n        CMP_SWP_KEY(K,rg_k2 ,rg_k13);\n        CMP_SWP_KEY(K,rg_k3 ,rg_k12);\n        CMP_SWP_KEY(K,rg_k4 ,rg_k11);\n        CMP_SWP_KEY(K,rg_k5 ,rg_k10);\n        CMP_SWP_KEY(K,rg_k6 ,rg_k9 );\n        CMP_SWP_KEY(K,rg_k7 ,rg_k8 );\n        // exch_paral: switch to exch_local()\n        CMP_SWP_KEY(K,rg_k0 ,rg_k4 );\n        CMP_SWP_KEY(K,rg_k1 ,rg_k5 );\n        CMP_SWP_KEY(K,rg_k2 ,rg_k6 );\n        CMP_SWP_KEY(K,rg_k3 ,rg_k7 );\n        CMP_SWP_KEY(K,rg_k8 ,rg_k12);\n        CMP_SWP_KEY(K,rg_k9 ,rg_k13);\n        CMP_SWP_KEY(K,rg_k10,rg_k14);\n        CMP_SWP_KEY(K,rg_k11,rg_k15);\n        // exch_paral: switch to exch_local()\n        CMP_SWP_KEY(K,rg_k0 ,rg_k2 );\n        CMP_SWP_KEY(K,rg_k1 ,rg_k3 );\n        CMP_SWP_KEY(K,rg_k4 ,rg_k6 );\n        CMP_SWP_KEY(K,rg_k5 ,rg_k7 );\n        CMP_SWP_KEY(K,rg_k8 ,rg_k10);\n        CMP_SWP_KEY(K,rg_k9 ,rg_k11);\n        CMP_SWP_KEY(K,rg_k12,rg_k14);\n        CMP_SWP_KEY(K,rg_k13,rg_k15);\n        // exch_paral: switch to exch_local()\n        CMP_SWP_KEY(K,rg_k0 ,rg_k1 );\n        CMP_SWP_KEY(K,rg_k2 ,rg_k3 );\n        CMP_SWP_KEY(K,rg_k4 ,rg_k5 );\n        CMP_SWP_KEY(K,rg_k6 ,rg_k7 );\n        CMP_SWP_KEY(K,rg_k8 ,rg_k9 );\n        CMP_SWP_KEY(K,rg_k10,rg_k11);\n        CMP_SWP_KEY(K,rg_k12,rg_k13);\n        CMP_SWP_KEY(K,rg_k14,rg_k15);\n        // exch_intxn: generate exch_intxn_keys()\n        exch_intxn_keys(rg_k0 ,rg_k1 ,rg_k2 ,rg_k3 ,rg_k4 ,rg_k5 ,rg_k6 ,rg_k7 ,\n                        rg_k8 ,rg_k9 ,rg_k10,rg_k11,rg_k12,rg_k13,rg_k14,rg_k15,\n                        0x1,bit1);\n        // exch_paral: switch to exch_local()\n        CMP_SWP_KEY(K,rg_k0 ,rg_k8 );\n        CMP_SWP_KEY(K,rg_k1 ,rg_k9 );\n        CMP_SWP_KEY(K,rg_k2 ,rg_k10);\n        CMP_SWP_KEY(K,rg_k3 ,rg_k11);\n        CMP_SWP_KEY(K,rg_k4 ,rg_k12);\n        CMP_SWP_KEY(K,rg_k5 ,rg_k13);\n        CMP_SWP_KEY(K,rg_k6 ,rg_k14);\n        CMP_SWP_KEY(K,rg_k7 ,rg_k15);\n        // exch_paral: switch to exch_local()\n        CMP_SWP_KEY(K,rg_k0 ,rg_k4 );\n        CMP_SWP_KEY(K,rg_k1 ,rg_k5 );\n        CMP_SWP_KEY(K,rg_k2 ,rg_k6 );\n        CMP_SWP_KEY(K,rg_k3 ,rg_k7 );\n        CMP_SWP_KEY(K,rg_k8 ,rg_k12);\n        CMP_SWP_KEY(K,rg_k9 ,rg_k13);\n        CMP_SWP_KEY(K,rg_k10,rg_k14);\n        CMP_SWP_KEY(K,rg_k11,rg_k15);\n        // exch_paral: switch to exch_local()\n        CMP_SWP_KEY(K,rg_k0 ,rg_k2 );\n        CMP_SWP_KEY(K,rg_k1 ,rg_k3 );\n        CMP_SWP_KEY(K,rg_k4 ,rg_k6 );\n        CMP_SWP_KEY(K,rg_k5 ,rg_k7 );\n        CMP_SWP_KEY(K,rg_k8 ,rg_k10);\n        CMP_SWP_KEY(K,rg_k9 ,rg_k11);\n        CMP_SWP_KEY(K,rg_k12,rg_k14);\n        CMP_SWP_KEY(K,rg_k13,rg_k15);\n        // exch_paral: switch to exch_local()\n        CMP_SWP_KEY(K,rg_k0 ,rg_k1 );\n        CMP_SWP_KEY(K,rg_k2 ,rg_k3 );\n        CMP_SWP_KEY(K,rg_k4 ,rg_k5 );\n        CMP_SWP_KEY(K,rg_k6 ,rg_k7 );\n        CMP_SWP_KEY(K,rg_k8 ,rg_k9 );\n        CMP_SWP_KEY(K,rg_k10,rg_k11);\n        CMP_SWP_KEY(K,rg_k12,rg_k13);\n        CMP_SWP_KEY(K,rg_k14,rg_k15);\n        // exch_intxn: generate exch_intxn_keys()\n        exch_intxn_keys(rg_k0 ,rg_k1 ,rg_k2 ,rg_k3 ,rg_k4 ,rg_k5 ,rg_k6 ,rg_k7 ,\n                        rg_k8 ,rg_k9 ,rg_k10,rg_k11,rg_k12,rg_k13,rg_k14,rg_k15,\n                        0x3,bit2);\n        // exch_paral: generate exch_paral_keys()\n        exch_paral_keys(rg_k0 ,rg_k1 ,rg_k2 ,rg_k3 ,rg_k4 ,rg_k5 ,rg_k6 ,rg_k7 ,\n                        rg_k8 ,rg_k9 ,rg_k10,rg_k11,rg_k12,rg_k13,rg_k14,rg_k15,\n                        0x1,bit1);\n        // exch_paral: switch to exch_local()\n        CMP_SWP_KEY(K,rg_k0 ,rg_k8 );\n        CMP_SWP_KEY(K,rg_k1 ,rg_k9 );\n        CMP_SWP_KEY(K,rg_k2 ,rg_k10);\n        CMP_SWP_KEY(K,rg_k3 ,rg_k11);\n        CMP_SWP_KEY(K,rg_k4 ,rg_k12);\n        CMP_SWP_KEY(K,rg_k5 ,rg_k13);\n        CMP_SWP_KEY(K,rg_k6 ,rg_k14);\n        CMP_SWP_KEY(K,rg_k7 ,rg_k15);\n        // exch_paral: switch to exch_local()\n        CMP_SWP_KEY(K,rg_k0 ,rg_k4 );\n        CMP_SWP_KEY(K,rg_k1 ,rg_k5 );\n        CMP_SWP_KEY(K,rg_k2 ,rg_k6 );\n        CMP_SWP_KEY(K,rg_k3 ,rg_k7 );\n        CMP_SWP_KEY(K,rg_k8 ,rg_k12);\n        CMP_SWP_KEY(K,rg_k9 ,rg_k13);\n        CMP_SWP_KEY(K,rg_k10,rg_k14);\n        CMP_SWP_KEY(K,rg_k11,rg_k15);\n        // exch_paral: switch to exch_local()\n        CMP_SWP_KEY(K,rg_k0 ,rg_k2 );\n        CMP_SWP_KEY(K,rg_k1 ,rg_k3 );\n        CMP_SWP_KEY(K,rg_k4 ,rg_k6 );\n        CMP_SWP_KEY(K,rg_k5 ,rg_k7 );\n        CMP_SWP_KEY(K,rg_k8 ,rg_k10);\n        CMP_SWP_KEY(K,rg_k9 ,rg_k11);\n        CMP_SWP_KEY(K,rg_k12,rg_k14);\n        CMP_SWP_KEY(K,rg_k13,rg_k15);\n        // exch_paral: switch to exch_local()\n        CMP_SWP_KEY(K,rg_k0 ,rg_k1 );\n        CMP_SWP_KEY(K,rg_k2 ,rg_k3 );\n        CMP_SWP_KEY(K,rg_k4 ,rg_k5 );\n        CMP_SWP_KEY(K,rg_k6 ,rg_k7 );\n        CMP_SWP_KEY(K,rg_k8 ,rg_k9 );\n        CMP_SWP_KEY(K,rg_k10,rg_k11);\n        CMP_SWP_KEY(K,rg_k12,rg_k13);\n        CMP_SWP_KEY(K,rg_k14,rg_k15);\n        // exch_intxn: generate exch_intxn_keys()\n        exch_intxn_keys(rg_k0 ,rg_k1 ,rg_k2 ,rg_k3 ,rg_k4 ,rg_k5 ,rg_k6 ,rg_k7 ,\n                        rg_k8 ,rg_k9 ,rg_k10,rg_k11,rg_k12,rg_k13,rg_k14,rg_k15,\n                        0x7,bit3);\n        // exch_paral: generate exch_paral_keys()\n        exch_paral_keys(rg_k0 ,rg_k1 ,rg_k2 ,rg_k3 ,rg_k4 ,rg_k5 ,rg_k6 ,rg_k7 ,\n                        rg_k8 ,rg_k9 ,rg_k10,rg_k11,rg_k12,rg_k13,rg_k14,rg_k15,\n                        0x2,bit2);\n        // exch_paral: generate exch_paral_keys()\n        exch_paral_keys(rg_k0 ,rg_k1 ,rg_k2 ,rg_k3 ,rg_k4 ,rg_k5 ,rg_k6 ,rg_k7 ,\n                        rg_k8 ,rg_k9 ,rg_k10,rg_k11,rg_k12,rg_k13,rg_k14,rg_k15,\n                        0x1,bit1);\n        // exch_paral: switch to exch_local()\n        CMP_SWP_KEY(K,rg_k0 ,rg_k8 );\n        CMP_SWP_KEY(K,rg_k1 ,rg_k9 );\n        CMP_SWP_KEY(K,rg_k2 ,rg_k10);\n        CMP_SWP_KEY(K,rg_k3 ,rg_k11);\n        CMP_SWP_KEY(K,rg_k4 ,rg_k12);\n        CMP_SWP_KEY(K,rg_k5 ,rg_k13);\n        CMP_SWP_KEY(K,rg_k6 ,rg_k14);\n        CMP_SWP_KEY(K,rg_k7 ,rg_k15);\n        // exch_paral: switch to exch_local()\n        CMP_SWP_KEY(K,rg_k0 ,rg_k4 );\n        CMP_SWP_KEY(K,rg_k1 ,rg_k5 );\n        CMP_SWP_KEY(K,rg_k2 ,rg_k6 );\n        CMP_SWP_KEY(K,rg_k3 ,rg_k7 );\n        CMP_SWP_KEY(K,rg_k8 ,rg_k12);\n        CMP_SWP_KEY(K,rg_k9 ,rg_k13);\n        CMP_SWP_KEY(K,rg_k10,rg_k14);\n        CMP_SWP_KEY(K,rg_k11,rg_k15);\n        // exch_paral: switch to exch_local()\n        CMP_SWP_KEY(K,rg_k0 ,rg_k2 );\n        CMP_SWP_KEY(K,rg_k1 ,rg_k3 );\n        CMP_SWP_KEY(K,rg_k4 ,rg_k6 );\n        CMP_SWP_KEY(K,rg_k5 ,rg_k7 );\n        CMP_SWP_KEY(K,rg_k8 ,rg_k10);\n        CMP_SWP_KEY(K,rg_k9 ,rg_k11);\n        CMP_SWP_KEY(K,rg_k12,rg_k14);\n        CMP_SWP_KEY(K,rg_k13,rg_k15);\n        // exch_paral: switch to exch_local()\n        CMP_SWP_KEY(K,rg_k0 ,rg_k1 );\n        CMP_SWP_KEY(K,rg_k2 ,rg_k3 );\n        CMP_SWP_KEY(K,rg_k4 ,rg_k5 );\n        CMP_SWP_KEY(K,rg_k6 ,rg_k7 );\n        CMP_SWP_KEY(K,rg_k8 ,rg_k9 );\n        CMP_SWP_KEY(K,rg_k10,rg_k11);\n        CMP_SWP_KEY(K,rg_k12,rg_k13);\n        CMP_SWP_KEY(K,rg_k14,rg_k15);\n    }\n\n    if(bin_it < normalized_bin_size) {\n        // store back the results\n        int lane_id = threadIdx.x & 31;\n        rg_k1  = __shfl_xor_sync(0xffffffff,rg_k1 , 0x1 );\n        rg_k3  = __shfl_xor_sync(0xffffffff,rg_k3 , 0x1 );\n        rg_k5  = __shfl_xor_sync(0xffffffff,rg_k5 , 0x1 );\n        rg_k7  = __shfl_xor_sync(0xffffffff,rg_k7 , 0x1 );\n        rg_k9  = __shfl_xor_sync(0xffffffff,rg_k9 , 0x1 );\n        rg_k11 = __shfl_xor_sync(0xffffffff,rg_k11, 0x1 );\n        rg_k13 = __shfl_xor_sync(0xffffffff,rg_k13, 0x1 );\n        rg_k15 = __shfl_xor_sync(0xffffffff,rg_k15, 0x1 );\n        if(lane_id&0x1 ) SWP_KEY(K, rg_k0 , rg_k1 );\n        if(lane_id&0x1 ) SWP_KEY(K, rg_k2 , rg_k3 );\n        if(lane_id&0x1 ) SWP_KEY(K, rg_k4 , rg_k5 );\n        if(lane_id&0x1 ) SWP_KEY(K, rg_k6 , rg_k7 );\n        if(lane_id&0x1 ) SWP_KEY(K, rg_k8 , rg_k9 );\n        if(lane_id&0x1 ) SWP_KEY(K, rg_k10, rg_k11);\n        if(lane_id&0x1 ) SWP_KEY(K, rg_k12, rg_k13);\n        if(lane_id&0x1 ) SWP_KEY(K, rg_k14, rg_k15);\n        rg_k1  = __shfl_xor_sync(0xffffffff,rg_k1 , 0x1 );\n        rg_k3  = __shfl_xor_sync(0xffffffff,rg_k3 , 0x1 );\n        rg_k5  = __shfl_xor_sync(0xffffffff,rg_k5 , 0x1 );\n        rg_k7  = __shfl_xor_sync(0xffffffff,rg_k7 , 0x1 );\n        rg_k9  = __shfl_xor_sync(0xffffffff,rg_k9 , 0x1 );\n        rg_k11 = __shfl_xor_sync(0xffffffff,rg_k11, 0x1 );\n        rg_k13 = __shfl_xor_sync(0xffffffff,rg_k13, 0x1 );\n        rg_k15 = __shfl_xor_sync(0xffffffff,rg_k15, 0x1 );\n        rg_k2  = __shfl_xor_sync(0xffffffff,rg_k2 , 0x2 );\n        rg_k3  = __shfl_xor_sync(0xffffffff,rg_k3 , 0x2 );\n        rg_k6  = __shfl_xor_sync(0xffffffff,rg_k6 , 0x2 );\n        rg_k7  = __shfl_xor_sync(0xffffffff,rg_k7 , 0x2 );\n        rg_k10 = __shfl_xor_sync(0xffffffff,rg_k10, 0x2 );\n        rg_k11 = __shfl_xor_sync(0xffffffff,rg_k11, 0x2 );\n        rg_k14 = __shfl_xor_sync(0xffffffff,rg_k14, 0x2 );\n        rg_k15 = __shfl_xor_sync(0xffffffff,rg_k15, 0x2 );\n        if(lane_id&0x2 ) SWP_KEY(K, rg_k0 , rg_k2 );\n        if(lane_id&0x2 ) SWP_KEY(K, rg_k1 , rg_k3 );\n        if(lane_id&0x2 ) SWP_KEY(K, rg_k4 , rg_k6 );\n        if(lane_id&0x2 ) SWP_KEY(K, rg_k5 , rg_k7 );\n        if(lane_id&0x2 ) SWP_KEY(K, rg_k8 , rg_k10);\n        if(lane_id&0x2 ) SWP_KEY(K, rg_k9 , rg_k11);\n        if(lane_id&0x2 ) SWP_KEY(K, rg_k12, rg_k14);\n        if(lane_id&0x2 ) SWP_KEY(K, rg_k13, rg_k15);\n        rg_k2  = __shfl_xor_sync(0xffffffff,rg_k2 , 0x2 );\n        rg_k3  = __shfl_xor_sync(0xffffffff,rg_k3 , 0x2 );\n        rg_k6  = __shfl_xor_sync(0xffffffff,rg_k6 , 0x2 );\n        rg_k7  = __shfl_xor_sync(0xffffffff,rg_k7 , 0x2 );\n        rg_k10 = __shfl_xor_sync(0xffffffff,rg_k10, 0x2 );\n        rg_k11 = __shfl_xor_sync(0xffffffff,rg_k11, 0x2 );\n        rg_k14 = __shfl_xor_sync(0xffffffff,rg_k14, 0x2 );\n        rg_k15 = __shfl_xor_sync(0xffffffff,rg_k15, 0x2 );\n        rg_k4  = __shfl_xor_sync(0xffffffff,rg_k4 , 0x4 );\n        rg_k5  = __shfl_xor_sync(0xffffffff,rg_k5 , 0x4 );\n        rg_k6  = __shfl_xor_sync(0xffffffff,rg_k6 , 0x4 );\n        rg_k7  = __shfl_xor_sync(0xffffffff,rg_k7 , 0x4 );\n        rg_k12 = __shfl_xor_sync(0xffffffff,rg_k12, 0x4 );\n        rg_k13 = __shfl_xor_sync(0xffffffff,rg_k13, 0x4 );\n        rg_k14 = __shfl_xor_sync(0xffffffff,rg_k14, 0x4 );\n        rg_k15 = __shfl_xor_sync(0xffffffff,rg_k15, 0x4 );\n        if(lane_id&0x4 ) SWP_KEY(K, rg_k0 , rg_k4 );\n        if(lane_id&0x4 ) SWP_KEY(K, rg_k1 , rg_k5 );\n        if(lane_id&0x4 ) SWP_KEY(K, rg_k2 , rg_k6 );\n        if(lane_id&0x4 ) SWP_KEY(K, rg_k3 , rg_k7 );\n        if(lane_id&0x4 ) SWP_KEY(K, rg_k8 , rg_k12);\n        if(lane_id&0x4 ) SWP_KEY(K, rg_k9 , rg_k13);\n        if(lane_id&0x4 ) SWP_KEY(K, rg_k10, rg_k14);\n        if(lane_id&0x4 ) SWP_KEY(K, rg_k11, rg_k15);\n        rg_k4  = __shfl_xor_sync(0xffffffff,rg_k4 , 0x4 );\n        rg_k5  = __shfl_xor_sync(0xffffffff,rg_k5 , 0x4 );\n        rg_k6  = __shfl_xor_sync(0xffffffff,rg_k6 , 0x4 );\n        rg_k7  = __shfl_xor_sync(0xffffffff,rg_k7 , 0x4 );\n        rg_k12 = __shfl_xor_sync(0xffffffff,rg_k12, 0x4 );\n        rg_k13 = __shfl_xor_sync(0xffffffff,rg_k13, 0x4 );\n        rg_k14 = __shfl_xor_sync(0xffffffff,rg_k14, 0x4 );\n        rg_k15 = __shfl_xor_sync(0xffffffff,rg_k15, 0x4 );\n        rg_k8  = __shfl_xor_sync(0xffffffff,rg_k8 , 0x8 );\n        rg_k9  = __shfl_xor_sync(0xffffffff,rg_k9 , 0x8 );\n        rg_k10 = __shfl_xor_sync(0xffffffff,rg_k10, 0x8 );\n        rg_k11 = __shfl_xor_sync(0xffffffff,rg_k11, 0x8 );\n        rg_k12 = __shfl_xor_sync(0xffffffff,rg_k12, 0x8 );\n        rg_k13 = __shfl_xor_sync(0xffffffff,rg_k13, 0x8 );\n        rg_k14 = __shfl_xor_sync(0xffffffff,rg_k14, 0x8 );\n        rg_k15 = __shfl_xor_sync(0xffffffff,rg_k15, 0x8 );\n        if(lane_id&0x8 ) SWP_KEY(K, rg_k0 , rg_k8 );\n        if(lane_id&0x8 ) SWP_KEY(K, rg_k1 , rg_k9 );\n        if(lane_id&0x8 ) SWP_KEY(K, rg_k2 , rg_k10);\n        if(lane_id&0x8 ) SWP_KEY(K, rg_k3 , rg_k11);\n        if(lane_id&0x8 ) SWP_KEY(K, rg_k4 , rg_k12);\n        if(lane_id&0x8 ) SWP_KEY(K, rg_k5 , rg_k13);\n        if(lane_id&0x8 ) SWP_KEY(K, rg_k6 , rg_k14);\n        if(lane_id&0x8 ) SWP_KEY(K, rg_k7 , rg_k15);\n        rg_k8  = __shfl_xor_sync(0xffffffff,rg_k8 , 0x8 );\n        rg_k9  = __shfl_xor_sync(0xffffffff,rg_k9 , 0x8 );\n        rg_k10 = __shfl_xor_sync(0xffffffff,rg_k10, 0x8 );\n        rg_k11 = __shfl_xor_sync(0xffffffff,rg_k11, 0x8 );\n        rg_k12 = __shfl_xor_sync(0xffffffff,rg_k12, 0x8 );\n        rg_k13 = __shfl_xor_sync(0xffffffff,rg_k13, 0x8 );\n        rg_k14 = __shfl_xor_sync(0xffffffff,rg_k14, 0x8 );\n        rg_k15 = __shfl_xor_sync(0xffffffff,rg_k15, 0x8 );\n        rg_k1  = __shfl_xor_sync(0xffffffff,rg_k1 , 0x10);\n        rg_k3  = __shfl_xor_sync(0xffffffff,rg_k3 , 0x10);\n        rg_k5  = __shfl_xor_sync(0xffffffff,rg_k5 , 0x10);\n        rg_k7  = __shfl_xor_sync(0xffffffff,rg_k7 , 0x10);\n        rg_k9  = __shfl_xor_sync(0xffffffff,rg_k9 , 0x10);\n        rg_k11 = __shfl_xor_sync(0xffffffff,rg_k11, 0x10);\n        rg_k13 = __shfl_xor_sync(0xffffffff,rg_k13, 0x10);\n        rg_k15 = __shfl_xor_sync(0xffffffff,rg_k15, 0x10);\n        if(lane_id&0x10) SWP_KEY(K, rg_k0 , rg_k1 );\n        if(lane_id&0x10) SWP_KEY(K, rg_k2 , rg_k3 );\n        if(lane_id&0x10) SWP_KEY(K, rg_k4 , rg_k5 );\n        if(lane_id&0x10) SWP_KEY(K, rg_k6 , rg_k7 );\n        if(lane_id&0x10) SWP_KEY(K, rg_k8 , rg_k9 );\n        if(lane_id&0x10) SWP_KEY(K, rg_k10, rg_k11);\n        if(lane_id&0x10) SWP_KEY(K, rg_k12, rg_k13);\n        if(lane_id&0x10) SWP_KEY(K, rg_k14, rg_k15);\n        rg_k1  = __shfl_xor_sync(0xffffffff,rg_k1 , 0x10);\n        rg_k3  = __shfl_xor_sync(0xffffffff,rg_k3 , 0x10);\n        rg_k5  = __shfl_xor_sync(0xffffffff,rg_k5 , 0x10);\n        rg_k7  = __shfl_xor_sync(0xffffffff,rg_k7 , 0x10);\n        rg_k9  = __shfl_xor_sync(0xffffffff,rg_k9 , 0x10);\n        rg_k11 = __shfl_xor_sync(0xffffffff,rg_k11, 0x10);\n        rg_k13 = __shfl_xor_sync(0xffffffff,rg_k13, 0x10);\n        rg_k15 = __shfl_xor_sync(0xffffffff,rg_k15, 0x10);\n        int kk;\n        int ss;\n        kk = __shfl_sync(0xffffffff,k, 0 );\n        ss = __shfl_sync(0xffffffff,seg_size, 0 );\n        if(lane_id+0  <ss) keyB[kk+lane_id+0  ] = rg_k0 ;\n        if(lane_id+32 <ss) keyB[kk+lane_id+32 ] = rg_k2 ;\n        if(lane_id+64 <ss) keyB[kk+lane_id+64 ] = rg_k4 ;\n        if(lane_id+96 <ss) keyB[kk+lane_id+96 ] = rg_k6 ;\n        kk = __shfl_sync(0xffffffff,k, 8 );\n        ss = __shfl_sync(0xffffffff,seg_size, 8 );\n        if(lane_id+0  <ss) keyB[kk+lane_id+0  ] = rg_k8 ;\n        if(lane_id+32 <ss) keyB[kk+lane_id+32 ] = rg_k10;\n        if(lane_id+64 <ss) keyB[kk+lane_id+64 ] = rg_k12;\n        if(lane_id+96 <ss) keyB[kk+lane_id+96 ] = rg_k14;\n        kk = __shfl_sync(0xffffffff,k, 16);\n        ss = __shfl_sync(0xffffffff,seg_size, 16);\n        if(lane_id+0  <ss) keyB[kk+lane_id+0  ] = rg_k1 ;\n        if(lane_id+32 <ss) keyB[kk+lane_id+32 ] = rg_k3 ;\n        if(lane_id+64 <ss) keyB[kk+lane_id+64 ] = rg_k5 ;\n        if(lane_id+96 <ss) keyB[kk+lane_id+96 ] = rg_k7 ;\n        kk = __shfl_sync(0xffffffff,k, 24);\n        ss = __shfl_sync(0xffffffff,seg_size, 24);\n        if(lane_id+0  <ss) keyB[kk+lane_id+0  ] = rg_k9 ;\n        if(lane_id+32 <ss) keyB[kk+lane_id+32 ] = rg_k11;\n        if(lane_id+64 <ss) keyB[kk+lane_id+64 ] = rg_k13;\n        if(lane_id+96 <ss) keyB[kk+lane_id+96 ] = rg_k15;\n    } else if(bin_it < bin_size) {\n        if((tid<<4)+0 <seg_size) keyB[k+(tid<<4)+0 ] = rg_k0 ;\n        if((tid<<4)+1 <seg_size) keyB[k+(tid<<4)+1 ] = rg_k1 ;\n        if((tid<<4)+2 <seg_size) keyB[k+(tid<<4)+2 ] = rg_k2 ;\n        if((tid<<4)+3 <seg_size) keyB[k+(tid<<4)+3 ] = rg_k3 ;\n        if((tid<<4)+4 <seg_size) keyB[k+(tid<<4)+4 ] = rg_k4 ;\n        if((tid<<4)+5 <seg_size) keyB[k+(tid<<4)+5 ] = rg_k5 ;\n        if((tid<<4)+6 <seg_size) keyB[k+(tid<<4)+6 ] = rg_k6 ;\n        if((tid<<4)+7 <seg_size) keyB[k+(tid<<4)+7 ] = rg_k7 ;\n        if((tid<<4)+8 <seg_size) keyB[k+(tid<<4)+8 ] = rg_k8 ;\n        if((tid<<4)+9 <seg_size) keyB[k+(tid<<4)+9 ] = rg_k9 ;\n        if((tid<<4)+10<seg_size) keyB[k+(tid<<4)+10] = rg_k10;\n        if((tid<<4)+11<seg_size) keyB[k+(tid<<4)+11] = rg_k11;\n        if((tid<<4)+12<seg_size) keyB[k+(tid<<4)+12] = rg_k12;\n        if((tid<<4)+13<seg_size) keyB[k+(tid<<4)+13] = rg_k13;\n        if((tid<<4)+14<seg_size) keyB[k+(tid<<4)+14] = rg_k14;\n        if((tid<<4)+15<seg_size) keyB[k+(tid<<4)+15] = rg_k15;\n    }\n}",
            "#define K 1.38065e-23        // J/K, Boltzmann constant\n\n\n__device__ inline void exch_intxn_keys(K &k0, K &k1, K &k2, K &k3, K &k4, K &k5, K &k6, K &k7, K &k8, K &k9, K &k10, K &k11, K &k12, K &k13, K &k14, K &k15, K &k16, K &k17, K &k18, K &k19, K &k20, K &k21, K &k22, K &k23, K &k24, K &k25, K &k26, K &k27, K &k28, K &k29, K &k30, K &k31, int mask, const int bit) {\n    K ex_k0, ex_k1;\n    if(bit) SWP_KEY(K, k0, k30);\n    if(bit) SWP_KEY(K, k1, k31);\n    if(bit) SWP_KEY(K, k2, k28);\n    if(bit) SWP_KEY(K, k3, k29);\n    if(bit) SWP_KEY(K, k4, k26);\n    if(bit) SWP_KEY(K, k5, k27);\n    if(bit) SWP_KEY(K, k6, k24);\n    if(bit) SWP_KEY(K, k7, k25);\n    if(bit) SWP_KEY(K, k8, k22);\n    if(bit) SWP_KEY(K, k9, k23);\n    if(bit) SWP_KEY(K, k10, k20);\n    if(bit) SWP_KEY(K, k11, k21);\n    if(bit) SWP_KEY(K, k12, k18);\n    if(bit) SWP_KEY(K, k13, k19);\n    if(bit) SWP_KEY(K, k14, k16);\n    if(bit) SWP_KEY(K, k15, k17);\n    ex_k0 = k0;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k1, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k0 = ex_k0;\n    k1 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k2;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k3, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k2 = ex_k0;\n    k3 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k4;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k5, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k4 = ex_k0;\n    k5 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k6;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k7, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k6 = ex_k0;\n    k7 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k8;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k9, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k8 = ex_k0;\n    k9 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k10;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k11, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k10 = ex_k0;\n    k11 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k12;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k13, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k12 = ex_k0;\n    k13 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k14;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k15, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k14 = ex_k0;\n    k15 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k16;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k17, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k16 = ex_k0;\n    k17 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k18;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k19, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k18 = ex_k0;\n    k19 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k20;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k21, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k20 = ex_k0;\n    k21 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k22;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k23, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k22 = ex_k0;\n    k23 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k24;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k25, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k24 = ex_k0;\n    k25 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k26;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k27, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k26 = ex_k0;\n    k27 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k28;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k29, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k28 = ex_k0;\n    k29 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k30;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k31, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k30 = ex_k0;\n    k31 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    if(bit) SWP_KEY(K, k0, k30);\n    if(bit) SWP_KEY(K, k1, k31);\n    if(bit) SWP_KEY(K, k2, k28);\n    if(bit) SWP_KEY(K, k3, k29);\n    if(bit) SWP_KEY(K, k4, k26);\n    if(bit) SWP_KEY(K, k5, k27);\n    if(bit) SWP_KEY(K, k6, k24);\n    if(bit) SWP_KEY(K, k7, k25);\n    if(bit) SWP_KEY(K, k8, k22);\n    if(bit) SWP_KEY(K, k9, k23);\n    if(bit) SWP_KEY(K, k10, k20);\n    if(bit) SWP_KEY(K, k11, k21);\n    if(bit) SWP_KEY(K, k12, k18);\n    if(bit) SWP_KEY(K, k13, k19);\n    if(bit) SWP_KEY(K, k14, k16);\n    if(bit) SWP_KEY(K, k15, k17);\n}\n\n__device__ inline void exch_paral_keys(K &k0, K &k1, K &k2, K &k3, K &k4, K &k5, K &k6, K &k7, K &k8, K &k9, K &k10, K &k11, K &k12, K &k13, K &k14, K &k15, K &k16, K &k17, K &k18, K &k19, K &k20, K &k21, K &k22, K &k23, K &k24, K &k25, K &k26, K &k27, K &k28, K &k29, K &k30, K &k31, int mask, const int bit) {\n    K ex_k0, ex_k1;\n    if(bit) SWP_KEY(K, k0, k1);\n    if(bit) SWP_KEY(K, k2, k3);\n    if(bit) SWP_KEY(K, k4, k5);\n    if(bit) SWP_KEY(K, k6, k7);\n    if(bit) SWP_KEY(K, k8, k9);\n    if(bit) SWP_KEY(K, k10, k11);\n    if(bit) SWP_KEY(K, k12, k13);\n    if(bit) SWP_KEY(K, k14, k15);\n    if(bit) SWP_KEY(K, k16, k17);\n    if(bit) SWP_KEY(K, k18, k19);\n    if(bit) SWP_KEY(K, k20, k21);\n    if(bit) SWP_KEY(K, k22, k23);\n    if(bit) SWP_KEY(K, k24, k25);\n    if(bit) SWP_KEY(K, k26, k27);\n    if(bit) SWP_KEY(K, k28, k29);\n    if(bit) SWP_KEY(K, k30, k31);\n    ex_k0 = k0;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k1, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k0 = ex_k0;\n    k1 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k2;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k3, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k2 = ex_k0;\n    k3 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k4;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k5, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k4 = ex_k0;\n    k5 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k6;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k7, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k6 = ex_k0;\n    k7 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k8;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k9, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k8 = ex_k0;\n    k9 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k10;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k11, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k10 = ex_k0;\n    k11 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k12;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k13, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k12 = ex_k0;\n    k13 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k14;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k15, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k14 = ex_k0;\n    k15 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k16;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k17, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k16 = ex_k0;\n    k17 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k18;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k19, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k18 = ex_k0;\n    k19 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k20;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k21, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k20 = ex_k0;\n    k21 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k22;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k23, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k22 = ex_k0;\n    k23 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k24;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k25, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k24 = ex_k0;\n    k25 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k26;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k27, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k26 = ex_k0;\n    k27 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k28;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k29, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k28 = ex_k0;\n    k29 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k30;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k31, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k30 = ex_k0;\n    k31 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    if(bit) SWP_KEY(K, k0, k1);\n    if(bit) SWP_KEY(K, k2, k3);\n    if(bit) SWP_KEY(K, k4, k5);\n    if(bit) SWP_KEY(K, k6, k7);\n    if(bit) SWP_KEY(K, k8, k9);\n    if(bit) SWP_KEY(K, k10, k11);\n    if(bit) SWP_KEY(K, k12, k13);\n    if(bit) SWP_KEY(K, k14, k15);\n    if(bit) SWP_KEY(K, k16, k17);\n    if(bit) SWP_KEY(K, k18, k19);\n    if(bit) SWP_KEY(K, k20, k21);\n    if(bit) SWP_KEY(K, k22, k23);\n    if(bit) SWP_KEY(K, k24, k25);\n    if(bit) SWP_KEY(K, k26, k27);\n    if(bit) SWP_KEY(K, k28, k29);\n    if(bit) SWP_KEY(K, k30, k31);\n}\n\n__global__\nvoid gen_bk256_wp32_tc8_r129_r256_strd(\n    K *key, K *keyB,\n    const Offset *seg_begins, const Offset *seg_ends,\n    const int *bin, const int bin_size)\n{\n    const int gid = threadIdx.x + blockIdx.x * blockDim.x;\n    const int bin_it = (gid>>5);\n    const int tid = (threadIdx.x & 31);\n    const int bit1 = (tid>>0)&0x1;\n    const int bit2 = (tid>>1)&0x1;\n    const int bit3 = (tid>>2)&0x1;\n    const int bit4 = (tid>>3)&0x1;\n    const int bit5 = (tid>>4)&0x1;\n    K rg_k0 ;\n    K rg_k1 ;\n    K rg_k2 ;\n    K rg_k3 ;\n    K rg_k4 ;\n    K rg_k5 ;\n    K rg_k6 ;\n    K rg_k7 ;\n    int normalized_bin_size = (bin_size/1)*1;\n    int k;\n    int seg_size;\n    if(bin_it < bin_size) {\n        k = seg_begins[bin[bin_it]];\n        seg_size = seg_ends[bin[bin_it]]-seg_begins[bin[bin_it]];\n        rg_k0  = (tid+0   <seg_size)?key[k+tid+0   ]:std::numeric_limits<K>::max();\n        rg_k1  = (tid+32  <seg_size)?key[k+tid+32  ]:std::numeric_limits<K>::max();\n        rg_k2  = (tid+64  <seg_size)?key[k+tid+64  ]:std::numeric_limits<K>::max();\n        rg_k3  = (tid+96  <seg_size)?key[k+tid+96  ]:std::numeric_limits<K>::max();\n        rg_k4  = (tid+128 <seg_size)?key[k+tid+128 ]:std::numeric_limits<K>::max();\n        rg_k5  = (tid+160 <seg_size)?key[k+tid+160 ]:std::numeric_limits<K>::max();\n        rg_k6  = (tid+192 <seg_size)?key[k+tid+192 ]:std::numeric_limits<K>::max();\n        rg_k7  = (tid+224 <seg_size)?key[k+tid+224 ]:std::numeric_limits<K>::max();\n        // sort 256 elements\n        // exch_intxn: switch to exch_local()\n        CMP_SWP_KEY(K,rg_k0 ,rg_k1 );\n        CMP_SWP_KEY(K,rg_k2 ,rg_k3 );\n        CMP_SWP_KEY(K,rg_k4 ,rg_k5 );\n        CMP_SWP_KEY(K,rg_k6 ,rg_k7 );\n        // exch_intxn: switch to exch_local()\n        CMP_SWP_KEY(K,rg_k0 ,rg_k3 );\n        CMP_SWP_KEY(K,rg_k1 ,rg_k2 );\n        CMP_SWP_KEY(K,rg_k4 ,rg_k7 );\n        CMP_SWP_KEY(K,rg_k5 ,rg_k6 );\n        // exch_paral: switch to exch_local()\n        CMP_SWP_KEY(K,rg_k0 ,rg_k1 );\n        CMP_SWP_KEY(K,rg_k2 ,rg_k3 );\n        CMP_SWP_KEY(K,rg_k4 ,rg_k5 );\n        CMP_SWP_KEY(K,rg_k6 ,rg_k7 );\n        // exch_intxn: switch to exch_local()\n        CMP_SWP_KEY(K,rg_k0 ,rg_k7 );\n        CMP_SWP_KEY(K,rg_k1 ,rg_k6 );\n        CMP_SWP_KEY(K,rg_k2 ,rg_k5 );\n        CMP_SWP_KEY(K,rg_k3 ,rg_k4 );\n        // exch_paral: switch to exch_local()\n        CMP_SWP_KEY(K,rg_k0 ,rg_k2 );\n        CMP_SWP_KEY(K,rg_k1 ,rg_k3 );\n        CMP_SWP_KEY(K,rg_k4 ,rg_k6 );\n        CMP_SWP_KEY(K,rg_k5 ,rg_k7 );\n        // exch_paral: switch to exch_local()\n        CMP_SWP_KEY(K,rg_k0 ,rg_k1 );\n        CMP_SWP_KEY(K,rg_k2 ,rg_k3 );\n        CMP_SWP_KEY(K,rg_k4 ,rg_k5 );\n        CMP_SWP_KEY(K,rg_k6 ,rg_k7 );\n        // exch_intxn: generate exch_intxn_keys()\n        exch_intxn_keys(rg_k0 ,rg_k1 ,rg_k2 ,rg_k3 ,rg_k4 ,rg_k5 ,rg_k6 ,rg_k7 ,\n                        0x1,bit1);\n        // exch_paral: switch to exch_local()\n        CMP_SWP_KEY(K,rg_k0 ,rg_k4 );\n        CMP_SWP_KEY(K,rg_k1 ,rg_k5 );\n        CMP_SWP_KEY(K,rg_k2 ,rg_k6 );\n        CMP_SWP_KEY(K,rg_k3 ,rg_k7 );\n        // exch_paral: switch to exch_local()\n        CMP_SWP_KEY(K,rg_k0 ,rg_k2 );\n        CMP_SWP_KEY(K,rg_k1 ,rg_k3 );\n        CMP_SWP_KEY(K,rg_k4 ,rg_k6 );\n        CMP_SWP_KEY(K,rg_k5 ,rg_k7 );\n        // exch_paral: switch to exch_local()\n        CMP_SWP_KEY(K,rg_k0 ,rg_k1 );\n        CMP_SWP_KEY(K,rg_k2 ,rg_k3 );\n        CMP_SWP_KEY(K,rg_k4 ,rg_k5 );\n        CMP_SWP_KEY(K,rg_k6 ,rg_k7 );\n        // exch_intxn: generate exch_intxn_keys()\n        exch_intxn_keys(rg_k0 ,rg_k1 ,rg_k2 ,rg_k3 ,rg_k4 ,rg_k5 ,rg_k6 ,rg_k7 ,\n                        0x3,bit2);\n        // exch_paral: generate exch_paral_keys()\n        exch_paral_keys(rg_k0 ,rg_k1 ,rg_k2 ,rg_k3 ,rg_k4 ,rg_k5 ,rg_k6 ,rg_k7 ,\n                        0x1,bit1);\n        // exch_paral: switch to exch_local()\n        CMP_SWP_KEY(K,rg_k0 ,rg_k4 );\n        CMP_SWP_KEY(K,rg_k1 ,rg_k5 );\n        CMP_SWP_KEY(K,rg_k2 ,rg_k6 );\n        CMP_SWP_KEY(K,rg_k3 ,rg_k7 );\n        // exch_paral: switch to exch_local()\n        CMP_SWP_KEY(K,rg_k0 ,rg_k2 );\n        CMP_SWP_KEY(K,rg_k1 ,rg_k3 );\n        CMP_SWP_KEY(K,rg_k4 ,rg_k6 );\n        CMP_SWP_KEY(K,rg_k5 ,rg_k7 );\n        // exch_paral: switch to exch_local()\n        CMP_SWP_KEY(K,rg_k0 ,rg_k1 );\n        CMP_SWP_KEY(K,rg_k2 ,rg_k3 );\n        CMP_SWP_KEY(K,rg_k4 ,rg_k5 );\n        CMP_SWP_KEY(K,rg_k6 ,rg_k7 );\n        // exch_intxn: generate exch_intxn_keys()\n        exch_intxn_keys(rg_k0 ,rg_k1 ,rg_k2 ,rg_k3 ,rg_k4 ,rg_k5 ,rg_k6 ,rg_k7 ,\n                        0x7,bit3);\n        // exch_paral: generate exch_paral_keys()\n        exch_paral_keys(rg_k0 ,rg_k1 ,rg_k2 ,rg_k3 ,rg_k4 ,rg_k5 ,rg_k6 ,rg_k7 ,\n                        0x2,bit2);\n        // exch_paral: generate exch_paral_keys()\n        exch_paral_keys(rg_k0 ,rg_k1 ,rg_k2 ,rg_k3 ,rg_k4 ,rg_k5 ,rg_k6 ,rg_k7 ,\n                        0x1,bit1);\n        // exch_paral: switch to exch_local()\n        CMP_SWP_KEY(K,rg_k0 ,rg_k4 );\n        CMP_SWP_KEY(K,rg_k1 ,rg_k5 );\n        CMP_SWP_KEY(K,rg_k2 ,rg_k6 );\n        CMP_SWP_KEY(K,rg_k3 ,rg_k7 );\n        // exch_paral: switch to exch_local()\n        CMP_SWP_KEY(K,rg_k0 ,rg_k2 );\n        CMP_SWP_KEY(K,rg_k1 ,rg_k3 );\n        CMP_SWP_KEY(K,rg_k4 ,rg_k6 );\n        CMP_SWP_KEY(K,rg_k5 ,rg_k7 );\n        // exch_paral: switch to exch_local()\n        CMP_SWP_KEY(K,rg_k0 ,rg_k1 );\n        CMP_SWP_KEY(K,rg_k2 ,rg_k3 );\n        CMP_SWP_KEY(K,rg_k4 ,rg_k5 );\n        CMP_SWP_KEY(K,rg_k6 ,rg_k7 );\n        // exch_intxn: generate exch_intxn_keys()\n        exch_intxn_keys(rg_k0 ,rg_k1 ,rg_k2 ,rg_k3 ,rg_k4 ,rg_k5 ,rg_k6 ,rg_k7 ,\n                        0xf,bit4);\n        // exch_paral: generate exch_paral_keys()\n        exch_paral_keys(rg_k0 ,rg_k1 ,rg_k2 ,rg_k3 ,rg_k4 ,rg_k5 ,rg_k6 ,rg_k7 ,\n                        0x4,bit3);\n        // exch_paral: generate exch_paral_keys()\n        exch_paral_keys(rg_k0 ,rg_k1 ,rg_k2 ,rg_k3 ,rg_k4 ,rg_k5 ,rg_k6 ,rg_k7 ,\n                        0x2,bit2);\n        // exch_paral: generate exch_paral_keys()\n        exch_paral_keys(rg_k0 ,rg_k1 ,rg_k2 ,rg_k3 ,rg_k4 ,rg_k5 ,rg_k6 ,rg_k7 ,\n                        0x1,bit1);\n        // exch_paral: switch to exch_local()\n        CMP_SWP_KEY(K,rg_k0 ,rg_k4 );\n        CMP_SWP_KEY(K,rg_k1 ,rg_k5 );\n        CMP_SWP_KEY(K,rg_k2 ,rg_k6 );\n        CMP_SWP_KEY(K,rg_k3 ,rg_k7 );\n        // exch_paral: switch to exch_local()\n        CMP_SWP_KEY(K,rg_k0 ,rg_k2 );\n        CMP_SWP_KEY(K,rg_k1 ,rg_k3 );\n        CMP_SWP_KEY(K,rg_k4 ,rg_k6 );\n        CMP_SWP_KEY(K,rg_k5 ,rg_k7 );\n        // exch_paral: switch to exch_local()\n        CMP_SWP_KEY(K,rg_k0 ,rg_k1 );\n        CMP_SWP_KEY(K,rg_k2 ,rg_k3 );\n        CMP_SWP_KEY(K,rg_k4 ,rg_k5 );\n        CMP_SWP_KEY(K,rg_k6 ,rg_k7 );\n        // exch_intxn: generate exch_intxn_keys()\n        exch_intxn_keys(rg_k0 ,rg_k1 ,rg_k2 ,rg_k3 ,rg_k4 ,rg_k5 ,rg_k6 ,rg_k7 ,\n                        0x1f,bit5);\n        // exch_paral: generate exch_paral_keys()\n        exch_paral_keys(rg_k0 ,rg_k1 ,rg_k2 ,rg_k3 ,rg_k4 ,rg_k5 ,rg_k6 ,rg_k7 ,\n                        0x8,bit4);\n        // exch_paral: generate exch_paral_keys()\n        exch_paral_keys(rg_k0 ,rg_k1 ,rg_k2 ,rg_k3 ,rg_k4 ,rg_k5 ,rg_k6 ,rg_k7 ,\n                        0x4,bit3);\n        // exch_paral: generate exch_paral_keys()\n        exch_paral_keys(rg_k0 ,rg_k1 ,rg_k2 ,rg_k3 ,rg_k4 ,rg_k5 ,rg_k6 ,rg_k7 ,\n                        0x2,bit2);\n        // exch_paral: generate exch_paral_keys()\n        exch_paral_keys(rg_k0 ,rg_k1 ,rg_k2 ,rg_k3 ,rg_k4 ,rg_k5 ,rg_k6 ,rg_k7 ,\n                        0x1,bit1);\n        // exch_paral: switch to exch_local()\n        CMP_SWP_KEY(K,rg_k0 ,rg_k4 );\n        CMP_SWP_KEY(K,rg_k1 ,rg_k5 );\n        CMP_SWP_KEY(K,rg_k2 ,rg_k6 );\n        CMP_SWP_KEY(K,rg_k3 ,rg_k7 );\n        // exch_paral: switch to exch_local()\n        CMP_SWP_KEY(K,rg_k0 ,rg_k2 );\n        CMP_SWP_KEY(K,rg_k1 ,rg_k3 );\n        CMP_SWP_KEY(K,rg_k4 ,rg_k6 );\n        CMP_SWP_KEY(K,rg_k5 ,rg_k7 );\n        // exch_paral: switch to exch_local()\n        CMP_SWP_KEY(K,rg_k0 ,rg_k1 );\n        CMP_SWP_KEY(K,rg_k2 ,rg_k3 );\n        CMP_SWP_KEY(K,rg_k4 ,rg_k5 );\n        CMP_SWP_KEY(K,rg_k6 ,rg_k7 );\n    }\n\n    if(bin_it < normalized_bin_size) {\n        // store back the results\n        int lane_id = threadIdx.x & 31;\n        rg_k1  = __shfl_xor_sync(0xffffffff,rg_k1 , 0x1 );\n        rg_k3  = __shfl_xor_sync(0xffffffff,rg_k3 , 0x1 );\n        rg_k5  = __shfl_xor_sync(0xffffffff,rg_k5 , 0x1 );\n        rg_k7  = __shfl_xor_sync(0xffffffff,rg_k7 , 0x1 );\n        if(lane_id&0x1 ) SWP_KEY(K, rg_k0 , rg_k1 );\n        if(lane_id&0x1 ) SWP_KEY(K, rg_k2 , rg_k3 );\n        if(lane_id&0x1 ) SWP_KEY(K, rg_k4 , rg_k5 );\n        if(lane_id&0x1 ) SWP_KEY(K, rg_k6 , rg_k7 );\n        rg_k1  = __shfl_xor_sync(0xffffffff,rg_k1 , 0x1 );\n        rg_k3  = __shfl_xor_sync(0xffffffff,rg_k3 , 0x1 );\n        rg_k5  = __shfl_xor_sync(0xffffffff,rg_k5 , 0x1 );\n        rg_k7  = __shfl_xor_sync(0xffffffff,rg_k7 , 0x1 );\n        rg_k2  = __shfl_xor_sync(0xffffffff,rg_k2 , 0x2 );\n        rg_k3  = __shfl_xor_sync(0xffffffff,rg_k3 , 0x2 );\n        rg_k6  = __shfl_xor_sync(0xffffffff,rg_k6 , 0x2 );\n        rg_k7  = __shfl_xor_sync(0xffffffff,rg_k7 , 0x2 );\n        if(lane_id&0x2 ) SWP_KEY(K, rg_k0 , rg_k2 );\n        if(lane_id&0x2 ) SWP_KEY(K, rg_k1 , rg_k3 );\n        if(lane_id&0x2 ) SWP_KEY(K, rg_k4 , rg_k6 );\n        if(lane_id&0x2 ) SWP_KEY(K, rg_k5 , rg_k7 );\n        rg_k2  = __shfl_xor_sync(0xffffffff,rg_k2 , 0x2 );\n        rg_k3  = __shfl_xor_sync(0xffffffff,rg_k3 , 0x2 );\n        rg_k6  = __shfl_xor_sync(0xffffffff,rg_k6 , 0x2 );\n        rg_k7  = __shfl_xor_sync(0xffffffff,rg_k7 , 0x2 );\n        rg_k4  = __shfl_xor_sync(0xffffffff,rg_k4 , 0x4 );\n        rg_k5  = __shfl_xor_sync(0xffffffff,rg_k5 , 0x4 );\n        rg_k6  = __shfl_xor_sync(0xffffffff,rg_k6 , 0x4 );\n        rg_k7  = __shfl_xor_sync(0xffffffff,rg_k7 , 0x4 );\n        if(lane_id&0x4 ) SWP_KEY(K, rg_k0 , rg_k4 );\n        if(lane_id&0x4 ) SWP_KEY(K, rg_k1 , rg_k5 );\n        if(lane_id&0x4 ) SWP_KEY(K, rg_k2 , rg_k6 );\n        if(lane_id&0x4 ) SWP_KEY(K, rg_k3 , rg_k7 );\n        rg_k4  = __shfl_xor_sync(0xffffffff,rg_k4 , 0x4 );\n        rg_k5  = __shfl_xor_sync(0xffffffff,rg_k5 , 0x4 );\n        rg_k6  = __shfl_xor_sync(0xffffffff,rg_k6 , 0x4 );\n        rg_k7  = __shfl_xor_sync(0xffffffff,rg_k7 , 0x4 );\n        rg_k1  = __shfl_xor_sync(0xffffffff,rg_k1 , 0x8 );\n        rg_k3  = __shfl_xor_sync(0xffffffff,rg_k3 , 0x8 );\n        rg_k5  = __shfl_xor_sync(0xffffffff,rg_k5 , 0x8 );\n        rg_k7  = __shfl_xor_sync(0xffffffff,rg_k7 , 0x8 );\n        if(lane_id&0x8 ) SWP_KEY(K, rg_k0 , rg_k1 );\n        if(lane_id&0x8 ) SWP_KEY(K, rg_k2 , rg_k3 );\n        if(lane_id&0x8 ) SWP_KEY(K, rg_k4 , rg_k5 );\n        if(lane_id&0x8 ) SWP_KEY(K, rg_k6 , rg_k7 );\n        rg_k1  = __shfl_xor_sync(0xffffffff,rg_k1 , 0x8 );\n        rg_k3  = __shfl_xor_sync(0xffffffff,rg_k3 , 0x8 );\n        rg_k5  = __shfl_xor_sync(0xffffffff,rg_k5 , 0x8 );\n        rg_k7  = __shfl_xor_sync(0xffffffff,rg_k7 , 0x8 );\n        rg_k2  = __shfl_xor_sync(0xffffffff,rg_k2 , 0x10);\n        rg_k3  = __shfl_xor_sync(0xffffffff,rg_k3 , 0x10);\n        rg_k6  = __shfl_xor_sync(0xffffffff,rg_k6 , 0x10);\n        rg_k7  = __shfl_xor_sync(0xffffffff,rg_k7 , 0x10);\n        if(lane_id&0x10) SWP_KEY(K, rg_k0 , rg_k2 );\n        if(lane_id&0x10) SWP_KEY(K, rg_k1 , rg_k3 );\n        if(lane_id&0x10) SWP_KEY(K, rg_k4 , rg_k6 );\n        if(lane_id&0x10) SWP_KEY(K, rg_k5 , rg_k7 );\n        rg_k2  = __shfl_xor_sync(0xffffffff,rg_k2 , 0x10);\n        rg_k3  = __shfl_xor_sync(0xffffffff,rg_k3 , 0x10);\n        rg_k6  = __shfl_xor_sync(0xffffffff,rg_k6 , 0x10);\n        rg_k7  = __shfl_xor_sync(0xffffffff,rg_k7 , 0x10);\n        int kk;\n        int ss;\n        kk = __shfl_sync(0xffffffff,k, 0 );\n        ss = __shfl_sync(0xffffffff,seg_size, 0 );\n        if(lane_id+0  <ss) keyB[kk+lane_id+0  ] = rg_k0 ;\n        if(lane_id+32 <ss) keyB[kk+lane_id+32 ] = rg_k4 ;\n        if(lane_id+64 <ss) keyB[kk+lane_id+64 ] = rg_k1 ;\n        if(lane_id+96 <ss) keyB[kk+lane_id+96 ] = rg_k5 ;\n        if(lane_id+128<ss) keyB[kk+lane_id+128] = rg_k2 ;\n        if(lane_id+160<ss) keyB[kk+lane_id+160] = rg_k6 ;\n        if(lane_id+192<ss) keyB[kk+lane_id+192] = rg_k3 ;\n        if(lane_id+224<ss) keyB[kk+lane_id+224] = rg_k7 ;\n    } else if(bin_it < bin_size) {\n        if((tid<<3)+0 <seg_size) keyB[k+(tid<<3)+0 ] = rg_k0 ;\n        if((tid<<3)+1 <seg_size) keyB[k+(tid<<3)+1 ] = rg_k1 ;\n        if((tid<<3)+2 <seg_size) keyB[k+(tid<<3)+2 ] = rg_k2 ;\n        if((tid<<3)+3 <seg_size) keyB[k+(tid<<3)+3 ] = rg_k3 ;\n        if((tid<<3)+4 <seg_size) keyB[k+(tid<<3)+4 ] = rg_k4 ;\n        if((tid<<3)+5 <seg_size) keyB[k+(tid<<3)+5 ] = rg_k5 ;\n        if((tid<<3)+6 <seg_size) keyB[k+(tid<<3)+6 ] = rg_k6 ;\n        if((tid<<3)+7 <seg_size) keyB[k+(tid<<3)+7 ] = rg_k7 ;\n    }\n}",
            "#define K 1.38065e-23        // J/K, Boltzmann constant\n\n\ninline __host__ __device__ uint4 make_uint4(int4 a)\n{\n    return make_uint4(uint(a.x), uint(a.y), uint(a.z), uint(a.w));\n}\n\ninline __host__ __device__ uint4 max(uint4 a, uint4 b)\n{\n    return make_uint4(max(a.x,b.x), max(a.y,b.y), max(a.z,b.z), max(a.w,b.w));\n}\n\ninline __host__ __device__ uint4 min(uint4 a, uint4 b)\n{\n    return make_uint4(min(a.x,b.x), min(a.y,b.y), min(a.z,b.z), min(a.w,b.w));\n}\n\n__device__ inline void exch_intxn_keys(K &k0, K &k1, K &k2, K &k3, K &k4, K &k5, K &k6, K &k7, K &k8, K &k9, K &k10, K &k11, K &k12, K &k13, K &k14, K &k15, K &k16, K &k17, K &k18, K &k19, K &k20, K &k21, K &k22, K &k23, K &k24, K &k25, K &k26, K &k27, K &k28, K &k29, K &k30, K &k31, int mask, const int bit) {\n    K ex_k0, ex_k1;\n    if(bit) SWP_KEY(K, k0, k30);\n    if(bit) SWP_KEY(K, k1, k31);\n    if(bit) SWP_KEY(K, k2, k28);\n    if(bit) SWP_KEY(K, k3, k29);\n    if(bit) SWP_KEY(K, k4, k26);\n    if(bit) SWP_KEY(K, k5, k27);\n    if(bit) SWP_KEY(K, k6, k24);\n    if(bit) SWP_KEY(K, k7, k25);\n    if(bit) SWP_KEY(K, k8, k22);\n    if(bit) SWP_KEY(K, k9, k23);\n    if(bit) SWP_KEY(K, k10, k20);\n    if(bit) SWP_KEY(K, k11, k21);\n    if(bit) SWP_KEY(K, k12, k18);\n    if(bit) SWP_KEY(K, k13, k19);\n    if(bit) SWP_KEY(K, k14, k16);\n    if(bit) SWP_KEY(K, k15, k17);\n    ex_k0 = k0;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k1, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k0 = ex_k0;\n    k1 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k2;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k3, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k2 = ex_k0;\n    k3 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k4;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k5, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k4 = ex_k0;\n    k5 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k6;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k7, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k6 = ex_k0;\n    k7 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k8;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k9, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k8 = ex_k0;\n    k9 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k10;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k11, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k10 = ex_k0;\n    k11 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k12;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k13, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k12 = ex_k0;\n    k13 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k14;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k15, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k14 = ex_k0;\n    k15 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k16;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k17, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k16 = ex_k0;\n    k17 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k18;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k19, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k18 = ex_k0;\n    k19 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k20;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k21, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k20 = ex_k0;\n    k21 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k22;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k23, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k22 = ex_k0;\n    k23 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k24;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k25, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k24 = ex_k0;\n    k25 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k26;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k27, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k26 = ex_k0;\n    k27 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k28;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k29, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k28 = ex_k0;\n    k29 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k30;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k31, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k30 = ex_k0;\n    k31 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    if(bit) SWP_KEY(K, k0, k30);\n    if(bit) SWP_KEY(K, k1, k31);\n    if(bit) SWP_KEY(K, k2, k28);\n    if(bit) SWP_KEY(K, k3, k29);\n    if(bit) SWP_KEY(K, k4, k26);\n    if(bit) SWP_KEY(K, k5, k27);\n    if(bit) SWP_KEY(K, k6, k24);\n    if(bit) SWP_KEY(K, k7, k25);\n    if(bit) SWP_KEY(K, k8, k22);\n    if(bit) SWP_KEY(K, k9, k23);\n    if(bit) SWP_KEY(K, k10, k20);\n    if(bit) SWP_KEY(K, k11, k21);\n    if(bit) SWP_KEY(K, k12, k18);\n    if(bit) SWP_KEY(K, k13, k19);\n    if(bit) SWP_KEY(K, k14, k16);\n    if(bit) SWP_KEY(K, k15, k17);\n}\n\n__device__ inline void exch_paral_keys(K &k0, K &k1, K &k2, K &k3, K &k4, K &k5, K &k6, K &k7, K &k8, K &k9, K &k10, K &k11, K &k12, K &k13, K &k14, K &k15, K &k16, K &k17, K &k18, K &k19, K &k20, K &k21, K &k22, K &k23, K &k24, K &k25, K &k26, K &k27, K &k28, K &k29, K &k30, K &k31, int mask, const int bit) {\n    K ex_k0, ex_k1;\n    if(bit) SWP_KEY(K, k0, k1);\n    if(bit) SWP_KEY(K, k2, k3);\n    if(bit) SWP_KEY(K, k4, k5);\n    if(bit) SWP_KEY(K, k6, k7);\n    if(bit) SWP_KEY(K, k8, k9);\n    if(bit) SWP_KEY(K, k10, k11);\n    if(bit) SWP_KEY(K, k12, k13);\n    if(bit) SWP_KEY(K, k14, k15);\n    if(bit) SWP_KEY(K, k16, k17);\n    if(bit) SWP_KEY(K, k18, k19);\n    if(bit) SWP_KEY(K, k20, k21);\n    if(bit) SWP_KEY(K, k22, k23);\n    if(bit) SWP_KEY(K, k24, k25);\n    if(bit) SWP_KEY(K, k26, k27);\n    if(bit) SWP_KEY(K, k28, k29);\n    if(bit) SWP_KEY(K, k30, k31);\n    ex_k0 = k0;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k1, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k0 = ex_k0;\n    k1 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k2;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k3, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k2 = ex_k0;\n    k3 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k4;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k5, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k4 = ex_k0;\n    k5 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k6;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k7, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k6 = ex_k0;\n    k7 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k8;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k9, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k8 = ex_k0;\n    k9 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k10;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k11, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k10 = ex_k0;\n    k11 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k12;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k13, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k12 = ex_k0;\n    k13 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k14;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k15, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k14 = ex_k0;\n    k15 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k16;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k17, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k16 = ex_k0;\n    k17 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k18;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k19, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k18 = ex_k0;\n    k19 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k20;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k21, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k20 = ex_k0;\n    k21 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k22;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k23, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k22 = ex_k0;\n    k23 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k24;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k25, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k24 = ex_k0;\n    k25 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k26;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k27, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k26 = ex_k0;\n    k27 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k28;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k29, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k28 = ex_k0;\n    k29 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k30;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k31, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k30 = ex_k0;\n    k31 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    if(bit) SWP_KEY(K, k0, k1);\n    if(bit) SWP_KEY(K, k2, k3);\n    if(bit) SWP_KEY(K, k4, k5);\n    if(bit) SWP_KEY(K, k6, k7);\n    if(bit) SWP_KEY(K, k8, k9);\n    if(bit) SWP_KEY(K, k10, k11);\n    if(bit) SWP_KEY(K, k12, k13);\n    if(bit) SWP_KEY(K, k14, k15);\n    if(bit) SWP_KEY(K, k16, k17);\n    if(bit) SWP_KEY(K, k18, k19);\n    if(bit) SWP_KEY(K, k20, k21);\n    if(bit) SWP_KEY(K, k22, k23);\n    if(bit) SWP_KEY(K, k24, k25);\n    if(bit) SWP_KEY(K, k26, k27);\n    if(bit) SWP_KEY(K, k28, k29);\n    if(bit) SWP_KEY(K, k30, k31);\n}\n\n__device__\nint find_kth3(K* a, int aCount, K* b, int bCount, int diag)\n{\n    int begin = max(0, diag - bCount);\n    int end = min(diag, aCount);\n\n    while(begin < end) {\n        int mid = (begin + end)>> 1;\n        K aKey = a[mid];\n        K bKey = b[diag - 1 - mid];\n        bool pred = aKey <= bKey;\n        if(pred) begin = mid + 1;\n        else end = mid;\n    }\n    return begin;\n}\n\n__global__\nvoid gen_bk128_tc4_r257_r512_orig(\n    K *key, K *keyB,\n    const Offset *seg_begins, const Offset *seg_ends,\n    const int *bin, const int bin_size)\n{\n    const int tid = threadIdx.x;\n    const int bin_it = blockIdx.x;\n    __shared__ K smem[512];\n    const int bit1 = (tid>>0)&0x1;\n    const int bit2 = (tid>>1)&0x1;\n    const int bit3 = (tid>>2)&0x1;\n    const int bit4 = (tid>>3)&0x1;\n    const int bit5 = (tid>>4)&0x1;\n    const int tid1 = threadIdx.x & 31;\n    const int warp_id = threadIdx.x / 32;\n    K rg_k0 ;\n    K rg_k1 ;\n    K rg_k2 ;\n    K rg_k3 ;\n    int k;\n    int seg_size;\n    int ext_seg_size;\n    if(bin_it < bin_size) {\n        k = seg_begins[bin[bin_it]];\n        seg_size = seg_ends[bin[bin_it]]-seg_begins[bin[bin_it]];\n        ext_seg_size = ((seg_size + 63) / 64) * 64;\n        int big_wp = (ext_seg_size - blockDim.x * 2) / 64;\n        int sml_wp = blockDim.x / 32 - big_wp;\n        int sml_len = sml_wp * 64;\n        const int big_warp_id = (warp_id - sml_wp < 0)? 0: warp_id - sml_wp;\n        bool sml_warp = warp_id < sml_wp;\n        if(sml_warp) {\n            rg_k0 = key[k+(warp_id<<6)+tid1+0   ];\n            rg_k1 = key[k+(warp_id<<6)+tid1+32  ];\n            // exch_intxn: switch to exch_local()\n            CMP_SWP_KEY(K,rg_k0 ,rg_k1 );\n            // exch_intxn: generate exch_intxn_keys()\n            exch_intxn_keys(rg_k0 ,rg_k1 ,\n                            0x1,bit1);\n            // exch_paral: switch to exch_local()\n            CMP_SWP_KEY(K,rg_k0 ,rg_k1 );\n            // exch_intxn: generate exch_intxn_keys()\n            exch_intxn_keys(rg_k0 ,rg_k1 ,\n                            0x3,bit2);\n            // exch_paral: generate exch_paral_keys()\n            exch_paral_keys(rg_k0 ,rg_k1 ,\n                            0x1,bit1);\n            // exch_paral: switch to exch_local()\n            CMP_SWP_KEY(K,rg_k0 ,rg_k1 );\n            // exch_intxn: generate exch_intxn_keys()\n            exch_intxn_keys(rg_k0 ,rg_k1 ,\n                            0x7,bit3);\n            // exch_paral: generate exch_paral_keys()\n            exch_paral_keys(rg_k0 ,rg_k1 ,\n                            0x2,bit2);\n            // exch_paral: generate exch_paral_keys()\n            exch_paral_keys(rg_k0 ,rg_k1 ,\n                            0x1,bit1);\n            // exch_paral: switch to exch_local()\n            CMP_SWP_KEY(K,rg_k0 ,rg_k1 );\n            // exch_intxn: generate exch_intxn_keys()\n            exch_intxn_keys(rg_k0 ,rg_k1 ,\n                            0xf,bit4);\n            // exch_paral: generate exch_paral_keys()\n            exch_paral_keys(rg_k0 ,rg_k1 ,\n                            0x4,bit3);\n            // exch_paral: generate exch_paral_keys()\n            exch_paral_keys(rg_k0 ,rg_k1 ,\n                            0x2,bit2);\n            // exch_paral: generate exch_paral_keys()\n            exch_paral_keys(rg_k0 ,rg_k1 ,\n                            0x1,bit1);\n            // exch_paral: switch to exch_local()\n            CMP_SWP_KEY(K,rg_k0 ,rg_k1 );\n            // exch_intxn: generate exch_intxn_keys()\n            exch_intxn_keys(rg_k0 ,rg_k1 ,\n                            0x1f,bit5);\n            // exch_paral: generate exch_paral_keys()\n            exch_paral_keys(rg_k0 ,rg_k1 ,\n                            0x8,bit4);\n            // exch_paral: generate exch_paral_keys()\n            exch_paral_keys(rg_k0 ,rg_k1 ,\n                            0x4,bit3);\n            // exch_paral: generate exch_paral_keys()\n            exch_paral_keys(rg_k0 ,rg_k1 ,\n                            0x2,bit2);\n            // exch_paral: generate exch_paral_keys()\n            exch_paral_keys(rg_k0 ,rg_k1 ,\n                            0x1,bit1);\n            // exch_paral: switch to exch_local()\n            CMP_SWP_KEY(K,rg_k0 ,rg_k1 );\n        } else {\n            rg_k0  = (sml_len+tid1+(big_warp_id<<7)+0   <seg_size)?key[k+sml_len+tid1+(big_warp_id<<7)+0   ]:std::numeric_limits<K>::max();\n            rg_k1  = (sml_len+tid1+(big_warp_id<<7)+32  <seg_size)?key[k+sml_len+tid1+(big_warp_id<<7)+32  ]:std::numeric_limits<K>::max();\n            rg_k2  = (sml_len+tid1+(big_warp_id<<7)+64  <seg_size)?key[k+sml_len+tid1+(big_warp_id<<7)+64  ]:std::numeric_limits<K>::max();\n            rg_k3  = (sml_len+tid1+(big_warp_id<<7)+96  <seg_size)?key[k+sml_len+tid1+(big_warp_id<<7)+96  ]:std::numeric_limits<K>::max();\n            // exch_intxn: switch to exch_local()\n            CMP_SWP_KEY(K,rg_k0 ,rg_k1 );\n            CMP_SWP_KEY(K,rg_k2 ,rg_k3 );\n            // exch_intxn: switch to exch_local()\n            CMP_SWP_KEY(K,rg_k0 ,rg_k3 );\n            CMP_SWP_KEY(K,rg_k1 ,rg_k2 );\n            // exch_paral: switch to exch_local()\n            CMP_SWP_KEY(K,rg_k0 ,rg_k1 );\n            CMP_SWP_KEY(K,rg_k2 ,rg_k3 );\n            // exch_intxn: generate exch_intxn_keys()\n            exch_intxn_keys(rg_k0 ,rg_k1 ,rg_k2 ,rg_k3 ,\n                            0x1,bit1);\n            // exch_paral: switch to exch_local()\n            CMP_SWP_KEY(K,rg_k0 ,rg_k2 );\n            CMP_SWP_KEY(K,rg_k1 ,rg_k3 );\n            // exch_paral: switch to exch_local()\n            CMP_SWP_KEY(K,rg_k0 ,rg_k1 );\n            CMP_SWP_KEY(K,rg_k2 ,rg_k3 );\n            // exch_intxn: generate exch_intxn_keys()\n            exch_intxn_keys(rg_k0 ,rg_k1 ,rg_k2 ,rg_k3 ,\n                            0x3,bit2);\n            // exch_paral: generate exch_paral_keys()\n            exch_paral_keys(rg_k0 ,rg_k1 ,rg_k2 ,rg_k3 ,\n                            0x1,bit1);\n            // exch_paral: switch to exch_local()\n            CMP_SWP_KEY(K,rg_k0 ,rg_k2 );\n            CMP_SWP_KEY(K,rg_k1 ,rg_k3 );\n            // exch_paral: switch to exch_local()\n            CMP_SWP_KEY(K,rg_k0 ,rg_k1 );\n            CMP_SWP_KEY(K,rg_k2 ,rg_k3 );\n            // exch_intxn: generate exch_intxn_keys()\n            exch_intxn_keys(rg_k0 ,rg_k1 ,rg_k2 ,rg_k3 ,\n                            0x7,bit3);\n            // exch_paral: generate exch_paral_keys()\n            exch_paral_keys(rg_k0 ,rg_k1 ,rg_k2 ,rg_k3 ,\n                            0x2,bit2);\n            // exch_paral: generate exch_paral_keys()\n            exch_paral_keys(rg_k0 ,rg_k1 ,rg_k2 ,rg_k3 ,\n                            0x1,bit1);\n            // exch_paral: switch to exch_local()\n            CMP_SWP_KEY(K,rg_k0 ,rg_k2 );\n            CMP_SWP_KEY(K,rg_k1 ,rg_k3 );\n            // exch_paral: switch to exch_local()\n            CMP_SWP_KEY(K,rg_k0 ,rg_k1 );\n            CMP_SWP_KEY(K,rg_k2 ,rg_k3 );\n            // exch_intxn: generate exch_intxn_keys()\n            exch_intxn_keys(rg_k0 ,rg_k1 ,rg_k2 ,rg_k3 ,\n                            0xf,bit4);\n            // exch_paral: generate exch_paral_keys()\n            exch_paral_keys(rg_k0 ,rg_k1 ,rg_k2 ,rg_k3 ,\n                            0x4,bit3);\n            // exch_paral: generate exch_paral_keys()\n            exch_paral_keys(rg_k0 ,rg_k1 ,rg_k2 ,rg_k3 ,\n                            0x2,bit2);\n            // exch_paral: generate exch_paral_keys()\n            exch_paral_keys(rg_k0 ,rg_k1 ,rg_k2 ,rg_k3 ,\n                            0x1,bit1);\n            // exch_paral: switch to exch_local()\n            CMP_SWP_KEY(K,rg_k0 ,rg_k2 );\n            CMP_SWP_KEY(K,rg_k1 ,rg_k3 );\n            // exch_paral: switch to exch_local()\n            CMP_SWP_KEY(K,rg_k0 ,rg_k1 );\n            CMP_SWP_KEY(K,rg_k2 ,rg_k3 );\n            // exch_intxn: generate exch_intxn_keys()\n            exch_intxn_keys(rg_k0 ,rg_k1 ,rg_k2 ,rg_k3 ,\n                            0x1f,bit5);\n            // exch_paral: generate exch_paral_keys()\n            exch_paral_keys(rg_k0 ,rg_k1 ,rg_k2 ,rg_k3 ,\n                            0x8,bit4);\n            // exch_paral: generate exch_paral_keys()\n            exch_paral_keys(rg_k0 ,rg_k1 ,rg_k2 ,rg_k3 ,\n                            0x4,bit3);\n            // exch_paral: generate exch_paral_keys()\n            exch_paral_keys(rg_k0 ,rg_k1 ,rg_k2 ,rg_k3 ,\n                            0x2,bit2);\n            // exch_paral: generate exch_paral_keys()\n            exch_paral_keys(rg_k0 ,rg_k1 ,rg_k2 ,rg_k3 ,\n                            0x1,bit1);\n            // exch_paral: switch to exch_local()\n            CMP_SWP_KEY(K,rg_k0 ,rg_k2 );\n            CMP_SWP_KEY(K,rg_k1 ,rg_k3 );\n            // exch_paral: switch to exch_local()\n            CMP_SWP_KEY(K,rg_k0 ,rg_k1 );\n            CMP_SWP_KEY(K,rg_k2 ,rg_k3 );\n        }\n        // Store register results to shared memory\n        if(sml_warp) {\n            smem[(warp_id<<6)+(tid1<<1)+0 ] = rg_k0 ;\n            smem[(warp_id<<6)+(tid1<<1)+1 ] = rg_k1 ;\n        } else {\n            smem[sml_len+(big_warp_id<<7)+(tid1<<2)+0 ] = rg_k0 ;\n            smem[sml_len+(big_warp_id<<7)+(tid1<<2)+1 ] = rg_k1 ;\n            smem[sml_len+(big_warp_id<<7)+(tid1<<2)+2 ] = rg_k2 ;\n            smem[sml_len+(big_warp_id<<7)+(tid1<<2)+3 ] = rg_k3 ;\n        }\n        __syncthreads();\n        // Merge in 2 steps\n        int grp_start_wp_id;\n        int grp_start_off;\n        int tmp_wp_id;\n        int lhs_len;\n        int rhs_len;\n        int gran;\n        int s_a;\n        int s_b;\n        bool p;\n        K tmp_k0;\n        K tmp_k1;\n        K *start;\n        // Step 0\n        grp_start_wp_id = ((warp_id>>1)<<1);\n        grp_start_off = (grp_start_wp_id<sml_wp)?grp_start_wp_id*64:sml_len+(grp_start_wp_id-sml_wp)*128;\n        tmp_wp_id = grp_start_wp_id;\n        lhs_len = ((tmp_wp_id+0<sml_wp)?64  :128 );\n        rhs_len = ((tmp_wp_id+1<sml_wp)?64  :128 );\n        gran = (warp_id<sml_wp)?(tid1<<1): (tid1<<2);\n        if((warp_id&1)==0){\n            gran += 0;\n        }\n        if((warp_id&1)==1){\n            gran += ((tmp_wp_id+0<sml_wp)?64  :128 );\n        }\n        start = smem + grp_start_off;\n        s_a = find_kth3(start, lhs_len, start+lhs_len, rhs_len, gran);\n        s_b = lhs_len + gran - s_a;\n        if(sml_warp){\n            tmp_k0 = (s_a<lhs_len        )?smem[grp_start_off+s_a]:std::numeric_limits<K>::max();\n            tmp_k1 = (s_b<lhs_len+rhs_len)?smem[grp_start_off+s_b]:std::numeric_limits<K>::max();\n            p = (s_b>=lhs_len+rhs_len)||((s_a<lhs_len)&&(tmp_k0<=tmp_k1));\n            rg_k0 = p ? tmp_k0 : tmp_k1;\n            if(p) {\n                ++s_a;\n                tmp_k0 = (s_a<lhs_len)?smem[grp_start_off+s_a]:std::numeric_limits<K>::max();\n            } else {\n                ++s_b;\n                tmp_k1 = (s_b<lhs_len+rhs_len)?smem[grp_start_off+s_b]:std::numeric_limits<K>::max();\n            }\n            p = (s_b>=lhs_len+rhs_len)||((s_a<lhs_len)&&(tmp_k0<=tmp_k1));\n            rg_k1 = p ? tmp_k0 : tmp_k1;\n        } else {\n            tmp_k0 = (s_a<lhs_len        )?smem[grp_start_off+s_a]:std::numeric_limits<K>::max();\n            tmp_k1 = (s_b<lhs_len+rhs_len)?smem[grp_start_off+s_b]:std::numeric_limits<K>::max();\n            p = (s_b>=lhs_len+rhs_len)||((s_a<lhs_len)&&(tmp_k0<=tmp_k1));\n            rg_k0 = p ? tmp_k0 : tmp_k1;\n            if(p) {\n                ++s_a;\n                tmp_k0 = (s_a<lhs_len)?smem[grp_start_off+s_a]:std::numeric_limits<K>::max();\n            } else {\n                ++s_b;\n                tmp_k1 = (s_b<lhs_len+rhs_len)?smem[grp_start_off+s_b]:std::numeric_limits<K>::max();\n            }\n            p = (s_b>=lhs_len+rhs_len)||((s_a<lhs_len)&&(tmp_k0<=tmp_k1));\n            rg_k1 = p ? tmp_k0 : tmp_k1;\n            if(p) {\n                ++s_a;\n                tmp_k0 = (s_a<lhs_len)?smem[grp_start_off+s_a]:std::numeric_limits<K>::max();\n            } else {\n                ++s_b;\n                tmp_k1 = (s_b<lhs_len+rhs_len)?smem[grp_start_off+s_b]:std::numeric_limits<K>::max();\n            }\n            p = (s_b>=lhs_len+rhs_len)||((s_a<lhs_len)&&(tmp_k0<=tmp_k1));\n            rg_k2 = p ? tmp_k0 : tmp_k1;\n            if(p) {\n                ++s_a;\n                tmp_k0 = (s_a<lhs_len)?smem[grp_start_off+s_a]:std::numeric_limits<K>::max();\n            } else {\n                ++s_b;\n                tmp_k1 = (s_b<lhs_len+rhs_len)?smem[grp_start_off+s_b]:std::numeric_limits<K>::max();\n            }\n            p = (s_b>=lhs_len+rhs_len)||((s_a<lhs_len)&&(tmp_k0<=tmp_k1));\n            rg_k3 = p ? tmp_k0 : tmp_k1;\n        }\n        __syncthreads();\n        // Store merged results back to shared memory\n        if(sml_warp){\n            smem[grp_start_off+gran+0 ] = rg_k0 ;\n            smem[grp_start_off+gran+1 ] = rg_k1 ;\n        } else {\n            smem[grp_start_off+gran+0 ] = rg_k0 ;\n            smem[grp_start_off+gran+1 ] = rg_k1 ;\n            smem[grp_start_off+gran+2 ] = rg_k2 ;\n            smem[grp_start_off+gran+3 ] = rg_k3 ;\n        }\n        __syncthreads();\n        // Step 1\n        grp_start_wp_id = ((warp_id>>2)<<2);\n        grp_start_off = (grp_start_wp_id<sml_wp)?grp_start_wp_id*64:sml_len+(grp_start_wp_id-sml_wp)*128;\n        tmp_wp_id = grp_start_wp_id;\n        lhs_len = ((tmp_wp_id+0<sml_wp)?64  :128 )+\n                  ((tmp_wp_id+1<sml_wp)?64  :128 );\n        rhs_len = ((tmp_wp_id+2<sml_wp)?64  :128 )+\n                  ((tmp_wp_id+3<sml_wp)?64  :128 );\n        gran = (warp_id<sml_wp)?(tid1<<1): (tid1<<2);\n        if((warp_id&3)==0){\n            gran += 0;\n        }\n        if((warp_id&3)==1){\n            gran += ((tmp_wp_id+0<sml_wp)?64  :128 );\n        }\n        if((warp_id&3)==2){\n            gran += ((tmp_wp_id+0<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+1<sml_wp)?64  :128 );\n        }\n        if((warp_id&3)==3){\n            gran += ((tmp_wp_id+0<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+1<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+2<sml_wp)?64  :128 );\n        }\n        start = smem + grp_start_off;\n        s_a = find_kth3(start, lhs_len, start+lhs_len, rhs_len, gran);\n        s_b = lhs_len + gran - s_a;\n        if(sml_warp){\n            tmp_k0 = (s_a<lhs_len        )?smem[grp_start_off+s_a]:std::numeric_limits<K>::max();\n            tmp_k1 = (s_b<lhs_len+rhs_len)?smem[grp_start_off+s_b]:std::numeric_limits<K>::max();\n            p = (s_b>=lhs_len+rhs_len)||((s_a<lhs_len)&&(tmp_k0<=tmp_k1));\n            rg_k0 = p ? tmp_k0 : tmp_k1;\n            if(p) {\n                ++s_a;\n                tmp_k0 = (s_a<lhs_len)?smem[grp_start_off+s_a]:std::numeric_limits<K>::max();\n            } else {\n                ++s_b;\n                tmp_k1 = (s_b<lhs_len+rhs_len)?smem[grp_start_off+s_b]:std::numeric_limits<K>::max();\n            }\n            p = (s_b>=lhs_len+rhs_len)||((s_a<lhs_len)&&(tmp_k0<=tmp_k1));\n            rg_k1 = p ? tmp_k0 : tmp_k1;\n        } else {\n            tmp_k0 = (s_a<lhs_len        )?smem[grp_start_off+s_a]:std::numeric_limits<K>::max();\n            tmp_k1 = (s_b<lhs_len+rhs_len)?smem[grp_start_off+s_b]:std::numeric_limits<K>::max();\n            p = (s_b>=lhs_len+rhs_len)||((s_a<lhs_len)&&(tmp_k0<=tmp_k1));\n            rg_k0 = p ? tmp_k0 : tmp_k1;\n            if(p) {\n                ++s_a;\n                tmp_k0 = (s_a<lhs_len)?smem[grp_start_off+s_a]:std::numeric_limits<K>::max();\n            } else {\n                ++s_b;\n                tmp_k1 = (s_b<lhs_len+rhs_len)?smem[grp_start_off+s_b]:std::numeric_limits<K>::max();\n            }\n            p = (s_b>=lhs_len+rhs_len)||((s_a<lhs_len)&&(tmp_k0<=tmp_k1));\n            rg_k1 = p ? tmp_k0 : tmp_k1;\n            if(p) {\n                ++s_a;\n                tmp_k0 = (s_a<lhs_len)?smem[grp_start_off+s_a]:std::numeric_limits<K>::max();\n            } else {\n                ++s_b;\n                tmp_k1 = (s_b<lhs_len+rhs_len)?smem[grp_start_off+s_b]:std::numeric_limits<K>::max();\n            }\n            p = (s_b>=lhs_len+rhs_len)||((s_a<lhs_len)&&(tmp_k0<=tmp_k1));\n            rg_k2 = p ? tmp_k0 : tmp_k1;\n            if(p) {\n                ++s_a;\n                tmp_k0 = (s_a<lhs_len)?smem[grp_start_off+s_a]:std::numeric_limits<K>::max();\n            } else {\n                ++s_b;\n                tmp_k1 = (s_b<lhs_len+rhs_len)?smem[grp_start_off+s_b]:std::numeric_limits<K>::max();\n            }\n            p = (s_b>=lhs_len+rhs_len)||((s_a<lhs_len)&&(tmp_k0<=tmp_k1));\n            rg_k3 = p ? tmp_k0 : tmp_k1;\n        }\n        if(sml_warp){\n        } else {\n        }\n        if(sml_warp){\n            if((tid<<1)+0 <seg_size) keyB[k+(tid<<1)+0 ] = rg_k0 ;\n            if((tid<<1)+1 <seg_size) keyB[k+(tid<<1)+1 ] = rg_k1 ;\n        } else {\n            if((tid<<2)+0 -sml_len<seg_size) keyB[k+(tid<<2)+0 -sml_len] = rg_k0 ;\n            if((tid<<2)+1 -sml_len<seg_size) keyB[k+(tid<<2)+1 -sml_len] = rg_k1 ;\n            if((tid<<2)+2 -sml_len<seg_size) keyB[k+(tid<<2)+2 -sml_len] = rg_k2 ;\n            if((tid<<2)+3 -sml_len<seg_size) keyB[k+(tid<<2)+3 -sml_len] = rg_k3 ;\n        }\n    }\n}",
            "#define K 1.38065e-23        // J/K, Boltzmann constant\n\n\ninline __host__ __device__ uint4 make_uint4(int4 a)\n{\n    return make_uint4(uint(a.x), uint(a.y), uint(a.z), uint(a.w));\n}\n\ninline __host__ __device__ uint4 max(uint4 a, uint4 b)\n{\n    return make_uint4(max(a.x,b.x), max(a.y,b.y), max(a.z,b.z), max(a.w,b.w));\n}\n\ninline __host__ __device__ uint4 min(uint4 a, uint4 b)\n{\n    return make_uint4(min(a.x,b.x), min(a.y,b.y), min(a.z,b.z), min(a.w,b.w));\n}\n\n__device__ inline void exch_intxn_keys(K &k0, K &k1, K &k2, K &k3, K &k4, K &k5, K &k6, K &k7, K &k8, K &k9, K &k10, K &k11, K &k12, K &k13, K &k14, K &k15, K &k16, K &k17, K &k18, K &k19, K &k20, K &k21, K &k22, K &k23, K &k24, K &k25, K &k26, K &k27, K &k28, K &k29, K &k30, K &k31, int mask, const int bit) {\n    K ex_k0, ex_k1;\n    if(bit) SWP_KEY(K, k0, k30);\n    if(bit) SWP_KEY(K, k1, k31);\n    if(bit) SWP_KEY(K, k2, k28);\n    if(bit) SWP_KEY(K, k3, k29);\n    if(bit) SWP_KEY(K, k4, k26);\n    if(bit) SWP_KEY(K, k5, k27);\n    if(bit) SWP_KEY(K, k6, k24);\n    if(bit) SWP_KEY(K, k7, k25);\n    if(bit) SWP_KEY(K, k8, k22);\n    if(bit) SWP_KEY(K, k9, k23);\n    if(bit) SWP_KEY(K, k10, k20);\n    if(bit) SWP_KEY(K, k11, k21);\n    if(bit) SWP_KEY(K, k12, k18);\n    if(bit) SWP_KEY(K, k13, k19);\n    if(bit) SWP_KEY(K, k14, k16);\n    if(bit) SWP_KEY(K, k15, k17);\n    ex_k0 = k0;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k1, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k0 = ex_k0;\n    k1 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k2;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k3, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k2 = ex_k0;\n    k3 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k4;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k5, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k4 = ex_k0;\n    k5 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k6;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k7, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k6 = ex_k0;\n    k7 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k8;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k9, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k8 = ex_k0;\n    k9 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k10;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k11, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k10 = ex_k0;\n    k11 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k12;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k13, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k12 = ex_k0;\n    k13 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k14;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k15, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k14 = ex_k0;\n    k15 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k16;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k17, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k16 = ex_k0;\n    k17 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k18;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k19, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k18 = ex_k0;\n    k19 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k20;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k21, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k20 = ex_k0;\n    k21 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k22;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k23, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k22 = ex_k0;\n    k23 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k24;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k25, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k24 = ex_k0;\n    k25 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k26;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k27, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k26 = ex_k0;\n    k27 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k28;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k29, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k28 = ex_k0;\n    k29 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k30;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k31, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k30 = ex_k0;\n    k31 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    if(bit) SWP_KEY(K, k0, k30);\n    if(bit) SWP_KEY(K, k1, k31);\n    if(bit) SWP_KEY(K, k2, k28);\n    if(bit) SWP_KEY(K, k3, k29);\n    if(bit) SWP_KEY(K, k4, k26);\n    if(bit) SWP_KEY(K, k5, k27);\n    if(bit) SWP_KEY(K, k6, k24);\n    if(bit) SWP_KEY(K, k7, k25);\n    if(bit) SWP_KEY(K, k8, k22);\n    if(bit) SWP_KEY(K, k9, k23);\n    if(bit) SWP_KEY(K, k10, k20);\n    if(bit) SWP_KEY(K, k11, k21);\n    if(bit) SWP_KEY(K, k12, k18);\n    if(bit) SWP_KEY(K, k13, k19);\n    if(bit) SWP_KEY(K, k14, k16);\n    if(bit) SWP_KEY(K, k15, k17);\n}\n\n__device__ inline void exch_paral_keys(K &k0, K &k1, K &k2, K &k3, K &k4, K &k5, K &k6, K &k7, K &k8, K &k9, K &k10, K &k11, K &k12, K &k13, K &k14, K &k15, K &k16, K &k17, K &k18, K &k19, K &k20, K &k21, K &k22, K &k23, K &k24, K &k25, K &k26, K &k27, K &k28, K &k29, K &k30, K &k31, int mask, const int bit) {\n    K ex_k0, ex_k1;\n    if(bit) SWP_KEY(K, k0, k1);\n    if(bit) SWP_KEY(K, k2, k3);\n    if(bit) SWP_KEY(K, k4, k5);\n    if(bit) SWP_KEY(K, k6, k7);\n    if(bit) SWP_KEY(K, k8, k9);\n    if(bit) SWP_KEY(K, k10, k11);\n    if(bit) SWP_KEY(K, k12, k13);\n    if(bit) SWP_KEY(K, k14, k15);\n    if(bit) SWP_KEY(K, k16, k17);\n    if(bit) SWP_KEY(K, k18, k19);\n    if(bit) SWP_KEY(K, k20, k21);\n    if(bit) SWP_KEY(K, k22, k23);\n    if(bit) SWP_KEY(K, k24, k25);\n    if(bit) SWP_KEY(K, k26, k27);\n    if(bit) SWP_KEY(K, k28, k29);\n    if(bit) SWP_KEY(K, k30, k31);\n    ex_k0 = k0;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k1, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k0 = ex_k0;\n    k1 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k2;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k3, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k2 = ex_k0;\n    k3 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k4;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k5, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k4 = ex_k0;\n    k5 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k6;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k7, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k6 = ex_k0;\n    k7 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k8;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k9, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k8 = ex_k0;\n    k9 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k10;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k11, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k10 = ex_k0;\n    k11 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k12;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k13, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k12 = ex_k0;\n    k13 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k14;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k15, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k14 = ex_k0;\n    k15 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k16;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k17, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k16 = ex_k0;\n    k17 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k18;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k19, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k18 = ex_k0;\n    k19 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k20;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k21, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k20 = ex_k0;\n    k21 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k22;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k23, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k22 = ex_k0;\n    k23 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k24;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k25, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k24 = ex_k0;\n    k25 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k26;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k27, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k26 = ex_k0;\n    k27 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k28;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k29, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k28 = ex_k0;\n    k29 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k30;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k31, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k30 = ex_k0;\n    k31 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    if(bit) SWP_KEY(K, k0, k1);\n    if(bit) SWP_KEY(K, k2, k3);\n    if(bit) SWP_KEY(K, k4, k5);\n    if(bit) SWP_KEY(K, k6, k7);\n    if(bit) SWP_KEY(K, k8, k9);\n    if(bit) SWP_KEY(K, k10, k11);\n    if(bit) SWP_KEY(K, k12, k13);\n    if(bit) SWP_KEY(K, k14, k15);\n    if(bit) SWP_KEY(K, k16, k17);\n    if(bit) SWP_KEY(K, k18, k19);\n    if(bit) SWP_KEY(K, k20, k21);\n    if(bit) SWP_KEY(K, k22, k23);\n    if(bit) SWP_KEY(K, k24, k25);\n    if(bit) SWP_KEY(K, k26, k27);\n    if(bit) SWP_KEY(K, k28, k29);\n    if(bit) SWP_KEY(K, k30, k31);\n}\n\n__device__\nint find_kth3(K* a, int aCount, K* b, int bCount, int diag)\n{\n    int begin = max(0, diag - bCount);\n    int end = min(diag, aCount);\n\n    while(begin < end) {\n        int mid = (begin + end)>> 1;\n        K aKey = a[mid];\n        K bKey = b[diag - 1 - mid];\n        bool pred = aKey <= bKey;\n        if(pred) begin = mid + 1;\n        else end = mid;\n    }\n    return begin;\n}\n\n__global__\nvoid gen_bk256_tc4_r513_r1024_orig(\n    K *key, K *keyB,\n    const Offset *seg_begins, const Offset *seg_ends,\n    const int *bin, const int bin_size)\n{\n    const int tid = threadIdx.x;\n    const int bin_it = blockIdx.x;\n    __shared__ K smem[1024];\n    const int bit1 = (tid>>0)&0x1;\n    const int bit2 = (tid>>1)&0x1;\n    const int bit3 = (tid>>2)&0x1;\n    const int bit4 = (tid>>3)&0x1;\n    const int bit5 = (tid>>4)&0x1;\n    const int tid1 = threadIdx.x & 31;\n    const int warp_id = threadIdx.x / 32;\n    K rg_k0 ;\n    K rg_k1 ;\n    K rg_k2 ;\n    K rg_k3 ;\n    int k;\n    int seg_size;\n    int ext_seg_size;\n    if(bin_it < bin_size) {\n        k = seg_begins[bin[bin_it]];\n        seg_size = seg_ends[bin[bin_it]]-seg_begins[bin[bin_it]];\n        ext_seg_size = ((seg_size + 63) / 64) * 64;\n        int big_wp = (ext_seg_size - blockDim.x * 2) / 64;\n        int sml_wp = blockDim.x / 32 - big_wp;\n        int sml_len = sml_wp * 64;\n        const int big_warp_id = (warp_id - sml_wp < 0)? 0: warp_id - sml_wp;\n        bool sml_warp = warp_id < sml_wp;\n        if(sml_warp) {\n            rg_k0 = key[k+(warp_id<<6)+tid1+0   ];\n            rg_k1 = key[k+(warp_id<<6)+tid1+32  ];\n            // exch_intxn: switch to exch_local()\n            CMP_SWP_KEY(K,rg_k0 ,rg_k1 );\n            // exch_intxn: generate exch_intxn_keys()\n            exch_intxn_keys(rg_k0 ,rg_k1 ,\n                            0x1,bit1);\n            // exch_paral: switch to exch_local()\n            CMP_SWP_KEY(K,rg_k0 ,rg_k1 );\n            // exch_intxn: generate exch_intxn_keys()\n            exch_intxn_keys(rg_k0 ,rg_k1 ,\n                            0x3,bit2);\n            // exch_paral: generate exch_paral_keys()\n            exch_paral_keys(rg_k0 ,rg_k1 ,\n                            0x1,bit1);\n            // exch_paral: switch to exch_local()\n            CMP_SWP_KEY(K,rg_k0 ,rg_k1 );\n            // exch_intxn: generate exch_intxn_keys()\n            exch_intxn_keys(rg_k0 ,rg_k1 ,\n                            0x7,bit3);\n            // exch_paral: generate exch_paral_keys()\n            exch_paral_keys(rg_k0 ,rg_k1 ,\n                            0x2,bit2);\n            // exch_paral: generate exch_paral_keys()\n            exch_paral_keys(rg_k0 ,rg_k1 ,\n                            0x1,bit1);\n            // exch_paral: switch to exch_local()\n            CMP_SWP_KEY(K,rg_k0 ,rg_k1 );\n            // exch_intxn: generate exch_intxn_keys()\n            exch_intxn_keys(rg_k0 ,rg_k1 ,\n                            0xf,bit4);\n            // exch_paral: generate exch_paral_keys()\n            exch_paral_keys(rg_k0 ,rg_k1 ,\n                            0x4,bit3);\n            // exch_paral: generate exch_paral_keys()\n            exch_paral_keys(rg_k0 ,rg_k1 ,\n                            0x2,bit2);\n            // exch_paral: generate exch_paral_keys()\n            exch_paral_keys(rg_k0 ,rg_k1 ,\n                            0x1,bit1);\n            // exch_paral: switch to exch_local()\n            CMP_SWP_KEY(K,rg_k0 ,rg_k1 );\n            // exch_intxn: generate exch_intxn_keys()\n            exch_intxn_keys(rg_k0 ,rg_k1 ,\n                            0x1f,bit5);\n            // exch_paral: generate exch_paral_keys()\n            exch_paral_keys(rg_k0 ,rg_k1 ,\n                            0x8,bit4);\n            // exch_paral: generate exch_paral_keys()\n            exch_paral_keys(rg_k0 ,rg_k1 ,\n                            0x4,bit3);\n            // exch_paral: generate exch_paral_keys()\n            exch_paral_keys(rg_k0 ,rg_k1 ,\n                            0x2,bit2);\n            // exch_paral: generate exch_paral_keys()\n            exch_paral_keys(rg_k0 ,rg_k1 ,\n                            0x1,bit1);\n            // exch_paral: switch to exch_local()\n            CMP_SWP_KEY(K,rg_k0 ,rg_k1 );\n        } else {\n            rg_k0  = (sml_len+tid1+(big_warp_id<<7)+0   <seg_size)?key[k+sml_len+tid1+(big_warp_id<<7)+0   ]:std::numeric_limits<K>::max();\n            rg_k1  = (sml_len+tid1+(big_warp_id<<7)+32  <seg_size)?key[k+sml_len+tid1+(big_warp_id<<7)+32  ]:std::numeric_limits<K>::max();\n            rg_k2  = (sml_len+tid1+(big_warp_id<<7)+64  <seg_size)?key[k+sml_len+tid1+(big_warp_id<<7)+64  ]:std::numeric_limits<K>::max();\n            rg_k3  = (sml_len+tid1+(big_warp_id<<7)+96  <seg_size)?key[k+sml_len+tid1+(big_warp_id<<7)+96  ]:std::numeric_limits<K>::max();\n            // exch_intxn: switch to exch_local()\n            CMP_SWP_KEY(K,rg_k0 ,rg_k1 );\n            CMP_SWP_KEY(K,rg_k2 ,rg_k3 );\n            // exch_intxn: switch to exch_local()\n            CMP_SWP_KEY(K,rg_k0 ,rg_k3 );\n            CMP_SWP_KEY(K,rg_k1 ,rg_k2 );\n            // exch_paral: switch to exch_local()\n            CMP_SWP_KEY(K,rg_k0 ,rg_k1 );\n            CMP_SWP_KEY(K,rg_k2 ,rg_k3 );\n            // exch_intxn: generate exch_intxn_keys()\n            exch_intxn_keys(rg_k0 ,rg_k1 ,rg_k2 ,rg_k3 ,\n                            0x1,bit1);\n            // exch_paral: switch to exch_local()\n            CMP_SWP_KEY(K,rg_k0 ,rg_k2 );\n            CMP_SWP_KEY(K,rg_k1 ,rg_k3 );\n            // exch_paral: switch to exch_local()\n            CMP_SWP_KEY(K,rg_k0 ,rg_k1 );\n            CMP_SWP_KEY(K,rg_k2 ,rg_k3 );\n            // exch_intxn: generate exch_intxn_keys()\n            exch_intxn_keys(rg_k0 ,rg_k1 ,rg_k2 ,rg_k3 ,\n                            0x3,bit2);\n            // exch_paral: generate exch_paral_keys()\n            exch_paral_keys(rg_k0 ,rg_k1 ,rg_k2 ,rg_k3 ,\n                            0x1,bit1);\n            // exch_paral: switch to exch_local()\n            CMP_SWP_KEY(K,rg_k0 ,rg_k2 );\n            CMP_SWP_KEY(K,rg_k1 ,rg_k3 );\n            // exch_paral: switch to exch_local()\n            CMP_SWP_KEY(K,rg_k0 ,rg_k1 );\n            CMP_SWP_KEY(K,rg_k2 ,rg_k3 );\n            // exch_intxn: generate exch_intxn_keys()\n            exch_intxn_keys(rg_k0 ,rg_k1 ,rg_k2 ,rg_k3 ,\n                            0x7,bit3);\n            // exch_paral: generate exch_paral_keys()\n            exch_paral_keys(rg_k0 ,rg_k1 ,rg_k2 ,rg_k3 ,\n                            0x2,bit2);\n            // exch_paral: generate exch_paral_keys()\n            exch_paral_keys(rg_k0 ,rg_k1 ,rg_k2 ,rg_k3 ,\n                            0x1,bit1);\n            // exch_paral: switch to exch_local()\n            CMP_SWP_KEY(K,rg_k0 ,rg_k2 );\n            CMP_SWP_KEY(K,rg_k1 ,rg_k3 );\n            // exch_paral: switch to exch_local()\n            CMP_SWP_KEY(K,rg_k0 ,rg_k1 );\n            CMP_SWP_KEY(K,rg_k2 ,rg_k3 );\n            // exch_intxn: generate exch_intxn_keys()\n            exch_intxn_keys(rg_k0 ,rg_k1 ,rg_k2 ,rg_k3 ,\n                            0xf,bit4);\n            // exch_paral: generate exch_paral_keys()\n            exch_paral_keys(rg_k0 ,rg_k1 ,rg_k2 ,rg_k3 ,\n                            0x4,bit3);\n            // exch_paral: generate exch_paral_keys()\n            exch_paral_keys(rg_k0 ,rg_k1 ,rg_k2 ,rg_k3 ,\n                            0x2,bit2);\n            // exch_paral: generate exch_paral_keys()\n            exch_paral_keys(rg_k0 ,rg_k1 ,rg_k2 ,rg_k3 ,\n                            0x1,bit1);\n            // exch_paral: switch to exch_local()\n            CMP_SWP_KEY(K,rg_k0 ,rg_k2 );\n            CMP_SWP_KEY(K,rg_k1 ,rg_k3 );\n            // exch_paral: switch to exch_local()\n            CMP_SWP_KEY(K,rg_k0 ,rg_k1 );\n            CMP_SWP_KEY(K,rg_k2 ,rg_k3 );\n            // exch_intxn: generate exch_intxn_keys()\n            exch_intxn_keys(rg_k0 ,rg_k1 ,rg_k2 ,rg_k3 ,\n                            0x1f,bit5);\n            // exch_paral: generate exch_paral_keys()\n            exch_paral_keys(rg_k0 ,rg_k1 ,rg_k2 ,rg_k3 ,\n                            0x8,bit4);\n            // exch_paral: generate exch_paral_keys()\n            exch_paral_keys(rg_k0 ,rg_k1 ,rg_k2 ,rg_k3 ,\n                            0x4,bit3);\n            // exch_paral: generate exch_paral_keys()\n            exch_paral_keys(rg_k0 ,rg_k1 ,rg_k2 ,rg_k3 ,\n                            0x2,bit2);\n            // exch_paral: generate exch_paral_keys()\n            exch_paral_keys(rg_k0 ,rg_k1 ,rg_k2 ,rg_k3 ,\n                            0x1,bit1);\n            // exch_paral: switch to exch_local()\n            CMP_SWP_KEY(K,rg_k0 ,rg_k2 );\n            CMP_SWP_KEY(K,rg_k1 ,rg_k3 );\n            // exch_paral: switch to exch_local()\n            CMP_SWP_KEY(K,rg_k0 ,rg_k1 );\n            CMP_SWP_KEY(K,rg_k2 ,rg_k3 );\n        }\n        // Store register results to shared memory\n        if(sml_warp) {\n            smem[(warp_id<<6)+(tid1<<1)+0 ] = rg_k0 ;\n            smem[(warp_id<<6)+(tid1<<1)+1 ] = rg_k1 ;\n        } else {\n            smem[sml_len+(big_warp_id<<7)+(tid1<<2)+0 ] = rg_k0 ;\n            smem[sml_len+(big_warp_id<<7)+(tid1<<2)+1 ] = rg_k1 ;\n            smem[sml_len+(big_warp_id<<7)+(tid1<<2)+2 ] = rg_k2 ;\n            smem[sml_len+(big_warp_id<<7)+(tid1<<2)+3 ] = rg_k3 ;\n        }\n        __syncthreads();\n        // Merge in 3 steps\n        int grp_start_wp_id;\n        int grp_start_off;\n        int tmp_wp_id;\n        int lhs_len;\n        int rhs_len;\n        int gran;\n        int s_a;\n        int s_b;\n        bool p;\n        K tmp_k0;\n        K tmp_k1;\n        K *start;\n        // Step 0\n        grp_start_wp_id = ((warp_id>>1)<<1);\n        grp_start_off = (grp_start_wp_id<sml_wp)?grp_start_wp_id*64:sml_len+(grp_start_wp_id-sml_wp)*128;\n        tmp_wp_id = grp_start_wp_id;\n        lhs_len = ((tmp_wp_id+0<sml_wp)?64  :128 );\n        rhs_len = ((tmp_wp_id+1<sml_wp)?64  :128 );\n        gran = (warp_id<sml_wp)?(tid1<<1): (tid1<<2);\n        if((warp_id&1)==0){\n            gran += 0;\n        }\n        if((warp_id&1)==1){\n            gran += ((tmp_wp_id+0<sml_wp)?64  :128 );\n        }\n        start = smem + grp_start_off;\n        s_a = find_kth3(start, lhs_len, start+lhs_len, rhs_len, gran);\n        s_b = lhs_len + gran - s_a;\n        if(sml_warp){\n            tmp_k0 = (s_a<lhs_len        )?smem[grp_start_off+s_a]:std::numeric_limits<K>::max();\n            tmp_k1 = (s_b<lhs_len+rhs_len)?smem[grp_start_off+s_b]:std::numeric_limits<K>::max();\n            p = (s_b>=lhs_len+rhs_len)||((s_a<lhs_len)&&(tmp_k0<=tmp_k1));\n            rg_k0 = p ? tmp_k0 : tmp_k1;\n            if(p) {\n                ++s_a;\n                tmp_k0 = (s_a<lhs_len)?smem[grp_start_off+s_a]:std::numeric_limits<K>::max();\n            } else {\n                ++s_b;\n                tmp_k1 = (s_b<lhs_len+rhs_len)?smem[grp_start_off+s_b]:std::numeric_limits<K>::max();\n            }\n            p = (s_b>=lhs_len+rhs_len)||((s_a<lhs_len)&&(tmp_k0<=tmp_k1));\n            rg_k1 = p ? tmp_k0 : tmp_k1;\n        } else {\n            tmp_k0 = (s_a<lhs_len        )?smem[grp_start_off+s_a]:std::numeric_limits<K>::max();\n            tmp_k1 = (s_b<lhs_len+rhs_len)?smem[grp_start_off+s_b]:std::numeric_limits<K>::max();\n            p = (s_b>=lhs_len+rhs_len)||((s_a<lhs_len)&&(tmp_k0<=tmp_k1));\n            rg_k0 = p ? tmp_k0 : tmp_k1;\n            if(p) {\n                ++s_a;\n                tmp_k0 = (s_a<lhs_len)?smem[grp_start_off+s_a]:std::numeric_limits<K>::max();\n            } else {\n                ++s_b;\n                tmp_k1 = (s_b<lhs_len+rhs_len)?smem[grp_start_off+s_b]:std::numeric_limits<K>::max();\n            }\n            p = (s_b>=lhs_len+rhs_len)||((s_a<lhs_len)&&(tmp_k0<=tmp_k1));\n            rg_k1 = p ? tmp_k0 : tmp_k1;\n            if(p) {\n                ++s_a;\n                tmp_k0 = (s_a<lhs_len)?smem[grp_start_off+s_a]:std::numeric_limits<K>::max();\n            } else {\n                ++s_b;\n                tmp_k1 = (s_b<lhs_len+rhs_len)?smem[grp_start_off+s_b]:std::numeric_limits<K>::max();\n            }\n            p = (s_b>=lhs_len+rhs_len)||((s_a<lhs_len)&&(tmp_k0<=tmp_k1));\n            rg_k2 = p ? tmp_k0 : tmp_k1;\n            if(p) {\n                ++s_a;\n                tmp_k0 = (s_a<lhs_len)?smem[grp_start_off+s_a]:std::numeric_limits<K>::max();\n            } else {\n                ++s_b;\n                tmp_k1 = (s_b<lhs_len+rhs_len)?smem[grp_start_off+s_b]:std::numeric_limits<K>::max();\n            }\n            p = (s_b>=lhs_len+rhs_len)||((s_a<lhs_len)&&(tmp_k0<=tmp_k1));\n            rg_k3 = p ? tmp_k0 : tmp_k1;\n        }\n        __syncthreads();\n        // Store merged results back to shared memory\n        if(sml_warp){\n            smem[grp_start_off+gran+0 ] = rg_k0 ;\n            smem[grp_start_off+gran+1 ] = rg_k1 ;\n        } else {\n            smem[grp_start_off+gran+0 ] = rg_k0 ;\n            smem[grp_start_off+gran+1 ] = rg_k1 ;\n            smem[grp_start_off+gran+2 ] = rg_k2 ;\n            smem[grp_start_off+gran+3 ] = rg_k3 ;\n        }\n        __syncthreads();\n        // Step 1\n        grp_start_wp_id = ((warp_id>>2)<<2);\n        grp_start_off = (grp_start_wp_id<sml_wp)?grp_start_wp_id*64:sml_len+(grp_start_wp_id-sml_wp)*128;\n        tmp_wp_id = grp_start_wp_id;\n        lhs_len = ((tmp_wp_id+0<sml_wp)?64  :128 )+\n                  ((tmp_wp_id+1<sml_wp)?64  :128 );\n        rhs_len = ((tmp_wp_id+2<sml_wp)?64  :128 )+\n                  ((tmp_wp_id+3<sml_wp)?64  :128 );\n        gran = (warp_id<sml_wp)?(tid1<<1): (tid1<<2);\n        if((warp_id&3)==0){\n            gran += 0;\n        }\n        if((warp_id&3)==1){\n            gran += ((tmp_wp_id+0<sml_wp)?64  :128 );\n        }\n        if((warp_id&3)==2){\n            gran += ((tmp_wp_id+0<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+1<sml_wp)?64  :128 );\n        }\n        if((warp_id&3)==3){\n            gran += ((tmp_wp_id+0<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+1<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+2<sml_wp)?64  :128 );\n        }\n        start = smem + grp_start_off;\n        s_a = find_kth3(start, lhs_len, start+lhs_len, rhs_len, gran);\n        s_b = lhs_len + gran - s_a;\n        if(sml_warp){\n            tmp_k0 = (s_a<lhs_len        )?smem[grp_start_off+s_a]:std::numeric_limits<K>::max();\n            tmp_k1 = (s_b<lhs_len+rhs_len)?smem[grp_start_off+s_b]:std::numeric_limits<K>::max();\n            p = (s_b>=lhs_len+rhs_len)||((s_a<lhs_len)&&(tmp_k0<=tmp_k1));\n            rg_k0 = p ? tmp_k0 : tmp_k1;\n            if(p) {\n                ++s_a;\n                tmp_k0 = (s_a<lhs_len)?smem[grp_start_off+s_a]:std::numeric_limits<K>::max();\n            } else {\n                ++s_b;\n                tmp_k1 = (s_b<lhs_len+rhs_len)?smem[grp_start_off+s_b]:std::numeric_limits<K>::max();\n            }\n            p = (s_b>=lhs_len+rhs_len)||((s_a<lhs_len)&&(tmp_k0<=tmp_k1));\n            rg_k1 = p ? tmp_k0 : tmp_k1;\n        } else {\n            tmp_k0 = (s_a<lhs_len        )?smem[grp_start_off+s_a]:std::numeric_limits<K>::max();\n            tmp_k1 = (s_b<lhs_len+rhs_len)?smem[grp_start_off+s_b]:std::numeric_limits<K>::max();\n            p = (s_b>=lhs_len+rhs_len)||((s_a<lhs_len)&&(tmp_k0<=tmp_k1));\n            rg_k0 = p ? tmp_k0 : tmp_k1;\n            if(p) {\n                ++s_a;\n                tmp_k0 = (s_a<lhs_len)?smem[grp_start_off+s_a]:std::numeric_limits<K>::max();\n            } else {\n                ++s_b;\n                tmp_k1 = (s_b<lhs_len+rhs_len)?smem[grp_start_off+s_b]:std::numeric_limits<K>::max();\n            }\n            p = (s_b>=lhs_len+rhs_len)||((s_a<lhs_len)&&(tmp_k0<=tmp_k1));\n            rg_k1 = p ? tmp_k0 : tmp_k1;\n            if(p) {\n                ++s_a;\n                tmp_k0 = (s_a<lhs_len)?smem[grp_start_off+s_a]:std::numeric_limits<K>::max();\n            } else {\n                ++s_b;\n                tmp_k1 = (s_b<lhs_len+rhs_len)?smem[grp_start_off+s_b]:std::numeric_limits<K>::max();\n            }\n            p = (s_b>=lhs_len+rhs_len)||((s_a<lhs_len)&&(tmp_k0<=tmp_k1));\n            rg_k2 = p ? tmp_k0 : tmp_k1;\n            if(p) {\n                ++s_a;\n                tmp_k0 = (s_a<lhs_len)?smem[grp_start_off+s_a]:std::numeric_limits<K>::max();\n            } else {\n                ++s_b;\n                tmp_k1 = (s_b<lhs_len+rhs_len)?smem[grp_start_off+s_b]:std::numeric_limits<K>::max();\n            }\n            p = (s_b>=lhs_len+rhs_len)||((s_a<lhs_len)&&(tmp_k0<=tmp_k1));\n            rg_k3 = p ? tmp_k0 : tmp_k1;\n        }\n        __syncthreads();\n        // Store merged results back to shared memory\n        if(sml_warp){\n            smem[grp_start_off+gran+0 ] = rg_k0 ;\n            smem[grp_start_off+gran+1 ] = rg_k1 ;\n        } else {\n            smem[grp_start_off+gran+0 ] = rg_k0 ;\n            smem[grp_start_off+gran+1 ] = rg_k1 ;\n            smem[grp_start_off+gran+2 ] = rg_k2 ;\n            smem[grp_start_off+gran+3 ] = rg_k3 ;\n        }\n        __syncthreads();\n        // Step 2\n        grp_start_wp_id = ((warp_id>>3)<<3);\n        grp_start_off = (grp_start_wp_id<sml_wp)?grp_start_wp_id*64:sml_len+(grp_start_wp_id-sml_wp)*128;\n        tmp_wp_id = grp_start_wp_id;\n        lhs_len = ((tmp_wp_id+0<sml_wp)?64  :128 )+\n                  ((tmp_wp_id+1<sml_wp)?64  :128 )+\n                  ((tmp_wp_id+2<sml_wp)?64  :128 )+\n                  ((tmp_wp_id+3<sml_wp)?64  :128 );\n        rhs_len = ((tmp_wp_id+4<sml_wp)?64  :128 )+\n                  ((tmp_wp_id+5<sml_wp)?64  :128 )+\n                  ((tmp_wp_id+6<sml_wp)?64  :128 )+\n                  ((tmp_wp_id+7<sml_wp)?64  :128 );\n        gran = (warp_id<sml_wp)?(tid1<<1): (tid1<<2);\n        if((warp_id&7)==0){\n            gran += 0;\n        }\n        if((warp_id&7)==1){\n            gran += ((tmp_wp_id+0<sml_wp)?64  :128 );\n        }\n        if((warp_id&7)==2){\n            gran += ((tmp_wp_id+0<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+1<sml_wp)?64  :128 );\n        }\n        if((warp_id&7)==3){\n            gran += ((tmp_wp_id+0<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+1<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+2<sml_wp)?64  :128 );\n        }\n        if((warp_id&7)==4){\n            gran += ((tmp_wp_id+0<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+1<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+2<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+3<sml_wp)?64  :128 );\n        }\n        if((warp_id&7)==5){\n            gran += ((tmp_wp_id+0<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+1<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+2<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+3<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+4<sml_wp)?64  :128 );\n        }\n        if((warp_id&7)==6){\n            gran += ((tmp_wp_id+0<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+1<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+2<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+3<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+4<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+5<sml_wp)?64  :128 );\n        }\n        if((warp_id&7)==7){\n            gran += ((tmp_wp_id+0<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+1<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+2<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+3<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+4<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+5<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+6<sml_wp)?64  :128 );\n        }\n        start = smem + grp_start_off;\n        s_a = find_kth3(start, lhs_len, start+lhs_len, rhs_len, gran);\n        s_b = lhs_len + gran - s_a;\n        if(sml_warp){\n            tmp_k0 = (s_a<lhs_len        )?smem[grp_start_off+s_a]:std::numeric_limits<K>::max();\n            tmp_k1 = (s_b<lhs_len+rhs_len)?smem[grp_start_off+s_b]:std::numeric_limits<K>::max();\n            p = (s_b>=lhs_len+rhs_len)||((s_a<lhs_len)&&(tmp_k0<=tmp_k1));\n            rg_k0 = p ? tmp_k0 : tmp_k1;\n            if(p) {\n                ++s_a;\n                tmp_k0 = (s_a<lhs_len)?smem[grp_start_off+s_a]:std::numeric_limits<K>::max();\n            } else {\n                ++s_b;\n                tmp_k1 = (s_b<lhs_len+rhs_len)?smem[grp_start_off+s_b]:std::numeric_limits<K>::max();\n            }\n            p = (s_b>=lhs_len+rhs_len)||((s_a<lhs_len)&&(tmp_k0<=tmp_k1));\n            rg_k1 = p ? tmp_k0 : tmp_k1;\n        } else {\n            tmp_k0 = (s_a<lhs_len        )?smem[grp_start_off+s_a]:std::numeric_limits<K>::max();\n            tmp_k1 = (s_b<lhs_len+rhs_len)?smem[grp_start_off+s_b]:std::numeric_limits<K>::max();\n            p = (s_b>=lhs_len+rhs_len)||((s_a<lhs_len)&&(tmp_k0<=tmp_k1));\n            rg_k0 = p ? tmp_k0 : tmp_k1;\n            if(p) {\n                ++s_a;\n                tmp_k0 = (s_a<lhs_len)?smem[grp_start_off+s_a]:std::numeric_limits<K>::max();\n            } else {\n                ++s_b;\n                tmp_k1 = (s_b<lhs_len+rhs_len)?smem[grp_start_off+s_b]:std::numeric_limits<K>::max();\n            }\n            p = (s_b>=lhs_len+rhs_len)||((s_a<lhs_len)&&(tmp_k0<=tmp_k1));\n            rg_k1 = p ? tmp_k0 : tmp_k1;\n            if(p) {\n                ++s_a;\n                tmp_k0 = (s_a<lhs_len)?smem[grp_start_off+s_a]:std::numeric_limits<K>::max();\n            } else {\n                ++s_b;\n                tmp_k1 = (s_b<lhs_len+rhs_len)?smem[grp_start_off+s_b]:std::numeric_limits<K>::max();\n            }\n            p = (s_b>=lhs_len+rhs_len)||((s_a<lhs_len)&&(tmp_k0<=tmp_k1));\n            rg_k2 = p ? tmp_k0 : tmp_k1;\n            if(p) {\n                ++s_a;\n                tmp_k0 = (s_a<lhs_len)?smem[grp_start_off+s_a]:std::numeric_limits<K>::max();\n            } else {\n                ++s_b;\n                tmp_k1 = (s_b<lhs_len+rhs_len)?smem[grp_start_off+s_b]:std::numeric_limits<K>::max();\n            }\n            p = (s_b>=lhs_len+rhs_len)||((s_a<lhs_len)&&(tmp_k0<=tmp_k1));\n            rg_k3 = p ? tmp_k0 : tmp_k1;\n        }\n        if(sml_warp){\n        } else {\n        }\n        if(sml_warp){\n            if((tid<<1)+0 <seg_size) keyB[k+(tid<<1)+0 ] = rg_k0 ;\n            if((tid<<1)+1 <seg_size) keyB[k+(tid<<1)+1 ] = rg_k1 ;\n        } else {\n            if((tid<<2)+0 -sml_len<seg_size) keyB[k+(tid<<2)+0 -sml_len] = rg_k0 ;\n            if((tid<<2)+1 -sml_len<seg_size) keyB[k+(tid<<2)+1 -sml_len] = rg_k1 ;\n            if((tid<<2)+2 -sml_len<seg_size) keyB[k+(tid<<2)+2 -sml_len] = rg_k2 ;\n            if((tid<<2)+3 -sml_len<seg_size) keyB[k+(tid<<2)+3 -sml_len] = rg_k3 ;\n        }\n    }\n}",
            "#define K 1.38065e-23        // J/K, Boltzmann constant\n\n\ninline __host__ __device__ uint4 make_uint4(int4 a)\n{\n    return make_uint4(uint(a.x), uint(a.y), uint(a.z), uint(a.w));\n}\n\ninline __host__ __device__ uint4 max(uint4 a, uint4 b)\n{\n    return make_uint4(max(a.x,b.x), max(a.y,b.y), max(a.z,b.z), max(a.w,b.w));\n}\n\ninline __host__ __device__ uint4 min(uint4 a, uint4 b)\n{\n    return make_uint4(min(a.x,b.x), min(a.y,b.y), min(a.z,b.z), min(a.w,b.w));\n}\n\n__device__ inline void exch_intxn_keys(K &k0, K &k1, K &k2, K &k3, K &k4, K &k5, K &k6, K &k7, K &k8, K &k9, K &k10, K &k11, K &k12, K &k13, K &k14, K &k15, K &k16, K &k17, K &k18, K &k19, K &k20, K &k21, K &k22, K &k23, K &k24, K &k25, K &k26, K &k27, K &k28, K &k29, K &k30, K &k31, int mask, const int bit) {\n    K ex_k0, ex_k1;\n    if(bit) SWP_KEY(K, k0, k30);\n    if(bit) SWP_KEY(K, k1, k31);\n    if(bit) SWP_KEY(K, k2, k28);\n    if(bit) SWP_KEY(K, k3, k29);\n    if(bit) SWP_KEY(K, k4, k26);\n    if(bit) SWP_KEY(K, k5, k27);\n    if(bit) SWP_KEY(K, k6, k24);\n    if(bit) SWP_KEY(K, k7, k25);\n    if(bit) SWP_KEY(K, k8, k22);\n    if(bit) SWP_KEY(K, k9, k23);\n    if(bit) SWP_KEY(K, k10, k20);\n    if(bit) SWP_KEY(K, k11, k21);\n    if(bit) SWP_KEY(K, k12, k18);\n    if(bit) SWP_KEY(K, k13, k19);\n    if(bit) SWP_KEY(K, k14, k16);\n    if(bit) SWP_KEY(K, k15, k17);\n    ex_k0 = k0;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k1, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k0 = ex_k0;\n    k1 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k2;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k3, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k2 = ex_k0;\n    k3 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k4;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k5, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k4 = ex_k0;\n    k5 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k6;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k7, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k6 = ex_k0;\n    k7 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k8;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k9, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k8 = ex_k0;\n    k9 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k10;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k11, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k10 = ex_k0;\n    k11 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k12;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k13, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k12 = ex_k0;\n    k13 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k14;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k15, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k14 = ex_k0;\n    k15 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k16;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k17, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k16 = ex_k0;\n    k17 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k18;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k19, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k18 = ex_k0;\n    k19 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k20;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k21, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k20 = ex_k0;\n    k21 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k22;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k23, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k22 = ex_k0;\n    k23 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k24;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k25, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k24 = ex_k0;\n    k25 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k26;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k27, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k26 = ex_k0;\n    k27 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k28;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k29, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k28 = ex_k0;\n    k29 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k30;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k31, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k30 = ex_k0;\n    k31 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    if(bit) SWP_KEY(K, k0, k30);\n    if(bit) SWP_KEY(K, k1, k31);\n    if(bit) SWP_KEY(K, k2, k28);\n    if(bit) SWP_KEY(K, k3, k29);\n    if(bit) SWP_KEY(K, k4, k26);\n    if(bit) SWP_KEY(K, k5, k27);\n    if(bit) SWP_KEY(K, k6, k24);\n    if(bit) SWP_KEY(K, k7, k25);\n    if(bit) SWP_KEY(K, k8, k22);\n    if(bit) SWP_KEY(K, k9, k23);\n    if(bit) SWP_KEY(K, k10, k20);\n    if(bit) SWP_KEY(K, k11, k21);\n    if(bit) SWP_KEY(K, k12, k18);\n    if(bit) SWP_KEY(K, k13, k19);\n    if(bit) SWP_KEY(K, k14, k16);\n    if(bit) SWP_KEY(K, k15, k17);\n}\n\n__device__ inline void exch_paral_keys(K &k0, K &k1, K &k2, K &k3, K &k4, K &k5, K &k6, K &k7, K &k8, K &k9, K &k10, K &k11, K &k12, K &k13, K &k14, K &k15, K &k16, K &k17, K &k18, K &k19, K &k20, K &k21, K &k22, K &k23, K &k24, K &k25, K &k26, K &k27, K &k28, K &k29, K &k30, K &k31, int mask, const int bit) {\n    K ex_k0, ex_k1;\n    if(bit) SWP_KEY(K, k0, k1);\n    if(bit) SWP_KEY(K, k2, k3);\n    if(bit) SWP_KEY(K, k4, k5);\n    if(bit) SWP_KEY(K, k6, k7);\n    if(bit) SWP_KEY(K, k8, k9);\n    if(bit) SWP_KEY(K, k10, k11);\n    if(bit) SWP_KEY(K, k12, k13);\n    if(bit) SWP_KEY(K, k14, k15);\n    if(bit) SWP_KEY(K, k16, k17);\n    if(bit) SWP_KEY(K, k18, k19);\n    if(bit) SWP_KEY(K, k20, k21);\n    if(bit) SWP_KEY(K, k22, k23);\n    if(bit) SWP_KEY(K, k24, k25);\n    if(bit) SWP_KEY(K, k26, k27);\n    if(bit) SWP_KEY(K, k28, k29);\n    if(bit) SWP_KEY(K, k30, k31);\n    ex_k0 = k0;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k1, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k0 = ex_k0;\n    k1 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k2;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k3, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k2 = ex_k0;\n    k3 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k4;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k5, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k4 = ex_k0;\n    k5 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k6;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k7, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k6 = ex_k0;\n    k7 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k8;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k9, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k8 = ex_k0;\n    k9 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k10;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k11, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k10 = ex_k0;\n    k11 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k12;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k13, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k12 = ex_k0;\n    k13 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k14;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k15, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k14 = ex_k0;\n    k15 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k16;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k17, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k16 = ex_k0;\n    k17 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k18;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k19, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k18 = ex_k0;\n    k19 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k20;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k21, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k20 = ex_k0;\n    k21 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k22;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k23, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k22 = ex_k0;\n    k23 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k24;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k25, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k24 = ex_k0;\n    k25 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k26;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k27, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k26 = ex_k0;\n    k27 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k28;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k29, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k28 = ex_k0;\n    k29 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    ex_k0 = k30;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k31, mask);\n    CMP_SWP_KEY(K, ex_k0, ex_k1);\n    if(bit) EQL_SWP_KEY(K, ex_k0, ex_k1);\n    k30 = ex_k0;\n    k31 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    if(bit) SWP_KEY(K, k0, k1);\n    if(bit) SWP_KEY(K, k2, k3);\n    if(bit) SWP_KEY(K, k4, k5);\n    if(bit) SWP_KEY(K, k6, k7);\n    if(bit) SWP_KEY(K, k8, k9);\n    if(bit) SWP_KEY(K, k10, k11);\n    if(bit) SWP_KEY(K, k12, k13);\n    if(bit) SWP_KEY(K, k14, k15);\n    if(bit) SWP_KEY(K, k16, k17);\n    if(bit) SWP_KEY(K, k18, k19);\n    if(bit) SWP_KEY(K, k20, k21);\n    if(bit) SWP_KEY(K, k22, k23);\n    if(bit) SWP_KEY(K, k24, k25);\n    if(bit) SWP_KEY(K, k26, k27);\n    if(bit) SWP_KEY(K, k28, k29);\n    if(bit) SWP_KEY(K, k30, k31);\n}\n\n__device__\nint find_kth3(K* a, int aCount, K* b, int bCount, int diag)\n{\n    int begin = max(0, diag - bCount);\n    int end = min(diag, aCount);\n\n    while(begin < end) {\n        int mid = (begin + end)>> 1;\n        K aKey = a[mid];\n        K bKey = b[diag - 1 - mid];\n        bool pred = aKey <= bKey;\n        if(pred) begin = mid + 1;\n        else end = mid;\n    }\n    return begin;\n}\n\n__global__\nvoid gen_bk512_tc4_r1025_r2048_orig(\n    K *key, K *keyB,\n    const Offset *seg_begins, const Offset *seg_ends,\n    const int *bin, const int bin_size)\n{\n    const int tid = threadIdx.x;\n    const int bin_it = blockIdx.x;\n    __shared__ K smem[2048];\n    const int bit1 = (tid>>0)&0x1;\n    const int bit2 = (tid>>1)&0x1;\n    const int bit3 = (tid>>2)&0x1;\n    const int bit4 = (tid>>3)&0x1;\n    const int bit5 = (tid>>4)&0x1;\n    const int tid1 = threadIdx.x & 31;\n    const int warp_id = threadIdx.x / 32;\n    K rg_k0 ;\n    K rg_k1 ;\n    K rg_k2 ;\n    K rg_k3 ;\n    int k;\n    int seg_size;\n    int ext_seg_size;\n    if(bin_it < bin_size) {\n        k = seg_begins[bin[bin_it]];\n        seg_size = seg_ends[bin[bin_it]]-seg_begins[bin[bin_it]];\n        ext_seg_size = ((seg_size + 63) / 64) * 64;\n        int big_wp = (ext_seg_size - blockDim.x * 2) / 64;\n        int sml_wp = blockDim.x / 32 - big_wp;\n        int sml_len = sml_wp * 64;\n        const int big_warp_id = (warp_id - sml_wp < 0)? 0: warp_id - sml_wp;\n        bool sml_warp = warp_id < sml_wp;\n        if(sml_warp) {\n            rg_k0 = key[k+(warp_id<<6)+tid1+0   ];\n            rg_k1 = key[k+(warp_id<<6)+tid1+32  ];\n            // exch_intxn: switch to exch_local()\n            CMP_SWP_KEY(K,rg_k0 ,rg_k1 );\n            // exch_intxn: generate exch_intxn_keys()\n            exch_intxn_keys(rg_k0 ,rg_k1 ,\n                            0x1,bit1);\n            // exch_paral: switch to exch_local()\n            CMP_SWP_KEY(K,rg_k0 ,rg_k1 );\n            // exch_intxn: generate exch_intxn_keys()\n            exch_intxn_keys(rg_k0 ,rg_k1 ,\n                            0x3,bit2);\n            // exch_paral: generate exch_paral_keys()\n            exch_paral_keys(rg_k0 ,rg_k1 ,\n                            0x1,bit1);\n            // exch_paral: switch to exch_local()\n            CMP_SWP_KEY(K,rg_k0 ,rg_k1 );\n            // exch_intxn: generate exch_intxn_keys()\n            exch_intxn_keys(rg_k0 ,rg_k1 ,\n                            0x7,bit3);\n            // exch_paral: generate exch_paral_keys()\n            exch_paral_keys(rg_k0 ,rg_k1 ,\n                            0x2,bit2);\n            // exch_paral: generate exch_paral_keys()\n            exch_paral_keys(rg_k0 ,rg_k1 ,\n                            0x1,bit1);\n            // exch_paral: switch to exch_local()\n            CMP_SWP_KEY(K,rg_k0 ,rg_k1 );\n            // exch_intxn: generate exch_intxn_keys()\n            exch_intxn_keys(rg_k0 ,rg_k1 ,\n                            0xf,bit4);\n            // exch_paral: generate exch_paral_keys()\n            exch_paral_keys(rg_k0 ,rg_k1 ,\n                            0x4,bit3);\n            // exch_paral: generate exch_paral_keys()\n            exch_paral_keys(rg_k0 ,rg_k1 ,\n                            0x2,bit2);\n            // exch_paral: generate exch_paral_keys()\n            exch_paral_keys(rg_k0 ,rg_k1 ,\n                            0x1,bit1);\n            // exch_paral: switch to exch_local()\n            CMP_SWP_KEY(K,rg_k0 ,rg_k1 );\n            // exch_intxn: generate exch_intxn_keys()\n            exch_intxn_keys(rg_k0 ,rg_k1 ,\n                            0x1f,bit5);\n            // exch_paral: generate exch_paral_keys()\n            exch_paral_keys(rg_k0 ,rg_k1 ,\n                            0x8,bit4);\n            // exch_paral: generate exch_paral_keys()\n            exch_paral_keys(rg_k0 ,rg_k1 ,\n                            0x4,bit3);\n            // exch_paral: generate exch_paral_keys()\n            exch_paral_keys(rg_k0 ,rg_k1 ,\n                            0x2,bit2);\n            // exch_paral: generate exch_paral_keys()\n            exch_paral_keys(rg_k0 ,rg_k1 ,\n                            0x1,bit1);\n            // exch_paral: switch to exch_local()\n            CMP_SWP_KEY(K,rg_k0 ,rg_k1 );\n        } else {\n            rg_k0  = (sml_len+tid1+(big_warp_id<<7)+0   <seg_size)?key[k+sml_len+tid1+(big_warp_id<<7)+0   ]:std::numeric_limits<K>::max();\n            rg_k1  = (sml_len+tid1+(big_warp_id<<7)+32  <seg_size)?key[k+sml_len+tid1+(big_warp_id<<7)+32  ]:std::numeric_limits<K>::max();\n            rg_k2  = (sml_len+tid1+(big_warp_id<<7)+64  <seg_size)?key[k+sml_len+tid1+(big_warp_id<<7)+64  ]:std::numeric_limits<K>::max();\n            rg_k3  = (sml_len+tid1+(big_warp_id<<7)+96  <seg_size)?key[k+sml_len+tid1+(big_warp_id<<7)+96  ]:std::numeric_limits<K>::max();\n            // exch_intxn: switch to exch_local()\n            CMP_SWP_KEY(K,rg_k0 ,rg_k1 );\n            CMP_SWP_KEY(K,rg_k2 ,rg_k3 );\n            // exch_intxn: switch to exch_local()\n            CMP_SWP_KEY(K,rg_k0 ,rg_k3 );\n            CMP_SWP_KEY(K,rg_k1 ,rg_k2 );\n            // exch_paral: switch to exch_local()\n            CMP_SWP_KEY(K,rg_k0 ,rg_k1 );\n            CMP_SWP_KEY(K,rg_k2 ,rg_k3 );\n            // exch_intxn: generate exch_intxn_keys()\n            exch_intxn_keys(rg_k0 ,rg_k1 ,rg_k2 ,rg_k3 ,\n                            0x1,bit1);\n            // exch_paral: switch to exch_local()\n            CMP_SWP_KEY(K,rg_k0 ,rg_k2 );\n            CMP_SWP_KEY(K,rg_k1 ,rg_k3 );\n            // exch_paral: switch to exch_local()\n            CMP_SWP_KEY(K,rg_k0 ,rg_k1 );\n            CMP_SWP_KEY(K,rg_k2 ,rg_k3 );\n            // exch_intxn: generate exch_intxn_keys()\n            exch_intxn_keys(rg_k0 ,rg_k1 ,rg_k2 ,rg_k3 ,\n                            0x3,bit2);\n            // exch_paral: generate exch_paral_keys()\n            exch_paral_keys(rg_k0 ,rg_k1 ,rg_k2 ,rg_k3 ,\n                            0x1,bit1);\n            // exch_paral: switch to exch_local()\n            CMP_SWP_KEY(K,rg_k0 ,rg_k2 );\n            CMP_SWP_KEY(K,rg_k1 ,rg_k3 );\n            // exch_paral: switch to exch_local()\n            CMP_SWP_KEY(K,rg_k0 ,rg_k1 );\n            CMP_SWP_KEY(K,rg_k2 ,rg_k3 );\n            // exch_intxn: generate exch_intxn_keys()\n            exch_intxn_keys(rg_k0 ,rg_k1 ,rg_k2 ,rg_k3 ,\n                            0x7,bit3);\n            // exch_paral: generate exch_paral_keys()\n            exch_paral_keys(rg_k0 ,rg_k1 ,rg_k2 ,rg_k3 ,\n                            0x2,bit2);\n            // exch_paral: generate exch_paral_keys()\n            exch_paral_keys(rg_k0 ,rg_k1 ,rg_k2 ,rg_k3 ,\n                            0x1,bit1);\n            // exch_paral: switch to exch_local()\n            CMP_SWP_KEY(K,rg_k0 ,rg_k2 );\n            CMP_SWP_KEY(K,rg_k1 ,rg_k3 );\n            // exch_paral: switch to exch_local()\n            CMP_SWP_KEY(K,rg_k0 ,rg_k1 );\n            CMP_SWP_KEY(K,rg_k2 ,rg_k3 );\n            // exch_intxn: generate exch_intxn_keys()\n            exch_intxn_keys(rg_k0 ,rg_k1 ,rg_k2 ,rg_k3 ,\n                            0xf,bit4);\n            // exch_paral: generate exch_paral_keys()\n            exch_paral_keys(rg_k0 ,rg_k1 ,rg_k2 ,rg_k3 ,\n                            0x4,bit3);\n            // exch_paral: generate exch_paral_keys()\n            exch_paral_keys(rg_k0 ,rg_k1 ,rg_k2 ,rg_k3 ,\n                            0x2,bit2);\n            // exch_paral: generate exch_paral_keys()\n            exch_paral_keys(rg_k0 ,rg_k1 ,rg_k2 ,rg_k3 ,\n                            0x1,bit1);\n            // exch_paral: switch to exch_local()\n            CMP_SWP_KEY(K,rg_k0 ,rg_k2 );\n            CMP_SWP_KEY(K,rg_k1 ,rg_k3 );\n            // exch_paral: switch to exch_local()\n            CMP_SWP_KEY(K,rg_k0 ,rg_k1 );\n            CMP_SWP_KEY(K,rg_k2 ,rg_k3 );\n            // exch_intxn: generate exch_intxn_keys()\n            exch_intxn_keys(rg_k0 ,rg_k1 ,rg_k2 ,rg_k3 ,\n                            0x1f,bit5);\n            // exch_paral: generate exch_paral_keys()\n            exch_paral_keys(rg_k0 ,rg_k1 ,rg_k2 ,rg_k3 ,\n                            0x8,bit4);\n            // exch_paral: generate exch_paral_keys()\n            exch_paral_keys(rg_k0 ,rg_k1 ,rg_k2 ,rg_k3 ,\n                            0x4,bit3);\n            // exch_paral: generate exch_paral_keys()\n            exch_paral_keys(rg_k0 ,rg_k1 ,rg_k2 ,rg_k3 ,\n                            0x2,bit2);\n            // exch_paral: generate exch_paral_keys()\n            exch_paral_keys(rg_k0 ,rg_k1 ,rg_k2 ,rg_k3 ,\n                            0x1,bit1);\n            // exch_paral: switch to exch_local()\n            CMP_SWP_KEY(K,rg_k0 ,rg_k2 );\n            CMP_SWP_KEY(K,rg_k1 ,rg_k3 );\n            // exch_paral: switch to exch_local()\n            CMP_SWP_KEY(K,rg_k0 ,rg_k1 );\n            CMP_SWP_KEY(K,rg_k2 ,rg_k3 );\n        }\n        // Store register results to shared memory\n        if(sml_warp) {\n            smem[(warp_id<<6)+(tid1<<1)+0 ] = rg_k0 ;\n            smem[(warp_id<<6)+(tid1<<1)+1 ] = rg_k1 ;\n        } else {\n            smem[sml_len+(big_warp_id<<7)+(tid1<<2)+0 ] = rg_k0 ;\n            smem[sml_len+(big_warp_id<<7)+(tid1<<2)+1 ] = rg_k1 ;\n            smem[sml_len+(big_warp_id<<7)+(tid1<<2)+2 ] = rg_k2 ;\n            smem[sml_len+(big_warp_id<<7)+(tid1<<2)+3 ] = rg_k3 ;\n        }\n        __syncthreads();\n        // Merge in 4 steps\n        int grp_start_wp_id;\n        int grp_start_off;\n        int tmp_wp_id;\n        int lhs_len;\n        int rhs_len;\n        int gran;\n        int s_a;\n        int s_b;\n        bool p;\n        K tmp_k0;\n        K tmp_k1;\n        K *start;\n        // Step 0\n        grp_start_wp_id = ((warp_id>>1)<<1);\n        grp_start_off = (grp_start_wp_id<sml_wp)?grp_start_wp_id*64:sml_len+(grp_start_wp_id-sml_wp)*128;\n        tmp_wp_id = grp_start_wp_id;\n        lhs_len = ((tmp_wp_id+0<sml_wp)?64  :128 );\n        rhs_len = ((tmp_wp_id+1<sml_wp)?64  :128 );\n        gran = (warp_id<sml_wp)?(tid1<<1): (tid1<<2);\n        if((warp_id&1)==0){\n            gran += 0;\n        }\n        if((warp_id&1)==1){\n            gran += ((tmp_wp_id+0<sml_wp)?64  :128 );\n        }\n        start = smem + grp_start_off;\n        s_a = find_kth3(start, lhs_len, start+lhs_len, rhs_len, gran);\n        s_b = lhs_len + gran - s_a;\n        if(sml_warp){\n            tmp_k0 = (s_a<lhs_len        )?smem[grp_start_off+s_a]:std::numeric_limits<K>::max();\n            tmp_k1 = (s_b<lhs_len+rhs_len)?smem[grp_start_off+s_b]:std::numeric_limits<K>::max();\n            p = (s_b>=lhs_len+rhs_len)||((s_a<lhs_len)&&(tmp_k0<=tmp_k1));\n            rg_k0 = p ? tmp_k0 : tmp_k1;\n            if(p) {\n                ++s_a;\n                tmp_k0 = (s_a<lhs_len)?smem[grp_start_off+s_a]:std::numeric_limits<K>::max();\n            } else {\n                ++s_b;\n                tmp_k1 = (s_b<lhs_len+rhs_len)?smem[grp_start_off+s_b]:std::numeric_limits<K>::max();\n            }\n            p = (s_b>=lhs_len+rhs_len)||((s_a<lhs_len)&&(tmp_k0<=tmp_k1));\n            rg_k1 = p ? tmp_k0 : tmp_k1;\n        } else {\n            tmp_k0 = (s_a<lhs_len        )?smem[grp_start_off+s_a]:std::numeric_limits<K>::max();\n            tmp_k1 = (s_b<lhs_len+rhs_len)?smem[grp_start_off+s_b]:std::numeric_limits<K>::max();\n            p = (s_b>=lhs_len+rhs_len)||((s_a<lhs_len)&&(tmp_k0<=tmp_k1));\n            rg_k0 = p ? tmp_k0 : tmp_k1;\n            if(p) {\n                ++s_a;\n                tmp_k0 = (s_a<lhs_len)?smem[grp_start_off+s_a]:std::numeric_limits<K>::max();\n            } else {\n                ++s_b;\n                tmp_k1 = (s_b<lhs_len+rhs_len)?smem[grp_start_off+s_b]:std::numeric_limits<K>::max();\n            }\n            p = (s_b>=lhs_len+rhs_len)||((s_a<lhs_len)&&(tmp_k0<=tmp_k1));\n            rg_k1 = p ? tmp_k0 : tmp_k1;\n            if(p) {\n                ++s_a;\n                tmp_k0 = (s_a<lhs_len)?smem[grp_start_off+s_a]:std::numeric_limits<K>::max();\n            } else {\n                ++s_b;\n                tmp_k1 = (s_b<lhs_len+rhs_len)?smem[grp_start_off+s_b]:std::numeric_limits<K>::max();\n            }\n            p = (s_b>=lhs_len+rhs_len)||((s_a<lhs_len)&&(tmp_k0<=tmp_k1));\n            rg_k2 = p ? tmp_k0 : tmp_k1;\n            if(p) {\n                ++s_a;\n                tmp_k0 = (s_a<lhs_len)?smem[grp_start_off+s_a]:std::numeric_limits<K>::max();\n            } else {\n                ++s_b;\n                tmp_k1 = (s_b<lhs_len+rhs_len)?smem[grp_start_off+s_b]:std::numeric_limits<K>::max();\n            }\n            p = (s_b>=lhs_len+rhs_len)||((s_a<lhs_len)&&(tmp_k0<=tmp_k1));\n            rg_k3 = p ? tmp_k0 : tmp_k1;\n        }\n        __syncthreads();\n        // Store merged results back to shared memory\n        if(sml_warp){\n            smem[grp_start_off+gran+0 ] = rg_k0 ;\n            smem[grp_start_off+gran+1 ] = rg_k1 ;\n        } else {\n            smem[grp_start_off+gran+0 ] = rg_k0 ;\n            smem[grp_start_off+gran+1 ] = rg_k1 ;\n            smem[grp_start_off+gran+2 ] = rg_k2 ;\n            smem[grp_start_off+gran+3 ] = rg_k3 ;\n        }\n        __syncthreads();\n        // Step 1\n        grp_start_wp_id = ((warp_id>>2)<<2);\n        grp_start_off = (grp_start_wp_id<sml_wp)?grp_start_wp_id*64:sml_len+(grp_start_wp_id-sml_wp)*128;\n        tmp_wp_id = grp_start_wp_id;\n        lhs_len = ((tmp_wp_id+0<sml_wp)?64  :128 )+\n                  ((tmp_wp_id+1<sml_wp)?64  :128 );\n        rhs_len = ((tmp_wp_id+2<sml_wp)?64  :128 )+\n                  ((tmp_wp_id+3<sml_wp)?64  :128 );\n        gran = (warp_id<sml_wp)?(tid1<<1): (tid1<<2);\n        if((warp_id&3)==0){\n            gran += 0;\n        }\n        if((warp_id&3)==1){\n            gran += ((tmp_wp_id+0<sml_wp)?64  :128 );\n        }\n        if((warp_id&3)==2){\n            gran += ((tmp_wp_id+0<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+1<sml_wp)?64  :128 );\n        }\n        if((warp_id&3)==3){\n            gran += ((tmp_wp_id+0<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+1<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+2<sml_wp)?64  :128 );\n        }\n        start = smem + grp_start_off;\n        s_a = find_kth3(start, lhs_len, start+lhs_len, rhs_len, gran);\n        s_b = lhs_len + gran - s_a;\n        if(sml_warp){\n            tmp_k0 = (s_a<lhs_len        )?smem[grp_start_off+s_a]:std::numeric_limits<K>::max();\n            tmp_k1 = (s_b<lhs_len+rhs_len)?smem[grp_start_off+s_b]:std::numeric_limits<K>::max();\n            p = (s_b>=lhs_len+rhs_len)||((s_a<lhs_len)&&(tmp_k0<=tmp_k1));\n            rg_k0 = p ? tmp_k0 : tmp_k1;\n            if(p) {\n                ++s_a;\n                tmp_k0 = (s_a<lhs_len)?smem[grp_start_off+s_a]:std::numeric_limits<K>::max();\n            } else {\n                ++s_b;\n                tmp_k1 = (s_b<lhs_len+rhs_len)?smem[grp_start_off+s_b]:std::numeric_limits<K>::max();\n            }\n            p = (s_b>=lhs_len+rhs_len)||((s_a<lhs_len)&&(tmp_k0<=tmp_k1));\n            rg_k1 = p ? tmp_k0 : tmp_k1;\n        } else {\n            tmp_k0 = (s_a<lhs_len        )?smem[grp_start_off+s_a]:std::numeric_limits<K>::max();\n            tmp_k1 = (s_b<lhs_len+rhs_len)?smem[grp_start_off+s_b]:std::numeric_limits<K>::max();\n            p = (s_b>=lhs_len+rhs_len)||((s_a<lhs_len)&&(tmp_k0<=tmp_k1));\n            rg_k0 = p ? tmp_k0 : tmp_k1;\n            if(p) {\n                ++s_a;\n                tmp_k0 = (s_a<lhs_len)?smem[grp_start_off+s_a]:std::numeric_limits<K>::max();\n            } else {\n                ++s_b;\n                tmp_k1 = (s_b<lhs_len+rhs_len)?smem[grp_start_off+s_b]:std::numeric_limits<K>::max();\n            }\n            p = (s_b>=lhs_len+rhs_len)||((s_a<lhs_len)&&(tmp_k0<=tmp_k1));\n            rg_k1 = p ? tmp_k0 : tmp_k1;\n            if(p) {\n                ++s_a;\n                tmp_k0 = (s_a<lhs_len)?smem[grp_start_off+s_a]:std::numeric_limits<K>::max();\n            } else {\n                ++s_b;\n                tmp_k1 = (s_b<lhs_len+rhs_len)?smem[grp_start_off+s_b]:std::numeric_limits<K>::max();\n            }\n            p = (s_b>=lhs_len+rhs_len)||((s_a<lhs_len)&&(tmp_k0<=tmp_k1));\n            rg_k2 = p ? tmp_k0 : tmp_k1;\n            if(p) {\n                ++s_a;\n                tmp_k0 = (s_a<lhs_len)?smem[grp_start_off+s_a]:std::numeric_limits<K>::max();\n            } else {\n                ++s_b;\n                tmp_k1 = (s_b<lhs_len+rhs_len)?smem[grp_start_off+s_b]:std::numeric_limits<K>::max();\n            }\n            p = (s_b>=lhs_len+rhs_len)||((s_a<lhs_len)&&(tmp_k0<=tmp_k1));\n            rg_k3 = p ? tmp_k0 : tmp_k1;\n        }\n        __syncthreads();\n        // Store merged results back to shared memory\n        if(sml_warp){\n            smem[grp_start_off+gran+0 ] = rg_k0 ;\n            smem[grp_start_off+gran+1 ] = rg_k1 ;\n        } else {\n            smem[grp_start_off+gran+0 ] = rg_k0 ;\n            smem[grp_start_off+gran+1 ] = rg_k1 ;\n            smem[grp_start_off+gran+2 ] = rg_k2 ;\n            smem[grp_start_off+gran+3 ] = rg_k3 ;\n        }\n        __syncthreads();\n        // Step 2\n        grp_start_wp_id = ((warp_id>>3)<<3);\n        grp_start_off = (grp_start_wp_id<sml_wp)?grp_start_wp_id*64:sml_len+(grp_start_wp_id-sml_wp)*128;\n        tmp_wp_id = grp_start_wp_id;\n        lhs_len = ((tmp_wp_id+0<sml_wp)?64  :128 )+\n                  ((tmp_wp_id+1<sml_wp)?64  :128 )+\n                  ((tmp_wp_id+2<sml_wp)?64  :128 )+\n                  ((tmp_wp_id+3<sml_wp)?64  :128 );\n        rhs_len = ((tmp_wp_id+4<sml_wp)?64  :128 )+\n                  ((tmp_wp_id+5<sml_wp)?64  :128 )+\n                  ((tmp_wp_id+6<sml_wp)?64  :128 )+\n                  ((tmp_wp_id+7<sml_wp)?64  :128 );\n        gran = (warp_id<sml_wp)?(tid1<<1): (tid1<<2);\n        if((warp_id&7)==0){\n            gran += 0;\n        }\n        if((warp_id&7)==1){\n            gran += ((tmp_wp_id+0<sml_wp)?64  :128 );\n        }\n        if((warp_id&7)==2){\n            gran += ((tmp_wp_id+0<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+1<sml_wp)?64  :128 );\n        }\n        if((warp_id&7)==3){\n            gran += ((tmp_wp_id+0<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+1<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+2<sml_wp)?64  :128 );\n        }\n        if((warp_id&7)==4){\n            gran += ((tmp_wp_id+0<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+1<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+2<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+3<sml_wp)?64  :128 );\n        }\n        if((warp_id&7)==5){\n            gran += ((tmp_wp_id+0<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+1<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+2<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+3<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+4<sml_wp)?64  :128 );\n        }\n        if((warp_id&7)==6){\n            gran += ((tmp_wp_id+0<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+1<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+2<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+3<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+4<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+5<sml_wp)?64  :128 );\n        }\n        if((warp_id&7)==7){\n            gran += ((tmp_wp_id+0<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+1<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+2<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+3<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+4<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+5<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+6<sml_wp)?64  :128 );\n        }\n        start = smem + grp_start_off;\n        s_a = find_kth3(start, lhs_len, start+lhs_len, rhs_len, gran);\n        s_b = lhs_len + gran - s_a;\n        if(sml_warp){\n            tmp_k0 = (s_a<lhs_len        )?smem[grp_start_off+s_a]:std::numeric_limits<K>::max();\n            tmp_k1 = (s_b<lhs_len+rhs_len)?smem[grp_start_off+s_b]:std::numeric_limits<K>::max();\n            p = (s_b>=lhs_len+rhs_len)||((s_a<lhs_len)&&(tmp_k0<=tmp_k1));\n            rg_k0 = p ? tmp_k0 : tmp_k1;\n            if(p) {\n                ++s_a;\n                tmp_k0 = (s_a<lhs_len)?smem[grp_start_off+s_a]:std::numeric_limits<K>::max();\n            } else {\n                ++s_b;\n                tmp_k1 = (s_b<lhs_len+rhs_len)?smem[grp_start_off+s_b]:std::numeric_limits<K>::max();\n            }\n            p = (s_b>=lhs_len+rhs_len)||((s_a<lhs_len)&&(tmp_k0<=tmp_k1));\n            rg_k1 = p ? tmp_k0 : tmp_k1;\n        } else {\n            tmp_k0 = (s_a<lhs_len        )?smem[grp_start_off+s_a]:std::numeric_limits<K>::max();\n            tmp_k1 = (s_b<lhs_len+rhs_len)?smem[grp_start_off+s_b]:std::numeric_limits<K>::max();\n            p = (s_b>=lhs_len+rhs_len)||((s_a<lhs_len)&&(tmp_k0<=tmp_k1));\n            rg_k0 = p ? tmp_k0 : tmp_k1;\n            if(p) {\n                ++s_a;\n                tmp_k0 = (s_a<lhs_len)?smem[grp_start_off+s_a]:std::numeric_limits<K>::max();\n            } else {\n                ++s_b;\n                tmp_k1 = (s_b<lhs_len+rhs_len)?smem[grp_start_off+s_b]:std::numeric_limits<K>::max();\n            }\n            p = (s_b>=lhs_len+rhs_len)||((s_a<lhs_len)&&(tmp_k0<=tmp_k1));\n            rg_k1 = p ? tmp_k0 : tmp_k1;\n            if(p) {\n                ++s_a;\n                tmp_k0 = (s_a<lhs_len)?smem[grp_start_off+s_a]:std::numeric_limits<K>::max();\n            } else {\n                ++s_b;\n                tmp_k1 = (s_b<lhs_len+rhs_len)?smem[grp_start_off+s_b]:std::numeric_limits<K>::max();\n            }\n            p = (s_b>=lhs_len+rhs_len)||((s_a<lhs_len)&&(tmp_k0<=tmp_k1));\n            rg_k2 = p ? tmp_k0 : tmp_k1;\n            if(p) {\n                ++s_a;\n                tmp_k0 = (s_a<lhs_len)?smem[grp_start_off+s_a]:std::numeric_limits<K>::max();\n            } else {\n                ++s_b;\n                tmp_k1 = (s_b<lhs_len+rhs_len)?smem[grp_start_off+s_b]:std::numeric_limits<K>::max();\n            }\n            p = (s_b>=lhs_len+rhs_len)||((s_a<lhs_len)&&(tmp_k0<=tmp_k1));\n            rg_k3 = p ? tmp_k0 : tmp_k1;\n        }\n        __syncthreads();\n        // Store merged results back to shared memory\n        if(sml_warp){\n            smem[grp_start_off+gran+0 ] = rg_k0 ;\n            smem[grp_start_off+gran+1 ] = rg_k1 ;\n        } else {\n            smem[grp_start_off+gran+0 ] = rg_k0 ;\n            smem[grp_start_off+gran+1 ] = rg_k1 ;\n            smem[grp_start_off+gran+2 ] = rg_k2 ;\n            smem[grp_start_off+gran+3 ] = rg_k3 ;\n        }\n        __syncthreads();\n        // Step 3\n        grp_start_wp_id = ((warp_id>>4)<<4);\n        grp_start_off = (grp_start_wp_id<sml_wp)?grp_start_wp_id*64:sml_len+(grp_start_wp_id-sml_wp)*128;\n        tmp_wp_id = grp_start_wp_id;\n        lhs_len = ((tmp_wp_id+0<sml_wp)?64  :128 )+\n                  ((tmp_wp_id+1<sml_wp)?64  :128 )+\n                  ((tmp_wp_id+2<sml_wp)?64  :128 )+\n                  ((tmp_wp_id+3<sml_wp)?64  :128 )+\n                  ((tmp_wp_id+4<sml_wp)?64  :128 )+\n                  ((tmp_wp_id+5<sml_wp)?64  :128 )+\n                  ((tmp_wp_id+6<sml_wp)?64  :128 )+\n                  ((tmp_wp_id+7<sml_wp)?64  :128 );\n        rhs_len = ((tmp_wp_id+8<sml_wp)?64  :128 )+\n                  ((tmp_wp_id+9<sml_wp)?64  :128 )+\n                  ((tmp_wp_id+10<sml_wp)?64  :128 )+\n                  ((tmp_wp_id+11<sml_wp)?64  :128 )+\n                  ((tmp_wp_id+12<sml_wp)?64  :128 )+\n                  ((tmp_wp_id+13<sml_wp)?64  :128 )+\n                  ((tmp_wp_id+14<sml_wp)?64  :128 )+\n                  ((tmp_wp_id+15<sml_wp)?64  :128 );\n        gran = (warp_id<sml_wp)?(tid1<<1): (tid1<<2);\n        if((warp_id&15)==0){\n            gran += 0;\n        }\n        if((warp_id&15)==1){\n            gran += ((tmp_wp_id+0<sml_wp)?64  :128 );\n        }\n        if((warp_id&15)==2){\n            gran += ((tmp_wp_id+0<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+1<sml_wp)?64  :128 );\n        }\n        if((warp_id&15)==3){\n            gran += ((tmp_wp_id+0<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+1<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+2<sml_wp)?64  :128 );\n        }\n        if((warp_id&15)==4){\n            gran += ((tmp_wp_id+0<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+1<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+2<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+3<sml_wp)?64  :128 );\n        }\n        if((warp_id&15)==5){\n            gran += ((tmp_wp_id+0<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+1<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+2<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+3<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+4<sml_wp)?64  :128 );\n        }\n        if((warp_id&15)==6){\n            gran += ((tmp_wp_id+0<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+1<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+2<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+3<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+4<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+5<sml_wp)?64  :128 );\n        }\n        if((warp_id&15)==7){\n            gran += ((tmp_wp_id+0<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+1<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+2<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+3<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+4<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+5<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+6<sml_wp)?64  :128 );\n        }\n        if((warp_id&15)==8){\n            gran += ((tmp_wp_id+0<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+1<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+2<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+3<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+4<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+5<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+6<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+7<sml_wp)?64  :128 );\n        }\n        if((warp_id&15)==9){\n            gran += ((tmp_wp_id+0<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+1<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+2<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+3<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+4<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+5<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+6<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+7<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+8<sml_wp)?64  :128 );\n        }\n        if((warp_id&15)==10){\n            gran += ((tmp_wp_id+0<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+1<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+2<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+3<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+4<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+5<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+6<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+7<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+8<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+9<sml_wp)?64  :128 );\n        }\n        if((warp_id&15)==11){\n            gran += ((tmp_wp_id+0<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+1<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+2<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+3<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+4<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+5<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+6<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+7<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+8<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+9<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+10<sml_wp)?64  :128 );\n        }\n        if((warp_id&15)==12){\n            gran += ((tmp_wp_id+0<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+1<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+2<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+3<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+4<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+5<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+6<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+7<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+8<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+9<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+10<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+11<sml_wp)?64  :128 );\n        }\n        if((warp_id&15)==13){\n            gran += ((tmp_wp_id+0<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+1<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+2<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+3<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+4<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+5<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+6<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+7<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+8<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+9<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+10<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+11<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+12<sml_wp)?64  :128 );\n        }\n        if((warp_id&15)==14){\n            gran += ((tmp_wp_id+0<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+1<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+2<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+3<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+4<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+5<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+6<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+7<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+8<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+9<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+10<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+11<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+12<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+13<sml_wp)?64  :128 );\n        }\n        if((warp_id&15)==15){\n            gran += ((tmp_wp_id+0<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+1<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+2<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+3<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+4<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+5<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+6<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+7<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+8<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+9<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+10<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+11<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+12<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+13<sml_wp)?64  :128 )+\n                    ((tmp_wp_id+14<sml_wp)?64  :128 );\n        }\n        start = smem + grp_start_off;\n        s_a = find_kth3(start, lhs_len, start+lhs_len, rhs_len, gran);\n        s_b = lhs_len + gran - s_a;\n        if(sml_warp){\n            tmp_k0 = (s_a<lhs_len        )?smem[grp_start_off+s_a]:std::numeric_limits<K>::max();\n            tmp_k1 = (s_b<lhs_len+rhs_len)?smem[grp_start_off+s_b]:std::numeric_limits<K>::max();\n            p = (s_b>=lhs_len+rhs_len)||((s_a<lhs_len)&&(tmp_k0<=tmp_k1));\n            rg_k0 = p ? tmp_k0 : tmp_k1;\n            if(p) {\n                ++s_a;\n                tmp_k0 = (s_a<lhs_len)?smem[grp_start_off+s_a]:std::numeric_limits<K>::max();\n            } else {\n                ++s_b;\n                tmp_k1 = (s_b<lhs_len+rhs_len)?smem[grp_start_off+s_b]:std::numeric_limits<K>::max();\n            }\n            p = (s_b>=lhs_len+rhs_len)||((s_a<lhs_len)&&(tmp_k0<=tmp_k1));\n            rg_k1 = p ? tmp_k0 : tmp_k1;\n        } else {\n            tmp_k0 = (s_a<lhs_len        )?smem[grp_start_off+s_a]:std::numeric_limits<K>::max();\n            tmp_k1 = (s_b<lhs_len+rhs_len)?smem[grp_start_off+s_b]:std::numeric_limits<K>::max();\n            p = (s_b>=lhs_len+rhs_len)||((s_a<lhs_len)&&(tmp_k0<=tmp_k1));\n            rg_k0 = p ? tmp_k0 : tmp_k1;\n            if(p) {\n                ++s_a;\n                tmp_k0 = (s_a<lhs_len)?smem[grp_start_off+s_a]:std::numeric_limits<K>::max();\n            } else {\n                ++s_b;\n                tmp_k1 = (s_b<lhs_len+rhs_len)?smem[grp_start_off+s_b]:std::numeric_limits<K>::max();\n            }\n            p = (s_b>=lhs_len+rhs_len)||((s_a<lhs_len)&&(tmp_k0<=tmp_k1));\n            rg_k1 = p ? tmp_k0 : tmp_k1;\n            if(p) {\n                ++s_a;\n                tmp_k0 = (s_a<lhs_len)?smem[grp_start_off+s_a]:std::numeric_limits<K>::max();\n            } else {\n                ++s_b;\n                tmp_k1 = (s_b<lhs_len+rhs_len)?smem[grp_start_off+s_b]:std::numeric_limits<K>::max();\n            }\n            p = (s_b>=lhs_len+rhs_len)||((s_a<lhs_len)&&(tmp_k0<=tmp_k1));\n            rg_k2 = p ? tmp_k0 : tmp_k1;\n            if(p) {\n                ++s_a;\n                tmp_k0 = (s_a<lhs_len)?smem[grp_start_off+s_a]:std::numeric_limits<K>::max();\n            } else {\n                ++s_b;\n                tmp_k1 = (s_b<lhs_len+rhs_len)?smem[grp_start_off+s_b]:std::numeric_limits<K>::max();\n            }\n            p = (s_b>=lhs_len+rhs_len)||((s_a<lhs_len)&&(tmp_k0<=tmp_k1));\n            rg_k3 = p ? tmp_k0 : tmp_k1;\n        }\n        if(sml_warp){\n        } else {\n        }\n        if(sml_warp){\n            if((tid<<1)+0 <seg_size) keyB[k+(tid<<1)+0 ] = rg_k0 ;\n            if((tid<<1)+1 <seg_size) keyB[k+(tid<<1)+1 ] = rg_k1 ;\n        } else {\n            if((tid<<2)+0 -sml_len<seg_size) keyB[k+(tid<<2)+0 -sml_len] = rg_k0 ;\n            if((tid<<2)+1 -sml_len<seg_size) keyB[k+(tid<<2)+1 -sml_len] = rg_k1 ;\n            if((tid<<2)+2 -sml_len<seg_size) keyB[k+(tid<<2)+2 -sml_len] = rg_k2 ;\n            if((tid<<2)+3 -sml_len<seg_size) keyB[k+(tid<<2)+3 -sml_len] = rg_k3 ;\n        }\n    }\n}"
        ],
        "/Users/gbolet/hecbench-roofline/src/segsort-cuda/src/bb_comput_l.cuh": [
            "#define K 1.38065e-23        // J/K, Boltzmann constant\n\n\n#define T ((int)32)\n\n\ninline __host__ __device__ uint4 make_uint4(int4 a)\n{\n    return make_uint4(uint(a.x), uint(a.y), uint(a.z), uint(a.w));\n}\n\ninline __host__ __device__ uint4 max(uint4 a, uint4 b)\n{\n    return make_uint4(max(a.x,b.x), max(a.y,b.y), max(a.z,b.z), max(a.w,b.w));\n}\n\ninline __host__ __device__ uint4 min(uint4 a, uint4 b)\n{\n    return make_uint4(min(a.x,b.x), min(a.y,b.y), min(a.z,b.z), min(a.w,b.w));\n}\n\n__device__ inline void exch_intxn(K &k0, K &k1, K &k2, K &k3, K &k4, K &k5, K &k6, K &k7, K &k8, K &k9, K &k10, K &k11, K &k12, K &k13, K &k14, K &k15, K &k16, K &k17, K &k18, K &k19, K &k20, K &k21, K &k22, K &k23, K &k24, K &k25, K &k26, K &k27, K &k28, K &k29, K &k30, K &k31, int &v0, int &v1, int &v2, int &v3, int &v4, int &v5, int &v6, int &v7, int &v8, int &v9, int &v10, int &v11, int &v12, int &v13, int &v14, int &v15, int &v16, int &v17, int &v18, int &v19, int &v20, int &v21, int &v22, int &v23, int &v24, int &v25, int &v26, int &v27, int &v28, int &v29, int &v30, int &v31, int mask, const int bit) {\n    K ex_k0, ex_k1;\n    int ex_v0, ex_v1;\n    if(bit) SWP(K, k0, k30, int, v0, v30);\n    if(bit) SWP(K, k1, k31, int, v1, v31);\n    if(bit) SWP(K, k2, k28, int, v2, v28);\n    if(bit) SWP(K, k3, k29, int, v3, v29);\n    if(bit) SWP(K, k4, k26, int, v4, v26);\n    if(bit) SWP(K, k5, k27, int, v5, v27);\n    if(bit) SWP(K, k6, k24, int, v6, v24);\n    if(bit) SWP(K, k7, k25, int, v7, v25);\n    if(bit) SWP(K, k8, k22, int, v8, v22);\n    if(bit) SWP(K, k9, k23, int, v9, v23);\n    if(bit) SWP(K, k10, k20, int, v10, v20);\n    if(bit) SWP(K, k11, k21, int, v11, v21);\n    if(bit) SWP(K, k12, k18, int, v12, v18);\n    if(bit) SWP(K, k13, k19, int, v13, v19);\n    if(bit) SWP(K, k14, k16, int, v14, v16);\n    if(bit) SWP(K, k15, k17, int, v15, v17);\n    ex_k0 = k0;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k1, mask);\n    ex_v0 = v0;\n    ex_v1 = __shfl_xor_sync(0xffffffff,v1, mask);\n    CMP_SWP(K, ex_k0, ex_k1, int, ex_v0, ex_v1);\n    if(bit) EQL_SWP(K, ex_k0, ex_k1, int, ex_v0, ex_v1);\n    k0 = ex_k0;\n    k1 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    v0 = ex_v0;\n    v1 = __shfl_xor_sync(0xffffffff,ex_v1, mask);\n    ex_k0 = k2;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k3, mask);\n    ex_v0 = v2;\n    ex_v1 = __shfl_xor_sync(0xffffffff,v3, mask);\n    CMP_SWP(K, ex_k0, ex_k1, int, ex_v0, ex_v1);\n    if(bit) EQL_SWP(K, ex_k0, ex_k1, int, ex_v0, ex_v1);\n    k2 = ex_k0;\n    k3 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    v2 = ex_v0;\n    v3 = __shfl_xor_sync(0xffffffff,ex_v1, mask);\n    ex_k0 = k4;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k5, mask);\n    ex_v0 = v4;\n    ex_v1 = __shfl_xor_sync(0xffffffff,v5, mask);\n    CMP_SWP(K, ex_k0, ex_k1, int, ex_v0, ex_v1);\n    if(bit) EQL_SWP(K, ex_k0, ex_k1, int, ex_v0, ex_v1);\n    k4 = ex_k0;\n    k5 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    v4 = ex_v0;\n    v5 = __shfl_xor_sync(0xffffffff,ex_v1, mask);\n    ex_k0 = k6;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k7, mask);\n    ex_v0 = v6;\n    ex_v1 = __shfl_xor_sync(0xffffffff,v7, mask);\n    CMP_SWP(K, ex_k0, ex_k1, int, ex_v0, ex_v1);\n    if(bit) EQL_SWP(K, ex_k0, ex_k1, int, ex_v0, ex_v1);\n    k6 = ex_k0;\n    k7 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    v6 = ex_v0;\n    v7 = __shfl_xor_sync(0xffffffff,ex_v1, mask);\n    ex_k0 = k8;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k9, mask);\n    ex_v0 = v8;\n    ex_v1 = __shfl_xor_sync(0xffffffff,v9, mask);\n    CMP_SWP(K, ex_k0, ex_k1, int, ex_v0, ex_v1);\n    if(bit) EQL_SWP(K, ex_k0, ex_k1, int, ex_v0, ex_v1);\n    k8 = ex_k0;\n    k9 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    v8 = ex_v0;\n    v9 = __shfl_xor_sync(0xffffffff,ex_v1, mask);\n    ex_k0 = k10;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k11, mask);\n    ex_v0 = v10;\n    ex_v1 = __shfl_xor_sync(0xffffffff,v11, mask);\n    CMP_SWP(K, ex_k0, ex_k1, int, ex_v0, ex_v1);\n    if(bit) EQL_SWP(K, ex_k0, ex_k1, int, ex_v0, ex_v1);\n    k10 = ex_k0;\n    k11 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    v10 = ex_v0;\n    v11 = __shfl_xor_sync(0xffffffff,ex_v1, mask);\n    ex_k0 = k12;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k13, mask);\n    ex_v0 = v12;\n    ex_v1 = __shfl_xor_sync(0xffffffff,v13, mask);\n    CMP_SWP(K, ex_k0, ex_k1, int, ex_v0, ex_v1);\n    if(bit) EQL_SWP(K, ex_k0, ex_k1, int, ex_v0, ex_v1);\n    k12 = ex_k0;\n    k13 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    v12 = ex_v0;\n    v13 = __shfl_xor_sync(0xffffffff,ex_v1, mask);\n    ex_k0 = k14;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k15, mask);\n    ex_v0 = v14;\n    ex_v1 = __shfl_xor_sync(0xffffffff,v15, mask);\n    CMP_SWP(K, ex_k0, ex_k1, int, ex_v0, ex_v1);\n    if(bit) EQL_SWP(K, ex_k0, ex_k1, int, ex_v0, ex_v1);\n    k14 = ex_k0;\n    k15 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    v14 = ex_v0;\n    v15 = __shfl_xor_sync(0xffffffff,ex_v1, mask);\n    ex_k0 = k16;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k17, mask);\n    ex_v0 = v16;\n    ex_v1 = __shfl_xor_sync(0xffffffff,v17, mask);\n    CMP_SWP(K, ex_k0, ex_k1, int, ex_v0, ex_v1);\n    if(bit) EQL_SWP(K, ex_k0, ex_k1, int, ex_v0, ex_v1);\n    k16 = ex_k0;\n    k17 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    v16 = ex_v0;\n    v17 = __shfl_xor_sync(0xffffffff,ex_v1, mask);\n    ex_k0 = k18;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k19, mask);\n    ex_v0 = v18;\n    ex_v1 = __shfl_xor_sync(0xffffffff,v19, mask);\n    CMP_SWP(K, ex_k0, ex_k1, int, ex_v0, ex_v1);\n    if(bit) EQL_SWP(K, ex_k0, ex_k1, int, ex_v0, ex_v1);\n    k18 = ex_k0;\n    k19 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    v18 = ex_v0;\n    v19 = __shfl_xor_sync(0xffffffff,ex_v1, mask);\n    ex_k0 = k20;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k21, mask);\n    ex_v0 = v20;\n    ex_v1 = __shfl_xor_sync(0xffffffff,v21, mask);\n    CMP_SWP(K, ex_k0, ex_k1, int, ex_v0, ex_v1);\n    if(bit) EQL_SWP(K, ex_k0, ex_k1, int, ex_v0, ex_v1);\n    k20 = ex_k0;\n    k21 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    v20 = ex_v0;\n    v21 = __shfl_xor_sync(0xffffffff,ex_v1, mask);\n    ex_k0 = k22;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k23, mask);\n    ex_v0 = v22;\n    ex_v1 = __shfl_xor_sync(0xffffffff,v23, mask);\n    CMP_SWP(K, ex_k0, ex_k1, int, ex_v0, ex_v1);\n    if(bit) EQL_SWP(K, ex_k0, ex_k1, int, ex_v0, ex_v1);\n    k22 = ex_k0;\n    k23 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    v22 = ex_v0;\n    v23 = __shfl_xor_sync(0xffffffff,ex_v1, mask);\n    ex_k0 = k24;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k25, mask);\n    ex_v0 = v24;\n    ex_v1 = __shfl_xor_sync(0xffffffff,v25, mask);\n    CMP_SWP(K, ex_k0, ex_k1, int, ex_v0, ex_v1);\n    if(bit) EQL_SWP(K, ex_k0, ex_k1, int, ex_v0, ex_v1);\n    k24 = ex_k0;\n    k25 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    v24 = ex_v0;\n    v25 = __shfl_xor_sync(0xffffffff,ex_v1, mask);\n    ex_k0 = k26;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k27, mask);\n    ex_v0 = v26;\n    ex_v1 = __shfl_xor_sync(0xffffffff,v27, mask);\n    CMP_SWP(K, ex_k0, ex_k1, int, ex_v0, ex_v1);\n    if(bit) EQL_SWP(K, ex_k0, ex_k1, int, ex_v0, ex_v1);\n    k26 = ex_k0;\n    k27 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    v26 = ex_v0;\n    v27 = __shfl_xor_sync(0xffffffff,ex_v1, mask);\n    ex_k0 = k28;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k29, mask);\n    ex_v0 = v28;\n    ex_v1 = __shfl_xor_sync(0xffffffff,v29, mask);\n    CMP_SWP(K, ex_k0, ex_k1, int, ex_v0, ex_v1);\n    if(bit) EQL_SWP(K, ex_k0, ex_k1, int, ex_v0, ex_v1);\n    k28 = ex_k0;\n    k29 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    v28 = ex_v0;\n    v29 = __shfl_xor_sync(0xffffffff,ex_v1, mask);\n    ex_k0 = k30;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k31, mask);\n    ex_v0 = v30;\n    ex_v1 = __shfl_xor_sync(0xffffffff,v31, mask);\n    CMP_SWP(K, ex_k0, ex_k1, int, ex_v0, ex_v1);\n    if(bit) EQL_SWP(K, ex_k0, ex_k1, int, ex_v0, ex_v1);\n    k30 = ex_k0;\n    k31 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    v30 = ex_v0;\n    v31 = __shfl_xor_sync(0xffffffff,ex_v1, mask);\n    if(bit) SWP(K, k0, k30, int, v0, v30);\n    if(bit) SWP(K, k1, k31, int, v1, v31);\n    if(bit) SWP(K, k2, k28, int, v2, v28);\n    if(bit) SWP(K, k3, k29, int, v3, v29);\n    if(bit) SWP(K, k4, k26, int, v4, v26);\n    if(bit) SWP(K, k5, k27, int, v5, v27);\n    if(bit) SWP(K, k6, k24, int, v6, v24);\n    if(bit) SWP(K, k7, k25, int, v7, v25);\n    if(bit) SWP(K, k8, k22, int, v8, v22);\n    if(bit) SWP(K, k9, k23, int, v9, v23);\n    if(bit) SWP(K, k10, k20, int, v10, v20);\n    if(bit) SWP(K, k11, k21, int, v11, v21);\n    if(bit) SWP(K, k12, k18, int, v12, v18);\n    if(bit) SWP(K, k13, k19, int, v13, v19);\n    if(bit) SWP(K, k14, k16, int, v14, v16);\n    if(bit) SWP(K, k15, k17, int, v15, v17);\n}\n\n__device__ inline void exch_paral(K &k0, K &k1, K &k2, K &k3, K &k4, K &k5, K &k6, K &k7, K &k8, K &k9, K &k10, K &k11, K &k12, K &k13, K &k14, K &k15, K &k16, K &k17, K &k18, K &k19, K &k20, K &k21, K &k22, K &k23, K &k24, K &k25, K &k26, K &k27, K &k28, K &k29, K &k30, K &k31, int &v0, int &v1, int &v2, int &v3, int &v4, int &v5, int &v6, int &v7, int &v8, int &v9, int &v10, int &v11, int &v12, int &v13, int &v14, int &v15, int &v16, int &v17, int &v18, int &v19, int &v20, int &v21, int &v22, int &v23, int &v24, int &v25, int &v26, int &v27, int &v28, int &v29, int &v30, int &v31, int mask, const int bit) {\n    K ex_k0, ex_k1;\n    int ex_v0, ex_v1;\n    if(bit) SWP(K, k0, k1, int, v0, v1);\n    if(bit) SWP(K, k2, k3, int, v2, v3);\n    if(bit) SWP(K, k4, k5, int, v4, v5);\n    if(bit) SWP(K, k6, k7, int, v6, v7);\n    if(bit) SWP(K, k8, k9, int, v8, v9);\n    if(bit) SWP(K, k10, k11, int, v10, v11);\n    if(bit) SWP(K, k12, k13, int, v12, v13);\n    if(bit) SWP(K, k14, k15, int, v14, v15);\n    if(bit) SWP(K, k16, k17, int, v16, v17);\n    if(bit) SWP(K, k18, k19, int, v18, v19);\n    if(bit) SWP(K, k20, k21, int, v20, v21);\n    if(bit) SWP(K, k22, k23, int, v22, v23);\n    if(bit) SWP(K, k24, k25, int, v24, v25);\n    if(bit) SWP(K, k26, k27, int, v26, v27);\n    if(bit) SWP(K, k28, k29, int, v28, v29);\n    if(bit) SWP(K, k30, k31, int, v30, v31);\n    ex_k0 = k0;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k1, mask);\n    ex_v0 = v0;\n    ex_v1 = __shfl_xor_sync(0xffffffff,v1, mask);\n    CMP_SWP(K, ex_k0, ex_k1, int, ex_v0, ex_v1);\n    if(bit) EQL_SWP(K, ex_k0, ex_k1, int, ex_v0, ex_v1);\n    k0 = ex_k0;\n    k1 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    v0 = ex_v0;\n    v1 = __shfl_xor_sync(0xffffffff,ex_v1, mask);\n    ex_k0 = k2;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k3, mask);\n    ex_v0 = v2;\n    ex_v1 = __shfl_xor_sync(0xffffffff,v3, mask);\n    CMP_SWP(K, ex_k0, ex_k1, int, ex_v0, ex_v1);\n    if(bit) EQL_SWP(K, ex_k0, ex_k1, int, ex_v0, ex_v1);\n    k2 = ex_k0;\n    k3 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    v2 = ex_v0;\n    v3 = __shfl_xor_sync(0xffffffff,ex_v1, mask);\n    ex_k0 = k4;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k5, mask);\n    ex_v0 = v4;\n    ex_v1 = __shfl_xor_sync(0xffffffff,v5, mask);\n    CMP_SWP(K, ex_k0, ex_k1, int, ex_v0, ex_v1);\n    if(bit) EQL_SWP(K, ex_k0, ex_k1, int, ex_v0, ex_v1);\n    k4 = ex_k0;\n    k5 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    v4 = ex_v0;\n    v5 = __shfl_xor_sync(0xffffffff,ex_v1, mask);\n    ex_k0 = k6;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k7, mask);\n    ex_v0 = v6;\n    ex_v1 = __shfl_xor_sync(0xffffffff,v7, mask);\n    CMP_SWP(K, ex_k0, ex_k1, int, ex_v0, ex_v1);\n    if(bit) EQL_SWP(K, ex_k0, ex_k1, int, ex_v0, ex_v1);\n    k6 = ex_k0;\n    k7 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    v6 = ex_v0;\n    v7 = __shfl_xor_sync(0xffffffff,ex_v1, mask);\n    ex_k0 = k8;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k9, mask);\n    ex_v0 = v8;\n    ex_v1 = __shfl_xor_sync(0xffffffff,v9, mask);\n    CMP_SWP(K, ex_k0, ex_k1, int, ex_v0, ex_v1);\n    if(bit) EQL_SWP(K, ex_k0, ex_k1, int, ex_v0, ex_v1);\n    k8 = ex_k0;\n    k9 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    v8 = ex_v0;\n    v9 = __shfl_xor_sync(0xffffffff,ex_v1, mask);\n    ex_k0 = k10;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k11, mask);\n    ex_v0 = v10;\n    ex_v1 = __shfl_xor_sync(0xffffffff,v11, mask);\n    CMP_SWP(K, ex_k0, ex_k1, int, ex_v0, ex_v1);\n    if(bit) EQL_SWP(K, ex_k0, ex_k1, int, ex_v0, ex_v1);\n    k10 = ex_k0;\n    k11 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    v10 = ex_v0;\n    v11 = __shfl_xor_sync(0xffffffff,ex_v1, mask);\n    ex_k0 = k12;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k13, mask);\n    ex_v0 = v12;\n    ex_v1 = __shfl_xor_sync(0xffffffff,v13, mask);\n    CMP_SWP(K, ex_k0, ex_k1, int, ex_v0, ex_v1);\n    if(bit) EQL_SWP(K, ex_k0, ex_k1, int, ex_v0, ex_v1);\n    k12 = ex_k0;\n    k13 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    v12 = ex_v0;\n    v13 = __shfl_xor_sync(0xffffffff,ex_v1, mask);\n    ex_k0 = k14;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k15, mask);\n    ex_v0 = v14;\n    ex_v1 = __shfl_xor_sync(0xffffffff,v15, mask);\n    CMP_SWP(K, ex_k0, ex_k1, int, ex_v0, ex_v1);\n    if(bit) EQL_SWP(K, ex_k0, ex_k1, int, ex_v0, ex_v1);\n    k14 = ex_k0;\n    k15 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    v14 = ex_v0;\n    v15 = __shfl_xor_sync(0xffffffff,ex_v1, mask);\n    ex_k0 = k16;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k17, mask);\n    ex_v0 = v16;\n    ex_v1 = __shfl_xor_sync(0xffffffff,v17, mask);\n    CMP_SWP(K, ex_k0, ex_k1, int, ex_v0, ex_v1);\n    if(bit) EQL_SWP(K, ex_k0, ex_k1, int, ex_v0, ex_v1);\n    k16 = ex_k0;\n    k17 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    v16 = ex_v0;\n    v17 = __shfl_xor_sync(0xffffffff,ex_v1, mask);\n    ex_k0 = k18;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k19, mask);\n    ex_v0 = v18;\n    ex_v1 = __shfl_xor_sync(0xffffffff,v19, mask);\n    CMP_SWP(K, ex_k0, ex_k1, int, ex_v0, ex_v1);\n    if(bit) EQL_SWP(K, ex_k0, ex_k1, int, ex_v0, ex_v1);\n    k18 = ex_k0;\n    k19 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    v18 = ex_v0;\n    v19 = __shfl_xor_sync(0xffffffff,ex_v1, mask);\n    ex_k0 = k20;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k21, mask);\n    ex_v0 = v20;\n    ex_v1 = __shfl_xor_sync(0xffffffff,v21, mask);\n    CMP_SWP(K, ex_k0, ex_k1, int, ex_v0, ex_v1);\n    if(bit) EQL_SWP(K, ex_k0, ex_k1, int, ex_v0, ex_v1);\n    k20 = ex_k0;\n    k21 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    v20 = ex_v0;\n    v21 = __shfl_xor_sync(0xffffffff,ex_v1, mask);\n    ex_k0 = k22;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k23, mask);\n    ex_v0 = v22;\n    ex_v1 = __shfl_xor_sync(0xffffffff,v23, mask);\n    CMP_SWP(K, ex_k0, ex_k1, int, ex_v0, ex_v1);\n    if(bit) EQL_SWP(K, ex_k0, ex_k1, int, ex_v0, ex_v1);\n    k22 = ex_k0;\n    k23 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    v22 = ex_v0;\n    v23 = __shfl_xor_sync(0xffffffff,ex_v1, mask);\n    ex_k0 = k24;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k25, mask);\n    ex_v0 = v24;\n    ex_v1 = __shfl_xor_sync(0xffffffff,v25, mask);\n    CMP_SWP(K, ex_k0, ex_k1, int, ex_v0, ex_v1);\n    if(bit) EQL_SWP(K, ex_k0, ex_k1, int, ex_v0, ex_v1);\n    k24 = ex_k0;\n    k25 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    v24 = ex_v0;\n    v25 = __shfl_xor_sync(0xffffffff,ex_v1, mask);\n    ex_k0 = k26;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k27, mask);\n    ex_v0 = v26;\n    ex_v1 = __shfl_xor_sync(0xffffffff,v27, mask);\n    CMP_SWP(K, ex_k0, ex_k1, int, ex_v0, ex_v1);\n    if(bit) EQL_SWP(K, ex_k0, ex_k1, int, ex_v0, ex_v1);\n    k26 = ex_k0;\n    k27 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    v26 = ex_v0;\n    v27 = __shfl_xor_sync(0xffffffff,ex_v1, mask);\n    ex_k0 = k28;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k29, mask);\n    ex_v0 = v28;\n    ex_v1 = __shfl_xor_sync(0xffffffff,v29, mask);\n    CMP_SWP(K, ex_k0, ex_k1, int, ex_v0, ex_v1);\n    if(bit) EQL_SWP(K, ex_k0, ex_k1, int, ex_v0, ex_v1);\n    k28 = ex_k0;\n    k29 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    v28 = ex_v0;\n    v29 = __shfl_xor_sync(0xffffffff,ex_v1, mask);\n    ex_k0 = k30;\n    ex_k1 = __shfl_xor_sync(0xffffffff,k31, mask);\n    ex_v0 = v30;\n    ex_v1 = __shfl_xor_sync(0xffffffff,v31, mask);\n    CMP_SWP(K, ex_k0, ex_k1, int, ex_v0, ex_v1);\n    if(bit) EQL_SWP(K, ex_k0, ex_k1, int, ex_v0, ex_v1);\n    k30 = ex_k0;\n    k31 = __shfl_xor_sync(0xffffffff,ex_k1, mask);\n    v30 = ex_v0;\n    v31 = __shfl_xor_sync(0xffffffff,ex_v1, mask);\n    if(bit) SWP(K, k0, k1, int, v0, v1);\n    if(bit) SWP(K, k2, k3, int, v2, v3);\n    if(bit) SWP(K, k4, k5, int, v4, v5);\n    if(bit) SWP(K, k6, k7, int, v6, v7);\n    if(bit) SWP(K, k8, k9, int, v8, v9);\n    if(bit) SWP(K, k10, k11, int, v10, v11);\n    if(bit) SWP(K, k12, k13, int, v12, v13);\n    if(bit) SWP(K, k14, k15, int, v14, v15);\n    if(bit) SWP(K, k16, k17, int, v16, v17);\n    if(bit) SWP(K, k18, k19, int, v18, v19);\n    if(bit) SWP(K, k20, k21, int, v20, v21);\n    if(bit) SWP(K, k22, k23, int, v22, v23);\n    if(bit) SWP(K, k24, k25, int, v24, v25);\n    if(bit) SWP(K, k26, k27, int, v26, v27);\n    if(bit) SWP(K, k28, k29, int, v28, v29);\n    if(bit) SWP(K, k30, k31, int, v30, v31);\n}\n\n__device__\nint find_kth3(K* a, int aCount, K* b, int bCount, int diag)\n{\n    int begin = max(0, diag - bCount);\n    int end = min(diag, aCount);\n\n    while(begin < end) {\n        int mid = (begin + end)>> 1;\n        K aKey = a[mid];\n        K bKey = b[diag - 1 - mid];\n        bool pred = aKey <= bKey;\n        if(pred) begin = mid + 1;\n        else end = mid;\n    }\n    return begin;\n}\n\n__global__\nvoid kern_block_sort(\n    const K *key, const T *val, K *keyB, T *valB,\n    const Offset *seg_begins, const Offset *seg_ends, const int *bin,\n    const int workloads_per_block)\n{\n    const int bin_it = blockIdx.x;\n    const int innerbid = blockIdx.y;\n\n    const int seg_size = seg_ends[bin[bin_it]]-seg_begins[bin[bin_it]];\n    const int blk_stat = (seg_size+workloads_per_block-1)/workloads_per_block;\n\n    if(innerbid < blk_stat)\n    {\n        const int tid = threadIdx.x;\n        __shared__ K smem[2048];\n        __shared__ int tmem[2048];\n        const int bit1 = (tid>>0)&0x1;\n        const int bit2 = (tid>>1)&0x1;\n        const int bit3 = (tid>>2)&0x1;\n        const int bit4 = (tid>>3)&0x1;\n        const int bit5 = (tid>>4)&0x1;\n        const int warp_lane = threadIdx.x & 31;\n        const int warp_id = threadIdx.x / 32;\n        K rg_k0 ;\n        K rg_k1 ;\n        K rg_k2 ;\n        K rg_k3 ;\n        int rg_v0 ;\n        int rg_v1 ;\n        int rg_v2 ;\n        int rg_v3 ;\n        // int k;\n        // int ext_seg_size;\n        /*** codegen ***/\n        int k = seg_begins[bin[bin_it]];\n        k = k + (innerbid<<11);\n        int inner_seg_size = min(seg_size-(innerbid<<11), 2048);\n        /*** codegen ***/\n        rg_k0  = (warp_lane+(warp_id<<7)+0   <inner_seg_size)?key[k+warp_lane+(warp_id<<7)+0   ]:std::numeric_limits<K>::max();\n        rg_k1  = (warp_lane+(warp_id<<7)+32  <inner_seg_size)?key[k+warp_lane+(warp_id<<7)+32  ]:std::numeric_limits<K>::max();\n        rg_k2  = (warp_lane+(warp_id<<7)+64  <inner_seg_size)?key[k+warp_lane+(warp_id<<7)+64  ]:std::numeric_limits<K>::max();\n        rg_k3  = (warp_lane+(warp_id<<7)+96  <inner_seg_size)?key[k+warp_lane+(warp_id<<7)+96  ]:std::numeric_limits<K>::max();\n        if(warp_lane+(warp_id<<7)+0   <inner_seg_size) rg_v0  = warp_lane+(warp_id<<7)+0   ;\n        if(warp_lane+(warp_id<<7)+32  <inner_seg_size) rg_v1  = warp_lane+(warp_id<<7)+32  ;\n        if(warp_lane+(warp_id<<7)+64  <inner_seg_size) rg_v2  = warp_lane+(warp_id<<7)+64  ;\n        if(warp_lane+(warp_id<<7)+96  <inner_seg_size) rg_v3  = warp_lane+(warp_id<<7)+96  ;\n        // exch_intxn: switch to exch_local()\n        CMP_SWP(K,rg_k0 ,rg_k1 ,int,rg_v0 ,rg_v1 );\n        CMP_SWP(K,rg_k2 ,rg_k3 ,int,rg_v2 ,rg_v3 );\n        // exch_intxn: switch to exch_local()\n        CMP_SWP(K,rg_k0 ,rg_k3 ,int,rg_v0 ,rg_v3 );\n        CMP_SWP(K,rg_k1 ,rg_k2 ,int,rg_v1 ,rg_v2 );\n        // exch_paral: switch to exch_local()\n        CMP_SWP(K,rg_k0 ,rg_k1 ,int,rg_v0 ,rg_v1 );\n        CMP_SWP(K,rg_k2 ,rg_k3 ,int,rg_v2 ,rg_v3 );\n        // exch_intxn: generate exch_intxn()\n        exch_intxn(rg_k0 ,rg_k1 ,rg_k2 ,rg_k3 ,\n                rg_v0 ,rg_v1 ,rg_v2 ,rg_v3 ,\n                0x1,bit1);\n        // exch_paral: switch to exch_local()\n        CMP_SWP(K,rg_k0 ,rg_k2 ,int,rg_v0 ,rg_v2 );\n        CMP_SWP(K,rg_k1 ,rg_k3 ,int,rg_v1 ,rg_v3 );\n        // exch_paral: switch to exch_local()\n        CMP_SWP(K,rg_k0 ,rg_k1 ,int,rg_v0 ,rg_v1 );\n        CMP_SWP(K,rg_k2 ,rg_k3 ,int,rg_v2 ,rg_v3 );\n        // exch_intxn: generate exch_intxn()\n        exch_intxn(rg_k0 ,rg_k1 ,rg_k2 ,rg_k3 ,\n                rg_v0 ,rg_v1 ,rg_v2 ,rg_v3 ,\n                0x3,bit2);\n        // exch_paral: generate exch_paral()\n        exch_paral(rg_k0 ,rg_k1 ,rg_k2 ,rg_k3 ,\n                rg_v0 ,rg_v1 ,rg_v2 ,rg_v3 ,\n                0x1,bit1);\n        // exch_paral: switch to exch_local()\n        CMP_SWP(K,rg_k0 ,rg_k2 ,int,rg_v0 ,rg_v2 );\n        CMP_SWP(K,rg_k1 ,rg_k3 ,int,rg_v1 ,rg_v3 );\n        // exch_paral: switch to exch_local()\n        CMP_SWP(K,rg_k0 ,rg_k1 ,int,rg_v0 ,rg_v1 );\n        CMP_SWP(K,rg_k2 ,rg_k3 ,int,rg_v2 ,rg_v3 );\n        // exch_intxn: generate exch_intxn()\n        exch_intxn(rg_k0 ,rg_k1 ,rg_k2 ,rg_k3 ,\n                rg_v0 ,rg_v1 ,rg_v2 ,rg_v3 ,\n                0x7,bit3);\n        // exch_paral: generate exch_paral()\n        exch_paral(rg_k0 ,rg_k1 ,rg_k2 ,rg_k3 ,\n                rg_v0 ,rg_v1 ,rg_v2 ,rg_v3 ,\n                0x2,bit2);\n        // exch_paral: generate exch_paral()\n        exch_paral(rg_k0 ,rg_k1 ,rg_k2 ,rg_k3 ,\n                rg_v0 ,rg_v1 ,rg_v2 ,rg_v3 ,\n                0x1,bit1);\n        // exch_paral: switch to exch_local()\n        CMP_SWP(K,rg_k0 ,rg_k2 ,int,rg_v0 ,rg_v2 );\n        CMP_SWP(K,rg_k1 ,rg_k3 ,int,rg_v1 ,rg_v3 );\n        // exch_paral: switch to exch_local()\n        CMP_SWP(K,rg_k0 ,rg_k1 ,int,rg_v0 ,rg_v1 );\n        CMP_SWP(K,rg_k2 ,rg_k3 ,int,rg_v2 ,rg_v3 );\n        // exch_intxn: generate exch_intxn()\n        exch_intxn(rg_k0 ,rg_k1 ,rg_k2 ,rg_k3 ,\n                rg_v0 ,rg_v1 ,rg_v2 ,rg_v3 ,\n                0xf,bit4);\n        // exch_paral: generate exch_paral()\n        exch_paral(rg_k0 ,rg_k1 ,rg_k2 ,rg_k3 ,\n                rg_v0 ,rg_v1 ,rg_v2 ,rg_v3 ,\n                0x4,bit3);\n        // exch_paral: generate exch_paral()\n        exch_paral(rg_k0 ,rg_k1 ,rg_k2 ,rg_k3 ,\n                rg_v0 ,rg_v1 ,rg_v2 ,rg_v3 ,\n                0x2,bit2);\n        // exch_paral: generate exch_paral()\n        exch_paral(rg_k0 ,rg_k1 ,rg_k2 ,rg_k3 ,\n                rg_v0 ,rg_v1 ,rg_v2 ,rg_v3 ,\n                0x1,bit1);\n        // exch_paral: switch to exch_local()\n        CMP_SWP(K,rg_k0 ,rg_k2 ,int,rg_v0 ,rg_v2 );\n        CMP_SWP(K,rg_k1 ,rg_k3 ,int,rg_v1 ,rg_v3 );\n        // exch_paral: switch to exch_local()\n        CMP_SWP(K,rg_k0 ,rg_k1 ,int,rg_v0 ,rg_v1 );\n        CMP_SWP(K,rg_k2 ,rg_k3 ,int,rg_v2 ,rg_v3 );\n        // exch_intxn: generate exch_intxn()\n        exch_intxn(rg_k0 ,rg_k1 ,rg_k2 ,rg_k3 ,\n                rg_v0 ,rg_v1 ,rg_v2 ,rg_v3 ,\n                0x1f,bit5);\n        // exch_paral: generate exch_paral()\n        exch_paral(rg_k0 ,rg_k1 ,rg_k2 ,rg_k3 ,\n                rg_v0 ,rg_v1 ,rg_v2 ,rg_v3 ,\n                0x8,bit4);\n        // exch_paral: generate exch_paral()\n        exch_paral(rg_k0 ,rg_k1 ,rg_k2 ,rg_k3 ,\n                rg_v0 ,rg_v1 ,rg_v2 ,rg_v3 ,\n                0x4,bit3);\n        // exch_paral: generate exch_paral()\n        exch_paral(rg_k0 ,rg_k1 ,rg_k2 ,rg_k3 ,\n                rg_v0 ,rg_v1 ,rg_v2 ,rg_v3 ,\n                0x2,bit2);\n        // exch_paral: generate exch_paral()\n        exch_paral(rg_k0 ,rg_k1 ,rg_k2 ,rg_k3 ,\n                rg_v0 ,rg_v1 ,rg_v2 ,rg_v3 ,\n                0x1,bit1);\n        // exch_paral: switch to exch_local()\n        CMP_SWP(K,rg_k0 ,rg_k2 ,int,rg_v0 ,rg_v2 );\n        CMP_SWP(K,rg_k1 ,rg_k3 ,int,rg_v1 ,rg_v3 );\n        // exch_paral: switch to exch_local()\n        CMP_SWP(K,rg_k0 ,rg_k1 ,int,rg_v0 ,rg_v1 );\n        CMP_SWP(K,rg_k2 ,rg_k3 ,int,rg_v2 ,rg_v3 );\n\n        smem[(warp_id<<7)+(warp_lane<<2)+0 ] = rg_k0 ;\n        smem[(warp_id<<7)+(warp_lane<<2)+1 ] = rg_k1 ;\n        smem[(warp_id<<7)+(warp_lane<<2)+2 ] = rg_k2 ;\n        smem[(warp_id<<7)+(warp_lane<<2)+3 ] = rg_k3 ;\n        tmem[(warp_id<<7)+(warp_lane<<2)+0 ] = rg_v0 ;\n        tmem[(warp_id<<7)+(warp_lane<<2)+1 ] = rg_v1 ;\n        tmem[(warp_id<<7)+(warp_lane<<2)+2 ] = rg_v2 ;\n        tmem[(warp_id<<7)+(warp_lane<<2)+3 ] = rg_v3 ;\n        __syncthreads();\n        // Merge in 4 steps\n        int grp_start_wp_id;\n        int grp_start_off;\n        // int tmp_wp_id;\n        int lhs_len;\n        int rhs_len;\n        int gran;\n        int s_a;\n        int s_b;\n        bool p;\n        K tmp_k0;\n        K tmp_k1;\n        int tmp_v0;\n        int tmp_v1;\n        K *start;\n        // Step 0\n        grp_start_wp_id = ((warp_id>>1)<<1);\n        grp_start_off = (grp_start_wp_id)*128;\n        // tmp_wp_id = grp_start_wp_id;\n        lhs_len = (128 );\n        rhs_len = (128 );\n        gran = (warp_lane<<2);\n        if((warp_id&1)==0){\n            gran += 0;\n        }\n        if((warp_id&1)==1){\n            gran += (128 );\n        }\n        start = smem + grp_start_off;\n        s_a = find_kth3(start, lhs_len, start+lhs_len, rhs_len, gran);\n        s_b = lhs_len + gran - s_a;\n\n        tmp_k0 = (s_a<lhs_len        )?smem[grp_start_off+s_a]:std::numeric_limits<K>::max();\n        tmp_k1 = (s_b<lhs_len+rhs_len)?smem[grp_start_off+s_b]:std::numeric_limits<K>::max();\n        if(s_a<lhs_len        ) tmp_v0 = tmem[grp_start_off+s_a];\n        if(s_b<lhs_len+rhs_len) tmp_v1 = tmem[grp_start_off+s_b];\n        p = (s_b>=lhs_len+rhs_len)||((s_a<lhs_len)&&(tmp_k0<=tmp_k1));\n        rg_k0 = p ? tmp_k0 : tmp_k1;\n        rg_v0 = p ? tmp_v0 : tmp_v1;\n        if(p) {\n            ++s_a;\n            tmp_k0 = (s_a<lhs_len)?smem[grp_start_off+s_a]:std::numeric_limits<K>::max();\n            if(s_a<lhs_len) tmp_v0 = tmem[grp_start_off+s_a];\n        } else {\n            ++s_b;\n            tmp_k1 = (s_b<lhs_len+rhs_len)?smem[grp_start_off+s_b]:std::numeric_limits<K>::max();\n            if(s_b<lhs_len+rhs_len) tmp_v1 = tmem[grp_start_off+s_b];\n        }\n        p = (s_b>=lhs_len+rhs_len)||((s_a<lhs_len)&&(tmp_k0<=tmp_k1));\n        rg_k1 = p ? tmp_k0 : tmp_k1;\n        rg_v1 = p ? tmp_v0 : tmp_v1;\n        if(p) {\n            ++s_a;\n            tmp_k0 = (s_a<lhs_len)?smem[grp_start_off+s_a]:std::numeric_limits<K>::max();\n            if(s_a<lhs_len) tmp_v0 = tmem[grp_start_off+s_a];\n        } else {\n            ++s_b;\n            tmp_k1 = (s_b<lhs_len+rhs_len)?smem[grp_start_off+s_b]:std::numeric_limits<K>::max();\n            if(s_b<lhs_len+rhs_len) tmp_v1 = tmem[grp_start_off+s_b];\n        }\n        p = (s_b>=lhs_len+rhs_len)||((s_a<lhs_len)&&(tmp_k0<=tmp_k1));\n        rg_k2 = p ? tmp_k0 : tmp_k1;\n        rg_v2 = p ? tmp_v0 : tmp_v1;\n        if(p) {\n            ++s_a;\n            tmp_k0 = (s_a<lhs_len)?smem[grp_start_off+s_a]:std::numeric_limits<K>::max();\n            if(s_a<lhs_len) tmp_v0 = tmem[grp_start_off+s_a];\n        } else {\n            ++s_b;\n            tmp_k1 = (s_b<lhs_len+rhs_len)?smem[grp_start_off+s_b]:std::numeric_limits<K>::max();\n            if(s_b<lhs_len+rhs_len) tmp_v1 = tmem[grp_start_off+s_b];\n        }\n        p = (s_b>=lhs_len+rhs_len)||((s_a<lhs_len)&&(tmp_k0<=tmp_k1));\n        rg_k3 = p ? tmp_k0 : tmp_k1;\n        rg_v3 = p ? tmp_v0 : tmp_v1;\n        __syncthreads();\n        // Store merged results back to shared memory\n\n        smem[grp_start_off+gran+0 ] = rg_k0 ;\n        smem[grp_start_off+gran+1 ] = rg_k1 ;\n        smem[grp_start_off+gran+2 ] = rg_k2 ;\n        smem[grp_start_off+gran+3 ] = rg_k3 ;\n        tmem[grp_start_off+gran+0 ] = rg_v0 ;\n        tmem[grp_start_off+gran+1 ] = rg_v1 ;\n        tmem[grp_start_off+gran+2 ] = rg_v2 ;\n        tmem[grp_start_off+gran+3 ] = rg_v3 ;\n        __syncthreads();\n        // Step 1\n        grp_start_wp_id = ((warp_id>>2)<<2);\n        grp_start_off = (grp_start_wp_id)*128;\n        // tmp_wp_id = grp_start_wp_id;\n        lhs_len = (256 );\n        rhs_len = (256 );\n        gran = (warp_lane<<2);\n        gran += (warp_id&3)*128;\n        start = smem + grp_start_off;\n        s_a = find_kth3(start, lhs_len, start+lhs_len, rhs_len, gran);\n        s_b = lhs_len + gran - s_a;\n\n        tmp_k0 = (s_a<lhs_len        )?smem[grp_start_off+s_a]:std::numeric_limits<K>::max();\n        tmp_k1 = (s_b<lhs_len+rhs_len)?smem[grp_start_off+s_b]:std::numeric_limits<K>::max();\n        if(s_a<lhs_len        ) tmp_v0 = tmem[grp_start_off+s_a];\n        if(s_b<lhs_len+rhs_len) tmp_v1 = tmem[grp_start_off+s_b];\n        p = (s_b>=lhs_len+rhs_len)||((s_a<lhs_len)&&(tmp_k0<=tmp_k1));\n        rg_k0 = p ? tmp_k0 : tmp_k1;\n        rg_v0 = p ? tmp_v0 : tmp_v1;\n        if(p) {\n            ++s_a;\n            tmp_k0 = (s_a<lhs_len)?smem[grp_start_off+s_a]:std::numeric_limits<K>::max();\n            if(s_a<lhs_len) tmp_v0 = tmem[grp_start_off+s_a];\n        } else {\n            ++s_b;\n            tmp_k1 = (s_b<lhs_len+rhs_len)?smem[grp_start_off+s_b]:std::numeric_limits<K>::max();\n            if(s_b<lhs_len+rhs_len) tmp_v1 = tmem[grp_start_off+s_b];\n        }\n        p = (s_b>=lhs_len+rhs_len)||((s_a<lhs_len)&&(tmp_k0<=tmp_k1));\n        rg_k1 = p ? tmp_k0 : tmp_k1;\n        rg_v1 = p ? tmp_v0 : tmp_v1;\n        if(p) {\n            ++s_a;\n            tmp_k0 = (s_a<lhs_len)?smem[grp_start_off+s_a]:std::numeric_limits<K>::max();\n            if(s_a<lhs_len) tmp_v0 = tmem[grp_start_off+s_a];\n        } else {\n            ++s_b;\n            tmp_k1 = (s_b<lhs_len+rhs_len)?smem[grp_start_off+s_b]:std::numeric_limits<K>::max();\n            if(s_b<lhs_len+rhs_len) tmp_v1 = tmem[grp_start_off+s_b];\n        }\n        p = (s_b>=lhs_len+rhs_len)||((s_a<lhs_len)&&(tmp_k0<=tmp_k1));\n        rg_k2 = p ? tmp_k0 : tmp_k1;\n        rg_v2 = p ? tmp_v0 : tmp_v1;\n        if(p) {\n            ++s_a;\n            tmp_k0 = (s_a<lhs_len)?smem[grp_start_off+s_a]:std::numeric_limits<K>::max();\n            if(s_a<lhs_len) tmp_v0 = tmem[grp_start_off+s_a];\n        } else {\n            ++s_b;\n            tmp_k1 = (s_b<lhs_len+rhs_len)?smem[grp_start_off+s_b]:std::numeric_limits<K>::max();\n            if(s_b<lhs_len+rhs_len) tmp_v1 = tmem[grp_start_off+s_b];\n        }\n        p = (s_b>=lhs_len+rhs_len)||((s_a<lhs_len)&&(tmp_k0<=tmp_k1));\n        rg_k3 = p ? tmp_k0 : tmp_k1;\n        rg_v3 = p ? tmp_v0 : tmp_v1;\n        __syncthreads();\n        // Store merged results back to shared memory\n\n        smem[grp_start_off+gran+0 ] = rg_k0 ;\n        smem[grp_start_off+gran+1 ] = rg_k1 ;\n        smem[grp_start_off+gran+2 ] = rg_k2 ;\n        smem[grp_start_off+gran+3 ] = rg_k3 ;\n        tmem[grp_start_off+gran+0 ] = rg_v0 ;\n        tmem[grp_start_off+gran+1 ] = rg_v1 ;\n        tmem[grp_start_off+gran+2 ] = rg_v2 ;\n        tmem[grp_start_off+gran+3 ] = rg_v3 ;\n        __syncthreads();\n        // Step 2\n        grp_start_wp_id = ((warp_id>>3)<<3);\n        grp_start_off = (grp_start_wp_id)*128;\n        // tmp_wp_id = grp_start_wp_id;\n        lhs_len = (512 );\n        rhs_len = (512 );\n        gran = (warp_lane<<2);\n        gran += (warp_id&7)*128;\n\n        start = smem + grp_start_off;\n        s_a = find_kth3(start, lhs_len, start+lhs_len, rhs_len, gran);\n        s_b = lhs_len + gran - s_a;\n        tmp_k0 = (s_a<lhs_len        )?smem[grp_start_off+s_a]:std::numeric_limits<K>::max();\n        tmp_k1 = (s_b<lhs_len+rhs_len)?smem[grp_start_off+s_b]:std::numeric_limits<K>::max();\n        if(s_a<lhs_len        ) tmp_v0 = tmem[grp_start_off+s_a];\n        if(s_b<lhs_len+rhs_len) tmp_v1 = tmem[grp_start_off+s_b];\n        p = (s_b>=lhs_len+rhs_len)||((s_a<lhs_len)&&(tmp_k0<=tmp_k1));\n        rg_k0 = p ? tmp_k0 : tmp_k1;\n        rg_v0 = p ? tmp_v0 : tmp_v1;\n        if(p) {\n            ++s_a;\n            tmp_k0 = (s_a<lhs_len)?smem[grp_start_off+s_a]:std::numeric_limits<K>::max();\n            if(s_a<lhs_len) tmp_v0 = tmem[grp_start_off+s_a];\n        } else {\n            ++s_b;\n            tmp_k1 = (s_b<lhs_len+rhs_len)?smem[grp_start_off+s_b]:std::numeric_limits<K>::max();\n            if(s_b<lhs_len+rhs_len) tmp_v1 = tmem[grp_start_off+s_b];\n        }\n        p = (s_b>=lhs_len+rhs_len)||((s_a<lhs_len)&&(tmp_k0<=tmp_k1));\n        rg_k1 = p ? tmp_k0 : tmp_k1;\n        rg_v1 = p ? tmp_v0 : tmp_v1;\n        if(p) {\n            ++s_a;\n            tmp_k0 = (s_a<lhs_len)?smem[grp_start_off+s_a]:std::numeric_limits<K>::max();\n            if(s_a<lhs_len) tmp_v0 = tmem[grp_start_off+s_a];\n        } else {\n            ++s_b;\n            tmp_k1 = (s_b<lhs_len+rhs_len)?smem[grp_start_off+s_b]:std::numeric_limits<K>::max();\n            if(s_b<lhs_len+rhs_len) tmp_v1 = tmem[grp_start_off+s_b];\n        }\n        p = (s_b>=lhs_len+rhs_len)||((s_a<lhs_len)&&(tmp_k0<=tmp_k1));\n        rg_k2 = p ? tmp_k0 : tmp_k1;\n        rg_v2 = p ? tmp_v0 : tmp_v1;\n        if(p) {\n            ++s_a;\n            tmp_k0 = (s_a<lhs_len)?smem[grp_start_off+s_a]:std::numeric_limits<K>::max();\n            if(s_a<lhs_len) tmp_v0 = tmem[grp_start_off+s_a];\n        } else {\n            ++s_b;\n            tmp_k1 = (s_b<lhs_len+rhs_len)?smem[grp_start_off+s_b]:std::numeric_limits<K>::max();\n            if(s_b<lhs_len+rhs_len) tmp_v1 = tmem[grp_start_off+s_b];\n        }\n        p = (s_b>=lhs_len+rhs_len)||((s_a<lhs_len)&&(tmp_k0<=tmp_k1));\n        rg_k3 = p ? tmp_k0 : tmp_k1;\n        rg_v3 = p ? tmp_v0 : tmp_v1;\n        __syncthreads();\n        // Store merged results back to shared memory\n\n        smem[grp_start_off+gran+0 ] = rg_k0 ;\n        smem[grp_start_off+gran+1 ] = rg_k1 ;\n        smem[grp_start_off+gran+2 ] = rg_k2 ;\n        smem[grp_start_off+gran+3 ] = rg_k3 ;\n        tmem[grp_start_off+gran+0 ] = rg_v0 ;\n        tmem[grp_start_off+gran+1 ] = rg_v1 ;\n        tmem[grp_start_off+gran+2 ] = rg_v2 ;\n        tmem[grp_start_off+gran+3 ] = rg_v3 ;\n        __syncthreads();\n        // Step 3\n        grp_start_wp_id = ((warp_id>>4)<<4);\n        grp_start_off = (grp_start_wp_id)*128;\n        // tmp_wp_id = grp_start_wp_id;\n        lhs_len = (1024);\n        rhs_len = (1024);\n        gran = (warp_lane<<2);\n        gran += (warp_id&15)*128;\n\n        start = smem + grp_start_off;\n        s_a = find_kth3(start, lhs_len, start+lhs_len, rhs_len, gran);\n        s_b = lhs_len + gran - s_a;\n\n        tmp_k0 = (s_a<lhs_len        )?smem[grp_start_off+s_a]:std::numeric_limits<K>::max();\n        tmp_k1 = (s_b<lhs_len+rhs_len)?smem[grp_start_off+s_b]:std::numeric_limits<K>::max();\n        if(s_a<lhs_len        ) tmp_v0 = tmem[grp_start_off+s_a];\n        if(s_b<lhs_len+rhs_len) tmp_v1 = tmem[grp_start_off+s_b];\n        p = (s_b>=lhs_len+rhs_len)||((s_a<lhs_len)&&(tmp_k0<=tmp_k1));\n        rg_k0 = p ? tmp_k0 : tmp_k1;\n        rg_v0 = p ? tmp_v0 : tmp_v1;\n        if(p) {\n            ++s_a;\n            tmp_k0 = (s_a<lhs_len)?smem[grp_start_off+s_a]:std::numeric_limits<K>::max();\n            if(s_a<lhs_len) tmp_v0 = tmem[grp_start_off+s_a];\n        } else {\n            ++s_b;\n            tmp_k1 = (s_b<lhs_len+rhs_len)?smem[grp_start_off+s_b]:std::numeric_limits<K>::max();\n            if(s_b<lhs_len+rhs_len) tmp_v1 = tmem[grp_start_off+s_b];\n        }\n        p = (s_b>=lhs_len+rhs_len)||((s_a<lhs_len)&&(tmp_k0<=tmp_k1));\n        rg_k1 = p ? tmp_k0 : tmp_k1;\n        rg_v1 = p ? tmp_v0 : tmp_v1;\n        if(p) {\n            ++s_a;\n            tmp_k0 = (s_a<lhs_len)?smem[grp_start_off+s_a]:std::numeric_limits<K>::max();\n            if(s_a<lhs_len) tmp_v0 = tmem[grp_start_off+s_a];\n        } else {\n            ++s_b;\n            tmp_k1 = (s_b<lhs_len+rhs_len)?smem[grp_start_off+s_b]:std::numeric_limits<K>::max();\n            if(s_b<lhs_len+rhs_len) tmp_v1 = tmem[grp_start_off+s_b];\n        }\n        p = (s_b>=lhs_len+rhs_len)||((s_a<lhs_len)&&(tmp_k0<=tmp_k1));\n        rg_k2 = p ? tmp_k0 : tmp_k1;\n        rg_v2 = p ? tmp_v0 : tmp_v1;\n        if(p) {\n            ++s_a;\n            tmp_k0 = (s_a<lhs_len)?smem[grp_start_off+s_a]:std::numeric_limits<K>::max();\n            if(s_a<lhs_len) tmp_v0 = tmem[grp_start_off+s_a];\n        } else {\n            ++s_b;\n            tmp_k1 = (s_b<lhs_len+rhs_len)?smem[grp_start_off+s_b]:std::numeric_limits<K>::max();\n            if(s_b<lhs_len+rhs_len) tmp_v1 = tmem[grp_start_off+s_b];\n        }\n        p = (s_b>=lhs_len+rhs_len)||((s_a<lhs_len)&&(tmp_k0<=tmp_k1));\n        rg_k3 = p ? tmp_k0 : tmp_k1;\n        rg_v3 = p ? tmp_v0 : tmp_v1;\n\n        if((tid<<2)+0 <inner_seg_size) keyB[k+(tid<<2)+0 ] = rg_k0 ;\n        if((tid<<2)+1 <inner_seg_size) keyB[k+(tid<<2)+1 ] = rg_k1 ;\n        if((tid<<2)+2 <inner_seg_size) keyB[k+(tid<<2)+2 ] = rg_k2 ;\n        if((tid<<2)+3 <inner_seg_size) keyB[k+(tid<<2)+3 ] = rg_k3 ;\n        T t_v0 ;\n        T t_v1 ;\n        T t_v2 ;\n        T t_v3 ;\n        if((tid<<2)+0 <inner_seg_size) t_v0  = val[k+rg_v0 ];\n        if((tid<<2)+1 <inner_seg_size) t_v1  = val[k+rg_v1 ];\n        if((tid<<2)+2 <inner_seg_size) t_v2  = val[k+rg_v2 ];\n        if((tid<<2)+3 <inner_seg_size) t_v3  = val[k+rg_v3 ];\n        if((tid<<2)+0 <inner_seg_size) valB[k+(tid<<2)+0 ] = t_v0 ;\n        if((tid<<2)+1 <inner_seg_size) valB[k+(tid<<2)+1 ] = t_v1 ;\n        if((tid<<2)+2 <inner_seg_size) valB[k+(tid<<2)+2 ] = t_v2 ;\n        if((tid<<2)+3 <inner_seg_size) valB[k+(tid<<2)+3 ] = t_v3 ;\n    }\n}",
            "#define K 1.38065e-23        // J/K, Boltzmann constant\n\n\n#define T ((int)32)\n\n\ninline __host__ __device__ uint4 make_uint4(int4 a)\n{\n    return make_uint4(uint(a.x), uint(a.y), uint(a.z), uint(a.w));\n}\n\ninline __host__ __device__ uint4 max(uint4 a, uint4 b)\n{\n    return make_uint4(max(a.x,b.x), max(a.y,b.y), max(a.z,b.z), max(a.w,b.w));\n}\n\ninline __host__ __device__ uint4 min(uint4 a, uint4 b)\n{\n    return make_uint4(min(a.x,b.x), min(a.y,b.y), min(a.z,b.z), min(a.w,b.w));\n}\n\n__device__\nint find_kth3(K* a, int aCount, K* b, int bCount, int diag)\n{\n    int begin = max(0, diag - bCount);\n    int end = min(diag, aCount);\n\n    while(begin < end) {\n        int mid = (begin + end)>> 1;\n        K aKey = a[mid];\n        K bKey = b[diag - 1 - mid];\n        bool pred = aKey <= bKey;\n        if(pred) begin = mid + 1;\n        else end = mid;\n    }\n    return begin;\n}\n\n__global__\nvoid kern_block_merge(\n    const K *keys, const T *vals, K *keysB, T *valsB,\n    const Offset *seg_begins, const Offset *seg_ends, const int *bin,  const int stride,\n    const int workloads_per_block)\n{\n    const int bin_it = blockIdx.x;\n    const int innerbid = blockIdx.y;\n\n    const int seg_size = seg_ends[bin[bin_it]]-seg_begins[bin[bin_it]];\n    const int blk_stat = (seg_size+workloads_per_block-1)/workloads_per_block;\n\n    if(innerbid < blk_stat && stride < seg_size)\n    {\n        const int tid = threadIdx.x;\n        __shared__ K smem[128*16];\n        const int k = seg_begins[bin[bin_it]];\n\n        int loc_a, loc_b;\n        int cnt_a, cnt_b;\n        int coop = (stride<<1)>>11; // how many blocks coop\n        int coop_bid = innerbid%coop;\n        int l_gran, r_gran;\n        loc_a = (innerbid/coop)*(stride<<1);\n        cnt_a = min(stride, seg_size-loc_a);\n        loc_b = min(loc_a + stride, seg_size);\n        cnt_b = min(stride, seg_size-loc_b);\n        l_gran = coop_bid<<11;\n        r_gran = min((coop_bid+1)<<11, seg_size-loc_a);\n        int l_s_a, l_s_b;\n        int r_s_a, r_s_b;\n        l_s_a = find_kth3(keys+k+loc_a, cnt_a, keys+k+loc_b, cnt_b, l_gran);\n        l_s_b = l_gran - l_s_a;\n        r_s_a = find_kth3(keys+k+loc_a, cnt_a, keys+k+loc_b, cnt_b, r_gran);\n        r_s_b = r_gran - r_s_a;\n        int l_st = 0;\n        int l_cnt = r_s_a - l_s_a;\n        if(l_s_a+tid     <r_s_a) smem[l_st+tid     ] = keys[k+loc_a+l_s_a+tid     ];\n        if(l_s_a+tid+128 <r_s_a) smem[l_st+tid+128 ] = keys[k+loc_a+l_s_a+tid+128 ];\n        if(l_s_a+tid+256 <r_s_a) smem[l_st+tid+256 ] = keys[k+loc_a+l_s_a+tid+256 ];\n        if(l_s_a+tid+384 <r_s_a) smem[l_st+tid+384 ] = keys[k+loc_a+l_s_a+tid+384 ];\n        if(l_s_a+tid+512 <r_s_a) smem[l_st+tid+512 ] = keys[k+loc_a+l_s_a+tid+512 ];\n        if(l_s_a+tid+640 <r_s_a) smem[l_st+tid+640 ] = keys[k+loc_a+l_s_a+tid+640 ];\n        if(l_s_a+tid+768 <r_s_a) smem[l_st+tid+768 ] = keys[k+loc_a+l_s_a+tid+768 ];\n        if(l_s_a+tid+896 <r_s_a) smem[l_st+tid+896 ] = keys[k+loc_a+l_s_a+tid+896 ];\n        if(l_s_a+tid+1024<r_s_a) smem[l_st+tid+1024] = keys[k+loc_a+l_s_a+tid+1024];\n        if(l_s_a+tid+1152<r_s_a) smem[l_st+tid+1152] = keys[k+loc_a+l_s_a+tid+1152];\n        if(l_s_a+tid+1280<r_s_a) smem[l_st+tid+1280] = keys[k+loc_a+l_s_a+tid+1280];\n        if(l_s_a+tid+1408<r_s_a) smem[l_st+tid+1408] = keys[k+loc_a+l_s_a+tid+1408];\n        if(l_s_a+tid+1536<r_s_a) smem[l_st+tid+1536] = keys[k+loc_a+l_s_a+tid+1536];\n        if(l_s_a+tid+1664<r_s_a) smem[l_st+tid+1664] = keys[k+loc_a+l_s_a+tid+1664];\n        if(l_s_a+tid+1792<r_s_a) smem[l_st+tid+1792] = keys[k+loc_a+l_s_a+tid+1792];\n        if(l_s_a+tid+1920<r_s_a) smem[l_st+tid+1920] = keys[k+loc_a+l_s_a+tid+1920];\n        int r_st = r_s_a - l_s_a;\n        int r_cnt = r_s_b - l_s_b;\n        if(l_s_b+tid     <r_s_b) smem[r_st+tid     ] = keys[k+loc_b+l_s_b+tid     ];\n        if(l_s_b+tid+128 <r_s_b) smem[r_st+tid+128 ] = keys[k+loc_b+l_s_b+tid+128 ];\n        if(l_s_b+tid+256 <r_s_b) smem[r_st+tid+256 ] = keys[k+loc_b+l_s_b+tid+256 ];\n        if(l_s_b+tid+384 <r_s_b) smem[r_st+tid+384 ] = keys[k+loc_b+l_s_b+tid+384 ];\n        if(l_s_b+tid+512 <r_s_b) smem[r_st+tid+512 ] = keys[k+loc_b+l_s_b+tid+512 ];\n        if(l_s_b+tid+640 <r_s_b) smem[r_st+tid+640 ] = keys[k+loc_b+l_s_b+tid+640 ];\n        if(l_s_b+tid+768 <r_s_b) smem[r_st+tid+768 ] = keys[k+loc_b+l_s_b+tid+768 ];\n        if(l_s_b+tid+896 <r_s_b) smem[r_st+tid+896 ] = keys[k+loc_b+l_s_b+tid+896 ];\n        if(l_s_b+tid+1024<r_s_b) smem[r_st+tid+1024] = keys[k+loc_b+l_s_b+tid+1024];\n        if(l_s_b+tid+1152<r_s_b) smem[r_st+tid+1152] = keys[k+loc_b+l_s_b+tid+1152];\n        if(l_s_b+tid+1280<r_s_b) smem[r_st+tid+1280] = keys[k+loc_b+l_s_b+tid+1280];\n        if(l_s_b+tid+1408<r_s_b) smem[r_st+tid+1408] = keys[k+loc_b+l_s_b+tid+1408];\n        if(l_s_b+tid+1536<r_s_b) smem[r_st+tid+1536] = keys[k+loc_b+l_s_b+tid+1536];\n        if(l_s_b+tid+1664<r_s_b) smem[r_st+tid+1664] = keys[k+loc_b+l_s_b+tid+1664];\n        if(l_s_b+tid+1792<r_s_b) smem[r_st+tid+1792] = keys[k+loc_b+l_s_b+tid+1792];\n        if(l_s_b+tid+1920<r_s_b) smem[r_st+tid+1920] = keys[k+loc_b+l_s_b+tid+1920];\n        __syncthreads();\n\n        int gran = tid<<4;\n        int s_a, s_b;\n        bool p;\n        K rg_k0 ;\n        K rg_k1 ;\n        K rg_k2 ;\n        K rg_k3 ;\n        K rg_k4 ;\n        K rg_k5 ;\n        K rg_k6 ;\n        K rg_k7 ;\n        K rg_k8 ;\n        K rg_k9 ;\n        K rg_k10;\n        K rg_k11;\n        K rg_k12;\n        K rg_k13;\n        K rg_k14;\n        K rg_k15;\n        int rg_v0 ;\n        int rg_v1 ;\n        int rg_v2 ;\n        int rg_v3 ;\n        int rg_v4 ;\n        int rg_v5 ;\n        int rg_v6 ;\n        int rg_v7 ;\n        int rg_v8 ;\n        int rg_v9 ;\n        int rg_v10;\n        int rg_v11;\n        int rg_v12;\n        int rg_v13;\n        int rg_v14;\n        int rg_v15;\n        K tmp_k0,tmp_k1;\n        int tmp_v0,tmp_v1;\n\n        s_a = find_kth3(smem+l_st, l_cnt, smem+r_st, r_cnt, gran);\n        s_b = gran - s_a;\n        tmp_k0 = (s_a < l_cnt)? smem[l_st+s_a]:std::numeric_limits<K>::max();\n        tmp_k1 = (s_b < r_cnt)? smem[r_st+s_b]:std::numeric_limits<K>::max();\n        if(s_a < l_cnt) tmp_v0 = (loc_a+l_s_a+s_a);\n        if(s_b < r_cnt) tmp_v1 = (loc_b+l_s_b+s_b);\n        p = (s_b >= r_cnt) || ((s_a < l_cnt) && (tmp_k0 <= tmp_k1));\n        rg_k0 = p ? tmp_k0 : tmp_k1;\n        rg_v0 = p ? tmp_v0 : tmp_v1;\n        if(p) {\n            ++s_a;\n            tmp_k0 = (s_a < l_cnt)? smem[l_st+s_a]:std::numeric_limits<K>::max();\n            if(s_a < l_cnt) tmp_v0 = (loc_a+l_s_a+s_a);\n        } else {\n            ++s_b;\n            tmp_k1 = (s_b < r_cnt)? smem[r_st+s_b]:std::numeric_limits<K>::max();\n            if(s_b < r_cnt) tmp_v1 = (loc_b+l_s_b+s_b);\n        }\n        p = (s_b >= r_cnt) || ((s_a < l_cnt) && (tmp_k0 <= tmp_k1));\n        rg_k1 = p ? tmp_k0 : tmp_k1;\n        rg_v1 = p ? tmp_v0 : tmp_v1;\n        if(p) {\n            ++s_a;\n            tmp_k0 = (s_a < l_cnt)? smem[l_st+s_a]:std::numeric_limits<K>::max();\n            if(s_a < l_cnt) tmp_v0 = (loc_a+l_s_a+s_a);\n        } else {\n            ++s_b;\n            tmp_k1 = (s_b < r_cnt)? smem[r_st+s_b]:std::numeric_limits<K>::max();\n            if(s_b < r_cnt) tmp_v1 = (loc_b+l_s_b+s_b);\n        }\n        p = (s_b >= r_cnt) || ((s_a < l_cnt) && (tmp_k0 <= tmp_k1));\n        rg_k2 = p ? tmp_k0 : tmp_k1;\n        rg_v2 = p ? tmp_v0 : tmp_v1;\n        if(p) {\n            ++s_a;\n            tmp_k0 = (s_a < l_cnt)? smem[l_st+s_a]:std::numeric_limits<K>::max();\n            if(s_a < l_cnt) tmp_v0 = (loc_a+l_s_a+s_a);\n        } else {\n            ++s_b;\n            tmp_k1 = (s_b < r_cnt)? smem[r_st+s_b]:std::numeric_limits<K>::max();\n            if(s_b < r_cnt) tmp_v1 = (loc_b+l_s_b+s_b);\n        }\n        p = (s_b >= r_cnt) || ((s_a < l_cnt) && (tmp_k0 <= tmp_k1));\n        rg_k3 = p ? tmp_k0 : tmp_k1;\n        rg_v3 = p ? tmp_v0 : tmp_v1;\n        if(p) {\n            ++s_a;\n            tmp_k0 = (s_a < l_cnt)? smem[l_st+s_a]:std::numeric_limits<K>::max();\n            if(s_a < l_cnt) tmp_v0 = (loc_a+l_s_a+s_a);\n        } else {\n            ++s_b;\n            tmp_k1 = (s_b < r_cnt)? smem[r_st+s_b]:std::numeric_limits<K>::max();\n            if(s_b < r_cnt) tmp_v1 = (loc_b+l_s_b+s_b);\n        }\n        p = (s_b >= r_cnt) || ((s_a < l_cnt) && (tmp_k0 <= tmp_k1));\n        rg_k4 = p ? tmp_k0 : tmp_k1;\n        rg_v4 = p ? tmp_v0 : tmp_v1;\n        if(p) {\n            ++s_a;\n            tmp_k0 = (s_a < l_cnt)? smem[l_st+s_a]:std::numeric_limits<K>::max();\n            if(s_a < l_cnt) tmp_v0 = (loc_a+l_s_a+s_a);\n        } else {\n            ++s_b;\n            tmp_k1 = (s_b < r_cnt)? smem[r_st+s_b]:std::numeric_limits<K>::max();\n            if(s_b < r_cnt) tmp_v1 = (loc_b+l_s_b+s_b);\n        }\n        p = (s_b >= r_cnt) || ((s_a < l_cnt) && (tmp_k0 <= tmp_k1));\n        rg_k5 = p ? tmp_k0 : tmp_k1;\n        rg_v5 = p ? tmp_v0 : tmp_v1;\n        if(p) {\n            ++s_a;\n            tmp_k0 = (s_a < l_cnt)? smem[l_st+s_a]:std::numeric_limits<K>::max();\n            if(s_a < l_cnt) tmp_v0 = (loc_a+l_s_a+s_a);\n        } else {\n            ++s_b;\n            tmp_k1 = (s_b < r_cnt)? smem[r_st+s_b]:std::numeric_limits<K>::max();\n            if(s_b < r_cnt) tmp_v1 = (loc_b+l_s_b+s_b);\n        }\n        p = (s_b >= r_cnt) || ((s_a < l_cnt) && (tmp_k0 <= tmp_k1));\n        rg_k6 = p ? tmp_k0 : tmp_k1;\n        rg_v6 = p ? tmp_v0 : tmp_v1;\n        if(p) {\n            ++s_a;\n            tmp_k0 = (s_a < l_cnt)? smem[l_st+s_a]:std::numeric_limits<K>::max();\n            if(s_a < l_cnt) tmp_v0 = (loc_a+l_s_a+s_a);\n        } else {\n            ++s_b;\n            tmp_k1 = (s_b < r_cnt)? smem[r_st+s_b]:std::numeric_limits<K>::max();\n            if(s_b < r_cnt) tmp_v1 = (loc_b+l_s_b+s_b);\n        }\n        p = (s_b >= r_cnt) || ((s_a < l_cnt) && (tmp_k0 <= tmp_k1));\n        rg_k7 = p ? tmp_k0 : tmp_k1;\n        rg_v7 = p ? tmp_v0 : tmp_v1;\n        if(p) {\n            ++s_a;\n            tmp_k0 = (s_a < l_cnt)? smem[l_st+s_a]:std::numeric_limits<K>::max();\n            if(s_a < l_cnt) tmp_v0 = (loc_a+l_s_a+s_a);\n        } else {\n            ++s_b;\n            tmp_k1 = (s_b < r_cnt)? smem[r_st+s_b]:std::numeric_limits<K>::max();\n            if(s_b < r_cnt) tmp_v1 = (loc_b+l_s_b+s_b);\n        }\n        p = (s_b >= r_cnt) || ((s_a < l_cnt) && (tmp_k0 <= tmp_k1));\n        rg_k8 = p ? tmp_k0 : tmp_k1;\n        rg_v8 = p ? tmp_v0 : tmp_v1;\n        if(p) {\n            ++s_a;\n            tmp_k0 = (s_a < l_cnt)? smem[l_st+s_a]:std::numeric_limits<K>::max();\n            if(s_a < l_cnt) tmp_v0 = (loc_a+l_s_a+s_a);\n        } else {\n            ++s_b;\n            tmp_k1 = (s_b < r_cnt)? smem[r_st+s_b]:std::numeric_limits<K>::max();\n            if(s_b < r_cnt) tmp_v1 = (loc_b+l_s_b+s_b);\n        }\n        p = (s_b >= r_cnt) || ((s_a < l_cnt) && (tmp_k0 <= tmp_k1));\n        rg_k9 = p ? tmp_k0 : tmp_k1;\n        rg_v9 = p ? tmp_v0 : tmp_v1;\n        if(p) {\n            ++s_a;\n            tmp_k0 = (s_a < l_cnt)? smem[l_st+s_a]:std::numeric_limits<K>::max();\n            if(s_a < l_cnt) tmp_v0 = (loc_a+l_s_a+s_a);\n        } else {\n            ++s_b;\n            tmp_k1 = (s_b < r_cnt)? smem[r_st+s_b]:std::numeric_limits<K>::max();\n            if(s_b < r_cnt) tmp_v1 = (loc_b+l_s_b+s_b);\n        }\n        p = (s_b >= r_cnt) || ((s_a < l_cnt) && (tmp_k0 <= tmp_k1));\n        rg_k10 = p ? tmp_k0 : tmp_k1;\n        rg_v10 = p ? tmp_v0 : tmp_v1;\n        if(p) {\n            ++s_a;\n            tmp_k0 = (s_a < l_cnt)? smem[l_st+s_a]:std::numeric_limits<K>::max();\n            if(s_a < l_cnt) tmp_v0 = (loc_a+l_s_a+s_a);\n        } else {\n            ++s_b;\n            tmp_k1 = (s_b < r_cnt)? smem[r_st+s_b]:std::numeric_limits<K>::max();\n            if(s_b < r_cnt) tmp_v1 = (loc_b+l_s_b+s_b);\n        }\n        p = (s_b >= r_cnt) || ((s_a < l_cnt) && (tmp_k0 <= tmp_k1));\n        rg_k11 = p ? tmp_k0 : tmp_k1;\n        rg_v11 = p ? tmp_v0 : tmp_v1;\n        if(p) {\n            ++s_a;\n            tmp_k0 = (s_a < l_cnt)? smem[l_st+s_a]:std::numeric_limits<K>::max();\n            if(s_a < l_cnt) tmp_v0 = (loc_a+l_s_a+s_a);\n        } else {\n            ++s_b;\n            tmp_k1 = (s_b < r_cnt)? smem[r_st+s_b]:std::numeric_limits<K>::max();\n            if(s_b < r_cnt) tmp_v1 = (loc_b+l_s_b+s_b);\n        }\n        p = (s_b >= r_cnt) || ((s_a < l_cnt) && (tmp_k0 <= tmp_k1));\n        rg_k12 = p ? tmp_k0 : tmp_k1;\n        rg_v12 = p ? tmp_v0 : tmp_v1;\n        if(p) {\n            ++s_a;\n            tmp_k0 = (s_a < l_cnt)? smem[l_st+s_a]:std::numeric_limits<K>::max();\n            if(s_a < l_cnt) tmp_v0 = (loc_a+l_s_a+s_a);\n        } else {\n            ++s_b;\n            tmp_k1 = (s_b < r_cnt)? smem[r_st+s_b]:std::numeric_limits<K>::max();\n            if(s_b < r_cnt) tmp_v1 = (loc_b+l_s_b+s_b);\n        }\n        p = (s_b >= r_cnt) || ((s_a < l_cnt) && (tmp_k0 <= tmp_k1));\n        rg_k13 = p ? tmp_k0 : tmp_k1;\n        rg_v13 = p ? tmp_v0 : tmp_v1;\n        if(p) {\n            ++s_a;\n            tmp_k0 = (s_a < l_cnt)? smem[l_st+s_a]:std::numeric_limits<K>::max();\n            if(s_a < l_cnt) tmp_v0 = (loc_a+l_s_a+s_a);\n        } else {\n            ++s_b;\n            tmp_k1 = (s_b < r_cnt)? smem[r_st+s_b]:std::numeric_limits<K>::max();\n            if(s_b < r_cnt) tmp_v1 = (loc_b+l_s_b+s_b);\n        }\n        p = (s_b >= r_cnt) || ((s_a < l_cnt) && (tmp_k0 <= tmp_k1));\n        rg_k14 = p ? tmp_k0 : tmp_k1;\n        rg_v14 = p ? tmp_v0 : tmp_v1;\n        if(p) {\n            ++s_a;\n            tmp_k0 = (s_a < l_cnt)? smem[l_st+s_a]:std::numeric_limits<K>::max();\n            if(s_a < l_cnt) tmp_v0 = (loc_a+l_s_a+s_a);\n        } else {\n            ++s_b;\n            tmp_k1 = (s_b < r_cnt)? smem[r_st+s_b]:std::numeric_limits<K>::max();\n            if(s_b < r_cnt) tmp_v1 = (loc_b+l_s_b+s_b);\n        }\n        p = (s_b >= r_cnt) || ((s_a < l_cnt) && (tmp_k0 <= tmp_k1));\n        rg_k15 = p ? tmp_k0 : tmp_k1;\n        rg_v15 = p ? tmp_v0 : tmp_v1;\n\n        int warp_id = threadIdx.x / 32;\n        int lane_id = threadIdx.x % 32;\n        rg_k1  = __shfl_xor_sync(0xffffffff,rg_k1 , 0x1 );\n        rg_k3  = __shfl_xor_sync(0xffffffff,rg_k3 , 0x1 );\n        rg_k5  = __shfl_xor_sync(0xffffffff,rg_k5 , 0x1 );\n        rg_k7  = __shfl_xor_sync(0xffffffff,rg_k7 , 0x1 );\n        rg_k9  = __shfl_xor_sync(0xffffffff,rg_k9 , 0x1 );\n        rg_k11 = __shfl_xor_sync(0xffffffff,rg_k11, 0x1 );\n        rg_k13 = __shfl_xor_sync(0xffffffff,rg_k13, 0x1 );\n        rg_k15 = __shfl_xor_sync(0xffffffff,rg_k15, 0x1 );\n        rg_v1  = __shfl_xor_sync(0xffffffff,rg_v1 , 0x1 );\n        rg_v3  = __shfl_xor_sync(0xffffffff,rg_v3 , 0x1 );\n        rg_v5  = __shfl_xor_sync(0xffffffff,rg_v5 , 0x1 );\n        rg_v7  = __shfl_xor_sync(0xffffffff,rg_v7 , 0x1 );\n        rg_v9  = __shfl_xor_sync(0xffffffff,rg_v9 , 0x1 );\n        rg_v11 = __shfl_xor_sync(0xffffffff,rg_v11, 0x1 );\n        rg_v13 = __shfl_xor_sync(0xffffffff,rg_v13, 0x1 );\n        rg_v15 = __shfl_xor_sync(0xffffffff,rg_v15, 0x1 );\n        if(lane_id&0x1) SWP(K, rg_k0 ,rg_k1 , int, rg_v0 ,rg_v1 );\n        if(lane_id&0x1) SWP(K, rg_k2 ,rg_k3 , int, rg_v2 ,rg_v3 );\n        if(lane_id&0x1) SWP(K, rg_k4 ,rg_k5 , int, rg_v4 ,rg_v5 );\n        if(lane_id&0x1) SWP(K, rg_k6 ,rg_k7 , int, rg_v6 ,rg_v7 );\n        if(lane_id&0x1) SWP(K, rg_k8 ,rg_k9 , int, rg_v8 ,rg_v9 );\n        if(lane_id&0x1) SWP(K, rg_k10,rg_k11, int, rg_v10,rg_v11);\n        if(lane_id&0x1) SWP(K, rg_k12,rg_k13, int, rg_v12,rg_v13);\n        if(lane_id&0x1) SWP(K, rg_k14,rg_k15, int, rg_v14,rg_v15);\n        rg_k1  = __shfl_xor_sync(0xffffffff,rg_k1 , 0x1 );\n        rg_k3  = __shfl_xor_sync(0xffffffff,rg_k3 , 0x1 );\n        rg_k5  = __shfl_xor_sync(0xffffffff,rg_k5 , 0x1 );\n        rg_k7  = __shfl_xor_sync(0xffffffff,rg_k7 , 0x1 );\n        rg_k9  = __shfl_xor_sync(0xffffffff,rg_k9 , 0x1 );\n        rg_k11 = __shfl_xor_sync(0xffffffff,rg_k11, 0x1 );\n        rg_k13 = __shfl_xor_sync(0xffffffff,rg_k13, 0x1 );\n        rg_k15 = __shfl_xor_sync(0xffffffff,rg_k15, 0x1 );\n        rg_v1  = __shfl_xor_sync(0xffffffff,rg_v1 , 0x1 );\n        rg_v3  = __shfl_xor_sync(0xffffffff,rg_v3 , 0x1 );\n        rg_v5  = __shfl_xor_sync(0xffffffff,rg_v5 , 0x1 );\n        rg_v7  = __shfl_xor_sync(0xffffffff,rg_v7 , 0x1 );\n        rg_v9  = __shfl_xor_sync(0xffffffff,rg_v9 , 0x1 );\n        rg_v11 = __shfl_xor_sync(0xffffffff,rg_v11, 0x1 );\n        rg_v13 = __shfl_xor_sync(0xffffffff,rg_v13, 0x1 );\n        rg_v15 = __shfl_xor_sync(0xffffffff,rg_v15, 0x1 );\n        rg_k2  = __shfl_xor_sync(0xffffffff,rg_k2 , 0x2 );\n        rg_k3  = __shfl_xor_sync(0xffffffff,rg_k3 , 0x2 );\n        rg_k6  = __shfl_xor_sync(0xffffffff,rg_k6 , 0x2 );\n        rg_k7  = __shfl_xor_sync(0xffffffff,rg_k7 , 0x2 );\n        rg_k10 = __shfl_xor_sync(0xffffffff,rg_k10, 0x2 );\n        rg_k11 = __shfl_xor_sync(0xffffffff,rg_k11, 0x2 );\n        rg_k14 = __shfl_xor_sync(0xffffffff,rg_k14, 0x2 );\n        rg_k15 = __shfl_xor_sync(0xffffffff,rg_k15, 0x2 );\n        rg_v2  = __shfl_xor_sync(0xffffffff,rg_v2 , 0x2 );\n        rg_v3  = __shfl_xor_sync(0xffffffff,rg_v3 , 0x2 );\n        rg_v6  = __shfl_xor_sync(0xffffffff,rg_v6 , 0x2 );\n        rg_v7  = __shfl_xor_sync(0xffffffff,rg_v7 , 0x2 );\n        rg_v10 = __shfl_xor_sync(0xffffffff,rg_v10, 0x2 );\n        rg_v11 = __shfl_xor_sync(0xffffffff,rg_v11, 0x2 );\n        rg_v14 = __shfl_xor_sync(0xffffffff,rg_v14, 0x2 );\n        rg_v15 = __shfl_xor_sync(0xffffffff,rg_v15, 0x2 );\n        if(lane_id&0x2 ) SWP(K, rg_k0 , rg_k2 , int, rg_v0 , rg_v2 );\n        if(lane_id&0x2 ) SWP(K, rg_k1 , rg_k3 , int, rg_v1 , rg_v3 );\n        if(lane_id&0x2 ) SWP(K, rg_k4 , rg_k6 , int, rg_v4 , rg_v6 );\n        if(lane_id&0x2 ) SWP(K, rg_k5 , rg_k7 , int, rg_v5 , rg_v7 );\n        if(lane_id&0x2 ) SWP(K, rg_k8 , rg_k10, int, rg_v8 , rg_v10);\n        if(lane_id&0x2 ) SWP(K, rg_k9 , rg_k11, int, rg_v9 , rg_v11);\n        if(lane_id&0x2 ) SWP(K, rg_k12, rg_k14, int, rg_v12, rg_v14);\n        if(lane_id&0x2 ) SWP(K, rg_k13, rg_k15, int, rg_v13, rg_v15);\n        rg_k2  = __shfl_xor_sync(0xffffffff,rg_k2 , 0x2 );\n        rg_k3  = __shfl_xor_sync(0xffffffff,rg_k3 , 0x2 );\n        rg_k6  = __shfl_xor_sync(0xffffffff,rg_k6 , 0x2 );\n        rg_k7  = __shfl_xor_sync(0xffffffff,rg_k7 , 0x2 );\n        rg_k10 = __shfl_xor_sync(0xffffffff,rg_k10, 0x2 );\n        rg_k11 = __shfl_xor_sync(0xffffffff,rg_k11, 0x2 );\n        rg_k14 = __shfl_xor_sync(0xffffffff,rg_k14, 0x2 );\n        rg_k15 = __shfl_xor_sync(0xffffffff,rg_k15, 0x2 );\n        rg_v2  = __shfl_xor_sync(0xffffffff,rg_v2 , 0x2 );\n        rg_v3  = __shfl_xor_sync(0xffffffff,rg_v3 , 0x2 );\n        rg_v6  = __shfl_xor_sync(0xffffffff,rg_v6 , 0x2 );\n        rg_v7  = __shfl_xor_sync(0xffffffff,rg_v7 , 0x2 );\n        rg_v10 = __shfl_xor_sync(0xffffffff,rg_v10, 0x2 );\n        rg_v11 = __shfl_xor_sync(0xffffffff,rg_v11, 0x2 );\n        rg_v14 = __shfl_xor_sync(0xffffffff,rg_v14, 0x2 );\n        rg_v15 = __shfl_xor_sync(0xffffffff,rg_v15, 0x2 );\n        rg_k4  = __shfl_xor_sync(0xffffffff,rg_k4 , 0x4 );\n        rg_k5  = __shfl_xor_sync(0xffffffff,rg_k5 , 0x4 );\n        rg_k6  = __shfl_xor_sync(0xffffffff,rg_k6 , 0x4 );\n        rg_k7  = __shfl_xor_sync(0xffffffff,rg_k7 , 0x4 );\n        rg_k12 = __shfl_xor_sync(0xffffffff,rg_k12, 0x4 );\n        rg_k13 = __shfl_xor_sync(0xffffffff,rg_k13, 0x4 );\n        rg_k14 = __shfl_xor_sync(0xffffffff,rg_k14, 0x4 );\n        rg_k15 = __shfl_xor_sync(0xffffffff,rg_k15, 0x4 );\n        rg_v4  = __shfl_xor_sync(0xffffffff,rg_v4 , 0x4 );\n        rg_v5  = __shfl_xor_sync(0xffffffff,rg_v5 , 0x4 );\n        rg_v6  = __shfl_xor_sync(0xffffffff,rg_v6 , 0x4 );\n        rg_v7  = __shfl_xor_sync(0xffffffff,rg_v7 , 0x4 );\n        rg_v12 = __shfl_xor_sync(0xffffffff,rg_v12, 0x4 );\n        rg_v13 = __shfl_xor_sync(0xffffffff,rg_v13, 0x4 );\n        rg_v14 = __shfl_xor_sync(0xffffffff,rg_v14, 0x4 );\n        rg_v15 = __shfl_xor_sync(0xffffffff,rg_v15, 0x4 );\n        if(lane_id&0x4 ) SWP(K, rg_k0 , rg_k4 , int, rg_v0 , rg_v4 );\n        if(lane_id&0x4 ) SWP(K, rg_k1 , rg_k5 , int, rg_v1 , rg_v5 );\n        if(lane_id&0x4 ) SWP(K, rg_k2 , rg_k6 , int, rg_v2 , rg_v6 );\n        if(lane_id&0x4 ) SWP(K, rg_k3 , rg_k7 , int, rg_v3 , rg_v7 );\n        if(lane_id&0x4 ) SWP(K, rg_k8 , rg_k12, int, rg_v8 , rg_v12);\n        if(lane_id&0x4 ) SWP(K, rg_k9 , rg_k13, int, rg_v9 , rg_v13);\n        if(lane_id&0x4 ) SWP(K, rg_k10, rg_k14, int, rg_v10, rg_v14);\n        if(lane_id&0x4 ) SWP(K, rg_k11, rg_k15, int, rg_v11, rg_v15);\n        rg_k4  = __shfl_xor_sync(0xffffffff,rg_k4 , 0x4 );\n        rg_k5  = __shfl_xor_sync(0xffffffff,rg_k5 , 0x4 );\n        rg_k6  = __shfl_xor_sync(0xffffffff,rg_k6 , 0x4 );\n        rg_k7  = __shfl_xor_sync(0xffffffff,rg_k7 , 0x4 );\n        rg_k12 = __shfl_xor_sync(0xffffffff,rg_k12, 0x4 );\n        rg_k13 = __shfl_xor_sync(0xffffffff,rg_k13, 0x4 );\n        rg_k14 = __shfl_xor_sync(0xffffffff,rg_k14, 0x4 );\n        rg_k15 = __shfl_xor_sync(0xffffffff,rg_k15, 0x4 );\n        rg_v4  = __shfl_xor_sync(0xffffffff,rg_v4 , 0x4 );\n        rg_v5  = __shfl_xor_sync(0xffffffff,rg_v5 , 0x4 );\n        rg_v6  = __shfl_xor_sync(0xffffffff,rg_v6 , 0x4 );\n        rg_v7  = __shfl_xor_sync(0xffffffff,rg_v7 , 0x4 );\n        rg_v12 = __shfl_xor_sync(0xffffffff,rg_v12, 0x4 );\n        rg_v13 = __shfl_xor_sync(0xffffffff,rg_v13, 0x4 );\n        rg_v14 = __shfl_xor_sync(0xffffffff,rg_v14, 0x4 );\n        rg_v15 = __shfl_xor_sync(0xffffffff,rg_v15, 0x4 );\n        rg_k8  = __shfl_xor_sync(0xffffffff,rg_k8 , 0x8 );\n        rg_k9  = __shfl_xor_sync(0xffffffff,rg_k9 , 0x8 );\n        rg_k10 = __shfl_xor_sync(0xffffffff,rg_k10, 0x8 );\n        rg_k11 = __shfl_xor_sync(0xffffffff,rg_k11, 0x8 );\n        rg_k12 = __shfl_xor_sync(0xffffffff,rg_k12, 0x8 );\n        rg_k13 = __shfl_xor_sync(0xffffffff,rg_k13, 0x8 );\n        rg_k14 = __shfl_xor_sync(0xffffffff,rg_k14, 0x8 );\n        rg_k15 = __shfl_xor_sync(0xffffffff,rg_k15, 0x8 );\n        rg_v8  = __shfl_xor_sync(0xffffffff,rg_v8 , 0x8 );\n        rg_v9  = __shfl_xor_sync(0xffffffff,rg_v9 , 0x8 );\n        rg_v10 = __shfl_xor_sync(0xffffffff,rg_v10, 0x8 );\n        rg_v11 = __shfl_xor_sync(0xffffffff,rg_v11, 0x8 );\n        rg_v12 = __shfl_xor_sync(0xffffffff,rg_v12, 0x8 );\n        rg_v13 = __shfl_xor_sync(0xffffffff,rg_v13, 0x8 );\n        rg_v14 = __shfl_xor_sync(0xffffffff,rg_v14, 0x8 );\n        rg_v15 = __shfl_xor_sync(0xffffffff,rg_v15, 0x8 );\n        if(lane_id&0x8 ) SWP(K, rg_k0 , rg_k8 , int, rg_v0 , rg_v8 );\n        if(lane_id&0x8 ) SWP(K, rg_k1 , rg_k9 , int, rg_v1 , rg_v9 );\n        if(lane_id&0x8 ) SWP(K, rg_k2 , rg_k10, int, rg_v2 , rg_v10);\n        if(lane_id&0x8 ) SWP(K, rg_k3 , rg_k11, int, rg_v3 , rg_v11);\n        if(lane_id&0x8 ) SWP(K, rg_k4 , rg_k12, int, rg_v4 , rg_v12);\n        if(lane_id&0x8 ) SWP(K, rg_k5 , rg_k13, int, rg_v5 , rg_v13);\n        if(lane_id&0x8 ) SWP(K, rg_k6 , rg_k14, int, rg_v6 , rg_v14);\n        if(lane_id&0x8 ) SWP(K, rg_k7 , rg_k15, int, rg_v7 , rg_v15);\n        rg_k8  = __shfl_xor_sync(0xffffffff,rg_k8 , 0x8 );\n        rg_k9  = __shfl_xor_sync(0xffffffff,rg_k9 , 0x8 );\n        rg_k10 = __shfl_xor_sync(0xffffffff,rg_k10, 0x8 );\n        rg_k11 = __shfl_xor_sync(0xffffffff,rg_k11, 0x8 );\n        rg_k12 = __shfl_xor_sync(0xffffffff,rg_k12, 0x8 );\n        rg_k13 = __shfl_xor_sync(0xffffffff,rg_k13, 0x8 );\n        rg_k14 = __shfl_xor_sync(0xffffffff,rg_k14, 0x8 );\n        rg_k15 = __shfl_xor_sync(0xffffffff,rg_k15, 0x8 );\n        rg_v8  = __shfl_xor_sync(0xffffffff,rg_v8 , 0x8 );\n        rg_v9  = __shfl_xor_sync(0xffffffff,rg_v9 , 0x8 );\n        rg_v10 = __shfl_xor_sync(0xffffffff,rg_v10, 0x8 );\n        rg_v11 = __shfl_xor_sync(0xffffffff,rg_v11, 0x8 );\n        rg_v12 = __shfl_xor_sync(0xffffffff,rg_v12, 0x8 );\n        rg_v13 = __shfl_xor_sync(0xffffffff,rg_v13, 0x8 );\n        rg_v14 = __shfl_xor_sync(0xffffffff,rg_v14, 0x8 );\n        rg_v15 = __shfl_xor_sync(0xffffffff,rg_v15, 0x8 );\n        rg_k1  = __shfl_xor_sync(0xffffffff,rg_k1 , 0x10);\n        rg_k3  = __shfl_xor_sync(0xffffffff,rg_k3 , 0x10);\n        rg_k5  = __shfl_xor_sync(0xffffffff,rg_k5 , 0x10);\n        rg_k7  = __shfl_xor_sync(0xffffffff,rg_k7 , 0x10);\n        rg_k9  = __shfl_xor_sync(0xffffffff,rg_k9 , 0x10);\n        rg_k11 = __shfl_xor_sync(0xffffffff,rg_k11, 0x10);\n        rg_k13 = __shfl_xor_sync(0xffffffff,rg_k13, 0x10);\n        rg_k15 = __shfl_xor_sync(0xffffffff,rg_k15, 0x10);\n        rg_v1  = __shfl_xor_sync(0xffffffff,rg_v1 , 0x10);\n        rg_v3  = __shfl_xor_sync(0xffffffff,rg_v3 , 0x10);\n        rg_v5  = __shfl_xor_sync(0xffffffff,rg_v5 , 0x10);\n        rg_v7  = __shfl_xor_sync(0xffffffff,rg_v7 , 0x10);\n        rg_v9  = __shfl_xor_sync(0xffffffff,rg_v9 , 0x10);\n        rg_v11 = __shfl_xor_sync(0xffffffff,rg_v11, 0x10);\n        rg_v13 = __shfl_xor_sync(0xffffffff,rg_v13, 0x10);\n        rg_v15 = __shfl_xor_sync(0xffffffff,rg_v15, 0x10);\n        if(lane_id&0x10) SWP(K, rg_k0 , rg_k1 , int, rg_v0 , rg_v1 );\n        if(lane_id&0x10) SWP(K, rg_k2 , rg_k3 , int, rg_v2 , rg_v3 );\n        if(lane_id&0x10) SWP(K, rg_k4 , rg_k5 , int, rg_v4 , rg_v5 );\n        if(lane_id&0x10) SWP(K, rg_k6 , rg_k7 , int, rg_v6 , rg_v7 );\n        if(lane_id&0x10) SWP(K, rg_k8 , rg_k9 , int, rg_v8 , rg_v9 );\n        if(lane_id&0x10) SWP(K, rg_k10, rg_k11, int, rg_v10, rg_v11);\n        if(lane_id&0x10) SWP(K, rg_k12, rg_k13, int, rg_v12, rg_v13);\n        if(lane_id&0x10) SWP(K, rg_k14, rg_k15, int, rg_v14, rg_v15);\n        rg_k1  = __shfl_xor_sync(0xffffffff,rg_k1 , 0x10);\n        rg_k3  = __shfl_xor_sync(0xffffffff,rg_k3 , 0x10);\n        rg_k5  = __shfl_xor_sync(0xffffffff,rg_k5 , 0x10);\n        rg_k7  = __shfl_xor_sync(0xffffffff,rg_k7 , 0x10);\n        rg_k9  = __shfl_xor_sync(0xffffffff,rg_k9 , 0x10);\n        rg_k11 = __shfl_xor_sync(0xffffffff,rg_k11, 0x10);\n        rg_k13 = __shfl_xor_sync(0xffffffff,rg_k13, 0x10);\n        rg_k15 = __shfl_xor_sync(0xffffffff,rg_k15, 0x10);\n        rg_v1  = __shfl_xor_sync(0xffffffff,rg_v1 , 0x10);\n        rg_v3  = __shfl_xor_sync(0xffffffff,rg_v3 , 0x10);\n        rg_v5  = __shfl_xor_sync(0xffffffff,rg_v5 , 0x10);\n        rg_v7  = __shfl_xor_sync(0xffffffff,rg_v7 , 0x10);\n        rg_v9  = __shfl_xor_sync(0xffffffff,rg_v9 , 0x10);\n        rg_v11 = __shfl_xor_sync(0xffffffff,rg_v11, 0x10);\n        rg_v13 = __shfl_xor_sync(0xffffffff,rg_v13, 0x10);\n        rg_v15 = __shfl_xor_sync(0xffffffff,rg_v15, 0x10);\n\n        if((innerbid<<11)+(warp_id<<9)+0  +lane_id<seg_size) keysB[k+(innerbid<<11)+(warp_id<<9)+0  +lane_id] = rg_k0 ;\n        if((innerbid<<11)+(warp_id<<9)+32 +lane_id<seg_size) keysB[k+(innerbid<<11)+(warp_id<<9)+32 +lane_id] = rg_k2 ;\n        if((innerbid<<11)+(warp_id<<9)+64 +lane_id<seg_size) keysB[k+(innerbid<<11)+(warp_id<<9)+64 +lane_id] = rg_k4 ;\n        if((innerbid<<11)+(warp_id<<9)+96 +lane_id<seg_size) keysB[k+(innerbid<<11)+(warp_id<<9)+96 +lane_id] = rg_k6 ;\n        if((innerbid<<11)+(warp_id<<9)+128+lane_id<seg_size) keysB[k+(innerbid<<11)+(warp_id<<9)+128+lane_id] = rg_k8 ;\n        if((innerbid<<11)+(warp_id<<9)+160+lane_id<seg_size) keysB[k+(innerbid<<11)+(warp_id<<9)+160+lane_id] = rg_k10;\n        if((innerbid<<11)+(warp_id<<9)+192+lane_id<seg_size) keysB[k+(innerbid<<11)+(warp_id<<9)+192+lane_id] = rg_k12;\n        if((innerbid<<11)+(warp_id<<9)+224+lane_id<seg_size) keysB[k+(innerbid<<11)+(warp_id<<9)+224+lane_id] = rg_k14;\n        if((innerbid<<11)+(warp_id<<9)+256+lane_id<seg_size) keysB[k+(innerbid<<11)+(warp_id<<9)+256+lane_id] = rg_k1 ;\n        if((innerbid<<11)+(warp_id<<9)+288+lane_id<seg_size) keysB[k+(innerbid<<11)+(warp_id<<9)+288+lane_id] = rg_k3 ;\n        if((innerbid<<11)+(warp_id<<9)+320+lane_id<seg_size) keysB[k+(innerbid<<11)+(warp_id<<9)+320+lane_id] = rg_k5 ;\n        if((innerbid<<11)+(warp_id<<9)+352+lane_id<seg_size) keysB[k+(innerbid<<11)+(warp_id<<9)+352+lane_id] = rg_k7 ;\n        if((innerbid<<11)+(warp_id<<9)+384+lane_id<seg_size) keysB[k+(innerbid<<11)+(warp_id<<9)+384+lane_id] = rg_k9 ;\n        if((innerbid<<11)+(warp_id<<9)+416+lane_id<seg_size) keysB[k+(innerbid<<11)+(warp_id<<9)+416+lane_id] = rg_k11;\n        if((innerbid<<11)+(warp_id<<9)+448+lane_id<seg_size) keysB[k+(innerbid<<11)+(warp_id<<9)+448+lane_id] = rg_k13;\n        if((innerbid<<11)+(warp_id<<9)+480+lane_id<seg_size) keysB[k+(innerbid<<11)+(warp_id<<9)+480+lane_id] = rg_k15;\n\n        if((innerbid<<11)+(warp_id<<9)+0  +lane_id<seg_size)valsB[k+(innerbid<<11)+(warp_id<<9)+0  +lane_id]=vals[k+rg_v0 ];\n        if((innerbid<<11)+(warp_id<<9)+32 +lane_id<seg_size)valsB[k+(innerbid<<11)+(warp_id<<9)+32 +lane_id]=vals[k+rg_v2 ];\n        if((innerbid<<11)+(warp_id<<9)+64 +lane_id<seg_size)valsB[k+(innerbid<<11)+(warp_id<<9)+64 +lane_id]=vals[k+rg_v4 ];\n        if((innerbid<<11)+(warp_id<<9)+96 +lane_id<seg_size)valsB[k+(innerbid<<11)+(warp_id<<9)+96 +lane_id]=vals[k+rg_v6 ];\n        if((innerbid<<11)+(warp_id<<9)+128+lane_id<seg_size)valsB[k+(innerbid<<11)+(warp_id<<9)+128+lane_id]=vals[k+rg_v8 ];\n        if((innerbid<<11)+(warp_id<<9)+160+lane_id<seg_size)valsB[k+(innerbid<<11)+(warp_id<<9)+160+lane_id]=vals[k+rg_v10];\n        if((innerbid<<11)+(warp_id<<9)+192+lane_id<seg_size)valsB[k+(innerbid<<11)+(warp_id<<9)+192+lane_id]=vals[k+rg_v12];\n        if((innerbid<<11)+(warp_id<<9)+224+lane_id<seg_size)valsB[k+(innerbid<<11)+(warp_id<<9)+224+lane_id]=vals[k+rg_v14];\n        if((innerbid<<11)+(warp_id<<9)+256+lane_id<seg_size)valsB[k+(innerbid<<11)+(warp_id<<9)+256+lane_id]=vals[k+rg_v1 ];\n        if((innerbid<<11)+(warp_id<<9)+288+lane_id<seg_size)valsB[k+(innerbid<<11)+(warp_id<<9)+288+lane_id]=vals[k+rg_v3 ];\n        if((innerbid<<11)+(warp_id<<9)+320+lane_id<seg_size)valsB[k+(innerbid<<11)+(warp_id<<9)+320+lane_id]=vals[k+rg_v5 ];\n        if((innerbid<<11)+(warp_id<<9)+352+lane_id<seg_size)valsB[k+(innerbid<<11)+(warp_id<<9)+352+lane_id]=vals[k+rg_v7 ];\n        if((innerbid<<11)+(warp_id<<9)+384+lane_id<seg_size)valsB[k+(innerbid<<11)+(warp_id<<9)+384+lane_id]=vals[k+rg_v9 ];\n        if((innerbid<<11)+(warp_id<<9)+416+lane_id<seg_size)valsB[k+(innerbid<<11)+(warp_id<<9)+416+lane_id]=vals[k+rg_v11];\n        if((innerbid<<11)+(warp_id<<9)+448+lane_id<seg_size)valsB[k+(innerbid<<11)+(warp_id<<9)+448+lane_id]=vals[k+rg_v13];\n        if((innerbid<<11)+(warp_id<<9)+480+lane_id<seg_size)valsB[k+(innerbid<<11)+(warp_id<<9)+480+lane_id]=vals[k+rg_v15];\n    }\n}",
            "#define K 1.38065e-23        // J/K, Boltzmann constant\n\n\n#define T ((int)32)\n\n\n__device__ inline\nint log2(int u)\n{\n    int s, t;\n    t = (u > 0xffff) << 4; u >>= t;\n    s = (u > 0xff  ) << 3; u >>= s, t |= s;\n    s = (u > 0xf   ) << 2; u >>= s, t |= s;\n    s = (u > 0x3   ) << 1; u >>= s, t |= s;\n    return (t | (u >> 1));\n}\n\n__device__ inline\nint upper_power_of_two(int v)\n{\n    v--;\n    v |= v >> 1;\n    v |= v >> 2;\n    v |= v >> 4;\n    v |= v >> 8;\n    v |= v >> 16;\n    v++;\n    return v;\n\n}\n\n__global__\nvoid kern_copy(\n    const K *srck, const T *srcv, K *dstk, T *dstv,\n    const Offset *seg_begins, const Offset *seg_ends, const int *bin,\n    const int workloads_per_block)\n{\n    const int bin_it = blockIdx.x;\n    const int innerbid = blockIdx.y;\n\n    const int seg_size = seg_ends[bin[bin_it]]-seg_begins[bin[bin_it]];\n    const int blk_stat = (seg_size+workloads_per_block-1)/workloads_per_block;\n\n    if(innerbid < blk_stat)\n    {\n        const int tid = threadIdx.x;\n        int k = seg_begins[bin[bin_it]];\n        int stride = upper_power_of_two(seg_size);\n        int steps = log2(stride/2048);\n\n        if((steps&1))\n        {\n            if((innerbid<<11)+tid     <seg_size) dstk[k+(innerbid<<11)+tid     ] = srck[k+(innerbid<<11)+tid     ];\n            if((innerbid<<11)+tid+128 <seg_size) dstk[k+(innerbid<<11)+tid+128 ] = srck[k+(innerbid<<11)+tid+128 ];\n            if((innerbid<<11)+tid+256 <seg_size) dstk[k+(innerbid<<11)+tid+256 ] = srck[k+(innerbid<<11)+tid+256 ];\n            if((innerbid<<11)+tid+384 <seg_size) dstk[k+(innerbid<<11)+tid+384 ] = srck[k+(innerbid<<11)+tid+384 ];\n            if((innerbid<<11)+tid+512 <seg_size) dstk[k+(innerbid<<11)+tid+512 ] = srck[k+(innerbid<<11)+tid+512 ];\n            if((innerbid<<11)+tid+640 <seg_size) dstk[k+(innerbid<<11)+tid+640 ] = srck[k+(innerbid<<11)+tid+640 ];\n            if((innerbid<<11)+tid+768 <seg_size) dstk[k+(innerbid<<11)+tid+768 ] = srck[k+(innerbid<<11)+tid+768 ];\n            if((innerbid<<11)+tid+896 <seg_size) dstk[k+(innerbid<<11)+tid+896 ] = srck[k+(innerbid<<11)+tid+896 ];\n            if((innerbid<<11)+tid+1024<seg_size) dstk[k+(innerbid<<11)+tid+1024] = srck[k+(innerbid<<11)+tid+1024];\n            if((innerbid<<11)+tid+1152<seg_size) dstk[k+(innerbid<<11)+tid+1152] = srck[k+(innerbid<<11)+tid+1152];\n            if((innerbid<<11)+tid+1280<seg_size) dstk[k+(innerbid<<11)+tid+1280] = srck[k+(innerbid<<11)+tid+1280];\n            if((innerbid<<11)+tid+1408<seg_size) dstk[k+(innerbid<<11)+tid+1408] = srck[k+(innerbid<<11)+tid+1408];\n            if((innerbid<<11)+tid+1536<seg_size) dstk[k+(innerbid<<11)+tid+1536] = srck[k+(innerbid<<11)+tid+1536];\n            if((innerbid<<11)+tid+1664<seg_size) dstk[k+(innerbid<<11)+tid+1664] = srck[k+(innerbid<<11)+tid+1664];\n            if((innerbid<<11)+tid+1792<seg_size) dstk[k+(innerbid<<11)+tid+1792] = srck[k+(innerbid<<11)+tid+1792];\n            if((innerbid<<11)+tid+1920<seg_size) dstk[k+(innerbid<<11)+tid+1920] = srck[k+(innerbid<<11)+tid+1920];\n\n            if((innerbid<<11)+tid     <seg_size) dstv[k+(innerbid<<11)+tid     ] = srcv[k+(innerbid<<11)+tid     ];\n            if((innerbid<<11)+tid+128 <seg_size) dstv[k+(innerbid<<11)+tid+128 ] = srcv[k+(innerbid<<11)+tid+128 ];\n            if((innerbid<<11)+tid+256 <seg_size) dstv[k+(innerbid<<11)+tid+256 ] = srcv[k+(innerbid<<11)+tid+256 ];\n            if((innerbid<<11)+tid+384 <seg_size) dstv[k+(innerbid<<11)+tid+384 ] = srcv[k+(innerbid<<11)+tid+384 ];\n            if((innerbid<<11)+tid+512 <seg_size) dstv[k+(innerbid<<11)+tid+512 ] = srcv[k+(innerbid<<11)+tid+512 ];\n            if((innerbid<<11)+tid+640 <seg_size) dstv[k+(innerbid<<11)+tid+640 ] = srcv[k+(innerbid<<11)+tid+640 ];\n            if((innerbid<<11)+tid+768 <seg_size) dstv[k+(innerbid<<11)+tid+768 ] = srcv[k+(innerbid<<11)+tid+768 ];\n            if((innerbid<<11)+tid+896 <seg_size) dstv[k+(innerbid<<11)+tid+896 ] = srcv[k+(innerbid<<11)+tid+896 ];\n            if((innerbid<<11)+tid+1024<seg_size) dstv[k+(innerbid<<11)+tid+1024] = srcv[k+(innerbid<<11)+tid+1024];\n            if((innerbid<<11)+tid+1152<seg_size) dstv[k+(innerbid<<11)+tid+1152] = srcv[k+(innerbid<<11)+tid+1152];\n            if((innerbid<<11)+tid+1280<seg_size) dstv[k+(innerbid<<11)+tid+1280] = srcv[k+(innerbid<<11)+tid+1280];\n            if((innerbid<<11)+tid+1408<seg_size) dstv[k+(innerbid<<11)+tid+1408] = srcv[k+(innerbid<<11)+tid+1408];\n            if((innerbid<<11)+tid+1536<seg_size) dstv[k+(innerbid<<11)+tid+1536] = srcv[k+(innerbid<<11)+tid+1536];\n            if((innerbid<<11)+tid+1664<seg_size) dstv[k+(innerbid<<11)+tid+1664] = srcv[k+(innerbid<<11)+tid+1664];\n            if((innerbid<<11)+tid+1792<seg_size) dstv[k+(innerbid<<11)+tid+1792] = srcv[k+(innerbid<<11)+tid+1792];\n            if((innerbid<<11)+tid+1920<seg_size) dstv[k+(innerbid<<11)+tid+1920] = srcv[k+(innerbid<<11)+tid+1920];\n        }\n    }\n}"
        ]
    },
    "ne-cuda": {
        "/Users/gbolet/hecbench-roofline/src/ne-cuda/main.cu": [
            "inline __host__ __device__ float3 make_float3(uint3 a)\n{\n    return make_float3(float(a.x), float(a.y), float(a.z));\n}\n\ninline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\ninline __host__ __device__ float4 sqrtf(float4 v)\n{\n    return make_float4(sqrtf(v.x), sqrtf(v.y), sqrtf(v.z), sqrtf(v.w));\n}\n\nstatic __forceinline__ __device__\nValueType dot(BasicVector<ValueType> a, BasicVector<ValueType> b)\n{\n    return a.dot(b);\n}\n\ninline __host__ __device__ float3 cross(float3 a, float3 b)\n{\n    return make_float3(a.y*b.z - a.z*b.y, a.z*b.x - a.x*b.z, a.x*b.y - a.y*b.x);\n}\n\ninline __host__ __device__ float length(float4 v)\n{\n    return sqrtf(dot(v, v));\n}\n\ninline __host__ __device__ float4 normalize(float4 v)\n{\n    float invLen = rsqrtf(dot(v, v));\n    return v * invLen;\n}\n\ninline __device__\nfloat4 normalEstimate(const float3 *points, int idx, int width, int height) \n{\n  float3 query_pt = points[idx];\n  if (isnan(query_pt.z))\n    return make_float4 (0.f,0.f,0.f,0.f);\n\n  int xIdx = idx % width;\n  int yIdx = idx / width;\n\n  // are we at a border? are our neighbor valid points?\n  bool west_valid  = (xIdx > 1)        && !isnan (points[idx-1].z) &&     fabsf (points[idx-1].z - query_pt.z) < 200.f;\n  bool east_valid  = (xIdx < width-1)  && !isnan (points[idx+1].z) &&     fabsf (points[idx+1].z - query_pt.z) < 200.f;\n  bool north_valid = (yIdx > 1)        && !isnan (points[idx-width].z) && fabsf (points[idx-width].z - query_pt.z) < 200.f;\n  bool south_valid = (yIdx < height-1) && !isnan (points[idx+width].z) && fabsf (points[idx+width].z - query_pt.z) < 200.f;\n\n  float3 horiz, vert;\n  if (west_valid & east_valid)\n    horiz = points[idx+1] - points[idx-1];\n  if (west_valid & !east_valid)\n    horiz = points[idx] - points[idx-1];\n  if (!west_valid & east_valid)\n    horiz = points[idx+1] - points[idx];\n  if (!west_valid & !east_valid)\n    return make_float4 (0.f,0.f,0.f,1.f);\n\n  if (south_valid & north_valid)\n    vert = points[idx-width] - points[idx+width];\n  if (south_valid & !north_valid)\n    vert = points[idx] - points[idx+width];\n  if (!south_valid & north_valid)\n    vert = points[idx-width] - points[idx];\n  if (!south_valid & !north_valid)\n    return make_float4 (0.f,0.f,0.f,1.f);\n\n  float3 normal = cross (horiz, vert);\n\n  float curvature = length (normal);\n  curvature = fabsf(horiz.z) > 0.04f || fabsf(vert.z) > 0.04f ||\n    !west_valid || !east_valid || !north_valid || !south_valid;\n\n  float3 mc = normalize (normal);\n  if ( dot (query_pt, mc) > 0.f )\n    mc = mc * -1.f;\n  return make_float4 (mc.x, mc.y, mc.z, curvature);\n}\n\n__global__ void ne (\n  const float3 *__restrict__ points,\n        float4 *__restrict__ normal_points,\n  const int width,\n  const int height,\n  const int numPts)\n{\n  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  if (idx < numPts) \n    normal_points[idx] = normalEstimate(points, idx, width, height);\n}"
        ]
    },
    "cfd-cuda": {
        "/Users/gbolet/hecbench-roofline/src/cfd-cuda/euler3d.cu": [
            "__global__ void initialize_buffer(float *d, const float val, const int nelr)\n{\n  const int i = (blockDim.x*blockIdx.x + threadIdx.x);\n  if (i < nelr) d[i] = val;\n}",
            "__global__ void initialize_variables(const int nelr, float* variables, const float* ff_variable)\n{\n  const int i = (blockDim.x*blockIdx.x + threadIdx.x);\n  for(int j = 0; j < NVAR; j++)\n    variables[i + j*nelr] = ff_variable[j];\n}",
            "__device__\ninline float compute_pressure(const float density, const float density_energy, const float speed_sqd){\n  return ((float)(GAMMA) - (float)(1.0f))*(density_energy - (float)(0.5f)*density*speed_sqd);\n}\n\n__device__\ninline float compute_speed_of_sound(const float density, const float pressure){\n  return sqrt((float)(GAMMA)*pressure/density);\n}\n\n__device__\ninline float compute_speed_sqd(const Float3 velocity){\n  return velocity.x*velocity.x + velocity.y*velocity.y + velocity.z*velocity.z;\n}\n\n__device__\ninline void compute_velocity(const float density, const Float3 momentum, Float3* velocity){\n  velocity->x = momentum.x / density;\n  velocity->y = momentum.y / density;\n  velocity->z = momentum.z / density;\n}\n\n__global__ void compute_step_factor(const int nelr, \n    float* variables, \n    float* areas, \n    float* step_factors){\n\n  const int i = (blockDim.x*blockIdx.x + threadIdx.x);\n  if( i >= nelr) return;\n\n  float density = variables[i + VAR_DENSITY*nelr];\n  Float3 momentum;\n  momentum.x = variables[i + (VAR_MOMENTUM+0)*nelr];\n  momentum.y = variables[i + (VAR_MOMENTUM+1)*nelr];\n  momentum.z = variables[i + (VAR_MOMENTUM+2)*nelr];\n\n  float density_energy = variables[i + VAR_DENSITY_ENERGY*nelr];\n\n  Float3 velocity;       compute_velocity(density, momentum, &velocity);\n  float speed_sqd      = compute_speed_sqd(velocity);\n\n  float pressure       = compute_pressure(density, density_energy, speed_sqd);\n  float speed_of_sound = compute_speed_of_sound(density, pressure);\n  step_factors[i] = (float)(0.5f) / (sqrt(areas[i]) * (sqrt(speed_sqd) + speed_of_sound));\n}",
            "__device__ __host__\ninline void compute_flux_contribution(const float density, \n    Float3 momentum, \n    const float density_energy, \n    const float pressure, \n    const Float3 velocity, \n    Float3* fc_momentum_x, \n    Float3* fc_momentum_y, \n    Float3* fc_momentum_z, \n    Float3* fc_density_energy)\n{\n  fc_momentum_x->x = velocity.x*momentum.x + pressure;\n  fc_momentum_x->y = velocity.x*momentum.y;\n  fc_momentum_x->z = velocity.x*momentum.z;\n\n\n  fc_momentum_y->x = fc_momentum_x->y;\n  fc_momentum_y->y = velocity.y*momentum.y + pressure;\n  fc_momentum_y->z = velocity.y*momentum.z;\n\n  fc_momentum_z->x = fc_momentum_x->z;\n  fc_momentum_z->y = fc_momentum_y->z;\n  fc_momentum_z->z = velocity.z*momentum.z + pressure;\n\n  const float de_p = density_energy+pressure;\n  fc_density_energy->x = velocity.x*de_p;\n  fc_density_energy->y = velocity.y*de_p;\n  fc_density_energy->z = velocity.z*de_p;\n}\n\n__device__\ninline float compute_pressure(const float density, const float density_energy, const float speed_sqd){\n  return ((float)(GAMMA) - (float)(1.0f))*(density_energy - (float)(0.5f)*density*speed_sqd);\n}\n\n__device__\ninline float compute_speed_of_sound(const float density, const float pressure){\n  return sqrt((float)(GAMMA)*pressure/density);\n}\n\n__device__\ninline float compute_speed_sqd(const Float3 velocity){\n  return velocity.x*velocity.x + velocity.y*velocity.y + velocity.z*velocity.z;\n}\n\n__device__\ninline void compute_velocity(const float density, const Float3 momentum, Float3* velocity){\n  velocity->x = momentum.x / density;\n  velocity->y = momentum.y / density;\n  velocity->z = momentum.z / density;\n}\n\n__global__ void \ncompute_flux(\n    int nelr, \n    int* elements_surrounding_elements,\n    float* normals,\n    float* variables,\n    float* ff_variable,\n    float* fluxes,\n    Float3* ff_flux_contribution_density_energy,\n    Float3* ff_flux_contribution_momentum_x,\n    Float3* ff_flux_contribution_momentum_y,\n    Float3* ff_flux_contribution_momentum_z){\n\n  const int i = (blockDim.x*blockIdx.x + threadIdx.x);\n\n  if( i >= nelr) return;\n  const float smoothing_coefficient = (float)(0.2f);\n  int j, nb;\n  Float3 normal; \n  float normal_len;\n  float factor;\n\n  float density_i = variables[i + VAR_DENSITY*nelr];\n  Float3 momentum_i;\n  momentum_i.x = variables[i + (VAR_MOMENTUM+0)*nelr];\n  momentum_i.y = variables[i + (VAR_MOMENTUM+1)*nelr];\n  momentum_i.z = variables[i + (VAR_MOMENTUM+2)*nelr];\n\n  float density_energy_i = variables[i + VAR_DENSITY_ENERGY*nelr];\n\n  Float3 velocity_i;                     \n  compute_velocity(density_i, momentum_i, &velocity_i);\n  float speed_sqd_i                          = compute_speed_sqd(velocity_i);\n  //float speed_sqd_i;\n  //compute_speed_sqd(velocity_i, speed_sqd_i);\n  float speed_i                              = sqrt(speed_sqd_i);\n  float pressure_i                           = compute_pressure(density_i, density_energy_i, speed_sqd_i);\n  float speed_of_sound_i                     = compute_speed_of_sound(density_i, pressure_i);\n  Float3 flux_contribution_i_momentum_x, flux_contribution_i_momentum_y, flux_contribution_i_momentum_z;\n  Float3 flux_contribution_i_density_energy;  \n  compute_flux_contribution(density_i, momentum_i, density_energy_i, pressure_i, velocity_i, \n      &flux_contribution_i_momentum_x, &flux_contribution_i_momentum_y, \n      &flux_contribution_i_momentum_z, &flux_contribution_i_density_energy);\n\n  float flux_i_density = (float)(0.0f);\n  Float3 flux_i_momentum;\n  flux_i_momentum.x = (float)(0.0f);\n  flux_i_momentum.y = (float)(0.0f);\n  flux_i_momentum.z = (float)(0.0f);\n  float flux_i_density_energy = (float)(0.0f);\n\n  Float3 velocity_nb;\n  float density_nb, density_energy_nb;\n  Float3 momentum_nb;\n  Float3 flux_contribution_nb_momentum_x, flux_contribution_nb_momentum_y, flux_contribution_nb_momentum_z;\n  Float3 flux_contribution_nb_density_energy;  \n  float speed_sqd_nb, speed_of_sound_nb, pressure_nb;\n\n#pragma unroll\n  for(j = 0; j < NNB; j++)\n  {\n    nb = elements_surrounding_elements[i + j*nelr];\n    normal.x = normals[i + (j + 0*NNB)*nelr];\n    normal.y = normals[i + (j + 1*NNB)*nelr];\n    normal.z = normals[i + (j + 2*NNB)*nelr];\n    normal_len = sqrt(normal.x*normal.x + normal.y*normal.y + normal.z*normal.z);\n\n    if(nb >= 0)   // a legitimate neighbor\n    {\n      density_nb = variables[nb + VAR_DENSITY*nelr];\n      momentum_nb.x = variables[nb + (VAR_MOMENTUM+0)*nelr];\n      momentum_nb.y = variables[nb + (VAR_MOMENTUM+1)*nelr];\n      momentum_nb.z = variables[nb + (VAR_MOMENTUM+2)*nelr];\n      density_energy_nb = variables[nb + VAR_DENSITY_ENERGY*nelr];\n      compute_velocity(density_nb, momentum_nb, &velocity_nb);\n      speed_sqd_nb                      = compute_speed_sqd(velocity_nb);\n      pressure_nb                       = compute_pressure(density_nb, density_energy_nb, speed_sqd_nb);\n      speed_of_sound_nb                 = compute_speed_of_sound(density_nb, pressure_nb);\n      compute_flux_contribution(density_nb, momentum_nb, density_energy_nb, pressure_nb, velocity_nb, \n          &flux_contribution_nb_momentum_x, &flux_contribution_nb_momentum_y, &flux_contribution_nb_momentum_z, \n          &flux_contribution_nb_density_energy);\n\n      // artificial viscosity\n      factor = -normal_len*smoothing_coefficient*(float)(0.5f)*(speed_i + sqrt(speed_sqd_nb) + speed_of_sound_i + speed_of_sound_nb);\n      flux_i_density += factor*(density_i-density_nb);\n      flux_i_density_energy += factor*(density_energy_i-density_energy_nb);\n      flux_i_momentum.x += factor*(momentum_i.x-momentum_nb.x);\n      flux_i_momentum.y += factor*(momentum_i.y-momentum_nb.y);\n      flux_i_momentum.z += factor*(momentum_i.z-momentum_nb.z);\n\n      // accumulate cell-centered fluxes\n      factor = (float)(0.5f)*normal.x;\n      flux_i_density += factor*(momentum_nb.x+momentum_i.x);\n      flux_i_density_energy += factor*(flux_contribution_nb_density_energy.x+flux_contribution_i_density_energy.x);\n      flux_i_momentum.x += factor*(flux_contribution_nb_momentum_x.x+flux_contribution_i_momentum_x.x);\n      flux_i_momentum.y += factor*(flux_contribution_nb_momentum_y.x+flux_contribution_i_momentum_y.x);\n      flux_i_momentum.z += factor*(flux_contribution_nb_momentum_z.x+flux_contribution_i_momentum_z.x);\n\n      factor = (float)(0.5f)*normal.y;\n      flux_i_density += factor*(momentum_nb.y+momentum_i.y);\n      flux_i_density_energy += factor*(flux_contribution_nb_density_energy.y+flux_contribution_i_density_energy.y);\n      flux_i_momentum.x += factor*(flux_contribution_nb_momentum_x.y+flux_contribution_i_momentum_x.y);\n      flux_i_momentum.y += factor*(flux_contribution_nb_momentum_y.y+flux_contribution_i_momentum_y.y);\n      flux_i_momentum.z += factor*(flux_contribution_nb_momentum_z.y+flux_contribution_i_momentum_z.y);\n\n      factor = (float)(0.5f)*normal.z;\n      flux_i_density += factor*(momentum_nb.z+momentum_i.z);\n      flux_i_density_energy += factor*(flux_contribution_nb_density_energy.z+flux_contribution_i_density_energy.z);\n      flux_i_momentum.x += factor*(flux_contribution_nb_momentum_x.z+flux_contribution_i_momentum_x.z);\n      flux_i_momentum.y += factor*(flux_contribution_nb_momentum_y.z+flux_contribution_i_momentum_y.z);\n      flux_i_momentum.z += factor*(flux_contribution_nb_momentum_z.z+flux_contribution_i_momentum_z.z);\n    }\n    else if(nb == -1)  // a wing boundary\n    {\n      flux_i_momentum.x += normal.x*pressure_i;\n      flux_i_momentum.y += normal.y*pressure_i;\n      flux_i_momentum.z += normal.z*pressure_i;\n    }\n    else if(nb == -2) // a far field boundary\n    {\n      factor = (float)(0.5f)*normal.x;\n      flux_i_density += factor*(ff_variable[VAR_MOMENTUM+0]+momentum_i.x);\n      flux_i_density_energy += factor*(ff_flux_contribution_density_energy[0].x+flux_contribution_i_density_energy.x);\n      flux_i_momentum.x += factor*(ff_flux_contribution_momentum_x[0].x + flux_contribution_i_momentum_x.x);\n      flux_i_momentum.y += factor*(ff_flux_contribution_momentum_y[0].x + flux_contribution_i_momentum_y.x);\n      flux_i_momentum.z += factor*(ff_flux_contribution_momentum_z[0].x + flux_contribution_i_momentum_z.x);\n\n      factor = (float)(0.5f)*normal.y;\n      flux_i_density += factor*(ff_variable[VAR_MOMENTUM+1]+momentum_i.y);\n      flux_i_density_energy += factor*(ff_flux_contribution_density_energy[0].y+flux_contribution_i_density_energy.y);\n      flux_i_momentum.x += factor*(ff_flux_contribution_momentum_x[0].y + flux_contribution_i_momentum_x.y);\n      flux_i_momentum.y += factor*(ff_flux_contribution_momentum_y[0].y + flux_contribution_i_momentum_y.y);\n      flux_i_momentum.z += factor*(ff_flux_contribution_momentum_z[0].y + flux_contribution_i_momentum_z.y);\n\n      factor = (float)(0.5f)*normal.z;\n      flux_i_density += factor*(ff_variable[VAR_MOMENTUM+2]+momentum_i.z);\n      flux_i_density_energy += factor*(ff_flux_contribution_density_energy[0].z+flux_contribution_i_density_energy.z);\n      flux_i_momentum.x += factor*(ff_flux_contribution_momentum_x[0].z + flux_contribution_i_momentum_x.z);\n      flux_i_momentum.y += factor*(ff_flux_contribution_momentum_y[0].z + flux_contribution_i_momentum_y.z);\n      flux_i_momentum.z += factor*(ff_flux_contribution_momentum_z[0].z + flux_contribution_i_momentum_z.z);\n\n    }\n  }\n\n  fluxes[i + VAR_DENSITY*nelr] = flux_i_density;\n  fluxes[i + (VAR_MOMENTUM+0)*nelr] = flux_i_momentum.x;\n  fluxes[i + (VAR_MOMENTUM+1)*nelr] = flux_i_momentum.y;\n  fluxes[i + (VAR_MOMENTUM+2)*nelr] = flux_i_momentum.z;\n  fluxes[i + VAR_DENSITY_ENERGY*nelr] = flux_i_density_energy;\n\n}",
            "__global__ void \ntime_step(int j, int nelr, \n    const float* old_variables, \n    float* variables, \n    const float* step_factors, \n    const float* fluxes) {\n\n  const int i = (blockDim.x*blockIdx.x + threadIdx.x);\n  if( i >= nelr) return;\n\n  float factor = step_factors[i]/(float)(RK+1-j);\n\n  variables[i + VAR_DENSITY*nelr] = old_variables[i + VAR_DENSITY*nelr] + factor*fluxes[i + VAR_DENSITY*nelr];\n  variables[i + VAR_DENSITY_ENERGY*nelr] = old_variables[i + VAR_DENSITY_ENERGY*nelr] + factor*fluxes[i + VAR_DENSITY_ENERGY*nelr];\n  variables[i + (VAR_MOMENTUM+0)*nelr] = old_variables[i + (VAR_MOMENTUM+0)*nelr] + factor*fluxes[i + (VAR_MOMENTUM+0)*nelr];\n  variables[i + (VAR_MOMENTUM+1)*nelr] = old_variables[i + (VAR_MOMENTUM+1)*nelr] + factor*fluxes[i + (VAR_MOMENTUM+1)*nelr];  \n  variables[i + (VAR_MOMENTUM+2)*nelr] = old_variables[i + (VAR_MOMENTUM+2)*nelr] + factor*fluxes[i + (VAR_MOMENTUM+2)*nelr];  \n}"
        ]
    },
    "floydwarshall2-cuda": {
        "/Users/gbolet/hecbench-roofline/src/floydwarshall2-cuda/main.cu": [
            "static __global__ void init2(\n  const ECLgraph g,\n  mtype* const AdjMat,\n  const int upper)\n{\n  const int i = threadIdx.x + blockIdx.x * blockDim.x;\n  if (i < g.nodes) {\n    for (int j = g.nindex[i]; j < g.nindex[i + 1]; j++) {\n      const int nei = g.nlist[j];\n      AdjMat[i * upper + nei] = g.eweight[j];\n    }\n  }\n}",
            "static __global__ void init1(\n  const int nodes,\n  mtype* const AdjMat,\n  const int upper)\n{\n  const int idx = threadIdx.x + blockIdx.x * blockDim.x;\n  const int i = idx / upper;\n  if (i < upper) {\n    const int j = idx % upper;\n    AdjMat[idx] = ((i == j) && (i < nodes)) ? 0 : (INT_MAX / 2);\n  }\n}",
            "inline __host__ __device__ uint4 make_uint4(int4 a)\n{\n    return make_uint4(uint(a.x), uint(a.y), uint(a.z), uint(a.w));\n}\n\ninline __host__ __device__ uint4 min(uint4 a, uint4 b)\n{\n    return make_uint4(min(a.x,b.x), min(a.y,b.y), min(a.z,b.z), min(a.w,b.w));\n}\n\nstatic __global__ __launch_bounds__(ThreadsPerBlock, 1)\nvoid FW0_64(\n  mtype* const __restrict__ AdjMat,\n  const int upper,\n  mtype* const __restrict__ krows,\n  mtype* const __restrict__ kcols)\n{\n  __shared__ mtype temp[tile * tile];\n  __shared__ mtype krow[tile * tile];\n\n  const int warp_a = threadIdx.x / ws; // i: 0-31, upper half\n  const int warp_b = warp_a + ws; // i: 32-63, lower half\n  const int lane_a = threadIdx.x % ws; // j: 0-31, left half\n  const int lane_b = lane_a + ws; // j: 32-63, right half\n\n  const int idx0_aa = warp_a * upper + lane_a; // upper left\n  const int idx0_ab = warp_a * upper + lane_b; // upper right\n  const int idx0_ba = warp_b * upper + lane_a; // lower left\n  const int idx0_bb = warp_b * upper + lane_b; // lower right\n\n  const int idx1_aa = lane_a * tile + warp_a;\n  const int idx1_ab = lane_b * tile + warp_a;\n  const int idx1_ba = lane_a * tile + warp_b;\n  const int idx1_bb = lane_b * tile + warp_b;\n\n  int idx2_a = lane_a;\n  int idx2_b = lane_b;\n\n  mtype ij_aa = AdjMat[idx0_aa];\n  mtype ij_ab = AdjMat[idx0_ab];\n  mtype ij_ba = AdjMat[idx0_ba];\n  mtype ij_bb = AdjMat[idx0_bb];\n\n  #pragma unroll 64\n  for (int k = 0; k < tile; k++) {\n    if (warp_a == k) krow[idx2_a] = ij_aa;\n    if (warp_a == k) krow[idx2_b] = ij_ab;\n    if (warp_b == k) krow[idx2_a] = ij_ba;\n    if (warp_b == k) krow[idx2_b] = ij_bb;\n    __syncthreads();\n\n    mtype ik_a, ik_b;\n    if (k < ws) {\n      ik_a = __shfl_sync(~0, ij_aa, k);\n      ik_b = __shfl_sync(~0, ij_ba, k);\n    } else {\n      ik_a = __shfl_sync(~0, ij_ab, k - ws);\n      ik_b = __shfl_sync(~0, ij_bb, k - ws);\n    }\n\n    const mtype kr_a = krow[idx2_a];\n    const mtype kr_b = krow[idx2_b];\n\n    ij_aa = min(ij_aa, ik_a + kr_a);\n    ij_ab = min(ij_ab, ik_a + kr_b);\n    ij_ba = min(ij_ba, ik_b + kr_a);\n    ij_bb = min(ij_bb, ik_b + kr_b);\n\n    if (warp_a == k) krows[idx0_aa] = ij_aa;\n    if (warp_a == k) krows[idx0_ab] = ij_ab;\n    if (warp_b == k) krows[idx0_ba] = ij_ba;\n    if (warp_b == k) krows[idx0_bb] = ij_bb;\n\n    if (lane_a == k) temp[idx1_aa] = ij_aa;\n    if (lane_a == k) temp[idx1_ba] = ij_ba;\n    if (lane_b == k) temp[idx1_ab] = ij_ab;\n    if (lane_b == k) temp[idx1_bb] = ij_bb;\n\n    idx2_a += tile;\n    idx2_b += tile;\n  }\n\n  __syncthreads();\n  kcols[idx0_aa] = temp[warp_a * tile + lane_a];\n  kcols[idx0_ab] = temp[warp_a * tile + lane_b];\n  kcols[idx0_ba] = temp[warp_b * tile + lane_a];\n  kcols[idx0_bb] = temp[warp_b * tile + lane_b];\n  AdjMat[idx0_aa] = ij_aa;\n  AdjMat[idx0_ab] = ij_ab;\n  AdjMat[idx0_ba] = ij_ba;\n  AdjMat[idx0_bb] = ij_bb;\n}",
            "inline __host__ __device__ uint4 make_uint4(int4 a)\n{\n    return make_uint4(uint(a.x), uint(a.y), uint(a.z), uint(a.w));\n}\n\ninline __host__ __device__ uint4 min(uint4 a, uint4 b)\n{\n    return make_uint4(min(a.x,b.x), min(a.y,b.y), min(a.z,b.z), min(a.w,b.w));\n}\n\nstatic __global__ __launch_bounds__(ThreadsPerBlock, 2048 / ThreadsPerBlock)\nvoid FWrowcol_64(\n  mtype* const __restrict__ AdjMat,\n  const int upper,\n  mtype* const __restrict__ krows,\n  mtype* const __restrict__ kcols,\n  const int x, const int subm1)\n{\n  __shared__ mtype temp[tile * tile];\n  __shared__ mtype krow[tile * tile];\n\n  const int warp_a = threadIdx.x / ws; // i: 0-31, upper half\n  const int warp_b = warp_a + ws; // i: 32-63, lower half\n  const int lane_a = threadIdx.x % ws; // j: 0-31, left half\n  const int lane_b = lane_a + ws; // j: 32-63, right half\n\n  int y = blockIdx.x;\n\n  if (y < subm1) {\n    if (y >= x) y++;\n    const int i_a = warp_a + x * tile;\n    const int i_b = warp_b + x * tile;\n    const int j_a = lane_a + y * tile;\n    const int j_b = lane_b + y * tile;\n\n    const int idx0_aa = i_a * upper + j_a;\n    const int idx0_ab = i_a * upper + j_b;\n    const int idx0_ba = i_b * upper + j_a;\n    const int idx0_bb = i_b * upper + j_b;\n\n    int idx1_a = warp_a;\n    int idx1_b = warp_b;\n    int idx2_a = lane_a;\n    int idx2_b = lane_b;\n\n    temp[warp_a * tile + lane_a] = kcols[i_a * upper + lane_a + x * tile];\n    temp[warp_a * tile + lane_b] = kcols[i_a * upper + lane_b + x * tile];\n    temp[warp_b * tile + lane_a] = kcols[i_b * upper + lane_a + x * tile];\n    temp[warp_b * tile + lane_b] = kcols[i_b * upper + lane_b + x * tile];\n    __syncthreads();\n\n    mtype ij_aa = AdjMat[idx0_aa];\n    mtype ij_ab = AdjMat[idx0_ab];\n    mtype ij_ba = AdjMat[idx0_ba];\n    mtype ij_bb = AdjMat[idx0_bb];\n\n    const mtype orig_aa = ij_aa;\n    const mtype orig_ab = ij_ab;\n    const mtype orig_ba = ij_ba;\n    const mtype orig_bb = ij_bb;\n\n    #pragma unroll 64\n    for (int k = 0; k < tile; k++) {\n      if (warp_a == k) krow[idx2_a] = ij_aa;\n      if (warp_a == k) krow[idx2_b] = ij_ab;\n      if (warp_b == k) krow[idx2_a] = ij_ba;\n      if (warp_b == k) krow[idx2_b] = ij_bb;\n      __syncthreads();\n\n      const mtype ik_a = temp[idx1_a];\n      const mtype ik_b = temp[idx1_b];\n      const mtype kr_a = krow[idx2_a];\n      const mtype kr_b = krow[idx2_b];\n\n      ij_aa = min(ij_aa, ik_a + kr_a);\n      ij_ab = min(ij_ab, ik_a + kr_b);\n      ij_ba = min(ij_ba, ik_b + kr_a);\n      ij_bb = min(ij_bb, ik_b + kr_b);\n\n      if (warp_a == k) krows[idx0_aa] = ij_aa;\n      if (warp_a == k) krows[idx0_ab] = ij_ab;\n      if (warp_b == k) krows[idx0_ba] = ij_ba;\n      if (warp_b == k) krows[idx0_bb] = ij_bb;\n\n      idx1_a += tile;\n      idx1_b += tile;\n      idx2_a += tile;\n      idx2_b += tile;\n    }\n    if (ij_aa != orig_aa) AdjMat[idx0_aa] = ij_aa;\n    if (ij_ab != orig_ab) AdjMat[idx0_ab] = ij_ab;\n    if (ij_ba != orig_ba) AdjMat[idx0_ba] = ij_ba;\n    if (ij_bb != orig_bb) AdjMat[idx0_bb] = ij_bb;\n  } else {\n    y -= subm1;\n    if (y >= x) y++;\n    const int i_a = warp_a + y * tile;\n    const int i_b = warp_b + y * tile;\n\n    const int j_a = lane_a + x * tile;\n    const int j_b = lane_b + x * tile;\n\n    const int idx0_aa = i_a * upper + j_a;\n    const int idx0_ab = i_a * upper + j_b;\n    const int idx0_ba = i_b * upper + j_a;\n    const int idx0_bb = i_b * upper + j_b;\n\n    const int idx1_aa = lane_a * tile + warp_a;\n    const int idx1_ab = lane_b * tile + warp_a;\n    const int idx1_ba = lane_a * tile + warp_b;\n    const int idx1_bb = lane_b * tile + warp_b;\n\n    int idx2_a = (x * tile) * upper + j_a;\n    int idx2_b = (x * tile) * upper + j_b;\n\n    mtype ij_aa = AdjMat[idx0_aa];\n    mtype ij_ab = AdjMat[idx0_ab];\n    mtype ij_ba = AdjMat[idx0_ba];\n    mtype ij_bb = AdjMat[idx0_bb];\n\n    const mtype orig_aa = ij_aa;\n    const mtype orig_ab = ij_ab;\n    const mtype orig_ba = ij_ba;\n    const mtype orig_bb = ij_bb;\n\n    #pragma unroll 64\n    for (int k = 0; k < tile; k++) {\n      mtype ik_a, ik_b;\n      if (k < ws) {\n        ik_a = __shfl_sync(~0, ij_aa, k);\n        ik_b = __shfl_sync(~0, ij_ba, k);\n      }\n      if (k >= ws) {\n        ik_a = __shfl_sync(~0, ij_ab, k - ws);\n        ik_b = __shfl_sync(~0, ij_bb, k - ws);\n      }\n      const mtype kr_a = krows[idx2_a];\n      const mtype kr_b = krows[idx2_b];\n\n      ij_aa = min(ij_aa, ik_a + kr_a);\n      ij_ab = min(ij_ab, ik_a + kr_b);\n      ij_ba = min(ij_ba, ik_b + kr_a);\n      ij_bb = min(ij_bb, ik_b + kr_b);\n\n      if (lane_a == k) temp[idx1_aa] = ij_aa;\n      if (lane_a == k) temp[idx1_ba] = ij_ba;\n      if (lane_b == k) temp[idx1_ab] = ij_ab;\n      if (lane_b == k) temp[idx1_bb] = ij_bb;\n\n      idx2_a += upper;\n      idx2_b += upper;\n    }\n    __syncthreads();\n\n    kcols[idx0_aa] = temp[warp_a * tile + lane_a];\n    kcols[idx0_ab] = temp[warp_a * tile + lane_b];\n    kcols[idx0_ba] = temp[warp_b * tile + lane_a];\n    kcols[idx0_bb] = temp[warp_b * tile + lane_b];\n\n    if (ij_aa != orig_aa) AdjMat[idx0_aa] = ij_aa;\n    if (ij_ab != orig_ab) AdjMat[idx0_ab] = ij_ab;\n    if (ij_ba != orig_ba) AdjMat[idx0_ba] = ij_ba;\n    if (ij_bb != orig_bb) AdjMat[idx0_bb] = ij_bb;\n  }\n}",
            "inline __host__ __device__ uint4 make_uint4(int4 a)\n{\n    return make_uint4(uint(a.x), uint(a.y), uint(a.z), uint(a.w));\n}\n\ninline __host__ __device__ uint4 min(uint4 a, uint4 b)\n{\n    return make_uint4(min(a.x,b.x), min(a.y,b.y), min(a.z,b.z), min(a.w,b.w));\n}\n\nstatic __global__ __launch_bounds__(ThreadsPerBlock, 2048 / ThreadsPerBlock)\nvoid FWrem_64(\n  mtype* const __restrict__ AdjMat, \n  const int upper,\n  mtype* const __restrict__ krows,\n  mtype* const __restrict__ kcols,\n  const int x, const int subm1)\n{\n  int y = blockIdx.x / subm1;\n  int z = blockIdx.x % subm1;\n  if (y >= x) y++;\n  if (z >= x) z++;\n\n  const int warp_a = threadIdx.x / ws;\n  const int warp_b = warp_a + ws;\n\n  const int lane_a = threadIdx.x % ws;\n  const int lane_b = lane_a + ws;\n\n  const int i_a = warp_a + y * tile;\n  const int i_b = warp_b + y * tile;\n  const int j_a = lane_a + z * tile;\n  const int j_b = lane_b + z * tile;\n\n  const int idx0_aa = i_a * upper + j_a; // upper left\n  const int idx0_ab = i_a * upper + j_b; // upper right\n  const int idx0_ba = i_b * upper + j_a; // lower left\n  const int idx0_bb = i_b * upper + j_b; // lower right\n\n  __shared__ mtype s_kj[tile * tile];\n  __shared__ mtype s_ik[tile * tile];\n\n  s_kj[warp_a * tile + lane_a] = krows[(x * tile + warp_a) * upper + j_a];\n  s_kj[warp_a * tile + lane_b] = krows[(x * tile + warp_a) * upper + j_b];\n  s_kj[warp_b * tile + lane_a] = krows[(x * tile + warp_b) * upper + j_a];\n  s_kj[warp_b * tile + lane_b] = krows[(x * tile + warp_b) * upper + j_b];\n\n  s_ik[warp_a * tile + lane_a] = kcols[i_a * upper + lane_a + x * tile];\n  s_ik[warp_a * tile + lane_b] = kcols[i_a * upper + lane_b + x * tile];\n  s_ik[warp_b * tile + lane_a] = kcols[i_b * upper + lane_a + x * tile];\n  s_ik[warp_b * tile + lane_b] = kcols[i_b * upper + lane_b + x * tile];\n\n  mtype ij_aa = AdjMat[idx0_aa];\n  mtype ij_ab = AdjMat[idx0_ab];\n  mtype ij_ba = AdjMat[idx0_ba];\n  mtype ij_bb = AdjMat[idx0_bb];\n\n  const mtype orig_aa = ij_aa;\n  const mtype orig_ab = ij_ab;\n  const mtype orig_ba = ij_ba;\n  const mtype orig_bb = ij_bb;\n\n  __syncthreads();\n  int idx1_a = warp_a;\n  int idx1_b = warp_b;\n\n  int idx2_a = lane_a;\n  int idx2_b = lane_b;\n\n  #pragma unroll 64\n  for (int k = 0; k < tile; k++) {\n    const mtype sk_a = s_kj[idx2_a];\n    const mtype sk_b = s_kj[idx2_b];\n\n    ij_aa = min(ij_aa, s_ik[idx1_a] + sk_a);\n    ij_ab = min(ij_ab, s_ik[idx1_a] + sk_b);\n    ij_ba = min(ij_ba, s_ik[idx1_b] + sk_a);\n    ij_bb = min(ij_bb, s_ik[idx1_b] + sk_b);\n\n    idx1_a += tile;\n    idx1_b += tile;\n\n    idx2_a += tile;\n    idx2_b += tile;\n  }\n\n  if ((y == z) && (y == x + 1) && (x != subm1)) { // the diagonal in next iteration\n    const int idx1_aa = lane_a * tile + warp_a;\n    const int idx1_ab = lane_b * tile + warp_a;\n    const int idx1_ba = lane_a * tile + warp_b;\n    const int idx1_bb = lane_b * tile + warp_b;\n\n    int idx2_a = lane_a;\n    int idx2_b = lane_b;\n\n    #pragma unroll 64\n    for (int k = 0; k < tile; k++) {\n      if (warp_a == k) s_kj[idx2_a] = ij_aa;\n      if (warp_a == k) s_kj[idx2_b] = ij_ab;\n      if (warp_b == k) s_kj[idx2_a] = ij_ba;\n      if (warp_b == k) s_kj[idx2_b] = ij_bb;\n      __syncthreads();\n\n      mtype ik_a, ik_b;\n      if (k < ws) {\n        ik_a = __shfl_sync(~0, ij_aa, k);\n        ik_b = __shfl_sync(~0, ij_ba, k);\n      }\n      else {\n        ik_a = __shfl_sync(~0, ij_ab, k - ws);\n        ik_b = __shfl_sync(~0, ij_bb, k - ws);\n      }\n\n      const mtype sk_a = s_kj[idx2_a];\n      const mtype sk_b = s_kj[idx2_b];\n\n      ij_aa = min(ij_aa, ik_a + sk_a);\n      ij_ab = min(ij_ab, ik_a + sk_b);\n      ij_ba = min(ij_ba, ik_b + sk_a);\n      ij_bb = min(ij_bb, ik_b + sk_b);\n\n      if (warp_a == k) krows[idx0_aa] = ij_aa;\n      if (warp_a == k) krows[idx0_ab] = ij_ab;\n      if (warp_b == k) krows[idx0_ba] = ij_ba;\n      if (warp_b == k) krows[idx0_bb] = ij_bb;\n\n      if (lane_a == k) s_ik[idx1_aa] = ij_aa;\n      if (lane_a == k) s_ik[idx1_ba] = ij_ba;\n      if (lane_b == k) s_ik[idx1_ab] = ij_ab;\n      if (lane_b == k) s_ik[idx1_bb] = ij_bb;\n      idx2_a += tile;\n      idx2_b += tile;\n    }\n    __syncthreads();\n\n    kcols[idx0_aa] = s_ik[warp_a * tile + lane_a];\n    kcols[idx0_ab] = s_ik[warp_a * tile + lane_b];\n    kcols[idx0_ba] = s_ik[warp_b * tile + lane_a];\n    kcols[idx0_bb] = s_ik[warp_b * tile + lane_b];\n  }\n\n  if (ij_aa != orig_aa) AdjMat[idx0_aa] = ij_aa;\n  if (ij_ab != orig_ab) AdjMat[idx0_ab] = ij_ab;\n  if (ij_ba != orig_ba) AdjMat[idx0_ba] = ij_ba;\n  if (ij_bb != orig_bb) AdjMat[idx0_bb] = ij_bb;\n}"
        ]
    },
    "graphB+-cuda": {
        "/Users/gbolet/hecbench-roofline/src/graphB+-cuda/kernels.h": [
            "__device__ __forceinline__ uint32 hash(int x) {\n    return hash((uint32)x);\n}\n\nstatic __global__ void generateSpanningTree(\n  const int nodes,\n  const int* const __restrict__ nindex,\n  const int* const __restrict__ nlist,\n  const int seed,\n  EdgeInfo* const einfo,\n  volatile int* const parent,\n  int* const queue,\n  const int level,\n  int* const tail,\n  int start,\n  int end)\n{\n  const int from = (threadIdx.x + blockIdx.x * ThreadsPerBlock) / warpsize;\n  const int incr = (gridDim.x * ThreadsPerBlock) / warpsize;\n  const int lane = threadIdx.x % warpsize;\n  const int seed2 = seed * seed + seed;\n  const int bit = (level & 1) | 2;\n\n  for (int i = start + from; i < end; i += incr) {\n    const int node = queue[i];\n    const int me = (node << 2) | bit;\n    if (lane == 0) atomicAnd((int*)&parent[node], ~3);\n    for (int j = nindex[node + 1] - 1 - lane; j >= nindex[node]; j -= warpsize) {  // reverse order on purpose\n      const int neighbor = nlist[j] >> 1;\n      const int seed3 = neighbor ^ seed2;\n      const int hash_me = hash(me ^ seed3);\n      int val, hash_val;\n      do {  // pick parent deterministically\n          val = parent[neighbor];\n          hash_val = hash(val ^ seed3);\n        } while (((val < 0) || (((val & 3) == bit) && ((hash_val < hash_me) || \n                 ((hash_val == hash_me) && (val < me))))) && \n                 (atomicCAS((int*)&parent[neighbor], val, me) != val));\n        if (val < 0) {\n          val = atomicAdd(tail, 1);\n          queue[val] = neighbor;\n      }\n    }\n    __syncwarp();\n  }\n}",
            "static __global__ void verify_generateSpanningTree(\n  const int nodes,\n  const int edges,\n  const int* const nindex,\n  const int* const nlist,\n  const int seed,\n  const int* const parent,\n  const int level,\n  const int* const tail,\n        int end)\n{\n  const int from = threadIdx.x + blockIdx.x * ThreadsPerBlock;\n  const int incr = gridDim.x * ThreadsPerBlock;\n  if (end != *tail) {printf(\"ERROR: head mismatch\\n\"); asm(\"trap;\");}\n  if (*tail != nodes) {printf(\"ERROR: tail mismatch tail %d nodes %d \\n\", *tail, nodes); asm(\"trap;\");}\n  for (int i = from; i < nodes; i += incr) {\n    if (parent[i] < 0) {printf(\"ERROR: found unvisited node %d\\n\", i); asm(\"trap;\");}\n  }\n}",
            "static __global__ void rootcount(\n  const int* const parent,\n  const int* const queue,\n        int* const __restrict__ label,\n  const int level,\n        int start,\n        int end)\n{\n  const int from = threadIdx.x + blockIdx.x * ThreadsPerBlock;\n  const int incr = gridDim.x * ThreadsPerBlock;\n  // bottom up: push counts\n  for (int i = start + from; i < end; i += incr) {\n    const int node = queue[i];\n    atomicAdd(&label[parent[node] >> 2], label[node]);\n  }\n}",
            "#define T ((int)32)\n\n\ninline __host__ __device__ uint4 make_uint4(int4 a)\n{\n    return make_uint4(uint(a.x), uint(a.y), uint(a.z), uint(a.w));\n}\n\ninline __host__ __device__ uint4 max(uint4 a, uint4 b)\n{\n    return make_uint4(max(a.x,b.x), max(a.y,b.y), max(a.z,b.z), max(a.w,b.w));\n}\n\ninline __host__ __device__ uint4 min(uint4 a, uint4 b)\n{\n    return make_uint4(min(a.x,b.x), min(a.y,b.y), min(a.z,b.z), min(a.w,b.w));\n}\n\n__device__ inline void swap(T &a, T &b) {\n  T tmp = a;\n  a = b;\n  b = tmp;\n}\n\nstatic __global__ void treelabel(\n  const int nodes,\n  const int* const __restrict__ nindex,\n  volatile int* const __restrict__ nlist,\n  EdgeInfo* const __restrict__ einfo,\n  volatile int* const __restrict__ inTree,\n  volatile int* const __restrict__ negCnt,\n  const int* const __restrict__ parent,\n  const int* const __restrict__ queue,\n        int* const __restrict__ label,\n  const int level,\n        int start,\n        int end)\n{\n  const int from = (threadIdx.x + blockIdx.x * ThreadsPerBlock) / warpsize;\n  const int incr = (gridDim.x * ThreadsPerBlock) / warpsize;\n  const int lane = threadIdx.x % warpsize;\n  //cuv\n  // top down: label tree + set nlist flag + set edge info + move tree nodes to front + make parent edge first in list\n  for (int i = start + from; i < end; i += incr) {\n    const int node = queue[i];\n    const int par = parent[node] >> 2;\n    const int nodelabel = label[node];\n    const int beg = nindex[node];\n    const int end = nindex[node + 1];\n\n    // set nlist flag + set edge info\n    int lbl = (nodelabel >> 1) + 1;\n    for (int j = beg + lane; __any_sync(mask, j < end); j += warpsize) {\n      int lblinc = 0;\n      int neighbor = -1;\n      bool cond = false;\n      if (j < end) {\n        neighbor = nlist[j] >> 1;\n        cond = (neighbor != par) && ((parent[neighbor] >> 2) == node);\n        if (cond) {\n          lblinc = label[neighbor];\n        }\n      }\n      const int currcount = lblinc;\n      for (int d = 1; d < 32; d *= 2) {\n        const int tmp = __shfl_up_sync(mask, lblinc, d);\n        if (lane >= d) lblinc += tmp;\n      }\n      lbl += lblinc;\n\n      if (cond) {\n        const int lblval = (lbl - currcount) << 1;\n        label[neighbor] = lblval;\n        einfo[j].beg = lblval;\n        einfo[j].end = (einfo[j].end & 1) | ((lbl - 1) << 1);\n        nlist[j] |= 1;  // child edge is in tree\n      }\n      lbl = __shfl_sync(mask, lbl, 31);\n    }\n\n    // move tree nodes to front\n    const int len = end - beg;\n    if (len > 0) {\n      enum {none, some, left, right};\n      if (len <= warpsize) {\n        const int src = beg + lane;\n        int b, e, in, neg,  n, state = none;\n        if (lane < len) {\n          b = einfo[src].beg;\n          e = einfo[src].end;\n          in = inTree[src];\n          neg = negCnt[src];\n          n = nlist[src];\n          const int neighbor = n >> 1;\n          state = ((neighbor != par) && ((parent[neighbor] >> 2) == node)) ? left : right;  // partitioning condition\n        }\n        const int ball = __ballot_sync(mask, state == left);\n        const int balr = __ballot_sync(mask, state == right);\n        const int pfsl = __popc(ball & ~(-1 << lane));\n        const int pfsr = __popc(balr & ~(-1 << lane));\n        const int pos = beg + ((state == right) ? (len - 1 - pfsr) : pfsl);\n        if (state != none) {\n          einfo[pos].beg = b;\n          einfo[pos].end = e;\n          inTree[pos] = in;\n          negCnt[pos] = neg;\n          nlist[pos] = n;\n        }\n      } else {\n        int lp = beg;\n        int rp = end - 1;\n        int state = some;\n        int read = beg + min(warpsize, len);\n        int src = beg + lane;\n        int b = einfo[src].beg;\n        int e = einfo[src].end;\n        int n = nlist[src];\n        int in = inTree[src];\n        int neg = negCnt[src];\n\n        do {\n          if (state == some) {\n            const int neighbor = n >> 1;\n            state = ((neighbor != par) && ((parent[neighbor] >> 2) == node)) ? left : right;  // partitioning condition\n          }\n          const int ball = __ballot_sync(mask, state == left);\n          const int pfsl = __popc(ball & ~(-1 << lane));\n          if (state == left) {\n            int oldb, olde, oldin, oldneg, oldn;\n            const int pos = lp + pfsl;\n            if (pos >= read) {\n              oldb = einfo[pos].beg;\n              olde = einfo[pos].end;\n              oldin = inTree[pos];\n              oldneg = negCnt[pos];\n              oldn = nlist[pos];\n            }\n            einfo[pos].beg = b;\n            einfo[pos].end = e;\n            inTree[pos] = in;\n            negCnt[pos] = neg;\n            nlist[pos] = n;\n            b = oldb;\n            e = olde;\n            in = oldin;\n            neg = oldneg;\n            n = oldn;\n            state = (pos < read) ? none : some;\n          }\n          lp += __popc(ball);\n          read = max(read, lp);\n          const int balr = __ballot_sync(mask, state == right);\n          const int pfsr = __popc(balr & ~(-1 << lane));\n          if (state == right) {\n            int oldb, olde, oldin, oldneg, oldn;\n            const int pos = rp - pfsr;\n            if (pos >= read) {\n              oldb = einfo[pos].beg;\n              olde = einfo[pos].end;\n              oldin = inTree[pos];\n              oldneg = negCnt[pos];\n              oldn = nlist[pos];\n            }\n            einfo[pos].beg = b;\n            einfo[pos].end = e;\n            inTree[pos] = in;\n            negCnt[pos] = neg;\n            nlist[pos] = n;\n            b = oldb;\n            e = olde;\n            in = oldin;\n            neg = oldneg;\n            n = oldn;\n            state = (pos < read) ? none : some;\n          }\n          rp -= __popc(balr);\n          if (read <= rp) {\n            const int bal = __ballot_sync(mask, state == none);\n            const int pfs = __popc(bal & ~(-1 << lane));\n            if (state == none) {\n              const int pos = read + pfs;\n              if (pos <= rp) {\n                b = einfo[pos].beg;\n                e = einfo[pos].end;\n                in = inTree[pos];\n                neg = negCnt[pos];\n                n = nlist[pos];\n                state = some;\n              }\n            }\n            read += __popc(bal);  // may be too high but okay\n          }\n        } while (__any_sync(mask, state == some));\n      }\n    }\n\n    //find paredge here\n    int paredge = -1;\n    for (int j = beg + lane; __any_sync(mask, j < end); j += warpsize) {\n      if (j < end) {\n        const int neighbor = nlist[j] >> 1;\n        if (neighbor == par) {\n          paredge = j;\n        }\n      }\n      if (__any_sync(mask, paredge >= 0)) break;\n    }\n    int pos = -1;\n    for (int j = beg + lane; __any_sync(mask, j < end); j += warpsize) {\n      if (j < end) {\n        const int neighbor = nlist[j] >> 1;\n        if (((parent[neighbor] >> 2) != node)) {\n          pos = j;\n        }\n      }\n      if (__any_sync(mask, pos >= 0)) break;\n    }\n    unsigned int bal = __ballot_sync(mask, pos >= 0);\n    const int lid = __ffs(bal) - 1;\n    pos = __shfl_sync(mask, pos, lid);\n    if (paredge >= 0) {  // only one thread per warp\n      einfo[paredge].beg = nodelabel | 1;\n      einfo[paredge].end = (einfo[paredge].end & 1) | ((lbl - 1) << 1);\n      nlist[paredge] |= 1;\n      if (paredge != beg) {\n        if (paredge != pos) {\n          swap(nlist[pos], nlist[paredge]);\n          swap(einfo[pos], einfo[paredge]);\n          swap(inTree[pos], inTree[paredge]);\n          swap(negCnt[pos], negCnt[paredge]);\n          paredge = pos;\n        }\n        if (paredge != beg) {\n          swap(nlist[beg], nlist[paredge]);\n          swap(einfo[beg], einfo[paredge]);\n          swap(inTree[beg], inTree[paredge]);\n          swap(negCnt[beg], negCnt[paredge]);\n        }\n      }\n    }\n    __syncwarp();\n\n#ifdef VERIFY\n    if (lane == 0) {\n      if (i == 0) {\n        if (lbl != nodes) {printf(\"ERROR: lbl mismatch, lbl %d nodes %d\\n\", lbl, nodes); asm(\"trap;\");}\n      }\n      int j = beg;\n      while ((j < end) && (nlist[j] & 1)) j++;\n      while ((j < end) && !(nlist[j] & 1)) j++;\n      if (j != end) {printf(\"ERROR: not moved %d %d %d\\n\", beg, j, end); asm(\"trap;\");}\n    }\n#endif\n  }\n}",
            "static __global__ void inTreeUpdate(\n  const int edges,\n  const int* const __restrict__ nlist,\n  volatile int* const __restrict__ inTree)\n{\n  const int from = threadIdx.x + blockIdx.x * ThreadsPerBlock;\n  const int incr = gridDim.x * ThreadsPerBlock;\n  // update inTree\n  for (int j = from; j < edges; j += incr) {\n    inTree[j] += nlist[j] & 1;\n  }\n}",
            "static __global__ void processCycles(\n  const int nodes,\n  const int* const __restrict__ nindex,\n  const int* const __restrict__ nlist,\n  const int* const __restrict__ label,\n  const EdgeInfo* const __restrict__ einfo,\n  bool* const  __restrict__ minus)\n{\n  const int from = (threadIdx.x + blockIdx.x * ThreadsPerBlock) / warpsize;\n  const int incr = (gridDim.x * ThreadsPerBlock) / warpsize;\n  const int lane = threadIdx.x % warpsize;\n  for (int i = from; i < nodes; i += incr) {\n    const int target0 = label[i];\n    const int target1 = target0 | 1;\n    int j = nindex[i + 1] - 1 - lane;\n    while ((j >= nindex[i]) && !(nlist[j] & 1)) {\n      int curr = nlist[j] >> 1;\n      if (curr > i) {  // only process edges in one direction\n        int sum = 0;\n        while (label[curr] != target0) {\n          int k = nindex[curr];\n          while ((einfo[k].beg & 1) == ((einfo[k].beg <= target1) && (target0 <= einfo[k].end))) k++;\n#ifdef VERIFY\n          if ((k >= nindex[curr + 1]) || !(nlist[k] & 1)) {printf(\"ERROR: couldn't find path\\n\"); asm(\"trap;\");}\n#endif\n          sum += einfo[k].end & 1;\n          curr = nlist[k] >> 1;\n        }\n        minus[j] = sum & 1;\n      }\n      j -= warpsize;\n    }\n   __syncwarp();\n  }\n}",
            "static __global__ void initMinus(\n  const int edges,\n  const int nodes,\n  const int* const __restrict__ nindex,\n  const int* const __restrict__ nlist,\n  const EdgeInfo* const einfo,\n  bool* const minus)\n{\n  const int from = threadIdx.x + blockIdx.x * ThreadsPerBlock;\n  const int incr = gridDim.x * ThreadsPerBlock;\n  // set minus info to true\n  for (int j = from; j < edges; j += incr) {\n    minus[j] = true;\n  }\n  // copy minus info of tree edges\n  for (int i = from; i < nodes; i += incr) {\n    int j = nindex[i];\n    while ((j < nindex[i + 1]) && (nlist[j] & 1)) {\n      minus[j] = einfo[j].end & 1;\n      j++;\n    }\n  }\n}",
            "static __global__ void init3(\n  const int nodes,\n  const int* const __restrict__ nidx,\n  const int* const __restrict__ nlist,\n  int* const __restrict__ label,\n  int* const __restrict__ count)\n{\n  const int from = threadIdx.x + blockIdx.x * ThreadsPerBlock;\n  const int incr = gridDim.x * ThreadsPerBlock;\n\n  for (int v = from; v < nodes; v += incr) {\n    label[v] = v;\n  }\n\n  for (int v = from; v < nodes; v += incr) {\n    count[v] = 0;\n  }\n}",
            "static __global__ void ccSize(\n  const int nodes,\n  const int* const __restrict__ label,\n        int* const __restrict__ count)\n{\n  const int from = threadIdx.x + blockIdx.x * ThreadsPerBlock;\n  const int incr = gridDim.x * ThreadsPerBlock;\n  if (from == 0)\n  {\n    hi = 0;\n    wSize = 0;\n  }\n  for (int v = from; v < nodes; v += incr) {\n    atomicAdd(&count[label[v]],1);;\n  }\n}",
            "__device__ __forceinline__\ndouble atomicMax(double *address, double val)\n{\n  unsigned long long ret = __double_as_longlong(*address);\n  while(val > __longlong_as_double(ret))\n  {\n    unsigned long long old = ret;\n    if((ret = atomicCAS((unsigned long long *)address, old, __double_as_longlong(val))) == old)\n      break;\n  }\n  return __longlong_as_double(ret);\n}\n\nstatic __global__ void largestCC(const int nodes, const int* const __restrict__ count)\n{\n  const int from = threadIdx.x + blockIdx.x * ThreadsPerBlock;\n  const int incr = gridDim.x * ThreadsPerBlock;\n  for (int v = from; v < nodes; v += incr) {\n    const unsigned long long d_hi = (((unsigned long long)count[v]) << 32)| v;\n    if (hi < d_hi) {\n      atomicMax(&hi, d_hi);\n    }\n  }\n}",
            "static __global__ void ccHopCount(\n  const int nodes,\n  const int* const __restrict__ nidx,\n  const int* const __restrict__ nlist,\n  const int* const __restrict__ label,\n  int* const __restrict__ count,\n  int* const __restrict__ ws1,\n  int* const __restrict__ ws2)\n{\n  const int from = (threadIdx.x + blockIdx.x * ThreadsPerBlock) / warpsize;\n  const int incr = (gridDim.x * ThreadsPerBlock) / warpsize;\n  const int lane = threadIdx.x % warpsize;\n\n  const int hi2 = hi & 0xffffffff;\n  for (int v = from; v < nodes; v += incr) {\n    const int lblv = label[v];\n    if (lblv == v) {\n      count[lblv] = (lblv == hi2) ? 0 : INT_MAX - 1;  // init count\n    }\n    for (int j = nidx[v] + lane; j < nidx[v + 1]; j += warpsize) {\n      const int nli = nlist[j] >> 1;\n      const int lbln = label[nli];\n      if (lblv < lbln) {  // only one direction\n        const int idx = atomicAdd(&wSize, 1); //get the return value and use it\n        ws1[idx] = lblv;\n        ws2[idx] = lbln;\n      }\n    }\n  }\n}",
            "static __global__ void BellmanFord(\n  int* const __restrict__ count,\n  bool* const __restrict__ changed,\n  const int* const __restrict__ ws1,\n  const int* const __restrict__ ws2)\n{\n  const int from = threadIdx.x + blockIdx.x * ThreadsPerBlock;\n  const int incr = gridDim.x * ThreadsPerBlock;\n  // use Bellman Ford to compute distances\n  for (int i = from; i < wSize; i += incr) {\n    const int lblv = ws1[i];\n    const int lbln = ws2[i];\n    const int distv = count[lblv];\n    const int distn = count[lbln];\n    if (distv + 1 < distn) {\n      count[lbln] = distv + 1;\n      *changed = true;\n    } else if (distn + 1 < distv) {\n      count[lblv] = distn + 1;\n      *changed = true;\n    }\n  }\n}",
            "static __global__ void incrementCC(\n  const int nodes,\n  const int* const __restrict__ label,\n  const int* const __restrict__ count,\n  int* const __restrict__ inCC)\n{\n  const int from = threadIdx.x + blockIdx.x * ThreadsPerBlock;\n  const int incr = gridDim.x * ThreadsPerBlock;\n  // increment inCC if node is at even hop count from source CC\n  for (int v = from; v < nodes; v += incr) {\n    inCC[v] += (count[label[v]] % 2) ^ 1;\n  }\n}"
        ]
    },
    "cc-cuda": {
        "/Users/gbolet/hecbench-roofline/src/cc-cuda/main.cu": [
            "static inline __device__ int representative(const int idx, int* const __restrict__ nstat)\n{\n  int curr = nstat[idx];\n  if (curr != idx) {\n    int next, prev = idx;\n    while (curr > (next = nstat[curr])) {\n      nstat[prev] = next;\n      prev = curr;\n      curr = next;\n    }\n  }\n  return curr;\n}\n\nstatic __global__ __launch_bounds__(ThreadsPerBlock, 2048 / ThreadsPerBlock)\nvoid compute1(const int nodes,\n              const int* const __restrict__ nidx,\n              const int* const __restrict__ nlist,\n                    int* const __restrict__ nstat,\n                    int* const __restrict__ wl)\n{\n  const int from = threadIdx.x + blockIdx.x * ThreadsPerBlock;\n  const int incr = gridDim.x * ThreadsPerBlock;\n\n  for (int v = from; v < nodes; v += incr) {\n    const int vstat = nstat[v];\n    if (v != vstat) {\n      const int beg = nidx[v];\n      const int end = nidx[v + 1];\n      int deg = end - beg;\n      if (deg > 16) {\n        int idx;\n        if (deg <= 352) {\n          idx = atomicAdd(&topL, 1);\n        } else {\n          idx = atomicAdd(&topH, -1);\n        }\n        wl[idx] = v;\n      } else {\n        int vstat = representative(v, nstat);\n        for (int i = beg; i < end; i++) {\n          const int nli = nlist[i];\n          if (v > nli) {\n            int ostat = representative(nli, nstat);\n            bool repeat;\n            do {\n              repeat = false;\n              if (vstat != ostat) {\n                int ret;\n                if (vstat < ostat) {\n                  if ((ret = atomicCAS(&nstat[ostat], ostat, vstat)) != ostat) {\n                    ostat = ret;\n                    repeat = true;\n                  }\n                } else {\n                  if ((ret = atomicCAS(&nstat[vstat], vstat, ostat)) != vstat) {\n                    vstat = ret;\n                    repeat = true;\n                  }\n                }\n              }\n            } while (repeat);\n          }\n        }\n      }\n    }\n  }\n}",
            "static __global__ __launch_bounds__(ThreadsPerBlock, 2048 / ThreadsPerBlock)\nvoid flatten(const int nodes,\n             const int* const __restrict__ nidx,\n             const int* const __restrict__ nlist,\n                   int* const __restrict__ nstat)\n{\n  const int from = threadIdx.x + blockIdx.x * ThreadsPerBlock;\n  const int incr = gridDim.x * ThreadsPerBlock;\n\n  for (int v = from; v < nodes; v += incr) {\n    int next, vstat = nstat[v];\n    const int old = vstat;\n    while (vstat > (next = nstat[vstat])) {\n      vstat = next;\n    }\n    if (old != vstat) nstat[v] = vstat;\n  }\n}",
            "static inline __device__ int representative(const int idx, int* const __restrict__ nstat)\n{\n  int curr = nstat[idx];\n  if (curr != idx) {\n    int next, prev = idx;\n    while (curr > (next = nstat[curr])) {\n      nstat[prev] = next;\n      prev = curr;\n      curr = next;\n    }\n  }\n  return curr;\n}\n\nstatic __global__ __launch_bounds__(ThreadsPerBlock, 2048 / ThreadsPerBlock)\nvoid compute2(const int nodes,\n              const int* const __restrict__ nidx,\n              const int* const __restrict__ nlist,\n                    int* const __restrict__ nstat,\n              const int* const __restrict__ wl)\n{\n  const int lane = threadIdx.x % warpsize;\n\n  int idx;\n  if (lane == 0) idx = atomicAdd(&posL, 1);\n  idx = __shfl_sync(0xffffffff, idx, 0);\n  while (idx < topL) {\n    const int v = wl[idx];\n    int vstat = representative(v, nstat);\n    for (int i = nidx[v] + lane; i < nidx[v + 1]; i += warpsize) {\n      const int nli = nlist[i];\n      if (v > nli) {\n        int ostat = representative(nli, nstat);\n        bool repeat;\n        do {\n          repeat = false;\n          if (vstat != ostat) {\n            int ret;\n            if (vstat < ostat) {\n              if ((ret = atomicCAS(&nstat[ostat], ostat, vstat)) != ostat) {\n                ostat = ret;\n                repeat = true;\n              }\n            } else {\n              if ((ret = atomicCAS(&nstat[vstat], vstat, ostat)) != vstat) {\n                vstat = ret;\n                repeat = true;\n              }\n            }\n          }\n        } while (repeat);\n      }\n    }\n    if (lane == 0) idx = atomicAdd(&posL, 1);\n    idx = __shfl_sync(0xffffffff, idx, 0);\n  }\n}",
            "static inline __device__ int representative(const int idx, int* const __restrict__ nstat)\n{\n  int curr = nstat[idx];\n  if (curr != idx) {\n    int next, prev = idx;\n    while (curr > (next = nstat[curr])) {\n      nstat[prev] = next;\n      prev = curr;\n      curr = next;\n    }\n  }\n  return curr;\n}\n\nstatic __global__ __launch_bounds__(ThreadsPerBlock, 2048 / ThreadsPerBlock)\nvoid compute3(const int nodes,\n              const int* const __restrict__ nidx,\n              const int* const __restrict__ nlist,\n                    int* const __restrict__ nstat,\n              const int* const __restrict__ wl)\n{\n  __shared__ int vB;\n  if (threadIdx.x == 0) {\n    const int idx = atomicAdd(&posH, -1);\n    vB = (idx > topH) ? wl[idx] : -1;\n  }\n  __syncthreads();\n  while (vB >= 0) {\n    const int v = vB;\n    __syncthreads();\n    int vstat = representative(v, nstat);\n    for (int i = nidx[v] + threadIdx.x; i < nidx[v + 1]; i += ThreadsPerBlock) {\n      const int nli = nlist[i];\n      if (v > nli) {\n        int ostat = representative(nli, nstat);\n        bool repeat;\n        do {\n          repeat = false;\n          if (vstat != ostat) {\n            int ret;\n            if (vstat < ostat) {\n              if ((ret = atomicCAS(&nstat[ostat], ostat, vstat)) != ostat) {\n                ostat = ret;\n                repeat = true;\n              }\n            } else {\n              if ((ret = atomicCAS(&nstat[vstat], vstat, ostat)) != vstat) {\n                vstat = ret;\n                repeat = true;\n              }\n            }\n          }\n        } while (repeat);\n      }\n    }\n    if (threadIdx.x == 0) {\n      const int idx = atomicAdd(&posH, -1);\n      vB = (idx > topH) ? wl[idx] : -1;\n    }\n    __syncthreads();\n  }\n}"
        ]
    },
    "softmax-cuda": {
        "/Users/gbolet/hecbench-roofline/src/softmax-cuda/main.cu": [
            "inline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\ninline __host__ __device__ float4 expf(float4 v)\n{\n    return make_float4(expf(v.x), expf(v.y), expf(v.z), expf(v.w));\n}\n\ninline __host__ __device__ uint4 make_uint4(int4 a)\n{\n    return make_uint4(uint(a.x), uint(a.y), uint(a.z), uint(a.w));\n}\n\ninline __host__ __device__ uint4 max(uint4 a, uint4 b)\n{\n    return make_uint4(max(a.x,b.x), max(a.y,b.y), max(a.z,b.z), max(a.w,b.w));\n}\n\n__global__\nvoid softMax (const int numSlice, const int sliceSize,\n              const float* src, float* dest)\n{\n  int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i >= numSlice) return;\n  float max_ = src[i * sliceSize];\n  for (int j = 0; j < sliceSize; j++) {\n    max_ = max(max_, src[i * sliceSize + j]);\n  }\n  float sum = 0;\n  for (int j = 0; j < sliceSize; j++) {\n    sum += expf(src[i * sliceSize + j] - max_);\n  }\n  for (int j = 0; j < sliceSize; j++) {\n    dest[i * sliceSize + j] = expf(src[i * sliceSize + j] - max_) / sum;\n  }\n}",
            "inline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\ninline __host__ __device__ float4 expf(float4 v)\n{\n    return make_float4(expf(v.x), expf(v.y), expf(v.z), expf(v.w));\n}\n\ninline __host__ __device__ uint4 make_uint4(int4 a)\n{\n    return make_uint4(uint(a.x), uint(a.y), uint(a.z), uint(a.w));\n}\n\ninline __host__ __device__ uint4 max(uint4 a, uint4 b)\n{\n    return make_uint4(max(a.x,b.x), max(a.y,b.y), max(a.z,b.z), max(a.w,b.w));\n}\n\n__global__\nvoid softMax2 (const int numSlice, const int sliceSize,\n              const float* src, float* dest)\n{\n  namespace cg = cooperative_groups;\n  cg::thread_block block = cg::this_thread_block();\n  cg::thread_block_tile<32> warp = cg::tiled_partition<32>(block);\n  int i = blockIdx.x * warp.meta_group_size() + warp.meta_group_rank();\n  if (i >= numSlice) return;\n  float max_ = src[i * sliceSize];\n  for (int j = warp.thread_rank(); j < sliceSize; j += warp.size()) {\n    max_ = max(max_, src[i * sliceSize + j]);\n  }\n  max_ = cg::reduce(warp, max_, cg::greater<float>{});\n  float sum = 0;\n  for (int j = warp.thread_rank(); j < sliceSize; j += warp.size()) {\n    sum += expf(src[i * sliceSize + j] - max_);\n  }\n  sum = cg::reduce(warp, sum, cg::plus<float>{});\n  for (int j = warp.thread_rank(); j < sliceSize; j += warp.size()) {\n    dest[i * sliceSize + j] = expf(src[i * sliceSize + j] - max_) / sum;\n  }\n}"
        ]
    },
    "romberg-cuda": {
        "/Users/gbolet/hecbench-roofline/src/romberg-cuda/main.cu": [
            "__device__ inline\nint log2(int u)\n{\n    int s, t;\n    t = (u > 0xffff) << 4; u >>= t;\n    s = (u > 0xff  ) << 3; u >>= s, t |= s;\n    s = (u > 0xf   ) << 2; u >>= s, t |= s;\n    s = (u > 0x3   ) << 1; u >>= s, t |= s;\n    return (t | (u >> 1));\n}\n\n__inline__ __device__ float f(unsigned int x, unsigned int y, unsigned int z)\n{\n  constexpr float d(2.0f / N);\n  float xf((int(x - Nd2)) * d);//[-1, 1)\n  float yf((int(z - Nd2)) * d);\n  float zf((int(z - Nd2)) * d);\n  return 1.f - 16.f * xf * yf * zf - 4.f * (xf * xf + yf * yf + zf * zf);\n}\n\n__device__ inline unsigned int getFirstSetBitPos(int n)\n{\n  return log2((float)(n&-n))+1;\n}\n\n__global__ void romberg(double a, double b, double *result)  \n{\n  extern __shared__ double smem[];\n  double diff = (b-a)/gridDim.x, step;\n  int k;\n  int max_eval = (1<<(ROW_SIZE-1));\n  b = a + (blockIdx.x+1)*diff;\n  a += blockIdx.x*diff;\n\n  step = (b-a)/max_eval;\n\n  double local_col[ROW_SIZE];  // specific to the row size\n  for(int i = 0; i < ROW_SIZE; i++) local_col[i] = 0.0;\n  if(!threadIdx.x)\n  {\n    k = blockDim.x;\n    local_col[0] = f(a) + f(b);\n  }\n  else\n    k = threadIdx.x;\n\n  for(; k < max_eval; k += blockDim.x)\n    local_col[ROW_SIZE - getFirstSetBitPos(k)] += 2.0*f(a + step*k);\n\n  for(int i = 0; i < ROW_SIZE; i++)\n    smem[ROW_SIZE*threadIdx.x + i] = local_col[i];\n  __syncthreads();\n\n  if(threadIdx.x < ROW_SIZE)\n  {\n    double sum = 0.0;\n    for(int i = threadIdx.x; i < blockDim.x*ROW_SIZE; i+=ROW_SIZE)\n      sum += smem[i];\n    smem[threadIdx.x] = sum;\n  }\n\n  if(!threadIdx.x)\n  {\n    double *table = local_col;\n    table[0] = smem[0];\n\n    for(int k = 1; k < ROW_SIZE; k++)\n      table[k] = table[k-1] + smem[k];\n\n    for(int k = 0; k < ROW_SIZE; k++)  \n      table[k]*= (b-a)/(1<<(k+1));\n\n    for(int col = 0 ; col < ROW_SIZE-1 ; col++)\n      for(int row = ROW_SIZE-1; row > col; row--)\n        table[row] = table[row] + (table[row] - table[row-1])/((1<<(2*col+1))-1);\n\n    result[blockIdx.x] = table[ROW_SIZE-1];\n  }\n}"
        ]
    },
    "assert-cuda": {
        "/Users/gbolet/hecbench-roofline/src/assert-cuda/main.cu": [
            "__global__ void perfKernel()\n{\n  int gid = blockIdx.x*blockDim.x + threadIdx.x ;\n  assert(gid <= blockDim.x * gridDim.x) ;\n  int s = 0;\n  for (int n = 1; n <= gid; n++) {\n    s++; assert(s <= gid);\n  }\n}",
            "__global__ void perfKernel2()\n{\n  int gid = blockIdx.x*blockDim.x + threadIdx.x ;\n  int s = 0;\n  for (int n = 1; n <= gid; n++) {\n    s++; assert(s <= gid);\n  }\n}"
        ]
    },
    "gaussian-cuda": {
        "/Users/gbolet/hecbench-roofline/src/gaussian-cuda/gaussianElim.cu": [
            "__global__ void\nfan1 (const float*__restrict__ a,\n            float*__restrict__ m,\n      const int size, const int t)\n{\n  int globalId = blockDim.x * blockIdx.x + threadIdx.x;\n  if (globalId < size-1-t) {\n    m[size * (globalId + t + 1)+t] = \n      a[size * (globalId + t + 1) + t] / a[size * t + t];\n  }\n}",
            "__global__ void\nfan2 (float*__restrict__ a,\n      float*__restrict__ b,\n      const float*__restrict__ m,\n      const int size, const int t)\n{\n  int globalIdy = blockDim.x * blockIdx.x + threadIdx.x;\n  int globalIdx = blockDim.y * blockIdx.y + threadIdx.y;\n  if (globalIdx < size-1-t && globalIdy < size-t) {\n    a[size*(globalIdx+1+t)+(globalIdy+t)] -= \n      m[size*(globalIdx+1+t)+t] * a[size*t+(globalIdy+t)];\n\n    if(globalIdy == 0){\n      b[globalIdx+1+t] -= \n        m[size*(globalIdx+1+t)+(globalIdy+t)] * b[t];\n    }\n  }\n}"
        ]
    },
    "dwconv1d-cuda": {
        "/Users/gbolet/hecbench-roofline/src/dwconv1d-cuda/src/timex.cu": [
            "#define F params.pre_transform_params[idx][5]\n\n\n__global__ void kernel_forward(const F *__restrict__ const __w, const F *__restrict__ const __k, F *__restrict__ const x,\n                               const F eps, const int B, const int C, const int T) {\n    const int i = blockIdx.y;\n    const int t = threadIdx.x << 2;\n    const int ti = t + T * i;\n    const int tj = T * (B * C) / BF;\n\n    __shared__ F ww[Tmax];\n    __shared__ F kk[Tmax * BF];\n    F4(ww, t) = F4(__w, t + T * (i % C));\n\n    #pragma unroll\n    for (int j = 0; j < BF; j++) {\n        F4(kk, t + Tmax * j) = F4(__k, ti + tj * j);\n    }\n    __syncthreads();\n\n    float4 ss[BF];\n    #pragma unroll\n    for (int j = 0; j < BF; j++) {\n        ss[j] = {eps, eps, eps, eps};\n    }\n    for (int u = 0; u <= t; u++) {\n        const F *__restrict__ const w = ww + T - t + u - 4;\n        #pragma unroll\n        for (int j = 0; j < BF; j++) {\n            float4 *__restrict__ const s = ss + j;\n            const F k = kk[u + Tmax * j];\n            s->x += w[3] * k;\n            s->y += w[2] * k;\n            s->z += w[1] * k;\n            s->w += w[0] * k;\n        }\n    }\n    #pragma unroll\n    for (int j = 0; j < BF; j++) {\n        float4 *__restrict__ const s = ss + j;\n        const F *__restrict__ const w = ww + T - 3;\n        const F *__restrict__ const k = kk + Tmax * j + t + 1;\n        s->y += w[2] * k[0];\n        s->z += w[1] * k[0];\n        s->z += w[2] * k[1];\n        s->w += w[0] * k[0];\n        s->w += w[1] * k[1];\n        s->w += w[2] * k[2];\n        F4(x, ti + tj * j) = *s;\n    }\n}",
            "#define F params.pre_transform_params[idx][5]\n\n\n__global__ void kernel_backward(const F *__restrict__ const __w, const F *__restrict__ const __k, const F *__restrict__ const __gwk,\n                                F *__restrict__ const gw, F *__restrict__ const gk,\n                                const int B, const int C, const int T) {\n    const int i = blockIdx.y;\n    const int t = threadIdx.x << 2;\n    const int ti = t + T * i;\n    const int tj = T * (B * C) / BB;\n\n    __shared__ F ww[Tmax];\n    __shared__ F kk[Tmax * BB];\n    __shared__ F gg[Tmax * BB];\n    F4(ww, t) = F4(__w, t + T * (i % C));\n\n    #pragma unroll\n    for (int j = 0; j < BB; j++) {\n        F4(kk, t + Tmax * j) = F4(__k, ti + tj * j);\n        F4(gg, t + Tmax * j) = F4(__gwk, ti + tj * j);\n    }\n    __syncthreads();\n\n    float4 ss[BB];\n    #pragma unroll\n    for (int j = 0; j < BB; j++) {\n        ss[j] = {0, 0, 0, 0};\n    }\n    for (int u = 0; u <= t; u++) {\n        #pragma unroll\n        for (int j = 0; j < BB; j++) {\n            float4 *__restrict__ const s = ss + j;\n            const F *__restrict__ const g = gg + Tmax * j + T - t + u - 4;\n            const F k = kk[u + Tmax * j];\n            s->x += g[3] * k;\n            s->y += g[2] * k;\n            s->z += g[1] * k;\n            s->w += g[0] * k;\n        }\n    }\n    #pragma unroll\n    for (int j = 0; j < BB; j++) {\n        float4 *__restrict__ const s = ss + j;\n        const F *__restrict__ const k = kk + Tmax * j + t + 1;\n        const F *__restrict__ const g = gg + Tmax * j + T - 3;\n        s->y += g[2] * k[0];\n        s->z += g[1] * k[0];\n        s->z += g[2] * k[1];\n        s->w += g[0] * k[0];\n        s->w += g[1] * k[1];\n        s->w += g[2] * k[2];\n        F4(gw, ti + tj * j) = *s;\n    }\n\n    #pragma unroll\n    for (int j = 0; j < BB; j++) {\n        ss[j] = {0, 0, 0, 0};\n    }\n    for (int u = t + 3; u < T; u++) {\n        const F w = ww[u];\n        #pragma unroll\n        for (int j = 0; j < BB; j++) {\n            float4 *__restrict__ const s = ss + j;\n            const F *__restrict__ const g = gg + Tmax * j + T + t - u - 1;\n            s->x += g[0] * w;\n            s->y += g[1] * w;\n            s->z += g[2] * w;\n            s->w += g[3] * w;\n        }        \n    }\n    #pragma unroll\n    for (int j = 0; j < BB; j++) {\n        float4 *__restrict__ const s = ss + j;\n        const F *__restrict__ const g = gg + Tmax * j + T - 3;\n        const F *__restrict__ const w = ww + t;\n        s->x += g[2] * w[0];\n        s->x += g[1] * w[1];\n        s->x += g[0] * w[2];\n        s->y += g[2] * w[1];\n        s->y += g[1] * w[2];\n        s->z += g[2] * w[2];\n        F4(gk, ti + tj * j) = *s;\n    }\n}"
        ]
    },
    "burger-cuda": {
        "/Users/gbolet/hecbench-roofline/src/burger-cuda/kernels.h": [
            "__global__ \nvoid core (\n    double *__restrict__ u_new,\n    double *__restrict__ v_new,\n    const double *__restrict__ u,\n    const double *__restrict__ v,\n    const int x_points,\n    const int y_points,\n    const double nu,\n    const double del_t,\n    const double del_x,\n    const double del_y)\n{\n  int i = blockIdx.y * blockDim.y + threadIdx.y + 1;\n  int j = blockIdx.x * blockDim.x + threadIdx.x + 1;\n  if (j < x_points - 1 && i < y_points - 1) {\n    u_new[idx(i,j)] = u[idx(i,j)] + \n      (nu*del_t/(del_x*del_x)) * (u[idx(i,j+1)] + u[idx(i,j-1)] - 2 * u[idx(i,j)]) + \n      (nu*del_t/(del_y*del_y)) * (u[idx(i+1,j)] + u[idx(i-1,j)] - 2 * u[idx(i,j)]) - \n      (del_t/del_x)*u[idx(i,j)] * (u[idx(i,j)] - u[idx(i,j-1)]) - \n      (del_t/del_y)*v[idx(i,j)] * (u[idx(i,j)] - u[idx(i-1,j)]);\n\n    v_new[idx(i,j)] = v[idx(i,j)] +\n      (nu*del_t/(del_x*del_x)) * (v[idx(i,j+1)] + v[idx(i,j-1)] - 2 * v[idx(i,j)]) + \n      (nu*del_t/(del_y*del_y)) * (v[idx(i+1,j)] + v[idx(i-1,j)] - 2 * v[idx(i,j)]) -\n      (del_t/del_x)*u[idx(i,j)] * (v[idx(i,j)] - v[idx(i,j-1)]) - \n      (del_t/del_y)*v[idx(i,j)] * (v[idx(i,j)] - v[idx(i-1,j)]);\n  }\n}",
            "__global__ \nvoid bound_h (\n    double *__restrict__ u_new,\n    double *__restrict__ v_new,\n    const int x_points,\n    const int y_points)\n{\n  int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < x_points) {\n    u_new[idx(0,i)] = 1.0;\n    v_new[idx(0,i)] = 1.0;\n    u_new[idx(y_points-1,i)] = 1.0;\n    v_new[idx(y_points-1,i)] = 1.0;\n  }\n}",
            "__global__ \nvoid bound_v (\n    double *__restrict__ u_new,\n    double *__restrict__ v_new,\n    const int x_points,\n    const int y_points)\n{\n  int j = blockIdx.x * blockDim.x + threadIdx.x;\n  if (j < y_points) {\n    u_new[idx(j,0)] = 1.0;\n    v_new[idx(j,0)] = 1.0;\n    u_new[idx(j,x_points-1)] = 1.0;\n    v_new[idx(j,x_points-1)] = 1.0;\n  }\n}"
        ]
    },
    "gd-cuda": {
        "/Users/gbolet/hecbench-roofline/src/gd-cuda/main.cu": [
            "__global__ void\nupdate(float * __restrict__ x,\n       const float * __restrict__ grad,\n       int m, int n, float lambda, float alpha) \n{\n  int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < n) {\n    float g = grad[i] / (float)m + lambda * x[i]; \n    x[i] = x[i] - alpha * g;\n  }\n}",
            "__global__ void \nL2_norm(const float *x, float* l2_norm, int n)\n{\n  int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < n) atomicAdd(l2_norm, x[i]*x[i]);\n}"
        ]
    },
    "rsmt-cuda": {
        "/Users/gbolet/hecbench-roofline/src/rsmt-cuda/main.cu": [
            "inline __host__ __device__ int4 make_int4(float4 a)\n{\n    return make_int4(int(a.x), int(a.y), int(a.z), int(a.w));\n}\n\ninline __host__ __device__ int4 abs(int4 v)\n{\n    return make_int4(abs(v.x), abs(v.y), abs(v.z), abs(v.w));\n}\n\ninline __host__ __device__ uint4 make_uint4(int4 a)\n{\n    return make_uint4(uint(a.x), uint(a.y), uint(a.z), uint(a.w));\n}\n\ninline __host__ __device__ uint4 max(uint4 a, uint4 b)\n{\n    return make_uint4(max(a.x,b.x), max(a.y,b.y), max(a.z,b.z), max(a.w,b.w));\n}\n\ninline __host__ __device__ uint4 min(uint4 a, uint4 b)\n{\n    return make_uint4(min(a.x,b.x), min(a.y,b.y), min(a.z,b.z), min(a.w,b.w));\n}\n\n__device__ __forceinline__\ndouble atomicMax(double *address, double val)\n{\n  unsigned long long ret = __double_as_longlong(*address);\n  while(val > __longlong_as_double(ret))\n  {\n    unsigned long long old = ret;\n    if((ret = atomicCAS((unsigned long long *)address, old, __double_as_longlong(val))) == old)\n      break;\n  }\n  return __longlong_as_double(ret);\n}\n\nstatic __device__\nvoid buildMST(const ID num, \n              const ctype* const __restrict__ x,\n              const ctype* const __restrict__ y,\n              edge* const __restrict__ edges,\n              ctype dist[PinLimit])\n{\n  __shared__ ID source[WarpsPerBlock][PinLimit];\n  __shared__ ID destin[WarpsPerBlock][PinLimit];\n  __shared__ ctype mindj[WarpsPerBlock];\n\n  const int lane = threadIdx.x % WS;\n  const int warp = threadIdx.x / WS;\n\n  // initialize\n  ID numItems = num - 1;\n  for (ID i = lane; i < numItems; i += WS) dist[i] = INT_MAX;  // change if ctype changed\n  for (ID i = lane; i < numItems; i += WS) destin[warp][i] = (ID)(i + 1);\n\n  // Prim's MST algorithm\n  ID src = 0;\n  for (ID cnt = 0; cnt < num - 1; cnt++) {\n    __syncwarp();\n    if (lane == 0) mindj[warp] = INT_MAX;\n\n    // update distances\n    __syncwarp();\n    for (ID j = lane; j < numItems; j += WS) {\n      const ID dst = destin[warp][j];\n      const ctype dnew = abs(x[src] - x[dst]) + abs(y[src] - y[dst]);\n      ctype d = dist[j];\n      if (d > dnew) {\n        d = dnew;\n        dist[j] = dnew;\n        source[warp][j] = src;\n      }\n      const int upv = d * (MaxPins * 2) + j;  // tie breaker for determinism\n      atomicMin((ctype*)&mindj[warp], upv);\n    }\n\n    // create new edge\n    __syncwarp();\n    const ID j = mindj[warp] % (MaxPins * 2);\n    src = destin[warp][j];\n    numItems--;\n    if (lane == 0) {\n      edges[cnt].src = source[warp][j];\n      edges[cnt].dst = src;\n      dist[j] = dist[numItems];\n      source[warp][j] = source[warp][numItems];\n      destin[warp][j] = destin[warp][numItems];\n    }\n  }\n}\n\nstatic __device__\nbool insertSteinerPoints(ID& num,\n                         ctype* const __restrict__ x,\n                         ctype* const __restrict__ y,\n                         const edge* const __restrict__ edges,\n                         ctype dist[PinLimit])\n{\n  __shared__ ID adj[WarpsPerBlock][PinLimit][8];\n  __shared__ int cnt[WarpsPerBlock][PinLimit];\n\n  const int lane = threadIdx.x % WS;\n  const int warp = threadIdx.x / WS;\n  const ID top = num;\n\n  // create adjacency lists\n  for (ID i = lane; i < top; i += WS) cnt[warp][i] = 0;\n  __syncwarp();\n  for (ID e = lane; e < top - 1; e += WS) {\n    dist[e] = -1;\n    const ID s = edges[e].src;\n    const ID d = edges[e].dst;\n    if ((x[d] != x[s]) || (y[d] != y[s])) {\n      const int ps = atomicAdd(&cnt[warp][s], 1);\n      adj[warp][s][ps] = e;\n      const int pd = atomicAdd(&cnt[warp][d], 1);\n      adj[warp][d][pd] = e;\n    }\n  }\n\n  // find best distance for each triangle\n  __syncwarp();\n  for (ID s = lane; s < top; s += WS) {\n    if (cnt[warp][s] >= 2) {\n      const ctype x0 = x[s];\n      const ctype y0 = y[s];\n      for (char j = 0; j < cnt[warp][s] - 1; j++) {\n        const ID e1 = adj[warp][s][j];\n        const ID d1 = (s != edges[e1].src) ? edges[e1].src : edges[e1].dst;\n        const ctype x1 = x[d1];\n        const ctype y1 = y[d1];\n        for (char k = j + 1; k < cnt[warp][s]; k++) {\n          const ID e2 = adj[warp][s][k];\n          const ID d2 = (s != edges[e2].src) ? edges[e2].src : edges[e2].dst;\n          const ctype stx = max(min(x0, x1), min(max(x0, x1), x[d2]));\n          const ctype sty = max(min(y0, y1), min(max(y0, y1), y[d2]));\n          const ctype rd = abs(stx - x0) + abs(sty - y0);\n          if (rd > 0) {\n            const ctype rd1 = rd * (MaxPins * 2) + e1;  // tie breaker\n            const ctype rd2 = rd * (MaxPins * 2) + e2;  // tie breaker\n            atomicMax((ctype*)&dist[e1], rd2);\n            atomicMax((ctype*)&dist[e2], rd1);\n          }\n        }\n      }\n    }\n  }\n\n  // process \"triangles\" to find best candidate Steiner points\n  __syncwarp();\n  bool updated = false;\n  for (ID e1 = lane; __any_sync(0xffffffff, e1 < top - 2); e1 += WS) {\n    bool insert = false;\n    ctype stx, sty;\n    if (e1 < top - 2) {\n      const ctype d1 = dist[e1];\n      if (d1 > 0) {\n        const ID e2 = d1 % (MaxPins * 2);\n        if (e2 > e1) {\n          const ctype d2 = dist[e2];\n          if (e1 == d2 % (MaxPins * 2)) {\n            const ctype x0 = x[edges[e1].src];\n            const ctype y0 = y[edges[e1].src];\n            const ctype x1 = x[edges[e1].dst];\n            const ctype y1 = y[edges[e1].dst];\n            ctype x2 = x[edges[e2].src];\n            ctype y2 = y[edges[e2].src];\n            if (((x2 == x0) && (y2 == y0)) || ((x2 == x1) && (y2 == y1))) {\n              x2 = x[edges[e2].dst];\n              y2 = y[edges[e2].dst];\n            }\n            updated = true;\n            insert = true;\n            stx = max(min(x0, x1), min(max(x0, x1), x2));\n            sty = max(min(y0, y1), min(max(y0, y1), y2));\n          }\n        }\n      }\n    }\n    const int bal = __ballot_sync(0xffffffff, insert);\n    const int pos = __popc(bal & ~(-1 << lane)) + num;\n    if (insert) {\n      x[pos] = stx;\n      y[pos] = sty;\n    }\n    num += __popc(bal);\n  }\n\n  return __any_sync(0xffffffff, updated);\n}\n\nstatic __device__\ninline void processSmallNet(const int i,\n                            const int* const __restrict__ idxin,\n                            const ctype* const __restrict__ xin,\n                            const ctype* const __restrict__ yin,\n                            int* const __restrict__ idxout,\n                            ctype* const __restrict__ xout,\n                            ctype* const __restrict__ yout,\n                             edge* const __restrict__ edges,\n                              int* const __restrict__ wl)\n{\n  __shared__ ctype dist[WarpsPerBlock][PinLimit];\n  const int lane = threadIdx.x % WS;\n  const int warp = threadIdx.x / WS;\n\n  // initialize arrays and copy input coords to output\n  const int pin = idxin[i];\n  const ID num = idxin[i + 1] - pin;\n  const int pout = 2 * pin;\n  if (lane == 0) idxout[i] = pout;\n  for (ID j = lane; j < num; j += WS) xout[pout + j] = xin[pin + j];\n  for (ID j = lane; j < num; j += WS) yout[pout + j] = yin[pin + j];\n\n  // process nets\n  if (num == 2) {\n    if (lane == 0) edges[pout] = edge{0, 1};\n  } else if (num == 3) {\n    ctype x0, y0;\n    if (lane < 3) {\n      edges[pout + lane] = edge{(short)lane, 3};\n      x0 = xout[pout + lane];\n      y0 = yout[pout + lane];\n    }\n    const ctype x1 = __shfl_sync(0xffffffff, x0, 1);\n    const ctype y1 = __shfl_sync(0xffffffff, y0, 1);\n    const ctype x2 = __shfl_sync(0xffffffff, x0, 2);\n    const ctype y2 = __shfl_sync(0xffffffff, y0, 2);\n    if (lane == 0) {\n      xout[pout + 3] = max(min(x0, x1), min(max(x0, x1), x2));\n      yout[pout + 3] = max(min(y0, y1), min(max(y0, y1), y2));\n    }\n  } else if (num <= 32) {\n    // iterate until all Steiner points added\n    ID cnt = num;\n    do {\n      buildMST<WarpsPerBlock, PinLimit>(cnt, &xout[pout], &yout[pout], &edges[pout], dist[warp]);\n    } while (insertSteinerPoints<WarpsPerBlock, PinLimit>(cnt, &xout[pout], &yout[pout], &edges[pout], dist[warp]));\n  } else {\n    if (lane == 0) wl[atomicAdd(&wlsize, 1)] = i;\n  }\n}\n\nstatic __global__ __launch_bounds__(WarpsPerBlock * WS, 2)\nvoid largeNetKernel(const int* const __restrict__ idxin,\n                    const ctype* const __restrict__ xin,\n                    const ctype* const __restrict__ yin,\n                    int* const __restrict__ idxout,\n                    ctype* __restrict__ xout,\n                    ctype* __restrict__ yout,\n                     edge* __restrict__ edges,\n                    const int numnets,\n                    int* const __restrict__ wl)\n{\n  // compute Steiner points and edges\n  const int lane = threadIdx.x % WS;\n  do {\n    int i;\n    if (lane == 0) i = atomicAdd(&currpos1, 1);\n    i = __shfl_sync(0xffffffff, i, 0);\n    if (i >= numnets) break;\n    processSmallNet<WarpsPerBlock, PinLimit>(i, idxin, xin, yin, idxout, xout, yout, edges, wl);\n  } while (true);\n\n  // set final element\n  if ((threadIdx.x == 0) && (blockIdx.x == 0)) {\n    idxout[numnets] = 2 * idxin[numnets];\n  }\n}",
            "inline __host__ __device__ int4 make_int4(float4 a)\n{\n    return make_int4(int(a.x), int(a.y), int(a.z), int(a.w));\n}\n\ninline __host__ __device__ int4 abs(int4 v)\n{\n    return make_int4(abs(v.x), abs(v.y), abs(v.z), abs(v.w));\n}\n\ninline __host__ __device__ uint4 make_uint4(int4 a)\n{\n    return make_uint4(uint(a.x), uint(a.y), uint(a.z), uint(a.w));\n}\n\ninline __host__ __device__ uint4 max(uint4 a, uint4 b)\n{\n    return make_uint4(max(a.x,b.x), max(a.y,b.y), max(a.z,b.z), max(a.w,b.w));\n}\n\ninline __host__ __device__ uint4 min(uint4 a, uint4 b)\n{\n    return make_uint4(min(a.x,b.x), min(a.y,b.y), min(a.z,b.z), min(a.w,b.w));\n}\n\n__device__ __forceinline__\ndouble atomicMax(double *address, double val)\n{\n  unsigned long long ret = __double_as_longlong(*address);\n  while(val > __longlong_as_double(ret))\n  {\n    unsigned long long old = ret;\n    if((ret = atomicCAS((unsigned long long *)address, old, __double_as_longlong(val))) == old)\n      break;\n  }\n  return __longlong_as_double(ret);\n}\n\nstatic __device__\nvoid buildMST(const ID num, \n              const ctype* const __restrict__ x,\n              const ctype* const __restrict__ y,\n              edge* const __restrict__ edges,\n              ctype dist[PinLimit])\n{\n  __shared__ ID source[WarpsPerBlock][PinLimit];\n  __shared__ ID destin[WarpsPerBlock][PinLimit];\n  __shared__ ctype mindj[WarpsPerBlock];\n\n  const int lane = threadIdx.x % WS;\n  const int warp = threadIdx.x / WS;\n\n  // initialize\n  ID numItems = num - 1;\n  for (ID i = lane; i < numItems; i += WS) dist[i] = INT_MAX;  // change if ctype changed\n  for (ID i = lane; i < numItems; i += WS) destin[warp][i] = (ID)(i + 1);\n\n  // Prim's MST algorithm\n  ID src = 0;\n  for (ID cnt = 0; cnt < num - 1; cnt++) {\n    __syncwarp();\n    if (lane == 0) mindj[warp] = INT_MAX;\n\n    // update distances\n    __syncwarp();\n    for (ID j = lane; j < numItems; j += WS) {\n      const ID dst = destin[warp][j];\n      const ctype dnew = abs(x[src] - x[dst]) + abs(y[src] - y[dst]);\n      ctype d = dist[j];\n      if (d > dnew) {\n        d = dnew;\n        dist[j] = dnew;\n        source[warp][j] = src;\n      }\n      const int upv = d * (MaxPins * 2) + j;  // tie breaker for determinism\n      atomicMin((ctype*)&mindj[warp], upv);\n    }\n\n    // create new edge\n    __syncwarp();\n    const ID j = mindj[warp] % (MaxPins * 2);\n    src = destin[warp][j];\n    numItems--;\n    if (lane == 0) {\n      edges[cnt].src = source[warp][j];\n      edges[cnt].dst = src;\n      dist[j] = dist[numItems];\n      source[warp][j] = source[warp][numItems];\n      destin[warp][j] = destin[warp][numItems];\n    }\n  }\n}\n\nstatic __device__\nbool insertSteinerPoints(ID& num,\n                         ctype* const __restrict__ x,\n                         ctype* const __restrict__ y,\n                         const edge* const __restrict__ edges,\n                         ctype dist[PinLimit])\n{\n  __shared__ ID adj[WarpsPerBlock][PinLimit][8];\n  __shared__ int cnt[WarpsPerBlock][PinLimit];\n\n  const int lane = threadIdx.x % WS;\n  const int warp = threadIdx.x / WS;\n  const ID top = num;\n\n  // create adjacency lists\n  for (ID i = lane; i < top; i += WS) cnt[warp][i] = 0;\n  __syncwarp();\n  for (ID e = lane; e < top - 1; e += WS) {\n    dist[e] = -1;\n    const ID s = edges[e].src;\n    const ID d = edges[e].dst;\n    if ((x[d] != x[s]) || (y[d] != y[s])) {\n      const int ps = atomicAdd(&cnt[warp][s], 1);\n      adj[warp][s][ps] = e;\n      const int pd = atomicAdd(&cnt[warp][d], 1);\n      adj[warp][d][pd] = e;\n    }\n  }\n\n  // find best distance for each triangle\n  __syncwarp();\n  for (ID s = lane; s < top; s += WS) {\n    if (cnt[warp][s] >= 2) {\n      const ctype x0 = x[s];\n      const ctype y0 = y[s];\n      for (char j = 0; j < cnt[warp][s] - 1; j++) {\n        const ID e1 = adj[warp][s][j];\n        const ID d1 = (s != edges[e1].src) ? edges[e1].src : edges[e1].dst;\n        const ctype x1 = x[d1];\n        const ctype y1 = y[d1];\n        for (char k = j + 1; k < cnt[warp][s]; k++) {\n          const ID e2 = adj[warp][s][k];\n          const ID d2 = (s != edges[e2].src) ? edges[e2].src : edges[e2].dst;\n          const ctype stx = max(min(x0, x1), min(max(x0, x1), x[d2]));\n          const ctype sty = max(min(y0, y1), min(max(y0, y1), y[d2]));\n          const ctype rd = abs(stx - x0) + abs(sty - y0);\n          if (rd > 0) {\n            const ctype rd1 = rd * (MaxPins * 2) + e1;  // tie breaker\n            const ctype rd2 = rd * (MaxPins * 2) + e2;  // tie breaker\n            atomicMax((ctype*)&dist[e1], rd2);\n            atomicMax((ctype*)&dist[e2], rd1);\n          }\n        }\n      }\n    }\n  }\n\n  // process \"triangles\" to find best candidate Steiner points\n  __syncwarp();\n  bool updated = false;\n  for (ID e1 = lane; __any_sync(0xffffffff, e1 < top - 2); e1 += WS) {\n    bool insert = false;\n    ctype stx, sty;\n    if (e1 < top - 2) {\n      const ctype d1 = dist[e1];\n      if (d1 > 0) {\n        const ID e2 = d1 % (MaxPins * 2);\n        if (e2 > e1) {\n          const ctype d2 = dist[e2];\n          if (e1 == d2 % (MaxPins * 2)) {\n            const ctype x0 = x[edges[e1].src];\n            const ctype y0 = y[edges[e1].src];\n            const ctype x1 = x[edges[e1].dst];\n            const ctype y1 = y[edges[e1].dst];\n            ctype x2 = x[edges[e2].src];\n            ctype y2 = y[edges[e2].src];\n            if (((x2 == x0) && (y2 == y0)) || ((x2 == x1) && (y2 == y1))) {\n              x2 = x[edges[e2].dst];\n              y2 = y[edges[e2].dst];\n            }\n            updated = true;\n            insert = true;\n            stx = max(min(x0, x1), min(max(x0, x1), x2));\n            sty = max(min(y0, y1), min(max(y0, y1), y2));\n          }\n        }\n      }\n    }\n    const int bal = __ballot_sync(0xffffffff, insert);\n    const int pos = __popc(bal & ~(-1 << lane)) + num;\n    if (insert) {\n      x[pos] = stx;\n      y[pos] = sty;\n    }\n    num += __popc(bal);\n  }\n\n  return __any_sync(0xffffffff, updated);\n}\n\nstatic __device__\ninline void processLargeNet(const int i,\n                            const int* const __restrict__ idxin,\n                            ctype* const __restrict__ xout,\n                            ctype* const __restrict__ yout,\n                             edge* const __restrict__ edges)\n{\n  __shared__ ctype dist[WarpsPerBlock][PinLimit];\n  const int warp = threadIdx.x / WS;\n\n  const int pin = idxin[i];\n  const ID num = idxin[i + 1] - pin;\n  const int pout = 2 * pin;\n\n  // iterate until all Steiner points added\n  ID cnt = num;\n  do {\n    buildMST<WarpsPerBlock, PinLimit>(cnt, &xout[pout], &yout[pout], &edges[pout], dist[warp]);\n  } while (insertSteinerPoints<WarpsPerBlock, PinLimit>(cnt, &xout[pout], &yout[pout], &edges[pout], dist[warp]));\n}\n\nstatic __global__ __launch_bounds__(WarpsPerBlock * WS, 2)\nvoid smallNetKernel(const int* const __restrict__ idxin,\n                    ctype* __restrict__ xout,\n                    ctype* __restrict__ yout,\n                     edge* __restrict__ edges,\n                      int* const __restrict__ wl)\n{\n  // compute Steiner points and edges\n  const int lane = threadIdx.x % WS;\n  do {\n    int i;\n    if (lane == 0) i = atomicAdd(&currpos2, 1);\n    i = __shfl_sync(0xffffffff, i, 0);\n    if (i >= wlsize) break;\n    processLargeNet<WarpsPerBlock, PinLimit>(wl[i], idxin, xout, yout, edges);\n  } while (true);\n}"
        ]
    },
    "f16sp-cuda": {
        "/Users/gbolet/hecbench-roofline/src/f16sp-cuda/kernels.h": [
            "__forceinline__ __device__\nvoid reduceInShared_intrinsics(half2 * const v)\n{\n  int lid = threadIdx.x;\n  #pragma unroll\n  for (int i = NUM_OF_THREADS/2; i >= 1; i = i / 2) {\n    if(lid<i) v[lid] = __hadd2(v[lid], v[lid+i]);\n    __syncthreads();\n  }\n}\n\n__global__\nvoid scalarProductKernel_intrinsics(\n    half2 const *__restrict__ const a,\n    half2 const *__restrict__ const b,\n    float *__restrict__ const results,\n    size_t const size)\n{\n  const int stride = gridDim.x*blockDim.x;\n  __shared__ half2 shArray[NUM_OF_THREADS];\n\n  half2 value = __float2half2_rn(0.f);\n\n  for (int i = threadIdx.x + blockDim.x * blockIdx.x; i < size; i+=stride)\n  {\n    value = __hfma2(a[i], b[i], value);\n  }\n\n  shArray[threadIdx.x] = value;\n  __syncthreads();\n  reduceInShared_intrinsics(shArray);\n\n  if (threadIdx.x == 0)\n  {\n    half2 result = shArray[0];\n    float f_result = __low2float(result) + __high2float(result);\n    atomicAdd(results, f_result);\n  }\n}",
            "__forceinline__ __device__\nvoid reduceInShared_native(float2 * const v)\n{\n  int lid = threadIdx.x;\n  #pragma unroll\n  for (int i = NUM_OF_THREADS/2; i >= 1; i = i / 2) {\n    if(lid<i) {\n      v[lid].x += v[lid+i].x;\n      v[lid].y += v[lid+i].y;\n    }\n    __syncthreads();\n  }\n}\n\n__global__\nvoid scalarProductKernel_native_fp32(\n    half2 const *__restrict__ const a,\n    half2 const *__restrict__ const b,\n    float *__restrict__ const results,\n    size_t const size)\n{\n  const int stride = gridDim.x*blockDim.x;\n  __shared__ float2 shArray[NUM_OF_THREADS];\n\n  float2 value = {0.f, 0.f};\n\n  for (int i = threadIdx.x + blockDim.x * blockIdx.x; i < size; i+=stride)\n  {\n    value.x += (float)a[i].x * (float)b[i].x;\n    value.y += (float)a[i].y * (float)b[i].y;\n  }\n\n  shArray[threadIdx.x] = value;\n  __syncthreads();\n  reduceInShared_native(shArray);\n\n  if (threadIdx.x == 0)\n  {\n    float2 result = shArray[0];\n    atomicAdd(results, result.x + result.y);\n  }\n}",
            "#define NUM_OF_THREADS 128\n\n\n__global__\nvoid scalarProductKernel_native2_fp32(\n    half2 const *__restrict__ const a,\n    half2 const *__restrict__ const b,\n    float *__restrict__ const results,\n    size_t const size)\n{\n  const int stride = gridDim.x*blockDim.x;\n\n  typedef cub::BlockReduce<float, NUM_OF_THREADS> BlockReduce;\n  __shared__ typename BlockReduce::TempStorage temp_storage;\n\n  float2 value = {0.f, 0.f};\n\n  for (int i = threadIdx.x + blockDim.x * blockIdx.x; i < size; i+=stride)\n  {\n    value.x += (float)a[i].x * (float)b[i].x;\n    value.y += (float)a[i].y * (float)b[i].y;\n  }\n\n  value.x = BlockReduce(temp_storage).Sum(value.x + value.y);\n\n  if (threadIdx.x == 0)\n  {\n    atomicAdd(results, value.x);\n  }\n}"
        ]
    },
    "miniDGS-cuda": {
        "/Users/gbolet/hecbench-roofline/src/miniDGS-cuda/src/MaxwellsKernel3d.cu": [
            "__global__ void MaxwellsGPU_VOL_Kernel3D(float *g_rhsQ){\n\n  /* fastest */\n  __shared__ float s_Q[p_Nfields*BSIZE];\n  __shared__ float s_facs[12];\n\n  const int n = threadIdx.x;\n  const int k = blockIdx.x;\n  \n  /* \"coalesced\"  */\n  int m = n+k*p_Nfields*BSIZE;\n  int id = n;\n  s_Q[id] = tex1Dfetch(t_Q, m); m+=BSIZE; id+=BSIZE;\n  s_Q[id] = tex1Dfetch(t_Q, m); m+=BSIZE; id+=BSIZE;\n  s_Q[id] = tex1Dfetch(t_Q, m); m+=BSIZE; id+=BSIZE;\n  s_Q[id] = tex1Dfetch(t_Q, m); m+=BSIZE; id+=BSIZE;\n  s_Q[id] = tex1Dfetch(t_Q, m); m+=BSIZE; id+=BSIZE;\n  s_Q[id] = tex1Dfetch(t_Q, m); \n\n  if(p_Np<12 && n==0)\n    for(m=0;m<12;++m)\n      s_facs[m] = tex1Dfetch(t_vgeo, 12*k+m);\n  else if(n<12 && p_Np>=12)\n    s_facs[n] = tex1Dfetch(t_vgeo, 12*k+n);\n\n  __syncthreads();\n\n  float dHxdr=0,dHxds=0,dHxdt=0;\n  float dHydr=0,dHyds=0,dHydt=0;\n  float dHzdr=0,dHzds=0,dHzdt=0;\n  float dExdr=0,dExds=0,dExdt=0;\n  float dEydr=0,dEyds=0,dEydt=0;\n  float dEzdr=0,dEzds=0,dEzdt=0;\n  float Q;\n\n  for(m=0;p_Np-m;){\n    float4 D = tex1Dfetch(t_DrDsDt, n+m*BSIZE);\n\n    id = m;\n    Q = s_Q[id]; dHxdr += D.x*Q; dHxds += D.y*Q; dHxdt += D.z*Q; id += BSIZE;\n    Q = s_Q[id]; dHydr += D.x*Q; dHyds += D.y*Q; dHydt += D.z*Q; id += BSIZE;\n    Q = s_Q[id]; dHzdr += D.x*Q; dHzds += D.y*Q; dHzdt += D.z*Q; id += BSIZE;\n    Q = s_Q[id]; dExdr += D.x*Q; dExds += D.y*Q; dExdt += D.z*Q; id += BSIZE;\n    Q = s_Q[id]; dEydr += D.x*Q; dEyds += D.y*Q; dEydt += D.z*Q; id += BSIZE;\n    Q = s_Q[id]; dEzdr += D.x*Q; dEzds += D.y*Q; dEzdt += D.z*Q; \n\n    ++m;\n#if ( (p_Np) % 2 )==0\n    D = tex1Dfetch(t_DrDsDt, n+m*BSIZE);\n\n    id = m;\n    Q = s_Q[id]; dHxdr += D.x*Q; dHxds += D.y*Q; dHxdt += D.z*Q; id += BSIZE;\n    Q = s_Q[id]; dHydr += D.x*Q; dHyds += D.y*Q; dHydt += D.z*Q; id += BSIZE;\n    Q = s_Q[id]; dHzdr += D.x*Q; dHzds += D.y*Q; dHzdt += D.z*Q; id += BSIZE;\n    Q = s_Q[id]; dExdr += D.x*Q; dExds += D.y*Q; dExdt += D.z*Q; id += BSIZE;\n    Q = s_Q[id]; dEydr += D.x*Q; dEyds += D.y*Q; dEydt += D.z*Q; id += BSIZE;\n    Q = s_Q[id]; dEzdr += D.x*Q; dEzds += D.y*Q; dEzdt += D.z*Q; \n\n    ++m;\n\n#if ( (p_Np)%3 )==0\n    D = tex1Dfetch(t_DrDsDt, n+m*BSIZE);\n\n    id = m;\n    Q = s_Q[id]; dHxdr += D.x*Q; dHxds += D.y*Q; dHxdt += D.z*Q; id += BSIZE;\n    Q = s_Q[id]; dHydr += D.x*Q; dHyds += D.y*Q; dHydt += D.z*Q; id += BSIZE;\n    Q = s_Q[id]; dHzdr += D.x*Q; dHzds += D.y*Q; dHzdt += D.z*Q; id += BSIZE;\n    Q = s_Q[id]; dExdr += D.x*Q; dExds += D.y*Q; dExdt += D.z*Q; id += BSIZE;\n    Q = s_Q[id]; dEydr += D.x*Q; dEyds += D.y*Q; dEydt += D.z*Q; id += BSIZE;\n    Q = s_Q[id]; dEzdr += D.x*Q; dEzds += D.y*Q; dEzdt += D.z*Q; \n\n    ++m;\n#endif\n#endif\n  }\n  \n  const float drdx= s_facs[0];\n  const float drdy= s_facs[1];\n  const float drdz= s_facs[2];\n  const float dsdx= s_facs[4];\n  const float dsdy= s_facs[5];\n  const float dsdz= s_facs[6];\n  const float dtdx= s_facs[8];\n  const float dtdy= s_facs[9];\n  const float dtdz= s_facs[10];\n  \n  m = n+p_Nfields*BSIZE*k;\n\n  g_rhsQ[m] = -(drdy*dEzdr+dsdy*dEzds+dtdy*dEzdt - drdz*dEydr-dsdz*dEyds-dtdz*dEydt); m += BSIZE;\n  g_rhsQ[m] = -(drdz*dExdr+dsdz*dExds+dtdz*dExdt - drdx*dEzdr-dsdx*dEzds-dtdx*dEzdt); m += BSIZE;\n  g_rhsQ[m] = -(drdx*dEydr+dsdx*dEyds+dtdx*dEydt - drdy*dExdr-dsdy*dExds-dtdy*dExdt); m += BSIZE;\n  g_rhsQ[m] =  (drdy*dHzdr+dsdy*dHzds+dtdy*dHzdt - drdz*dHydr-dsdz*dHyds-dtdz*dHydt); m += BSIZE;\n  g_rhsQ[m] =  (drdz*dHxdr+dsdz*dHxds+dtdz*dHxdt - drdx*dHzdr-dsdx*dHzds-dtdx*dHzdt); m += BSIZE;\n  g_rhsQ[m] =  (drdx*dHydr+dsdx*dHyds+dtdx*dHydt - drdy*dHxdr-dsdy*dHxds-dtdy*dHxdt); \n}",
            "__global__ void MaxwellsGPU_SURF_Kernel3D(float *g_rhsQ)\n{\n\n  __shared__ float s_fluxQ[p_Nfields*p_Nfp*p_Nfaces];\n\n  const int n = threadIdx.x;\n  const int k = blockIdx.x;\n  int m;\n\n  /* grab surface nodes and store flux in shared memory */\n  if(n< (p_Nfp*p_Nfaces) ){\n    /* coalesced reads (maybe) */\n    m = 7*(k*p_Nfp*p_Nfaces)+n;\n    const  int idM   = tex1Dfetch(t_surfinfo, m); m += p_Nfp*p_Nfaces;\n           int idP   = tex1Dfetch(t_surfinfo, m); m += p_Nfp*p_Nfaces;\n    const  float Fsc = tex1Dfetch(t_surfinfo, m); m += p_Nfp*p_Nfaces;\n    const  float Bsc = tex1Dfetch(t_surfinfo, m); m += p_Nfp*p_Nfaces;\n    const  float nx  = tex1Dfetch(t_surfinfo, m); m += p_Nfp*p_Nfaces;\n    const  float ny  = tex1Dfetch(t_surfinfo, m); m += p_Nfp*p_Nfaces;\n    const  float nz  = tex1Dfetch(t_surfinfo, m);\n\n    /* check if idP<0  */\n    double dHx, dHy, dHz, dEx, dEy, dEz;\n    if(idP<0){\n      idP = p_Nfields*(-1-idP);\n      \n      dHx = Fsc*(tex1Dfetch(t_partQ, idP+0) - tex1Dfetch(t_Q, idM+0*BSIZE));\n      dHy = Fsc*(tex1Dfetch(t_partQ, idP+1) - tex1Dfetch(t_Q, idM+1*BSIZE));\n      dHz = Fsc*(tex1Dfetch(t_partQ, idP+2) - tex1Dfetch(t_Q, idM+2*BSIZE));\n      \n      dEx = Fsc*(tex1Dfetch(t_partQ, idP+3) - tex1Dfetch(t_Q, idM+3*BSIZE));\n      dEy = Fsc*(tex1Dfetch(t_partQ, idP+4) - tex1Dfetch(t_Q, idM+4*BSIZE));\n      dEz = Fsc*(tex1Dfetch(t_partQ, idP+5) - tex1Dfetch(t_Q, idM+5*BSIZE));\n    }\n    else{\n      dHx = Fsc*(tex1Dfetch(t_Q, idP+0*BSIZE) - tex1Dfetch(t_Q, idM+0*BSIZE));\n      dHy = Fsc*(tex1Dfetch(t_Q, idP+1*BSIZE) - tex1Dfetch(t_Q, idM+1*BSIZE));\n      dHz = Fsc*(tex1Dfetch(t_Q, idP+2*BSIZE) - tex1Dfetch(t_Q, idM+2*BSIZE));\n      \n      dEx = Fsc*(Bsc*tex1Dfetch(t_Q, idP+3*BSIZE) - tex1Dfetch(t_Q, idM+3*BSIZE));\n      dEy = Fsc*(Bsc*tex1Dfetch(t_Q, idP+4*BSIZE) - tex1Dfetch(t_Q, idM+4*BSIZE));\n      dEz = Fsc*(Bsc*tex1Dfetch(t_Q, idP+5*BSIZE) - tex1Dfetch(t_Q, idM+5*BSIZE));\n    }\n\n    const double ndotdH = nx*dHx + ny*dHy + nz*dHz;\n    const double ndotdE = nx*dEx + ny*dEy + nz*dEz;\n\n    m = n;\n    s_fluxQ[m] = -ny*dEz + nz*dEy + dHx - ndotdH*nx; m += p_Nfp*p_Nfaces;\n    s_fluxQ[m] = -nz*dEx + nx*dEz + dHy - ndotdH*ny; m += p_Nfp*p_Nfaces;\n    s_fluxQ[m] = -nx*dEy + ny*dEx + dHz - ndotdH*nz; m += p_Nfp*p_Nfaces;\n\n    s_fluxQ[m] =  ny*dHz - nz*dHy + dEx - ndotdE*nx; m += p_Nfp*p_Nfaces;\n    s_fluxQ[m] =  nz*dHx - nx*dHz + dEy - ndotdE*ny; m += p_Nfp*p_Nfaces;\n    s_fluxQ[m] =  nx*dHy - ny*dHx + dEz - ndotdE*nz; \n  }\n\n  /* make sure all element data points are cached */\n  __syncthreads();\n\n  if(n< (p_Np))\n  {\n    float rhsHx = 0, rhsHy = 0, rhsHz = 0;\n    float rhsEx = 0, rhsEy = 0, rhsEz = 0;\n    \n    int sk = n;\n    /* can manually unroll to 4 because there are 4 faces */\n    for(m=0;p_Nfaces*p_Nfp-m;){\n      const float4 L = tex1Dfetch(t_LIFT, sk); sk+=p_Np;\n\n      /* broadcast */\n      int sk1 = m;\n      rhsHx += L.x*s_fluxQ[sk1]; sk1 += p_Nfp*p_Nfaces;\n      rhsHy += L.x*s_fluxQ[sk1]; sk1 += p_Nfp*p_Nfaces;\n      rhsHz += L.x*s_fluxQ[sk1]; sk1 += p_Nfp*p_Nfaces;\n      rhsEx += L.x*s_fluxQ[sk1]; sk1 += p_Nfp*p_Nfaces;\n      rhsEy += L.x*s_fluxQ[sk1]; sk1 += p_Nfp*p_Nfaces;\n      rhsEz += L.x*s_fluxQ[sk1]; sk1 += p_Nfp*p_Nfaces;\n      ++m;\n\n      /* broadcast */\n      sk1 = m;\n      rhsHx += L.y*s_fluxQ[sk1]; sk1 += p_Nfp*p_Nfaces;\n      rhsHy += L.y*s_fluxQ[sk1]; sk1 += p_Nfp*p_Nfaces;\n      rhsHz += L.y*s_fluxQ[sk1]; sk1 += p_Nfp*p_Nfaces;\n      rhsEx += L.y*s_fluxQ[sk1]; sk1 += p_Nfp*p_Nfaces;\n      rhsEy += L.y*s_fluxQ[sk1]; sk1 += p_Nfp*p_Nfaces;\n      rhsEz += L.y*s_fluxQ[sk1]; sk1 += p_Nfp*p_Nfaces;\n      ++m;\n\n      /* broadcast */\n      sk1 = m;\n      rhsHx += L.z*s_fluxQ[sk1]; sk1 += p_Nfp*p_Nfaces;\n      rhsHy += L.z*s_fluxQ[sk1]; sk1 += p_Nfp*p_Nfaces;\n      rhsHz += L.z*s_fluxQ[sk1]; sk1 += p_Nfp*p_Nfaces;\n      rhsEx += L.z*s_fluxQ[sk1]; sk1 += p_Nfp*p_Nfaces;\n      rhsEy += L.z*s_fluxQ[sk1]; sk1 += p_Nfp*p_Nfaces;\n      rhsEz += L.z*s_fluxQ[sk1]; sk1 += p_Nfp*p_Nfaces;\n      ++m;\n\n      /* broadcast */\n      sk1 = m;\n      rhsHx += L.w*s_fluxQ[sk1]; sk1 += p_Nfp*p_Nfaces;\n      rhsHy += L.w*s_fluxQ[sk1]; sk1 += p_Nfp*p_Nfaces;\n      rhsHz += L.w*s_fluxQ[sk1]; sk1 += p_Nfp*p_Nfaces;\n      rhsEx += L.w*s_fluxQ[sk1]; sk1 += p_Nfp*p_Nfaces;\n      rhsEy += L.w*s_fluxQ[sk1]; sk1 += p_Nfp*p_Nfaces;\n      rhsEz += L.w*s_fluxQ[sk1]; sk1 += p_Nfp*p_Nfaces;\n      ++m;\n\n    }\n    \n    m = n+p_Nfields*k*BSIZE;\n    g_rhsQ[m] += rhsHx; m += BSIZE;\n    g_rhsQ[m] += rhsHy; m += BSIZE;\n    g_rhsQ[m] += rhsHz; m += BSIZE;\n    g_rhsQ[m] += rhsEx; m += BSIZE;\n    g_rhsQ[m] += rhsEy; m += BSIZE;\n    g_rhsQ[m] += rhsEz; m += BSIZE;\n\n  }\n}",
            "__global__ void MaxwellsGPU_RK_Kernel3D(\n  int Ntotal,\n        float *__restrict__ g_resQ,\n  const float *__restrict__ g_rhsQ,\n        float *__restrict__ g_Q,\n  float fa, float fb, float fdt){\n  \n  int n = blockIdx.x * blockDim.x + threadIdx.x;\n    \n  if(n<Ntotal){\n    float rhs = g_rhsQ[n];\n    float res = g_resQ[n];\n    res = fa*res + fdt*rhs;\n    \n    g_resQ[n] = res;\n    g_Q[n]    += fb*res;\n  }\n\n}",
            "__global__ void partial_get_kernel3d(int Ntotal, int *g_index, float *g_partQ){\n  \n  int n = blockIdx.x * blockDim.x + threadIdx.x;\n    \n  if(n<Ntotal)\n    g_partQ[n] = tex1Dfetch(t_Q, g_index[n]);\n  \n}"
        ]
    },
    "feynman-kac-cuda": {
        "/Users/gbolet/hecbench-roofline/src/feynman-kac-cuda/kernel.h": [
            "__device__\ndouble potential ( double a, double b, double x, double y )\n\n/*\n  Purpose:\n\n    POTENTIAL evaluates the potential function V(X,Y,Z).\n\n  Licensing:\n\n    This code is distributed under the GNU LGPL license. \n\n  Modified:\n\n    19 February 2008\n\n  Author:\n\n    John Burkardt\n\n  Parameters:\n\n    Input, double A, B, the parameters that define the ellipse.\n\n    Input, double X, Y, the coordinates of the point.\n\n    Output, double POTENTIAL, the value of the potential function at (X,Y).\n*/\n{\n  double value;\n\n  value = 2.0 * ( pow ( x / a / a, 2.0 )\n                + pow ( y / b / b, 2.0 ) )\n              + 1.0 / a / a\n              + 1.0 / b / b;\n\n  return value;\n}\n\n__device__\ndouble r8_uniform_01 ( int *seed )\n\n/*\n  Purpose:\n\n    R8_UNIFORM_01 returns a unit pseudorandom R8.\n\n  Discussion:\n\n    This routine implements the recursion\n\n      seed = 16807 * seed mod ( 2^31 - 1 )\n      r8_uniform_01 = seed / ( 2^31 - 1 )\n\n    The integer arithmetic never requires more than 32 bits,\n    including a sign bit.\n\n    If the initial seed is 12345, then the first three computations are\n\n      Input     Output      R8_UNIFORM_01\n      SEED      SEED\n\n         12345   207482415  0.096616\n     207482415  1790989824  0.833995\n    1790989824  2035175616  0.947702\n\n  Licensing:\n\n    This code is distributed under the GNU LGPL license. \n\n  Modified:\n\n    11 August 2004\n\n  Author:\n\n    John Burkardt\n\n  Reference:\n\n    Paul Bratley, Bennett Fox, Linus Schrage,\n    A Guide to Simulation,\n    Springer Verlag, pages 201-202, 1983.\n\n    Pierre L'Ecuyer,\n    Random Number Generation,\n    in Handbook of Simulation\n    edited by Jerry Banks,\n    Wiley Interscience, page 95, 1998.\n\n    Bennett Fox,\n    Algorithm 647:\n    Implementation and Relative Efficiency of Quasirandom\n    Sequence Generators,\n    ACM Transactions on Mathematical Software,\n    Volume 12, Number 4, pages 362-376, 1986.\n\n    Peter Lewis, Allen Goodman, James Miller,\n    A Pseudo-Random Number Generator for the System/360,\n    IBM Systems Journal,\n    Volume 8, pages 136-143, 1969.\n\n  Parameters:\n\n    Input/output, int *SEED, the \"seed\" value.  Normally, this\n    value should not be 0.  On output, SEED has been updated.\n\n    Output, double R8_UNIFORM_01, a new pseudorandom variate, strictly between\n    0 and 1.\n*/\n{\n  int k;\n  double r;\n\n  k = *seed / 127773;\n\n  *seed = 16807 * ( *seed - k * 127773 ) - k * 2836;\n\n  if ( *seed < 0 )\n  {\n    *seed = *seed + 2147483647;\n  }\n/*\n  Although SEED can be represented exactly as a 32 bit integer,\n  it generally cannot be represented exactly as a 32 bit real number!\n*/\n  r = ( double ) ( *seed ) * 4.656612875E-10;\n\n  return r;\n}\n\n__global__ void fk (\n    const int ni,\n    const int nj,\n          int seed,\n    const int N,\n    const double a,\n    const double b,\n    const double h,\n    const double rth,\n    int *__restrict__ n_inside,\n    double *__restrict__ err)\n{\n  int i = blockIdx.x * blockDim.x + threadIdx.x + 1;\n  int j = blockIdx.y * blockDim.y + threadIdx.y + 1;\n  if (i <= ni && j <= nj) {\n    double x = ( ( double ) ( nj - j     ) * ( - a )\n               + ( double ) (      j - 1 ) *     a )\n               / ( double ) ( nj     - 1 );\n\n    double y = ( ( double ) ( ni - i     ) * ( - b )\n               + ( double ) (      i - 1 ) *     b ) \n               / ( double ) ( ni     - 1 );\n\n    double dx;\n    double dy;\n    double us;\n    double ut;\n    double vh;\n    double vs;\n    double x1;\n    double x2;\n    double w;\n    double w_exact;\n    double we;\n    double wt;\n    double chk = pow ( x / a, 2.0 ) + pow ( y / b, 2.0 );\n\n    if ( 1.0 < chk )\n    {\n      w_exact = 1.0;\n      wt = 1.0;\n    }\n    else {\n      atomicAdd(n_inside, 1);\n      w_exact = exp ( pow ( x / a, 2.0 ) + pow ( y / b, 2.0 ) - 1.0 );\n      wt = 0.0;\n      for ( int k = 0; k < N; k++ )\n      {\n        x1 = x;\n        x2 = y;\n        w = 1.0;  \n        chk = 0.0;\n        while ( chk < 1.0 )\n        {\n          ut = r8_uniform_01 ( &seed );\n          if ( ut < 1.0 / 2.0 )\n          {\n            us = r8_uniform_01 ( &seed ) - 0.5;\n            if ( us < 0.0)\n              dx = - rth;\n            else\n              dx = rth;\n          } \n          else\n          {\n            dx = 0.0;\n          }\n\n          ut = r8_uniform_01 ( &seed );\n          if ( ut < 1.0 / 2.0 )\n          {\n            us = r8_uniform_01 ( &seed ) - 0.5;\n            if ( us < 0.0 )\n              dy = - rth;\n            else\n              dy = rth;\n          }\n          else\n          {\n            dy = 0.0;\n          }\n          vs = potential ( a, b, x1, x2 );\n          x1 = x1 + dx;\n          x2 = x2 + dy;\n\n          vh = potential ( a, b, x1, x2 );\n\n          we = ( 1.0 - h * vs ) * w;\n          w = w - 0.5 * h * ( vh * we + vs * w ); \n\n          chk = pow ( x1 / a, 2.0 ) + pow ( x2 / b, 2.0 );\n        }\n        wt += w;\n      }\n      wt /= ( double ) ( N ); \n      atomicAdd(err, pow ( w_exact - wt, 2.0 ));\n    }\n  }\n}"
        ]
    },
    "rtm8-cuda": {
        "/Users/gbolet/hecbench-roofline/src/rtm8-cuda/rtm8.cu": [
            "inline __host__ __device__ int indexTo1D(int x, int y, int z){\n  return x + y*nx + z*nx*ny;\n}\n\n__global__\nvoid rtm8(\n  const float*__restrict__ vsq,\n  const float*__restrict__ current_s,\n  const float*__restrict__ current_r,\n        float*__restrict__ next_s,\n        float*__restrict__ next_r,\n        float*__restrict__ image,\n  const float*__restrict__ a,\n  size_t N)\n{\n  unsigned x = blockIdx.x * blockDim.x + threadIdx.x;\n  unsigned y = blockIdx.y * blockDim.y + threadIdx.y;\n  unsigned z = blockIdx.z * blockDim.z + threadIdx.z;\n  float div;\n  if ((4 <= x && x < (nx - 4) ) && (4 <= y && y < (ny - 4)) && (4 <= z && z < (nz - 4))){\n    div =\n      a[0] * current_s[indexTo1D(x,y,z)] +\n      a[1] * (current_s[indexTo1D(x+1,y,z)] + current_s[indexTo1D(x-1,y,z)] +\n          current_s[indexTo1D(x,y+1,z)] + current_s[indexTo1D(x,y-1,z)] +\n          current_s[indexTo1D(x,y,z+1)] + current_s[indexTo1D(x,y,z-1)]) +\n      a[2] * (current_s[indexTo1D(x+2,y,z)] + current_s[indexTo1D(x-2,y,z)] +\n          current_s[indexTo1D(x,y+2,z)] + current_s[indexTo1D(x,y-2,z)] +\n          current_s[indexTo1D(x,y,z+2)] + current_s[indexTo1D(x,y,z-2)]) +\n      a[3] * (current_s[indexTo1D(x+3,y,z)] + current_s[indexTo1D(x-3,y,z)] +\n          current_s[indexTo1D(x,y+3,z)] + current_s[indexTo1D(x,y-3,z)] +\n          current_s[indexTo1D(x,y,z+3)] + current_s[indexTo1D(x,y,z-3)]) +\n      a[4] * (current_s[indexTo1D(x+4,y,z)] + current_s[indexTo1D(x-4,y,z)] +\n          current_s[indexTo1D(x,y+4,z)] + current_s[indexTo1D(x,y-4,z)] +\n          current_s[indexTo1D(x,y,z+4)] + current_s[indexTo1D(x,y,z-4)]);\n\n    next_s[indexTo1D(x,y,z)] = 2*current_s[indexTo1D(x,y,z)] - next_s[indexTo1D(x,y,z)]\n      + vsq[indexTo1D(x,y,z)]*div;\n    div =\n      a[0] * current_r[indexTo1D(x,y,z)] +\n      a[1] * (current_r[indexTo1D(x+1,y,z)] + current_r[indexTo1D(x-1,y,z)] +\n          current_r[indexTo1D(x,y+1,z)] + current_r[indexTo1D(x,y-1,z)] +\n          current_r[indexTo1D(x,y,z+1)] + current_r[indexTo1D(x,y,z-1)]) +\n      a[2] * (current_r[indexTo1D(x+2,y,z)] + current_r[indexTo1D(x-2,y,z)] +\n          current_r[indexTo1D(x,y+2,z)] + current_r[indexTo1D(x,y-2,z)] +\n          current_r[indexTo1D(x,y,z+2)] + current_r[indexTo1D(x,y,z-2)]) +\n      a[3] * (current_r[indexTo1D(x+3,y,z)] + current_r[indexTo1D(x-3,y,z)] +\n          current_r[indexTo1D(x,y+3,z)] + current_r[indexTo1D(x,y-3,z)] +\n          current_r[indexTo1D(x,y,z+3)] + current_r[indexTo1D(x,y,z-3)]) +\n      a[4] * (current_r[indexTo1D(x+4,y,z)] + current_r[indexTo1D(x-4,y,z)] +\n          current_r[indexTo1D(x,y+4,z)] + current_r[indexTo1D(x,y-4,z)] +\n          current_r[indexTo1D(x,y,z+4)] + current_r[indexTo1D(x,y,z-4)]);\n\n    next_r[indexTo1D(x,y,z)] = 2 * current_r[indexTo1D(x,y,z)]\n      - next_r[indexTo1D(x,y,z)] + vsq[indexTo1D(x,y,z)] * div;\n\n    image[indexTo1D(x,y,z)] = next_s[indexTo1D(x,y,z)] * next_r[indexTo1D(x,y,z)];\n  }\n}"
        ]
    },
    "lsqt-cuda": {
        "/Users/gbolet/hecbench-roofline/src/lsqt-cuda/vector.cu": [
            "__global__ void gpu_set_zero(int number_of_elements, \n  real* __restrict__ g_state_real, \n  real* __restrict__ g_state_imag)\n{\n  int n = blockIdx.x * blockDim.x + threadIdx.x;\n  if (n < number_of_elements) {\n    g_state_real[n] = 0;\n    g_state_imag[n] = 0;\n  }\n}",
            "__global__ void gpu_copy_state(\n  const int N,\n  const real* __restrict__ in_real,\n  const real* __restrict__ in_imag, \n        real* __restrict__ out_real, \n        real* __restrict__ out_imag)\n{\n  int n = blockIdx.x * blockDim.x + threadIdx.x;\n  if (n < N) {\n    out_real[n] = in_real[n];\n    out_imag[n] = in_imag[n];\n  }\n}",
            "__global__ void gpu_add_state(\n  const int n, \n  const real*__restrict__ in_real,\n  const real*__restrict__ in_imag, \n        real*__restrict__ out_real, \n        real*__restrict__ out_imag)\n{\n  int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < n) {\n    out_real[i] += in_real[i];\n    out_imag[i] += in_imag[i];\n  }\n}",
            "__global__ void gpu_apply_sz(\n  const int n, \n  const real* __restrict__ in_real, \n  const real* __restrict__ in_imag, \n        real* __restrict__ out_real, \n        real* __restrict__ out_imag)\n{\n  int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < n) {\n    if (i % 2 == 0) {\n      out_real[i] = in_real[i];\n      out_imag[i] = in_imag[i];\n    } else {\n      out_real[i] = -in_real[i];\n      out_imag[i] = -in_imag[i];\n    }\n  }\n}",
            "#define T ((int)32)\n\n\n__device__ __forceinline__ T\nWARP_SHFL_XOR_NATIVE(T value, int laneMask, int width = warpSize,\n                     unsigned int mask = 0xffffffff) {\n  return __shfl_xor_sync(mask, value, laneMask, width);\n}\n\n__device__ __forceinline__ void warp_reduce(acc_t *sum) {\n  ReduceOp<acc_t> r;\n#pragma unroll\n  for (int offset = WARP_SIZE / 2; offset > 0; offset /= 2) {\n#pragma unroll\n    for (int i = 0; i < WARP_BATCH; ++i) {\n      acc_t b = WARP_SHFL_XOR_NATIVE(sum[i], offset, WARP_SIZE);\n      sum[i] = r(sum[i], b);\n    }\n  }\n}\n\n__global__ void gpu_find_inner_product_1(\n  const int number_of_atoms,\n  const real* __restrict__ g_final_state_real,\n  const real* __restrict__ g_final_state_imag,\n  const real* __restrict__ g_random_state_real,\n  const real* __restrict__ g_random_state_imag,\n        real* __restrict__ g_inner_product_real,\n        real* __restrict__ g_inner_product_imag,\n  const int g_offset)\n{\n  int tid = threadIdx.x;\n  int n = blockIdx.x * blockDim.x + tid;\n  int m;\n  real a, b, c, d;\n  __shared__ real s_data_real[BLOCK_SIZE];\n  __shared__ real s_data_imag[BLOCK_SIZE];\n  s_data_real[tid] = 0.0;\n  s_data_imag[tid] = 0.0;\n\n  if (n < number_of_atoms) {\n    a = g_final_state_real[n];\n    b = g_final_state_imag[n];\n    c = g_random_state_real[n];\n    d = g_random_state_imag[n];\n    s_data_real[tid] = (a * c + b * d);\n    s_data_imag[tid] = (b * c - a * d);\n  }\n  __syncthreads();\n\n/*\n  if (tid < 256) {\n    m = tid + 256;\n    s_data_real[tid] += s_data_real[m];\n    s_data_imag[tid] += s_data_imag[m];\n  }\n  __syncthreads();\n*/\n\n  if (tid < 128) {\n    m = tid + 128;\n    s_data_real[tid] += s_data_real[m];\n    s_data_imag[tid] += s_data_imag[m];\n  }\n  __syncthreads();\n  if (tid < 64) {\n    m = tid + 64;\n    s_data_real[tid] += s_data_real[m];\n    s_data_imag[tid] += s_data_imag[m];\n  }\n  __syncthreads();\n  if (tid < 32) {\n    warp_reduce(s_data_real, tid);\n    warp_reduce(s_data_imag, tid);\n  }\n  if (tid == 0) {\n    g_inner_product_real[blockIdx.x + g_offset] = s_data_real[0];\n    g_inner_product_imag[blockIdx.x + g_offset] = s_data_imag[0];\n  }\n}",
            "#define T ((int)32)\n\n\n__device__ __forceinline__ T\nWARP_SHFL_XOR_NATIVE(T value, int laneMask, int width = warpSize,\n                     unsigned int mask = 0xffffffff) {\n  return __shfl_xor_sync(mask, value, laneMask, width);\n}\n\n__device__ __forceinline__ void warp_reduce(acc_t *sum) {\n  ReduceOp<acc_t> r;\n#pragma unroll\n  for (int offset = WARP_SIZE / 2; offset > 0; offset /= 2) {\n#pragma unroll\n    for (int i = 0; i < WARP_BATCH; ++i) {\n      acc_t b = WARP_SHFL_XOR_NATIVE(sum[i], offset, WARP_SIZE);\n      sum[i] = r(sum[i], b);\n    }\n  }\n}\n\n__global__ void gpu_find_inner_product_2(\n  const int number_of_atoms,\n  const real* __restrict__ g_inner_product_1_real,\n  const real* __restrict__ g_inner_product_1_imag,\n        real* __restrict__ g_inner_product_2_real,\n        real* __restrict__ g_inner_product_2_imag)\n{\n  //<<<para.number_of_energy_points, BLOCK_SIZE)>>>\n  int tid = threadIdx.x;\n  int patch, n, m;\n\n  __shared__ real s_data_real[BLOCK_SIZE];\n  __shared__ real s_data_imag[BLOCK_SIZE];\n  s_data_real[tid] = 0.0;\n  s_data_imag[tid] = 0.0;\n  int number_of_blocks = (number_of_atoms - 1) / BLOCK_SIZE + 1;\n  int number_of_patches = (number_of_blocks - 1) / BLOCK_SIZE + 1;\n\n  for (patch = 0; patch < number_of_patches; ++patch) {\n    n = tid + patch * BLOCK_SIZE;\n    if (n < number_of_blocks) {\n      m = blockIdx.x * number_of_blocks + n;\n      s_data_real[tid] += g_inner_product_1_real[m];\n      s_data_imag[tid] += g_inner_product_1_imag[m];\n    }\n  }\n  __syncthreads();\n\n/*\n  if (tid < 256) {\n    m = tid + 256;\n    s_data_real[tid] += s_data_real[m];\n    s_data_imag[tid] += s_data_imag[m];\n  }\n  __syncthreads();\n*/\n\n  if (tid < 128) {\n    m = tid + 128;\n    s_data_real[tid] += s_data_real[m];\n    s_data_imag[tid] += s_data_imag[m];\n  }\n  __syncthreads();\n  if (tid < 64) {\n    m = tid + 64;\n    s_data_real[tid] += s_data_real[m];\n    s_data_imag[tid] += s_data_imag[m];\n  }\n  __syncthreads();\n  if (tid < 32) {\n    warp_reduce(s_data_real, tid);\n    warp_reduce(s_data_imag, tid);\n  }\n  if (tid == 0) {\n    g_inner_product_2_real[blockIdx.x] = s_data_real[0];\n    g_inner_product_2_imag[blockIdx.x] = s_data_imag[0];\n  }\n}"
        ],
        "/Users/gbolet/hecbench-roofline/src/lsqt-cuda/hamiltonian.cu": [
            "__global__ void gpu_apply_hamiltonian(\n  const int number_of_atoms,\n  const real energy_max,\n  const  int* __restrict__ g_neighbor_number,\n  const  int* __restrict__ g_neighbor_list,\n  const real* __restrict__ g_potential,\n  const real* __restrict__ g_hopping_real,\n  const real* __restrict__ g_hopping_imag,\n  const real* __restrict__ g_state_in_real,\n  const real* __restrict__ g_state_in_imag,\n        real* __restrict__ g_state_out_real,\n        real* __restrict__ g_state_out_imag)\n{\n  int n = blockIdx.x * blockDim.x + threadIdx.x;\n  if (n < number_of_atoms) {\n    real temp_real = g_potential[n] * g_state_in_real[n]; // on-site\n    real temp_imag = g_potential[n] * g_state_in_imag[n]; // on-site\n\n    for (int m = 0; m < g_neighbor_number[n]; ++m) {\n      int index_1 = m * number_of_atoms + n;\n      int index_2 = g_neighbor_list[index_1];\n      real a = g_hopping_real[index_1];\n      real b = g_hopping_imag[index_1];\n      real c = g_state_in_real[index_2];\n      real d = g_state_in_imag[index_2];\n      temp_real += a * c - b * d; // hopping\n      temp_imag += a * d + b * c; // hopping\n    }\n    temp_real /= energy_max; // scale\n    temp_imag /= energy_max; // scale\n    g_state_out_real[n] = temp_real;\n    g_state_out_imag[n] = temp_imag;\n  }\n}",
            "__global__ void gpu_apply_commutator(\n  int number_of_atoms,\n  real energy_max,\n  int* g_neighbor_number,\n  int* g_neighbor_list,\n  real* g_hopping_real,\n  real* g_hopping_imag,\n  real* g_xx,\n  real* g_state_in_real,\n  real* g_state_in_imag,\n  real* g_state_out_real,\n  real* g_state_out_imag)\n{\n  int n = blockIdx.x * blockDim.x + threadIdx.x;\n  if (n < number_of_atoms) {\n    real temp_real = 0.0;\n    real temp_imag = 0.0;\n    for (int m = 0; m < g_neighbor_number[n]; ++m) {\n      int index_1 = m * number_of_atoms + n;\n      int index_2 = g_neighbor_list[index_1];\n      real a = g_hopping_real[index_1];\n      real b = g_hopping_imag[index_1];\n      real c = g_state_in_real[index_2];\n      real d = g_state_in_imag[index_2];\n      real xx = g_xx[index_1];\n      temp_real -= (a * c - b * d) * xx;\n      temp_imag -= (a * d + b * c) * xx;\n    }\n    g_state_out_real[n] = temp_real / energy_max; // scale\n    g_state_out_imag[n] = temp_imag / energy_max; // scale\n  }\n}",
            "__global__ void gpu_apply_current(\n  const int number_of_atoms,\n  const  int* __restrict__ g_neighbor_number,\n  const  int* __restrict__ g_neighbor_list,\n  const real* __restrict__ g_hopping_real,\n  const real* __restrict__ g_hopping_imag,\n  const real* __restrict__ g_xx,\n  const real* __restrict__ g_state_in_real,\n  const real* __restrict__ g_state_in_imag,\n        real* __restrict__ g_state_out_real,\n        real* __restrict__ g_state_out_imag)\n{\n  int n = blockIdx.x * blockDim.x + threadIdx.x;\n  if (n < number_of_atoms) {\n    real temp_real = 0.0;\n    real temp_imag = 0.0;\n    for (int m = 0; m < g_neighbor_number[n]; ++m) {\n      int index_1 = m * number_of_atoms + n;\n      int index_2 = g_neighbor_list[index_1];\n      real a = g_hopping_real[index_1];\n      real b = g_hopping_imag[index_1];\n      real c = g_state_in_real[index_2];\n      real d = g_state_in_imag[index_2];\n      temp_real += (a * c - b * d) * g_xx[index_1];\n      temp_imag += (a * d + b * c) * g_xx[index_1];\n    }\n    g_state_out_real[n] = +temp_imag;\n    g_state_out_imag[n] = -temp_real;\n  }\n}",
            "__global__ void gpu_chebyshev_01(\n  const int number_of_atoms,\n  const real* __restrict__ g_state_0_real,\n  const real* __restrict__ g_state_0_imag,\n  const real* __restrict__ g_state_1_real,\n  const real* __restrict__ g_state_1_imag,\n        real* __restrict__ g_state_real,\n        real* __restrict__ g_state_imag,\n  const real b0,\n  const real b1,\n  const int direction)\n{\n  int n = blockIdx.x * blockDim.x + threadIdx.x;\n  if (n < number_of_atoms) {\n    real bessel_0 = b0;\n    real bessel_1 = b1 * direction;\n    g_state_real[n] = bessel_0 * g_state_0_real[n] + bessel_1 * g_state_1_imag[n];\n    g_state_imag[n] = bessel_0 * g_state_0_imag[n] - bessel_1 * g_state_1_real[n];\n  }\n}",
            "__global__ void gpu_chebyshev_2(\n  const int number_of_atoms,\n  const real energy_max,\n  const  int* __restrict__ g_neighbor_number,\n  const  int* __restrict__ g_neighbor_list,\n  const real* __restrict__ g_potential,\n  const real* __restrict__ g_hopping_real,\n  const real* __restrict__ g_hopping_imag,\n  const real* __restrict__ g_state_0_real,\n  const real* __restrict__ g_state_0_imag,\n  const real* __restrict__ g_state_1_real,\n  const real* __restrict__ g_state_1_imag,\n        real* __restrict__ g_state_2_real,\n        real* __restrict__ g_state_2_imag,\n        real* __restrict__ g_state_real,\n        real* __restrict__ g_state_imag,\n  const real bessel_m,\n  const int label)\n{\n  int n = blockIdx.x * blockDim.x + threadIdx.x;\n  if (n < number_of_atoms) {\n    real temp_real = g_potential[n] * g_state_1_real[n]; // on-site\n    real temp_imag = g_potential[n] * g_state_1_imag[n]; // on-site\n\n    for (int m = 0; m < g_neighbor_number[n]; ++m) {\n      int index_1 = m * number_of_atoms + n;\n      int index_2 = g_neighbor_list[index_1];\n      real a = g_hopping_real[index_1];\n      real b = g_hopping_imag[index_1];\n      real c = g_state_1_real[index_2];\n      real d = g_state_1_imag[index_2];\n      temp_real += a * c - b * d; // hopping\n      temp_imag += a * d + b * c; // hopping\n    }\n    temp_real /= energy_max; // scale\n    temp_imag /= energy_max; // scale\n\n    temp_real = 2.0 * temp_real - g_state_0_real[n];\n    temp_imag = 2.0 * temp_imag - g_state_0_imag[n];\n    switch (label) {\n      case 1: {\n        g_state_real[n] += bessel_m * temp_real;\n        g_state_imag[n] += bessel_m * temp_imag;\n        break;\n      }\n      case 2: {\n        g_state_real[n] -= bessel_m * temp_real;\n        g_state_imag[n] -= bessel_m * temp_imag;\n        break;\n      }\n      case 3: {\n        g_state_real[n] += bessel_m * temp_imag;\n        g_state_imag[n] -= bessel_m * temp_real;\n        break;\n      }\n      case 4: {\n        g_state_real[n] -= bessel_m * temp_imag;\n        g_state_imag[n] += bessel_m * temp_real;\n        break;\n      }\n    }\n    g_state_2_real[n] = temp_real;\n    g_state_2_imag[n] = temp_imag;\n  }\n}",
            "__global__ void gpu_chebyshev_1x(\n  const int number_of_atoms,\n  const real* __restrict__ g_state_1x_real,\n  const real* __restrict__ g_state_1x_imag,\n        real* __restrict__ g_state_real,\n        real* __restrict__ g_state_imag,\n  const real g_bessel_1)\n{\n  int n = blockIdx.x * blockDim.x + threadIdx.x;\n  if (n < number_of_atoms) {\n    real b1 = g_bessel_1;\n    g_state_real[n] = +b1 * g_state_1x_imag[n];\n    g_state_imag[n] = -b1 * g_state_1x_real[n];\n  }\n}",
            "__global__ void gpu_chebyshev_2x(\n  const int number_of_atoms,\n  const real energy_max,\n  const  int* __restrict__ g_neighbor_number,\n  const  int* __restrict__ g_neighbor_list,\n  const real* __restrict__ g_potential,\n  const real* __restrict__ g_hopping_real,\n  const real* __restrict__ g_hopping_imag,\n  const real* __restrict__ g_xx,\n  const real* __restrict__ g_state_0_real,\n  const real* __restrict__ g_state_0_imag,\n  const real* __restrict__ g_state_0x_real,\n  const real* __restrict__ g_state_0x_imag,\n  const real* __restrict__ g_state_1_real,\n  const real* __restrict__ g_state_1_imag,\n  const real* __restrict__ g_state_1x_real,\n  const real* __restrict__ g_state_1x_imag,\n        real* __restrict__ g_state_2_real,\n        real* __restrict__ g_state_2_imag,\n        real* __restrict__ g_state_2x_real,\n        real* __restrict__ g_state_2x_imag,\n        real* __restrict__ g_state_real,\n        real* __restrict__ g_state_imag,\n  const real g_bessel_m,\n  const int g_label)\n{\n  int n = blockIdx.x * blockDim.x + threadIdx.x;\n  if (n < number_of_atoms) {\n    real temp_real = g_potential[n] * g_state_1_real[n];    // on-site\n    real temp_imag = g_potential[n] * g_state_1_imag[n];    // on-site\n    real temp_x_real = g_potential[n] * g_state_1x_real[n]; // on-site\n    real temp_x_imag = g_potential[n] * g_state_1x_imag[n]; // on-site\n\n    for (int m = 0; m < g_neighbor_number[n]; ++m) {\n      int index_1 = m * number_of_atoms + n;\n      int index_2 = g_neighbor_list[index_1];\n\n      real a = g_hopping_real[index_1];\n      real b = g_hopping_imag[index_1];\n      real c = g_state_1_real[index_2];\n      real d = g_state_1_imag[index_2];\n      temp_real += a * c - b * d; // hopping\n      temp_imag += a * d + b * c; // hopping\n\n      real cx = g_state_1x_real[index_2];\n      real dx = g_state_1x_imag[index_2];\n      temp_x_real += a * cx - b * dx; // hopping\n      temp_x_imag += a * dx + b * cx; // hopping\n\n      real xx = g_xx[index_1];\n      temp_x_real -= (a * c - b * d) * xx; // hopping\n      temp_x_imag -= (a * d + b * c) * xx; // hopping\n    }\n\n    temp_real /= energy_max; // scale\n    temp_imag /= energy_max; // scale\n    temp_real = 2.0 * temp_real - g_state_0_real[n];\n    temp_imag = 2.0 * temp_imag - g_state_0_imag[n];\n    g_state_2_real[n] = temp_real;\n    g_state_2_imag[n] = temp_imag;\n\n    temp_x_real /= energy_max; // scale\n    temp_x_imag /= energy_max; // scale\n    temp_x_real = 2.0 * temp_x_real - g_state_0x_real[n];\n    temp_x_imag = 2.0 * temp_x_imag - g_state_0x_imag[n];\n    g_state_2x_real[n] = temp_x_real;\n    g_state_2x_imag[n] = temp_x_imag;\n\n    real bessel_m = g_bessel_m;\n    switch (g_label) {\n      case 1: {\n        g_state_real[n] += bessel_m * temp_x_real;\n        g_state_imag[n] += bessel_m * temp_x_imag;\n        break;\n      }\n      case 2: {\n        g_state_real[n] -= bessel_m * temp_x_real;\n        g_state_imag[n] -= bessel_m * temp_x_imag;\n        break;\n      }\n      case 3: {\n        g_state_real[n] += bessel_m * temp_x_imag;\n        g_state_imag[n] -= bessel_m * temp_x_real;\n        break;\n      }\n      case 4: {\n        g_state_real[n] -= bessel_m * temp_x_imag;\n        g_state_imag[n] += bessel_m * temp_x_real;\n        break;\n      }\n    }\n  }\n}",
            "__global__ void gpu_kernel_polynomial(\n  const int number_of_atoms,\n  const real energy_max,\n  const  int* __restrict__ g_neighbor_number,\n  const  int* __restrict__ g_neighbor_list,\n  const real* __restrict__ g_potential,\n  const real* __restrict__ g_hopping_real,\n  const real* __restrict__ g_hopping_imag,\n  const real* __restrict__ g_state_0_real,\n  const real* __restrict__ g_state_0_imag,\n  const real* __restrict__ g_state_1_real,\n  const real* __restrict__ g_state_1_imag,\n        real* __restrict__ g_state_2_real,\n        real* __restrict__ g_state_2_imag)\n{\n  int n = blockIdx.x * blockDim.x + threadIdx.x;\n  if (n < number_of_atoms) {\n    real temp_real = g_potential[n] * g_state_1_real[n]; // on-site\n    real temp_imag = g_potential[n] * g_state_1_imag[n]; // on-site\n\n    for (int m = 0; m < g_neighbor_number[n]; ++m) {\n      int index_1 = m * number_of_atoms + n;\n      int index_2 = g_neighbor_list[index_1];\n      real a = g_hopping_real[index_1];\n      real b = g_hopping_imag[index_1];\n      real c = g_state_1_real[index_2];\n      real d = g_state_1_imag[index_2];\n      temp_real += a * c - b * d; // hopping\n      temp_imag += a * d + b * c; // hopping\n    }\n\n    temp_real /= energy_max; // scale\n    temp_imag /= energy_max; // scale\n\n    temp_real = 2.0 * temp_real - g_state_0_real[n];\n    temp_imag = 2.0 * temp_imag - g_state_0_imag[n];\n    g_state_2_real[n] = temp_real;\n    g_state_2_imag[n] = temp_imag;\n  }\n}"
        ]
    },
    "ga-cuda": {
        "/Users/gbolet/hecbench-roofline/src/ga-cuda/main.cu": [
            "__global__ \nvoid ga(const char *__restrict__ target,\n        const char *__restrict__ query,\n              char *__restrict__ batch_result,\n              uint32_t length,\n              int query_sequence_length,\n              int coarse_match_length,\n              int coarse_match_threshold,\n              int current_position)\n{\n  uint tid = blockIdx.x * blockDim.x + threadIdx.x;\n  if (tid > length) return;\n  bool match = false;\n  int max_length = query_sequence_length - coarse_match_length;\n\n  for (int i = 0; i <= max_length; i++) {\n    int distance = 0;\n    for (int j = 0; j < coarse_match_length; j++) {\n      if (target[current_position + tid + j] != query[i + j]) {\n        distance++;\n      }\n    }\n\n    if (distance < coarse_match_threshold) {\n      match = true;\n      break;\n    }\n  }\n  if (match) {\n    batch_result[tid] = 1;\n  }\n}"
        ]
    },
    "layout-cuda": {
        "/Users/gbolet/hecbench-roofline/src/layout-cuda/main.cu": [
            "__global__\nvoid AoSKernel(const AppleTree *__restrict__ trees, \n               int *__restrict__ outBuf)\n{\n  uint gid = blockIdx.x * blockDim.x + threadIdx.x;\n  uint res = 0;\n  for(int i = 0; i < treeSize; i++)\n  {\n    res += trees[gid].apples[i];\n  }\n  outBuf[gid] = res;\n}",
            "__global__\nvoid SoAKernel(const ApplesOnTrees *__restrict__ applesOnTrees,\n               int *__restrict__ outBuf)\n{\n  uint gid = blockIdx.x * blockDim.x + threadIdx.x;\n  uint res = 0;\n  for(int i = 0; i < treeSize; i++)\n  {\n    res += applesOnTrees[i].trees[gid];\n  }\n  outBuf[gid] = res;\n}"
        ]
    },
    "fdtd3d-cuda": {
        "/Users/gbolet/hecbench-roofline/src/fdtd3d-cuda/FDTD3dGPU.cu": [
            "__global__ void finite_difference(\n        float*__restrict__ output,\n  const float*__restrict__ input,\n  const float*__restrict__ coef, \n  const int dimx, const int dimy, const int dimz,\n  const int padding)\n{\n  bool valid = true;\n  const int gtidx = blockIdx.x * blockDim.x + threadIdx.x;\n  const int gtidy = blockIdx.y * blockDim.y + threadIdx.y;\n  const int ltidx = threadIdx.x;\n  const int ltidy = threadIdx.y;\n  const int workx = blockDim.x;\n  const int worky = blockDim.y;\n\n  const int stride_y = dimx + 2 * k_radius_default;\n  const int stride_z = stride_y * (dimy + 2 * k_radius_default);\n\n  __shared__ float tile[k_blockDimMaxY + 2 * k_radius_default][k_blockDimMaxX + 2 * k_radius_default];\n\n  int inputIndex  = 0;\n  int outputIndex = 0;\n\n  // Advance inputIndex to start of inner volume\n  inputIndex += k_radius_default * stride_y + k_radius_default + padding;\n\n  // Advance inputIndex to target element\n  inputIndex += gtidy * stride_y + gtidx;\n\n  float infront[k_radius_default];\n  float behind[k_radius_default];\n  float current;\n\n  const int tx = ltidx + k_radius_default;\n  const int ty = ltidy + k_radius_default;\n\n  if (gtidx >= dimx) valid = false;\n  if (gtidy >= dimy) valid = false;\n\n  // For simplicity we assume that the global size is equal to the actual\n  // problem size; since the global size must be a multiple of the local size\n  // this means the problem size must be a multiple of the local size (or\n  // padded to meet this constraint).\n  // Preload the \"infront\" and \"behind\" data\n  for (int i = k_radius_default - 2 ; i >= 0 ; i--)\n  {\n    behind[i] = input[inputIndex];\n    inputIndex += stride_z;\n  }\n\n  current = input[inputIndex];\n  outputIndex = inputIndex;\n  inputIndex += stride_z;\n\n  for (int i = 0 ; i < k_radius_default ; i++)\n  {\n    infront[i] = input[inputIndex];\n    inputIndex += stride_z;\n  }\n\n  // Step through the xy-planes\n  for (int iz = 0 ; iz < dimz ; iz++)\n  {\n    // Advance the slice (move the thread-front)\n    for (int i = k_radius_default - 1 ; i > 0 ; i--)\n      behind[i] = behind[i - 1];\n    behind[0] = current;\n    current = infront[0];\n    for (int i = 0 ; i < k_radius_default - 1 ; i++)\n      infront[i] = infront[i + 1];\n    infront[k_radius_default - 1] = input[inputIndex];\n\n    inputIndex  += stride_z;\n    outputIndex += stride_z;\n    __syncthreads();\n\n      // Note that for the work items on the boundary of the problem, the\n      // supplied index when reading the halo (below) may wrap to the\n      // previous/next row or even the previous/next xy-plane. This is\n      // acceptable since a) we disable the output write for these work\n      // items and b) there is at least one xy-plane before/after the\n      // current plane, so the access will be within bounds.\n\n      // Update the data slice in the local tile\n      // Halo above & below\n      if (ltidy < k_radius_default)\n      {\n        tile[ltidy][tx]                  = input[outputIndex - k_radius_default * stride_y];\n        tile[ltidy + worky + k_radius_default][tx] = input[outputIndex + worky * stride_y];\n      }\n    // Halo left & right\n    if (ltidx < k_radius_default)\n    {\n      tile[ty][ltidx]                  = input[outputIndex - k_radius_default];\n      tile[ty][ltidx + workx + k_radius_default] = input[outputIndex + workx];\n    }\n    tile[ty][tx] = current;\n    __syncthreads();\n\n      // Compute the output value\n      float value = coef[0] * current;\n    for (int i = 1 ; i <= k_radius_default ; i++)\n    {\n      value += coef[i] * (infront[i-1] + behind[i-1] + tile[ty - i][tx] + \n          tile[ty + i][tx] + tile[ty][tx - i] + tile[ty][tx + i]);\n    }\n\n    // Store the output value\n    if (valid) output[outputIndex] = value;\n  }\n}"
        ]
    },
    "fpc-cuda": {
        "/Users/gbolet/hecbench-roofline/src/fpc-cuda/main.cu": [
            "__host__ __device__\nunsigned my_abs ( int x )\n{\n  unsigned t = x >> 31;\n  return (x ^ t) - t;\n}\n\n__global__ void\nfpc_kernel (const ulong* values, unsigned *cmp_size)\n{\n  __shared__ unsigned compressable;\n  int lid = threadIdx.x;\n  int WGS = blockDim.x;\n  int gid = blockIdx.x*WGS+lid;\n\n  ulong value = values[gid];\n  unsigned inc;\n\n  // 000\n  if (value == 0){\n    inc = 1;\n  }\n  // 001 010\n  else if ((my_abs((int)(value)) <= 0xFF)) {\n    inc = 1;\n  }\n  // 011\n  else if ((my_abs((int)(value)) <= 0xFFFF)) {\n    inc = 2;\n  }\n  //100  \n  else if ((((value) & 0xFFFF) == 0 )) {\n    inc = 2;\n  }\n  //101\n  else if ((my_abs((int)((value) & 0xFFFF))) <= 0xFF\n      && my_abs((int)((value >> 16) & 0xFFFF)) <= 0xFF ) {\n    inc = 2;\n  }\n  //110\n  else if( (((value) & 0xFF) == ((value >> 8) & 0xFF)) &&\n      (((value) & 0xFF) == ((value >> 16) & 0xFF)) &&\n      (((value) & 0xFF) == ((value >> 24) & 0xFF)) ) {\n    inc = 1;\n  } else { \n    inc = 4;\n  }\n\n  if (lid == 0) compressable = 0;\n  __syncthreads();\n\n  atomicAdd(&compressable, inc);\n  __syncthreads();\n  if (lid == WGS-1) {\n    atomicAdd(cmp_size, compressable);\n  }\n}",
            "__host__ __device__\nunsigned my_abs ( int x )\n{\n  unsigned t = x >> 31;\n  return (x ^ t) - t;\n}\n\n__device__\nunsigned f1(ulong value, bool* mask) {\n  if (value == 0) {\n    *mask = 1;\n  } \n  return 1;\n}\n\n__device__\nunsigned f2(ulong value, bool* mask) {\n  if (my_abs((int)(value)) <= 0xFF) *mask = 1;\n  return 1;\n}\n\n__device__\nunsigned f3(ulong value, bool* mask) {\n  if (my_abs((int)(value)) <= 0xFFFF) *mask = 1;\n  return 2;\n}\n\n__device__\nunsigned f4(ulong value, bool* mask) {\n  if (((value) & 0xFFFF) == 0 ) *mask = 1;\n  return 2;\n}\n\n__device__\nunsigned f5(ulong value, bool* mask) {\n  if ((my_abs((int)((value) & 0xFFFF))) <= 0xFF && \n      my_abs((int)((value >> 16) & 0xFFFF)) <= 0xFF) \n    *mask = 1;\n  return 2;\n}\n\n__device__\nunsigned f6(ulong value, bool* mask) {\n  unsigned byte0 = (value) & 0xFF;\n  unsigned byte1 = (value >> 8) & 0xFF;\n  unsigned byte2 = (value >> 16) & 0xFF;\n  unsigned byte3 = (value >> 24) & 0xFF;\n  if (byte0 == byte1 && byte0 == byte2 && byte0 == byte3) \n    *mask = 1;\n  return 1;\n}\n\n__device__\nunsigned f7(ulong value, bool* mask) {\n  *mask = 1;\n  return 4;\n}\n\n__global__ void\nfpc2_kernel (const ulong* values, unsigned *cmp_size)\n{\n  __shared__ unsigned compressable;\n  int lid = threadIdx.x;\n  int WGS = blockDim.x;\n  int gid = blockIdx.x*WGS+lid;\n\n  unsigned inc;\n\n  bool m1 = 0;\n  bool m2 = 0;\n  bool m3 = 0;\n  bool m4 = 0;\n  bool m5 = 0;\n  bool m6 = 0;\n  bool m7 = 0;\n\n  ulong value = values[gid];\n  unsigned inc1 = f1(value, &m1);\n  unsigned inc2 = f2(value, &m2);\n  unsigned inc3 = f3(value, &m3);\n  unsigned inc4 = f4(value, &m4);\n  unsigned inc5 = f5(value, &m5);\n  unsigned inc6 = f6(value, &m6);\n  unsigned inc7 = f7(value, &m7);\n\n  if (m1)\n    inc = inc1;\n  else if (m2)\n    inc = inc2;\n  else if (m3)\n    inc = inc3;\n  else if (m4)\n    inc = inc4;\n  else if (m5)\n    inc = inc5;\n  else if (m6)\n    inc = inc6;\n  else\n    inc = inc7;\n\n\n  if (lid == 0) compressable = 0;\n  __syncthreads();\n\n  atomicAdd(&compressable, inc);\n  __syncthreads();\n  if (lid == WGS-1) {\n    atomicAdd(cmp_size, compressable);\n  }\n}"
        ]
    },
    "thomas-cuda": {
        "/Users/gbolet/hecbench-roofline/src/thomas-cuda/cuThomasBatch.cu": [
            "__global__ void cuThomasBatch(\n            const double *L, const double *D, double *U, double *RHS,\n            const int M,\n            const int BATCHCOUNT\n    ) {\n\n        int tid = threadIdx.x + blockDim.x*blockIdx.x;\n\n        if(tid < BATCHCOUNT) {\n\n            int first = tid;\n            int last  = BATCHCOUNT*(M-1)+tid;\n\n            U[first] /= D[first];\n            RHS[first] /= D[first];\n\n            for (int i = first + BATCHCOUNT; i < last; i+=BATCHCOUNT) {\n                U[i] /= D[i] - L[i] * U[i-BATCHCOUNT];\n                RHS[i] = ( RHS[i] - L[i] * RHS[i-BATCHCOUNT] ) / \n\t\t\t\t\t\t\t( D[i] - L[i] * U[i-BATCHCOUNT] );\n            }\n\n            RHS[last] = ( RHS[last] - L[last] * RHS[last-BATCHCOUNT] ) / \n\t\t\t\t\t\t\t( D[last] - L[last] * U[last-BATCHCOUNT] );\n\n            for (int i = last-BATCHCOUNT; i >= first; i-=BATCHCOUNT) {\n                RHS[i] -= U[i] * RHS[i+BATCHCOUNT];\n            }\n       }\n        \n}"
        ]
    },
    "sampling-cuda": {
        "/Users/gbolet/hecbench-roofline/src/sampling-cuda/kernels.cu": [
            "__global__\nvoid exact_rows_kernel(\n  float*__restrict__ X,\n  const IdxT nrows_X,\n  const IdxT ncols,\n  const DataT*__restrict__ background,\n  const IdxT nrows_background,\n  DataT*__restrict__ dataset,\n  const DataT*__restrict__ observation)\n{\n  // Each block processes one row of X. Columns are iterated over by blockDim.x at a time to ensure data coelescing\n  int col = threadIdx.x;\n  int row = blockIdx.x * ncols;\n\n  while (col < ncols) {\n    // Load the X idx for the current column\n    int curr_X = (int)X[row + col];\n\n    // Iterate over nrows_background\n    for (int row_idx = blockIdx.x * nrows_background;\n         row_idx < blockIdx.x * nrows_background + nrows_background;\n         row_idx += 1) {\n      if (curr_X == 0) {\n        dataset[row_idx * ncols + col] =\n          background[(row_idx % nrows_background) * ncols + col];\n      } else {\n        dataset[row_idx * ncols + col] = observation[col];\n      }\n    }\n    // Increment the column\n    col += blockDim.x;\n  }\n}",
            "__device__\ndouble LCG_random_double(uint64_t * seed)\n{\n  const uint64_t m = 9223372036854775808ULL; // 2^63\n  const uint64_t a = 2806196910506780709ULL;\n  const uint64_t c = 1ULL;\n  *seed = (a * (*seed) + c) % m;\n  return (double) (*seed) / (double) m;\n}\n\n__global__\nvoid sampled_rows_kernel(\n  const IdxT*__restrict__ nsamples,\n  float*__restrict__ X,\n  const IdxT nrows_X,\n  const IdxT ncols,\n  DataT*__restrict__ background,\n  const IdxT nrows_background,\n  DataT*__restrict__ dataset,\n  const DataT*__restrict__ observation,\n  uint64_t seed)\n{\n  // int tid = threadIdx.x + blockIdx.x * blockDim.x;\n  // see what k this block will generate\n  int k_blk = nsamples[blockIdx.x];\n\n  // First k threads of block generate samples\n  if (threadIdx.x < k_blk) {\n    int rand_idx = (int)(LCG_random_double(&seed) * ncols);\n\n    // Since X is initialized to 0, we quickly check for collisions (if k_blk << ncols the likelyhood of collisions is low)\n    while (atomicExch(&(X[2 * blockIdx.x * ncols + rand_idx]), 1) == 1) {\n      rand_idx = (int)(LCG_random_double(&seed) * ncols);\n    }\n  }\n  __syncthreads();\n\n  // Each block processes one row of X. Columns are iterated over by blockDim.x at a time to ensure data coelescing\n  int col_idx = threadIdx.x;\n  while (col_idx < ncols) {\n    // Load the X idx for the current column\n    int curr_X = (int)X[2 * blockIdx.x * ncols + col_idx];\n    X[(2 * blockIdx.x + 1) * ncols + col_idx] = 1 - curr_X;\n\n    for (int bg_row_idx = 2 * blockIdx.x * nrows_background;\n         bg_row_idx < 2 * blockIdx.x * nrows_background + nrows_background;\n         bg_row_idx += 1) {\n      if (curr_X == 0) {\n        dataset[bg_row_idx * ncols + col_idx] =\n          background[(bg_row_idx % nrows_background) * ncols + col_idx];\n      } else {\n        dataset[bg_row_idx * ncols + col_idx] = observation[col_idx];\n      }\n    }\n\n    for (int bg_row_idx = (2 * blockIdx.x + 1) * nrows_background;\n         bg_row_idx <\n         (2 * blockIdx.x + 1) * nrows_background + nrows_background;\n         bg_row_idx += 1) {\n      if (curr_X == 0) {\n        dataset[bg_row_idx * ncols + col_idx] = observation[col_idx];\n      } else {\n        // if(threadIdx.x == 0) printf(\"tid bg_row_idx: %d %d\\n\", tid, bg_row_idx);\n        dataset[bg_row_idx * ncols + col_idx] =\n          background[(bg_row_idx) % nrows_background * ncols + col_idx];\n      }\n    }\n\n    col_idx += blockDim.x;\n  }\n}"
        ]
    },
    "ans-cuda": {
        "/Users/gbolet/hecbench-roofline/src/ans-cuda/src/multians_gpu_decoder.cu": [
            "#define STATE_TYPE std::uint32_t\n\n\n#define SYMBOL_TYPE std::uint8_t\n\n\n#define UNIT_TYPE std::uint32_t\n\n\n__device__ __forceinline__ void decode_subsequence(\n    std::uint32_t subsequence_size,\n    std::uint32_t current_subsequence,\n    std::uint32_t subsequences_processed,\n    UNIT_TYPE mask,\n    std::uint32_t shift,\n    std::uint32_t start_bit,\n    std::uint32_t &in_pos,\n    UNIT_TYPE* in_ptr,\n    UNIT_TYPE &window,\n    UNIT_TYPE &next,\n    STATE_TYPE &state,\n    std::uint32_t &last_word_unit,\n    std::uint32_t &last_word_bit,\n    std::uint32_t &num_symbols,\n    std::uint32_t &out_pos,\n    SYMBOL_TYPE* out_ptr,\n    std::uint32_t &next_out_pos,\n    const uint* __restrict__ table,\n    const std::uint32_t bits_in_unit,\n    const std::uint32_t number_of_states,\n    std::uint32_t &last_at,\n    STATE_TYPE &last_state,\n    bool overflow,\n    bool write_output) {\n\n      // current unit in this subsequence\n      std::uint32_t current_unit = 0;\n\n      // current bit position in unit\n      std::uint32_t at = start_bit;\n\n      // number of symbols found in this subsequence\n      std::uint32_t num_symbols_l = 0;\n\n      UNIT_TYPE copy_next = next;\n\n      auto load_next = [&]() {\n        window = in_ptr[in_pos];\n        next = in_ptr[in_pos + 1];\n        copy_next = next;\n      };\n\n      // shift to start\n      if(current_subsequence == 0 || subsequences_processed == 0) {\n\n        copy_next <<= bits_in_unit - at;\n\n        next >>= at;\n        window >>= at;\n        window |= copy_next;\n      }\n\n      // perform overflow from previous subsequence\n      if(overflow && current_subsequence > 0 && subsequences_processed == 0) {\n\n        // decode first symbol\n        const uint hit_p = table[state - number_of_states];\n        const STATE_TYPE next_state_p = (std::uint16_t) (hit_p & 0x0000FFFF);\n        std::uint32_t taken = hit_p >> 24;\n\n        state = (next_state_p << taken) | (~(mask << taken) & window);\n\n        while(state < number_of_states) {\n          std::uint32_t shift_w = window >> taken;\n          ++taken;\n          state = (state << 1) | (~(mask << 1) & shift_w);\n        }\n\n        if(at == 0) {\n          ++num_symbols_l;\n\n          if(write_output) {\n            if(out_pos < next_out_pos) {\n              out_ptr[out_pos] = (hit_p & ((std::uint32_t) 0x00FF0000)) >> 16;\n              ++out_pos;\n            }\n          }\n        }\n\n        if(taken > 0) {\n          copy_next = next;\n          copy_next <<= bits_in_unit - taken;\n        }\n\n        else copy_next = 0;\n\n        next >>= taken;\n        window >>= taken;\n        at += taken;\n        window |= copy_next;\n\n        // overflow\n        if(at > bits_in_unit) {\n          ++in_pos;\n          load_next();\n          at -= bits_in_unit;\n          window >>= at;\n          next >>= at;\n\n          copy_next <<= bits_in_unit - at;\n          window |= copy_next;\n        }\n\n        if(at == bits_in_unit) {\n          ++in_pos;\n          load_next();\n          at = 0;\n        }\n      }\n\n      while(current_unit < subsequence_size) {\n\n        while(at < bits_in_unit) {\n\n          last_state = state;\n\n          const uint hit = table[state - number_of_states];\n\n          // decode a symbol\n          const STATE_TYPE next_state = (std::uint16_t) (hit & 0x0000FFFF);\n          std::uint32_t taken = hit >> 24;\n\n          state = (next_state << taken) | (~(mask << taken) & window);\n\n          while(state < number_of_states) {\n            shift = window >> taken;\n            ++taken;\n            state = (state << 1) | (~(mask << 1) & shift);\n          }\n\n          ++num_symbols_l;\n\n          if(write_output) {\n            if(out_pos < next_out_pos) {\n              out_ptr[out_pos] = (hit & ((std::uint32_t) 0x00FF0000)) >> 16;\n              ++out_pos;\n            }\n          }\n\n          if(taken > 0) {\n            copy_next = next;\n            copy_next <<= bits_in_unit - taken;\n          }\n\n          else copy_next = 0;\n\n          next >>= taken;\n          window >>= taken;\n          last_word_bit = at;\n          at += taken;\n          window |= copy_next;\n          last_word_unit = current_unit;\n        }\n\n        // refill decoder window if necessary\n        ++current_unit;\n        ++in_pos;\n\n        load_next();\n\n        if(at == bits_in_unit) {\n          at = 0;\n        }\n\n        else {\n          at -= bits_in_unit;\n          window >>= at;\n          next >>= at;\n\n          copy_next <<= bits_in_unit - at;\n          window |= copy_next;\n        }\n      }\n\n      num_symbols = num_symbols_l;\n      last_at = at;\n    }\n\n__global__ void phase1_decode_subseq(\n    std::uint32_t subsequence_size,\n    std::uint32_t total_num_subsequences,\n    std::uint32_t table_size,\n    UNIT_TYPE* in_ptr,\n    const uint* __restrict__ table,\n    uint4* sync_points,\n    const std::uint32_t bits_in_unit,\n    const std::uint32_t number_of_states,\n    const STATE_TYPE initial_state,\n    const std::uint32_t initial_bit) {\n\n  const std::uint32_t gid = blockDim.x * blockIdx.x + threadIdx.x;\n\n  if(gid * 4 < total_num_subsequences) {\n    std::uint32_t current_subsequence = gid * 4;\n    std::uint32_t in_pos = gid * subsequence_size * 4;\n\n    // mask\n    const UNIT_TYPE mask = (UNIT_TYPE) (0) - 1;\n\n    // shift right\n    const std::uint32_t shift = bits_in_unit - table_size;\n\n    std::uint32_t out_pos = 0;\n    std::uint32_t next_out_pos = 0;\n    std::uint8_t* out_ptr = 0;\n\n    // current state\n    STATE_TYPE state = initial_state;\n\n    // sliding window\n    UNIT_TYPE window = in_ptr[in_pos];\n    UNIT_TYPE next = in_ptr[in_pos + 1];\n\n    // start bit of last codeword in this subsequence\n    std::uint32_t last_word_unit = 0;\n    std::uint32_t last_word_bit = 0;\n\n    // last state in this subsequence\n    STATE_TYPE last_state;\n\n    // number of symbols found in this subsequence\n    std::uint32_t num_symbols = 0;\n\n    // bit position of next codeword\n    std::uint32_t last_at = (gid == 0) ?\n      bits_in_unit - initial_bit : 0;\n\n    std::uint32_t subsequences_processed = 0;\n    bool synchronised_flag = false;\n\n    std::uint32_t last_subsequence = blockDim.x * (blockIdx.x + 1) * 4;\n    if(last_subsequence > total_num_subsequences)\n      last_subsequence = total_num_subsequences;\n\n    auto sync = [&](std::uint32_t i) {\n      if(subsequences_processed >= 4) {\n        uint4 sync_point = sync_points[current_subsequence + i];\n\n        if(sync_point.x == last_word_unit\n            && sync_point.y == last_word_bit\n            && sync_point.w == last_state) {\n          synchronised_flag = true;\n        }\n      }\n    };\n\n    uint4 s0, s1, s2, s3;\n    bool wrt0 = false;\n    bool wrt1 = false;\n    bool wrt2 = false;\n    bool wrt3 = false;\n\n    while(subsequences_processed < blockDim.x * 4) {\n\n      if(!synchronised_flag\n          && current_subsequence < last_subsequence) {\n\n        decode_subsequence(subsequence_size, current_subsequence,\n            subsequences_processed,\n            mask, shift, last_at, in_pos, in_ptr,\n            window, next,\n            state, last_word_unit, last_word_bit, num_symbols,\n            out_pos, out_ptr, next_out_pos, table, bits_in_unit,\n            number_of_states, last_at, last_state, false, false);\n\n        sync(0);\n        s0 = {last_word_unit, last_word_bit, num_symbols, last_state};\n        wrt0 = true;\n      }\n\n      if(!synchronised_flag\n          && current_subsequence < last_subsequence) {\n\n        decode_subsequence(subsequence_size, current_subsequence + 1,\n            subsequences_processed + 1,\n            mask, shift, last_at, in_pos, in_ptr,\n            window, next,\n            state, last_word_unit, last_word_bit, num_symbols,\n            out_pos, out_ptr, next_out_pos, table, bits_in_unit,\n            number_of_states, last_at, last_state, false, false);\n\n        sync(1);\n        s1 = {last_word_unit, last_word_bit, num_symbols, last_state};\n        wrt1 = true;\n      }\n\n      if(!synchronised_flag\n          && current_subsequence < last_subsequence) {\n\n        decode_subsequence(subsequence_size, current_subsequence + 2,\n            subsequences_processed + 2,\n            mask, shift, last_at, in_pos, in_ptr,\n            window, next,\n            state, last_word_unit, last_word_bit, num_symbols,\n            out_pos, out_ptr, next_out_pos, table, bits_in_unit,\n            number_of_states, last_at, last_state, false, false);\n\n        sync(2);\n        s2 = {last_word_unit, last_word_bit, num_symbols, last_state};\n        wrt2 = true;\n      }\n\n      if(!synchronised_flag\n          && current_subsequence < last_subsequence) {\n\n        decode_subsequence(subsequence_size, current_subsequence + 3,\n            subsequences_processed + 3,\n            mask, shift, last_at, in_pos, in_ptr,\n            window, next,\n            state, last_word_unit, last_word_bit, num_symbols,\n            out_pos, out_ptr, next_out_pos, table, bits_in_unit,\n            number_of_states, last_at, last_state, false, false);\n\n        sync(3);\n        s3 = {last_word_unit, last_word_bit, num_symbols, last_state};\n        wrt3 = true;\n      }\n\n      if(wrt0) {\n        sync_points[current_subsequence] = s0;\n        wrt0 = false;\n      }\n\n      if(wrt1) {\n        sync_points[current_subsequence + 1] = s1;\n        wrt1 = false;\n      }\n\n      if(wrt2) {\n        sync_points[current_subsequence + 2] = s2;\n        wrt2 = false;\n      }\n\n      if(wrt3) {\n        sync_points[current_subsequence + 3] = s3;\n        wrt3 = false;\n      }\n\n      current_subsequence += 4;\n      subsequences_processed += 4;\n\n      __syncthreads();\n    }\n  }\n}",
            "#define STATE_TYPE std::uint32_t\n\n\n#define SYMBOL_TYPE std::uint8_t\n\n\n#define UNIT_TYPE std::uint32_t\n\n\n__device__ __forceinline__ void decode_subsequence(\n    std::uint32_t subsequence_size,\n    std::uint32_t current_subsequence,\n    std::uint32_t subsequences_processed,\n    UNIT_TYPE mask,\n    std::uint32_t shift,\n    std::uint32_t start_bit,\n    std::uint32_t &in_pos,\n    UNIT_TYPE* in_ptr,\n    UNIT_TYPE &window,\n    UNIT_TYPE &next,\n    STATE_TYPE &state,\n    std::uint32_t &last_word_unit,\n    std::uint32_t &last_word_bit,\n    std::uint32_t &num_symbols,\n    std::uint32_t &out_pos,\n    SYMBOL_TYPE* out_ptr,\n    std::uint32_t &next_out_pos,\n    const uint* __restrict__ table,\n    const std::uint32_t bits_in_unit,\n    const std::uint32_t number_of_states,\n    std::uint32_t &last_at,\n    STATE_TYPE &last_state,\n    bool overflow,\n    bool write_output) {\n\n      // current unit in this subsequence\n      std::uint32_t current_unit = 0;\n\n      // current bit position in unit\n      std::uint32_t at = start_bit;\n\n      // number of symbols found in this subsequence\n      std::uint32_t num_symbols_l = 0;\n\n      UNIT_TYPE copy_next = next;\n\n      auto load_next = [&]() {\n        window = in_ptr[in_pos];\n        next = in_ptr[in_pos + 1];\n        copy_next = next;\n      };\n\n      // shift to start\n      if(current_subsequence == 0 || subsequences_processed == 0) {\n\n        copy_next <<= bits_in_unit - at;\n\n        next >>= at;\n        window >>= at;\n        window |= copy_next;\n      }\n\n      // perform overflow from previous subsequence\n      if(overflow && current_subsequence > 0 && subsequences_processed == 0) {\n\n        // decode first symbol\n        const uint hit_p = table[state - number_of_states];\n        const STATE_TYPE next_state_p = (std::uint16_t) (hit_p & 0x0000FFFF);\n        std::uint32_t taken = hit_p >> 24;\n\n        state = (next_state_p << taken) | (~(mask << taken) & window);\n\n        while(state < number_of_states) {\n          std::uint32_t shift_w = window >> taken;\n          ++taken;\n          state = (state << 1) | (~(mask << 1) & shift_w);\n        }\n\n        if(at == 0) {\n          ++num_symbols_l;\n\n          if(write_output) {\n            if(out_pos < next_out_pos) {\n              out_ptr[out_pos] = (hit_p & ((std::uint32_t) 0x00FF0000)) >> 16;\n              ++out_pos;\n            }\n          }\n        }\n\n        if(taken > 0) {\n          copy_next = next;\n          copy_next <<= bits_in_unit - taken;\n        }\n\n        else copy_next = 0;\n\n        next >>= taken;\n        window >>= taken;\n        at += taken;\n        window |= copy_next;\n\n        // overflow\n        if(at > bits_in_unit) {\n          ++in_pos;\n          load_next();\n          at -= bits_in_unit;\n          window >>= at;\n          next >>= at;\n\n          copy_next <<= bits_in_unit - at;\n          window |= copy_next;\n        }\n\n        if(at == bits_in_unit) {\n          ++in_pos;\n          load_next();\n          at = 0;\n        }\n      }\n\n      while(current_unit < subsequence_size) {\n\n        while(at < bits_in_unit) {\n\n          last_state = state;\n\n          const uint hit = table[state - number_of_states];\n\n          // decode a symbol\n          const STATE_TYPE next_state = (std::uint16_t) (hit & 0x0000FFFF);\n          std::uint32_t taken = hit >> 24;\n\n          state = (next_state << taken) | (~(mask << taken) & window);\n\n          while(state < number_of_states) {\n            shift = window >> taken;\n            ++taken;\n            state = (state << 1) | (~(mask << 1) & shift);\n          }\n\n          ++num_symbols_l;\n\n          if(write_output) {\n            if(out_pos < next_out_pos) {\n              out_ptr[out_pos] = (hit & ((std::uint32_t) 0x00FF0000)) >> 16;\n              ++out_pos;\n            }\n          }\n\n          if(taken > 0) {\n            copy_next = next;\n            copy_next <<= bits_in_unit - taken;\n          }\n\n          else copy_next = 0;\n\n          next >>= taken;\n          window >>= taken;\n          last_word_bit = at;\n          at += taken;\n          window |= copy_next;\n          last_word_unit = current_unit;\n        }\n\n        // refill decoder window if necessary\n        ++current_unit;\n        ++in_pos;\n\n        load_next();\n\n        if(at == bits_in_unit) {\n          at = 0;\n        }\n\n        else {\n          at -= bits_in_unit;\n          window >>= at;\n          next >>= at;\n\n          copy_next <<= bits_in_unit - at;\n          window |= copy_next;\n        }\n      }\n\n      num_symbols = num_symbols_l;\n      last_at = at;\n    }\n\n__global__ void phase2_synchronise_blocks(\n    std::uint32_t subsequence_size,\n    std::uint32_t total_num_subsequences,\n    std::uint32_t table_size,\n    std::uint32_t num_blocks,\n    UNIT_TYPE* in_ptr,\n    const uint* __restrict__ table,\n    uint4* sync_points,\n    SYMBOL_TYPE* block_synchronised,\n    const std::uint32_t bits_in_unit,\n    const std::uint32_t number_of_states,\n    const STATE_TYPE initial_state) {\n\n  const std::uint32_t gid = blockIdx.x;\n  const std::uint32_t num_of_seams = num_blocks - 1;\n\n  if(gid < num_of_seams) {\n\n    // mask\n    const UNIT_TYPE mask = (UNIT_TYPE) (0) - 1;\n\n    // shift\n    const std::uint32_t shift = bits_in_unit - table_size;\n\n    std::uint32_t out_pos = 0;\n    std::uint32_t next_out_pos = 0;\n    std::uint8_t* out_ptr = 0;\n\n    // jump to first sequence of the block\n    std::uint32_t current_subsequence = (gid + 1) * blockDim.x;\n\n    // search for synchronised sequences at the end of previous block\n    uint4 sync_point = sync_points[current_subsequence - 1];\n\n    // current unit\n    std::uint32_t in_pos = (current_subsequence - 1) * subsequence_size;\n\n    // start bit of last codeword in this subsequence\n    std::uint32_t last_word_unit = in_pos + sync_point.x;\n    std::uint32_t last_word_bit = sync_point.y;\n\n    // state\n    STATE_TYPE state = sync_point.w;\n\n    // last state in this subsequence\n    STATE_TYPE last_state;\n\n    // number of symbols found in this subsequence\n    std::uint32_t num_symbols = 0;\n\n    std::uint32_t last_at = sync_point.y;\n\n    in_pos += sync_point.x;\n\n    // sliding window\n    UNIT_TYPE window = in_ptr[in_pos];\n    UNIT_TYPE next = in_ptr[in_pos + 1];\n\n    std::uint32_t subsequences_processed = 0;\n    bool synchronised_flag = false;\n\n    while(subsequences_processed < blockDim.x) {\n\n      if(!synchronised_flag) {\n        decode_subsequence(subsequence_size, current_subsequence,\n            subsequences_processed,\n            mask, shift, last_at, in_pos, in_ptr, \n            window, next, state,\n            last_word_unit, last_word_bit, num_symbols,\n            out_pos, out_ptr, next_out_pos, table, bits_in_unit,\n            number_of_states, last_at, last_state, true, false);\n\n        sync_point = sync_points[current_subsequence];\n\n        // if sync point detected\n        if(sync_point.x == last_word_unit\n            && sync_point.y == last_word_bit\n            && sync_point.w == last_state) {\n          sync_point.z = num_symbols;\n\n          block_synchronised[gid + 1] = 1;\n          synchronised_flag = true;\n        }\n\n        // correct erroneous position data\n        else {\n          sync_point.x = last_word_unit;\n          sync_point.y = last_word_bit;\n          sync_point.z = num_symbols;\n          sync_point.w = last_state;\n\n          block_synchronised[gid + 1] = 0;\n        }\n\n        sync_points[current_subsequence] = sync_point;\n      }\n\n      ++current_subsequence;\n      ++subsequences_processed;\n\n      __syncthreads();\n    }\n  }\n}",
            "__global__ void phase3_copy_num_symbols_from_sync_points_to_aux(\n    std::uint32_t total_num_subsequences,\n    const uint4* __restrict__ sync_points,\n    std::uint32_t* subsequence_output_sizes) {\n\n  const std::uint32_t gid = blockDim.x * blockIdx.x + threadIdx.x;\n\n  if(gid < total_num_subsequences) {\n    subsequence_output_sizes[gid] = sync_points[gid].z;\n  }\n}",
            "__global__ void phase3_copy_num_symbols_from_aux_to_sync_points(\n    std::uint32_t total_num_subsequences,\n    uint4* sync_points,\n    const std::uint32_t* __restrict__ subsequence_output_sizes) {\n\n  const std::uint32_t gid = blockDim.x * blockIdx.x + threadIdx.x;\n\n  if(gid < total_num_subsequences) {\n    sync_points[gid].z = subsequence_output_sizes[gid];\n  }\n}",
            "#define STATE_TYPE std::uint32_t\n\n\n#define SYMBOL_TYPE std::uint8_t\n\n\n#define UNIT_TYPE std::uint32_t\n\n\n__device__ __forceinline__ void decode_subsequence(\n    std::uint32_t subsequence_size,\n    std::uint32_t current_subsequence,\n    std::uint32_t subsequences_processed,\n    UNIT_TYPE mask,\n    std::uint32_t shift,\n    std::uint32_t start_bit,\n    std::uint32_t &in_pos,\n    UNIT_TYPE* in_ptr,\n    UNIT_TYPE &window,\n    UNIT_TYPE &next,\n    STATE_TYPE &state,\n    std::uint32_t &last_word_unit,\n    std::uint32_t &last_word_bit,\n    std::uint32_t &num_symbols,\n    std::uint32_t &out_pos,\n    SYMBOL_TYPE* out_ptr,\n    std::uint32_t &next_out_pos,\n    const uint* __restrict__ table,\n    const std::uint32_t bits_in_unit,\n    const std::uint32_t number_of_states,\n    std::uint32_t &last_at,\n    STATE_TYPE &last_state,\n    bool overflow,\n    bool write_output) {\n\n      // current unit in this subsequence\n      std::uint32_t current_unit = 0;\n\n      // current bit position in unit\n      std::uint32_t at = start_bit;\n\n      // number of symbols found in this subsequence\n      std::uint32_t num_symbols_l = 0;\n\n      UNIT_TYPE copy_next = next;\n\n      auto load_next = [&]() {\n        window = in_ptr[in_pos];\n        next = in_ptr[in_pos + 1];\n        copy_next = next;\n      };\n\n      // shift to start\n      if(current_subsequence == 0 || subsequences_processed == 0) {\n\n        copy_next <<= bits_in_unit - at;\n\n        next >>= at;\n        window >>= at;\n        window |= copy_next;\n      }\n\n      // perform overflow from previous subsequence\n      if(overflow && current_subsequence > 0 && subsequences_processed == 0) {\n\n        // decode first symbol\n        const uint hit_p = table[state - number_of_states];\n        const STATE_TYPE next_state_p = (std::uint16_t) (hit_p & 0x0000FFFF);\n        std::uint32_t taken = hit_p >> 24;\n\n        state = (next_state_p << taken) | (~(mask << taken) & window);\n\n        while(state < number_of_states) {\n          std::uint32_t shift_w = window >> taken;\n          ++taken;\n          state = (state << 1) | (~(mask << 1) & shift_w);\n        }\n\n        if(at == 0) {\n          ++num_symbols_l;\n\n          if(write_output) {\n            if(out_pos < next_out_pos) {\n              out_ptr[out_pos] = (hit_p & ((std::uint32_t) 0x00FF0000)) >> 16;\n              ++out_pos;\n            }\n          }\n        }\n\n        if(taken > 0) {\n          copy_next = next;\n          copy_next <<= bits_in_unit - taken;\n        }\n\n        else copy_next = 0;\n\n        next >>= taken;\n        window >>= taken;\n        at += taken;\n        window |= copy_next;\n\n        // overflow\n        if(at > bits_in_unit) {\n          ++in_pos;\n          load_next();\n          at -= bits_in_unit;\n          window >>= at;\n          next >>= at;\n\n          copy_next <<= bits_in_unit - at;\n          window |= copy_next;\n        }\n\n        if(at == bits_in_unit) {\n          ++in_pos;\n          load_next();\n          at = 0;\n        }\n      }\n\n      while(current_unit < subsequence_size) {\n\n        while(at < bits_in_unit) {\n\n          last_state = state;\n\n          const uint hit = table[state - number_of_states];\n\n          // decode a symbol\n          const STATE_TYPE next_state = (std::uint16_t) (hit & 0x0000FFFF);\n          std::uint32_t taken = hit >> 24;\n\n          state = (next_state << taken) | (~(mask << taken) & window);\n\n          while(state < number_of_states) {\n            shift = window >> taken;\n            ++taken;\n            state = (state << 1) | (~(mask << 1) & shift);\n          }\n\n          ++num_symbols_l;\n\n          if(write_output) {\n            if(out_pos < next_out_pos) {\n              out_ptr[out_pos] = (hit & ((std::uint32_t) 0x00FF0000)) >> 16;\n              ++out_pos;\n            }\n          }\n\n          if(taken > 0) {\n            copy_next = next;\n            copy_next <<= bits_in_unit - taken;\n          }\n\n          else copy_next = 0;\n\n          next >>= taken;\n          window >>= taken;\n          last_word_bit = at;\n          at += taken;\n          window |= copy_next;\n          last_word_unit = current_unit;\n        }\n\n        // refill decoder window if necessary\n        ++current_unit;\n        ++in_pos;\n\n        load_next();\n\n        if(at == bits_in_unit) {\n          at = 0;\n        }\n\n        else {\n          at -= bits_in_unit;\n          window >>= at;\n          next >>= at;\n\n          copy_next <<= bits_in_unit - at;\n          window |= copy_next;\n        }\n      }\n\n      num_symbols = num_symbols_l;\n      last_at = at;\n    }\n\n__global__ void phase4_decode_write_output(\n    std::uint32_t subsequence_size,\n    std::uint32_t total_num_subsequences,\n    std::uint32_t table_size,\n    UNIT_TYPE* in_ptr,\n    SYMBOL_TYPE* out_ptr,\n    std::uint32_t output_size,\n    const uint* __restrict__ table,\n    const uint4* __restrict__ sync_points,\n    const std::uint32_t bits_in_unit,\n    const std::uint32_t number_of_states,\n    const STATE_TYPE initial_state,\n    const std::uint32_t initial_bit) {\n\n  const std::uint32_t gid = blockDim.x * blockIdx.x + threadIdx.x;\n\n  if(gid < total_num_subsequences) {\n\n    // mask\n    const UNIT_TYPE mask = (UNIT_TYPE) (0) - 1;\n\n    // shift\n    const size_t shift = bits_in_unit - table_size;\n\n    // start bit of last codeword in this subsequence\n    std::uint32_t last_word_unit = 0;\n    std::uint32_t last_word_bit = 0;\n\n    // state\n    STATE_TYPE state = initial_state;\n\n    // number of symbols found in this subsequence\n    std::uint32_t num_symbols = 0;\n\n    // bit position of next codeword\n    std::uint32_t last_at = 0;\n\n    // last state in this subsequence\n    STATE_TYPE last_state;\n\n    std::uint32_t subsequences_processed = 0;\n\n    std::uint32_t current_subsequence = gid;\n    std::uint32_t in_pos = current_subsequence * subsequence_size;\n\n    uint4 sync_point = sync_points[current_subsequence];\n    uint4 next_sync_point = sync_points[current_subsequence + 1];\n\n    std::uint32_t out_pos = sync_point.z;\n    std::uint32_t next_out_pos = gid == total_num_subsequences - 1 ?\n      output_size : next_sync_point.z;\n\n    if(gid > 0) {\n      sync_point = sync_points[current_subsequence - 1];\n      in_pos = (current_subsequence - 1) * subsequence_size;\n      state = sync_point.w;\n    }\n\n    // sliding window\n    UNIT_TYPE window = in_ptr[in_pos];\n    UNIT_TYPE next = in_ptr[in_pos + 1];\n\n    // start bit\n    std::uint32_t start = bits_in_unit - initial_bit;\n\n    if(gid > 0) {\n      in_pos += sync_point.x;\n      start = sync_point.y;\n\n      window = in_ptr[in_pos];\n      next = in_ptr[in_pos + 1];\n    }\n\n    // overflow from previous subsequence, decode, write output\n    decode_subsequence(subsequence_size, current_subsequence,\n        subsequences_processed, mask, shift,\n        start, in_pos, in_ptr, window, next, state,\n        last_word_unit, last_word_bit, num_symbols, out_pos, out_ptr,\n        next_out_pos, table, bits_in_unit, number_of_states,\n        last_at, last_state, true, true);\n  }\n}"
        ]
    },
    "pool-cuda": {
        "/Users/gbolet/hecbench-roofline/src/pool-cuda/main.cu": [
            "#define T ((int)32)\n\n\ninline __host__ __device__ uint4 make_uint4(int4 a)\n{\n    return make_uint4(uint(a.x), uint(a.y), uint(a.z), uint(a.w));\n}\n\ninline __host__ __device__ uint4 max(uint4 a, uint4 b)\n{\n    return make_uint4(max(a.x,b.x), max(a.y,b.y), max(a.z,b.z), max(a.w,b.w));\n}\n\ninline __host__ __device__ uint4 min(uint4 a, uint4 b)\n{\n    return make_uint4(min(a.x,b.x), min(a.y,b.y), min(a.z,b.z), min(a.w,b.w));\n}\n\n__global__ void KernelPool2DGrad(\n    const int nthreads,\n    const T*__restrict__ input_data,\n    const T*__restrict__ output_data,\n    const T*__restrict__ output_grad,\n    const int channels,\n    const int input_height,\n    const int input_width,\n    const int output_height,\n    const int output_width,\n    PoolProcess pool_process,\n    T*__restrict__ input_grad,\n    bool channel_last = false)\n{\n  for (int index = blockIdx.x * blockDim.x + threadIdx.x; index < nthreads;\n           index += blockDim.x * gridDim.x) {\n    int w_offset, h_offset, offsetC, batch_idx;\n    int tmp;\n    if (!channel_last) { /* NCHW */\n      w_offset = index % input_width + padding_width;\n      tmp = index / input_width;\n      h_offset = tmp % input_height + padding_height;\n      tmp = tmp / input_height;\n      offsetC = tmp % channels;\n      batch_idx = tmp / channels;\n    } else { /* NHWC */\n      offsetC = index % channels;\n      tmp = index / channels;\n      w_offset = tmp % input_width + padding_width;\n      tmp = tmp / input_width;\n      h_offset = tmp % input_height + padding_height;\n      batch_idx = tmp / input_height;\n    }\n\n    int phstart, phend;\n    int pwstart, pwend;\n    phstart = (h_offset < ksize_height) ? 0 : (h_offset - ksize_height) / stride_height + 1;\n    pwstart = (w_offset < ksize_width) ? 0 : (w_offset - ksize_width) / stride_width + 1;\n    phend = min(h_offset / stride_height + 1, output_height);\n    pwend = min(w_offset / stride_width + 1, output_width);\n\n    // initial gradient value\n    T gradient = static_cast<T>(0.0);\n    T input = input_data[index];\n\n    int output_stride = batch_idx * output_height * output_width * channels;\n    if (!channel_last)\n      output_stride += offsetC * output_height * output_width;\n\n    const T *__restrict__ output_data_t = output_data + output_stride;\n    const T *__restrict__ output_grad_t = output_grad + output_stride;\n\n    for (int ph = phstart; ph < phend; ++ph) {\n      for (int pw = pwstart; pw < pwend; ++pw) {\n        int pool_size;\n        int hstart = ph * stride_height - padding_height;\n        int wstart = pw * stride_width - padding_width;\n        int hend = min(hstart + ksize_height, input_height);\n        int wend = min(wstart + ksize_width, input_width);\n        hstart = max(hstart, 0);\n        wstart = max(wstart, 0);\n        pool_size = exclusive ? (hend - hstart) * (wend - wstart)\n          : ksize_height * ksize_width;\n\n        int output_sub_idx = channel_last\n          ? (ph * output_width + pw) * channels + offsetC\n          : ph * output_width + pw;\n        pool_process.compute(input, output_data_t[output_sub_idx],\n            output_grad_t[output_sub_idx],\n            static_cast<T>(1.f / pool_size), &gradient);\n      }\n    }\n    input_grad[index] = gradient;\n  }\n}"
        ]
    },
    "frechet-cuda": {
        "/Users/gbolet/hecbench-roofline/src/frechet-cuda/norm1.h": [
            "inline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\ninline __host__ __device__ float4 fabs(float4 v)\n{\n    return make_float4(fabs(v.x), fabs(v.y), fabs(v.z), fabs(v.w));\n}\n\n__device__\ndouble norm1(int i, int j, const double *c1, const double *c2)\n{\n  double dist, diff; /* Temp variables for simpler computations */\n  int k; /* Index for iterating over dimensions */\n\n  /* Initialise distance */\n  dist = 0.0;\n\n  for (k = 0; k < n_d; k++)\n  {\n    /*\n     * Compute the distance between the k-th component of the i-th point\n     * of the 1st curve and the k-th component of the j-th point of the\n     * 2nd curve.\n     *\n     * Notice the 1-offset added for better readability (as in [1]).\n     */\n    diff = *(c1 + (i - 1)*n_d + k) - *(c2 + (j - 1)*n_d + k);\n    /* Increment the accumulator variable with the absolute distance */\n    dist += fabs(diff);\n  }\n\n  return dist;\n}\n\n__device__\ndouble recursive_norm1(int i, int j, int n_2, double *ca,\n                       const double *c1, const double *c2)\n{\n  /*\n   * Target the shortcut to the (i, j)-th entry of the matrix `ca`\n   *\n   * Once again, notice the 1-offset.\n   */\n  double *ca_ij = ca + (i - 1)*n_2 + (j - 1);\n\n  /* This implements the algorithm from [1] */\n  if (*ca_ij > -1.0) \n  {\n    return *ca_ij;\n  }\n  else if ((i == 1) && (j == 1))\n  {\n    *ca_ij = norm1(1, 1, c1, c2);\n  }\n  else if ((i > 1) && (j == 1))\n  {\n    *ca_ij = fmax(recursive_norm1(i - 1, 1, n_2, ca, c1, c2), norm1(i, 1, c1, c2));\n  }\n  else if ((i == 1) && (j > 1))\n  {\n    *ca_ij = fmax(recursive_norm1(1, j - 1, n_2, ca, c1, c2), norm1(1, j, c1, c2));\n  }\n  else if ((i > 1) && (j > 1))\n  {\n    *ca_ij = fmax(\n        fmin(fmin(\n            recursive_norm1(i - 1, j    , n_2, ca, c1, c2),\n            recursive_norm1(i - 1, j - 1, n_2, ca, c1, c2)),\n            recursive_norm1(i,     j - 1, n_2, ca, c1, c2)),\n        norm1(i, j, c1, c2));\n  }\n  else\n  {\n    *ca_ij = INFINITY;\n  }\n\n  return *ca_ij;\n}\n\n__global__ void distance_norm1 (\n  int n_1, int n_2,\n  double *__restrict__ ca,\n  const double *__restrict__ c1,\n  const double *__restrict__ c2)\n{\n  int i = blockDim.x * blockIdx.x + threadIdx.x;\n  int j = blockDim.y * blockIdx.y + threadIdx.y;\n  if (j >= 1 && j <= n_2 && i >= 1 && i <= n_1)\n    recursive_norm1(i, j, n_2, ca, c1, c2);\n}"
        ],
        "/Users/gbolet/hecbench-roofline/src/frechet-cuda/norm2.h": [
            "static __forceinline__ __device__\nValueType norm2(BasicVector<ValueType> v)\n{\n    return v.norm2();\n}\n\n__device__\ndouble recursive_norm2(int i, int j, int n_2, double *ca,\n                       const double *c1, const double *c2)\n{\n  /*\n   * Target the shortcut to the (i, j)-th entry of the matrix `ca`\n   *\n   * Once again, notice the 1-offset.\n   */\n  double *ca_ij = ca + (i - 1)*n_2 + (j - 1);\n\n  /* This implements the algorithm from [1] */\n  if (*ca_ij > -1.0) \n  {\n    return *ca_ij;\n  }\n  else if ((i == 1) && (j == 1))\n  {\n    *ca_ij = norm2(1, 1, c1, c2);\n  }\n  else if ((i > 1) && (j == 1))\n  {\n    *ca_ij = fmax(recursive_norm2(i - 1, 1, n_2, ca, c1, c2), norm2(i, 1, c1, c2));\n  }\n  else if ((i == 1) && (j > 1))\n  {\n    *ca_ij = fmax(recursive_norm2(1, j - 1, n_2, ca, c1, c2), norm2(1, j, c1, c2));\n  }\n  else if ((i > 1) && (j > 1))\n  {\n    *ca_ij = fmax(\n        fmin(fmin(\n            recursive_norm2(i - 1, j    , n_2, ca, c1, c2),\n            recursive_norm2(i - 1, j - 1, n_2, ca, c1, c2)),\n            recursive_norm2(i,     j - 1, n_2, ca, c1, c2)),\n        norm2(i, j, c1, c2));\n  }\n  else\n  {\n    *ca_ij = INFINITY;\n  }\n\n  return *ca_ij;\n}\n\n__global__ void distance_norm2 (\n  int n_1, int n_2,\n  double *__restrict__ ca,\n  const double *__restrict__ c1,\n  const double *__restrict__ c2)\n{\n  int i = blockDim.x * blockIdx.x + threadIdx.x;\n  int j = blockDim.y * blockIdx.y + threadIdx.y;\n  if (j >= 1 && j <= n_2 && i >= 1 && i <= n_1)\n    recursive_norm2(i, j, n_2, ca, c1, c2);\n}"
        ],
        "/Users/gbolet/hecbench-roofline/src/frechet-cuda/norm3.h": [
            "inline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\ninline __host__ __device__ float4 fabs(float4 v)\n{\n    return make_float4(fabs(v.x), fabs(v.y), fabs(v.z), fabs(v.w));\n}\n\n__device__\ndouble norm3(int i, int j, const double *c1, const double *c2)\n{\n  double dist, diff; /* Temp variables for simpler computations */\n  int k; /* Index for iterating over dimensions */\n\n  /* Initialise distance */\n  dist = 0.0;\n\n  for (k = 0; k < n_d; k++)\n  {\n    /*\n     * Compute the distance between the k-th component of the i-th point\n     * of the 1st curve and the k-th component of the j-th point of the\n     * 2nd curve.\n     *\n     * Notice the 1-offset added for better readability (as in [1]).\n     */\n    diff = *(c1 + (i - 1)*n_d + k) - *(c2 + (j - 1)*n_d + k);\n    /* Update the current maximum  */\n    dist = fmax(dist, fabs(diff));\n  }\n\n  return dist;\n}\n\n__device__\ndouble recursive_norm3(int i, int j, int n_2, double *ca,\n                       const double *c1, const double *c2)\n{\n  /*\n   * Target the shortcut to the (i, j)-th entry of the matrix `ca`\n   *\n   * Once again, notice the 1-offset.\n   */\n  double *ca_ij = ca + (i - 1)*n_2 + (j - 1);\n\n  /* This implements the algorithm from [1] */\n  if (*ca_ij > -1.0) \n  {\n    return *ca_ij;\n  }\n  else if ((i == 1) && (j == 1))\n  {\n    *ca_ij = norm3(1, 1, c1, c2);\n  }\n  else if ((i > 1) && (j == 1))\n  {\n    *ca_ij = fmax(recursive_norm3(i - 1, 1, n_2, ca, c1, c2), norm3(i, 1, c1, c2));\n  }\n  else if ((i == 1) && (j > 1))\n  {\n    *ca_ij = fmax(recursive_norm3(1, j - 1, n_2, ca, c1, c2), norm3(1, j, c1, c2));\n  }\n  else if ((i > 1) && (j > 1))\n  {\n    *ca_ij = fmax(\n        fmin(fmin(\n            recursive_norm3(i - 1, j    , n_2, ca, c1, c2),\n            recursive_norm3(i - 1, j - 1, n_2, ca, c1, c2)),\n            recursive_norm3(i,     j - 1, n_2, ca, c1, c2)),\n        norm3(i, j, c1, c2));\n  }\n  else\n  {\n    *ca_ij = INFINITY;\n  }\n\n  return *ca_ij;\n}\n\n__global__ void distance_norm3 (\n  int n_1, int n_2,\n  double *__restrict__ ca,\n  const double *__restrict__ c1,\n  const double *__restrict__ c2)\n{\n  int i = blockDim.x * blockIdx.x + threadIdx.x;\n  int j = blockDim.y * blockIdx.y + threadIdx.y;\n  if (j >= 1 && j <= n_2 && i >= 1 && i <= n_1)\n    recursive_norm3(i, j, n_2, ca, c1, c2);\n}"
        ]
    },
    "reverse-cuda": {
        "/Users/gbolet/hecbench-roofline/src/reverse-cuda/main.cu": [
            "__global__ void reverse (int *d, const int len)\n{\n  __shared__ int s[256];\n  int t = threadIdx.x;\n  s[t] = d[t];\n  __syncthreads();\n  d[t] = s[len-t-1];\n}"
        ]
    },
    "kalman-cuda": {
        "/Users/gbolet/hecbench-roofline/src/kalman-cuda/main.cu": [
            "__global__ void kalman(\n  const double*__restrict__ ys,\n  int nobs,\n  const double*__restrict__ T,\n  const double*__restrict__ Z,\n  const double*__restrict__ RQR,\n  const double*__restrict__ P,\n  const double*__restrict__ alpha,\n  bool intercept,\n  const double*__restrict__ d_mu,\n  int batch_size,\n  double*__restrict__ vs,\n  double*__restrict__ Fs,\n  double*__restrict__ sum_logFs,\n  int n_diff,\n  int fc_steps = 0,\n  double*__restrict__ d_fc = nullptr,\n  bool conf_int = false,\n  double* d_F_fc = nullptr)\n{\n  constexpr int rd2 = rd * rd;\n  double l_RQR[rd2];\n  double l_T[rd2];\n  double l_Z[rd];\n  double l_P[rd2];\n  double l_alpha[rd];\n  double l_K[rd];\n  double l_tmp[rd2];\n  double l_TP[rd2];\n\n  int bid = blockDim.x * blockIdx.x + threadIdx.x;\n  if (bid < batch_size) {\n    // Load global mem into registers\n    int b_rd_offset  = bid * rd;\n    int b_rd2_offset = bid * rd2;\n    for (int i = 0; i < rd2; i++) {\n      l_RQR[i] = RQR[b_rd2_offset + i];\n      l_T[i]   = T[b_rd2_offset + i];\n      l_P[i]   = P[b_rd2_offset + i];\n    }\n    for (int i = 0; i < rd; i++) {\n      if (n_diff > 0) l_Z[i] = Z[b_rd_offset + i];\n      l_alpha[i] = alpha[b_rd_offset + i];\n    }\n\n    double b_sum_logFs = 0.0;\n    const double* b_ys = ys + bid * nobs;\n    double* b_vs       = vs + bid * nobs; \n    double* b_Fs       = Fs + bid * nobs;\n\n    double mu = intercept ? d_mu[bid] : 0.0;\n\n    for (int it = 0; it < nobs; it++) {\n      // 1. v = y - Z*alpha\n      double vs_it = b_ys[it];\n      if (n_diff == 0)\n        vs_it -= l_alpha[0];\n      else {\n        for (int i = 0; i < rd; i++) {\n          vs_it -= l_alpha[i] * l_Z[i];\n        }\n      }\n      b_vs[it] = vs_it;\n\n      // 2. F = Z*P*Z'\n      double _Fs;\n      if (n_diff == 0)\n        _Fs = l_P[0];\n      else {\n        _Fs = 0.0;\n        for (int i = 0; i < rd; i++) {\n          for (int j = 0; j < rd; j++) {\n            _Fs += l_P[j * rd + i] * l_Z[i] * l_Z[j];\n          }\n        }\n      }\n      b_Fs[it] = _Fs;\n      if (it >= n_diff) b_sum_logFs += log(_Fs);\n\n      // 3. K = 1/Fs[it] * T*P*Z'\n      // TP = T*P\n      MM_l<rd>(l_T, l_P, l_TP);\n      // K = 1/Fs[it] * TP*Z'\n      double _1_Fs = 1.0 / _Fs;\n      if (n_diff == 0) {\n        for (int i = 0; i < rd; i++) {\n          l_K[i] = _1_Fs * l_TP[i];\n        }\n      } else\n        Mv_l<rd>(_1_Fs, l_TP, l_Z, l_K);\n\n      // 4. alpha = T*alpha + K*vs[it] + c\n      // tmp = T*alpha\n      Mv_l<rd>(l_T, l_alpha, l_tmp);\n      // alpha = tmp + K*vs[it]\n      for (int i = 0; i < rd; i++) {\n        l_alpha[i] = l_tmp[i] + l_K[i] * vs_it;\n      }\n      // alpha = alpha + c\n      l_alpha[n_diff] += mu;\n\n      // 5. L = T - K * Z\n      // L = T (L is tmp)\n      for (int i = 0; i < rd2; i++) {\n        l_tmp[i] = l_T[i];\n      }\n      // L = L - K * Z\n      if (n_diff == 0) {\n        for (int i = 0; i < rd; i++) {\n          l_tmp[i] -= l_K[i];\n        }\n      } else {\n        for (int i = 0; i < rd; i++) {\n          for (int j = 0; j < rd; j++) {\n            l_tmp[j * rd + i] -= l_K[i] * l_Z[j];\n          }\n        }\n      }\n\n      // 6. P = T*P*L' + R*Q*R'\n      // P = TP*L'\n      MM_l<rd, false, true>(l_TP, l_tmp, l_P);\n      // P = P + RQR\n      for (int i = 0; i < rd2; i++) {\n        l_P[i] += l_RQR[i];\n      }\n    }\n    sum_logFs[bid] = b_sum_logFs;\n\n    // Forecast\n    double* b_fc   = fc_steps ? d_fc + bid * fc_steps : nullptr;\n    double* b_F_fc = conf_int ? d_F_fc + bid * fc_steps : nullptr;\n    for (int it = 0; it < fc_steps; it++) {\n      if (n_diff == 0)\n        b_fc[it] = l_alpha[0];\n      else {\n        double pred = 0.0;\n        for (int i = 0; i < rd; i++) {\n          pred += l_alpha[i] * l_Z[i];\n        }\n        b_fc[it] = pred;\n      }\n\n      // alpha = T*alpha + c\n      Mv_l<rd>(l_T, l_alpha, l_tmp);\n      for (int i = 0; i < rd; i++) {\n        l_alpha[i] = l_tmp[i];\n      }\n      l_alpha[n_diff] += mu;\n\n      if (conf_int) {\n        if (n_diff == 0)\n          b_F_fc[it] = l_P[0];\n        else {\n          double _Fs = 0.0;\n          for (int i = 0; i < rd; i++) {\n            for (int j = 0; j < rd; j++) {\n              _Fs += l_P[j * rd + i] * l_Z[i] * l_Z[j];\n            }\n          }\n          b_F_fc[it] = _Fs;\n        }\n\n        // P = T*P*T' + RR'\n        // TP = T*P\n        MM_l<rd>(l_T, l_P, l_TP);\n        // P = TP*T'\n        MM_l<rd, false, true>(l_TP, l_T, l_P);\n        // P = P + RR'\n        for (int i = 0; i < rd2; i++) {\n          l_P[i] += l_RQR[i];\n        }\n      }\n    }\n  }\n}"
        ]
    },
    "merkle-cuda": {
        "/Users/gbolet/hecbench-roofline/src/merkle-cuda/merkle_tree.cu": [
            "inline __host__ __device__ int4 make_int4(float4 a)\n{\n    return make_int4(int(a.x), int(a.y), int(a.z), int(a.w));\n}\n\ninline __host__ __device__ int4 abs(int4 v)\n{\n    return make_int4(abs(v.x), abs(v.y), abs(v.z), abs(v.w));\n}\n\ninline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\ninline __host__ __device__ float4 fmaxf(float4 a, float4 b)\n{\n    return make_float4(fmaxf(a.x,b.x), fmaxf(a.y,b.y), fmaxf(a.z,b.z), fmaxf(a.w,b.w));\n}\n\n__global__ void merge(\n  const size_t imgSize,\n  const unsigned char *__restrict__ Img,\n  const unsigned char *__restrict__ Img1,\n  const unsigned char *__restrict__ Img2,\n        unsigned char *__restrict__ Tn,\n        unsigned char *__restrict__ Bn)\n{\n  size_t i = blockDim.x * blockIdx.x + threadIdx.x;\n  if (i >= imgSize) return;\n  if ( abs(Img[i] - Img1[i]) <= Tn[i] && abs(Img[i] - Img2[i]) <= Tn[i] ) {\n    // update background\n    Bn[i] = 0.92f * Bn[i] + 0.08f * Img[i];\n\n    // update threshold\n    float th = 0.92f * Tn[i] + 0.24f * (Img[i] - Bn[i]);\n    Tn[i] = fmaxf(th, 20.f);\n  }\n}\n\n__global__ void MerklizeRescuePrimeApproach1Phase0 (\n  const size_t output_offset,\n  const ulong* __restrict__ leaves,\n        ulong* __restrict__ intermediates,\n  const ulong4* __restrict__ mds,\n  const ulong4* __restrict__ ark1,\n  const ulong4* __restrict__ ark2)\n{\n  const size_t idx = blockIdx.x * blockDim.x + threadIdx.x;\n  merge(leaves + idx * (DIGEST_SIZE >> 1),\n        intermediates + (output_offset + idx) * DIGEST_SIZE,\n        mds,\n        ark1,\n        ark2);\n}",
            "inline __host__ __device__ int4 make_int4(float4 a)\n{\n    return make_int4(int(a.x), int(a.y), int(a.z), int(a.w));\n}\n\ninline __host__ __device__ int4 abs(int4 v)\n{\n    return make_int4(abs(v.x), abs(v.y), abs(v.z), abs(v.w));\n}\n\ninline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\ninline __host__ __device__ float4 fmaxf(float4 a, float4 b)\n{\n    return make_float4(fmaxf(a.x,b.x), fmaxf(a.y,b.y), fmaxf(a.z,b.z), fmaxf(a.w,b.w));\n}\n\n__global__ void merge(\n  const size_t imgSize,\n  const unsigned char *__restrict__ Img,\n  const unsigned char *__restrict__ Img1,\n  const unsigned char *__restrict__ Img2,\n        unsigned char *__restrict__ Tn,\n        unsigned char *__restrict__ Bn)\n{\n  size_t i = blockDim.x * blockIdx.x + threadIdx.x;\n  if (i >= imgSize) return;\n  if ( abs(Img[i] - Img1[i]) <= Tn[i] && abs(Img[i] - Img2[i]) <= Tn[i] ) {\n    // update background\n    Bn[i] = 0.92f * Bn[i] + 0.08f * Img[i];\n\n    // update threshold\n    float th = 0.92f * Tn[i] + 0.24f * (Img[i] - Bn[i]);\n    Tn[i] = fmaxf(th, 20.f);\n  }\n}\n\n__global__ void MerklizeRescuePrimeApproach1Phase1(\n  const size_t offset,\n        ulong* __restrict__ intermediates,\n  const ulong4* __restrict__ mds,\n  const ulong4* __restrict__ ark1,\n  const ulong4* __restrict__ ark2)\n{\n  const size_t idx = blockIdx.x * blockDim.x + threadIdx.x;\n  merge(intermediates + (offset << 1) * DIGEST_SIZE +\n          idx * (DIGEST_SIZE >> 1),\n        intermediates + (offset + idx) * DIGEST_SIZE,\n        mds,\n        ark1,\n        ark2);\n}"
        ]
    },
    "xlqc-cuda": {
        "/Users/gbolet/hecbench-roofline/src/xlqc-cuda/cuda_rys_dp.cu": [
            "__device__ int cuda_fact(int n){\n  int result = 1;\n  for (int i = 2; i <= n; i++) result *= i;\n  return result;\n}\n\n__device__ int cuda_binomial(int a, int b){\n  return cuda_fact(a)/(cuda_fact(b)*cuda_fact(a-b));\n}\n\n__device__ void cuda_Root123_dp(int n, double X, double roots[], double weights[]){\n\n  double R12, PIE4, R22, W22, R13, R23, W23, R33, W33;\n  double RT1=0,RT2=0,RT3=0,WW1=0,WW2=0,WW3=0;\n  double F1,F2,E,T1,T2,T3,A1,A2,Y;\n\n  R12 = 2.75255128608411E-01;\n  PIE4 = 7.85398163397448E-01;\n  R22 =  2.72474487139158E+00;\n  W22 = 9.17517095361369E-02;\n  R13 = 1.90163509193487E-01;\n  R23 = 1.78449274854325E+00;\n  W23 = 1.77231492083829E-01;\n  R33 = 5.52534374226326E+00;\n  W33 = 5.11156880411248E-03;\n    \n  if (X < 3.e-7){\n    if (n == 1){\n      RT1 = 0.5E+00 -X/5.0E+00;\n      WW1 = 1.0E+00 -X/3.0E+00;\n    } else if (n == 2) {\n      RT1 = 1.30693606237085E-01 -2.90430236082028E-02 *X;\n      RT2 = 2.86930639376291E+00 -6.37623643058102E-01 *X;\n      WW1 = 6.52145154862545E-01 -1.22713621927067E-01 *X;\n      WW2 = 3.47854845137453E-01 -2.10619711404725E-01 *X;\n    } else if (n == 3) {\n      RT1 = 6.03769246832797E-02 -9.28875764357368E-03 *X;\n      RT2 = 7.76823355931043E-01 -1.19511285527878E-01 *X;\n      RT3 = 6.66279971938567E+00 -1.02504611068957E+00 *X;\n      WW1 = 4.67913934572691E-01 -5.64876917232519E-02 *X;\n      WW2 = 3.60761573048137E-01 -1.49077186455208E-01 *X;\n      WW3 = 1.71324492379169E-01 -1.27768455150979E-01 *X;\n    }\n  } else if (X < 1.) {\n    if (n == 1){\n      F1 = ((((((((-8.36313918003957E-08*X+1.21222603512827E-06 )*X-\n                  1.15662609053481E-05 )*X+9.25197374512647E-05 )*X-\n                6.40994113129432E-04 )*X+3.78787044215009E-03 )*X-\n              1.85185172458485E-02 )*X+7.14285713298222E-02 )*X-\n            1.99999999997023E-01 )*X+3.33333333333318E-01;\n      WW1 = (X+X)*F1+exp(-X);\n      RT1 = F1/(WW1-F1);\n    } else if (n == 2) {\n      F1 = ((((((((-8.36313918003957E-08*X+1.21222603512827E-06 )*X-\n                  1.15662609053481E-05 )*X+9.25197374512647E-05 )*X-\n                6.40994113129432E-04 )*X+3.78787044215009E-03 )*X-\n              1.85185172458485E-02 )*X+7.14285713298222E-02 )*X-\n            1.99999999997023E-01 )*X+3.33333333333318E-01;\n      WW1 = (X+X)*F1+exp(-X);\n      RT1 = (((((((-2.35234358048491E-09*X+2.49173650389842E-08)*X-\n                  4.558315364581E-08)*X-2.447252174587E-06)*X+\n                4.743292959463E-05)*X-5.33184749432408E-04 )*X+\n              4.44654947116579E-03 )*X-2.90430236084697E-02 )*X+\n        1.30693606237085E-01;\n      RT2 = (((((((-2.47404902329170E-08*X+2.36809910635906E-07)*X+\n                  1.835367736310E-06)*X-2.066168802076E-05)*X-\n                1.345693393936E-04)*X-5.88154362858038E-05 )*X+\n              5.32735082098139E-02 )*X-6.37623643056745E-01 )*X+\n        2.86930639376289E+00;\n      WW2 = ((F1-WW1)*RT1+F1)*(1.0E+00+RT2)/(RT2-RT1);\n      WW1 = WW1-WW2;\n    } else if (n==3){\n      RT1 = ((((((-5.10186691538870E-10*X+2.40134415703450E-08)*X-\n                 5.01081057744427E-07 )*X+7.58291285499256E-06 )*X-\n               9.55085533670919E-05 )*X+1.02893039315878E-03 )*X-\n             9.28875764374337E-03 )*X+6.03769246832810E-02;\n      RT2 = ((((((-1.29646524960555E-08*X+7.74602292865683E-08)*X+\n                 1.56022811158727E-06 )*X-1.58051990661661E-05 )*X-\n               3.30447806384059E-04 )*X+9.74266885190267E-03 )*X-\n             1.19511285526388E-01 )*X+7.76823355931033E-01;\n      RT3 = ((((((-9.28536484109606E-09*X-3.02786290067014E-07)*X-\n                 2.50734477064200E-06 )*X-7.32728109752881E-06 )*X+\n               2.44217481700129E-04 )*X+4.94758452357327E-02 )*X-\n             1.02504611065774E+00 )*X+6.66279971938553E+00;\n      F2 = ((((((((-7.60911486098850E-08*X+1.09552870123182E-06 )*X-\n                  1.03463270693454E-05 )*X+8.16324851790106E-05 )*X-\n                5.55526624875562E-04 )*X+3.20512054753924E-03 )*X-\n              1.51515139838540E-02 )*X+5.55555554649585E-02 )*X-\n            1.42857142854412E-01 )*X+1.99999999999986E-01;\n      E = exp(-X);\n      F1 = ((X+X)*F2+E)/3.0E+00;\n      WW1 = (X+X)*F1+E;\n      T1 = RT1/(RT1+1.0E+00);\n      T2 = RT2/(RT2+1.0E+00);\n      T3 = RT3/(RT3+1.0E+00);\n      A2 = F2-T1*F1;\n      A1 = F1-T1*WW1;\n      WW3 = (A2-T2*A1)/((T3-T2)*(T3-T1));\n      WW2 = (T3*A1-A2)/((T3-T2)*(T2-T1));\n      WW1 = WW1-WW2-WW3;\n    }\n  } else if (X < 3.) {\n    Y = X-2.0E+00;\n    if (n == 1) {\n      F1 = ((((((((((-1.61702782425558E-10*Y+1.96215250865776E-09 )*Y-\n                    2.14234468198419E-08 )*Y+2.17216556336318E-07 )*Y-\n                  1.98850171329371E-06 )*Y+1.62429321438911E-05 )*Y-\n                1.16740298039895E-04 )*Y+7.24888732052332E-04 )*Y-\n              3.79490003707156E-03 )*Y+1.61723488664661E-02 )*Y-\n            5.29428148329736E-02 )*Y+1.15702180856167E-01;\n      WW1 = (X+X)*F1+exp(-X);\n      RT1 = F1/(WW1-F1);\n    } else if (n == 2) {\n      F1 = ((((((((((-1.61702782425558E-10*Y+1.96215250865776E-09 )*Y-\n                    2.14234468198419E-08 )*Y+2.17216556336318E-07 )*Y-\n                  1.98850171329371E-06 )*Y+1.62429321438911E-05 )*Y-\n                1.16740298039895E-04 )*Y+7.24888732052332E-04 )*Y-\n              3.79490003707156E-03 )*Y+1.61723488664661E-02 )*Y-\n            5.29428148329736E-02 )*Y+1.15702180856167E-01;\n      WW1 = (X+X)*F1+exp(-X);\n      RT1 = (((((((((-6.36859636616415E-12*Y+8.47417064776270E-11)*Y-\n                    5.152207846962E-10)*Y-3.846389873308E-10)*Y+\n                  8.472253388380E-08)*Y-1.85306035634293E-06 )*Y+\n                2.47191693238413E-05 )*Y-2.49018321709815E-04 )*Y+\n              2.19173220020161E-03 )*Y-1.63329339286794E-02 )*Y+\n        8.68085688285261E-02;\n      RT2 = ((((((((( 1.45331350488343E-10*Y+2.07111465297976E-09)*Y-\n                    1.878920917404E-08)*Y-1.725838516261E-07)*Y+\n                  2.247389642339E-06)*Y+9.76783813082564E-06 )*Y-\n                1.93160765581969E-04 )*Y-1.58064140671893E-03 )*Y+\n              4.85928174507904E-02 )*Y-4.30761584997596E-01 )*Y+\n        1.80400974537950E+00;\n      WW2 = ((F1-WW1)*RT1+F1)*(1.0E+00+RT2)/(RT2-RT1);\n      WW1 = WW1-WW2;\n    } else if (n == 3) {\n      RT1 = (((((((( 1.44687969563318E-12*Y+4.85300143926755E-12)*Y-\n                   6.55098264095516E-10 )*Y+1.56592951656828E-08 )*Y-\n                 2.60122498274734E-07 )*Y+3.86118485517386E-06 )*Y-\n               5.13430986707889E-05 )*Y+6.03194524398109E-04 )*Y-\n             6.11219349825090E-03 )*Y+4.52578254679079E-02;\n      RT2 = ((((((( 6.95964248788138E-10*Y-5.35281831445517E-09)*Y-\n                  6.745205954533E-08)*Y+1.502366784525E-06)*Y+\n                9.923326947376E-07)*Y-3.89147469249594E-04 )*Y+\n              7.51549330892401E-03 )*Y-8.48778120363400E-02 )*Y+\n        5.73928229597613E-01;\n      RT3 = ((((((((-2.81496588401439E-10*Y+3.61058041895031E-09)*Y+\n                   4.53631789436255E-08 )*Y-1.40971837780847E-07 )*Y-\n                 6.05865557561067E-06 )*Y-5.15964042227127E-05 )*Y+\n               3.34761560498171E-05 )*Y+5.04871005319119E-02 )*Y-\n             8.24708946991557E-01 )*Y+4.81234667357205E+00;\n      F2 = ((((((((((-1.48044231072140E-10*Y+1.78157031325097E-09 )*Y-\n                    1.92514145088973E-08 )*Y+1.92804632038796E-07 )*Y-\n                  1.73806555021045E-06 )*Y+1.39195169625425E-05 )*Y-\n                9.74574633246452E-05 )*Y+5.83701488646511E-04 )*Y-\n              2.89955494844975E-03 )*Y+1.13847001113810E-02 )*Y-\n            3.23446977320647E-02 )*Y+5.29428148329709E-02;\n      E = exp(-X);\n      F1 = ((X+X)*F2+E)/3.0E+00;\n      WW1 = (X+X)*F1+E;\n      T1 = RT1/(RT1+1.0E+00);\n      T2 = RT2/(RT2+1.0E+00);\n      T3 = RT3/(RT3+1.0E+00);\n      A2 = F2-T1*F1;\n      A1 = F1-T1*WW1;\n      WW3 = (A2-T2*A1)/((T3-T2)*(T3-T1));\n      WW2 = (T3*A1-A2)/((T3-T2)*(T2-T1));\n      WW1 = WW1-WW2-WW3;\n    }\n  } else if (X < 5.){\n    Y = X-4.0E+00;\n    if (n == 1){\n      F1 = ((((((((((-2.62453564772299E-11*Y+3.24031041623823E-10 )*Y-\n                    3.614965656163E-09)*Y+3.760256799971E-08)*Y-\n                  3.553558319675E-07)*Y+3.022556449731E-06)*Y-\n                2.290098979647E-05)*Y+1.526537461148E-04)*Y-\n              8.81947375894379E-04 )*Y+4.33207949514611E-03 )*Y-\n            1.75257821619926E-02 )*Y+5.28406320615584E-02;\n      WW1 = (X+X)*F1+exp(-X);\n      RT1 = F1/(WW1-F1);\n    } else if (n == 2) {\n      F1 = ((((((((((-2.62453564772299E-11*Y+3.24031041623823E-10 )*Y-\n                    3.614965656163E-09)*Y+3.760256799971E-08)*Y-\n                  3.553558319675E-07)*Y+3.022556449731E-06)*Y-\n                2.290098979647E-05)*Y+1.526537461148E-04)*Y-\n              8.81947375894379E-04 )*Y+4.33207949514611E-03 )*Y-\n            1.75257821619926E-02 )*Y+5.28406320615584E-02;\n      WW1 = (X+X)*F1+exp(-X);\n      RT1 = ((((((((-4.11560117487296E-12*Y+7.10910223886747E-11)*Y-\n                   1.73508862390291E-09 )*Y+5.93066856324744E-08 )*Y-\n                 9.76085576741771E-07 )*Y+1.08484384385679E-05 )*Y-\n               1.12608004981982E-04 )*Y+1.16210907653515E-03 )*Y-\n             9.89572595720351E-03 )*Y+6.12589701086408E-02;\n      RT2 = (((((((((-1.80555625241001E-10*Y+5.44072475994123E-10)*Y+\n                    1.603498045240E-08)*Y-1.497986283037E-07)*Y-\n                  7.017002532106E-07)*Y+1.85882653064034E-05 )*Y-\n                2.04685420150802E-05 )*Y-2.49327728643089E-03 )*Y+\n              3.56550690684281E-02 )*Y-2.60417417692375E-01 )*Y+\n        1.12155283108289E+00;\n      WW2 = ((F1-WW1)*RT1+F1)*(1.0E+00+RT2)/(RT2-RT1);\n      WW1 = WW1-WW2;\n    } else if (n == 3) {\n      RT1 = ((((((( 1.44265709189601E-11*Y-4.66622033006074E-10)*Y+\n                  7.649155832025E-09)*Y-1.229940017368E-07)*Y+\n                2.026002142457E-06)*Y-2.87048671521677E-05 )*Y+\n              3.70326938096287E-04 )*Y-4.21006346373634E-03 )*Y+\n        3.50898470729044E-02;\n      RT2 = ((((((((-2.65526039155651E-11*Y+1.97549041402552E-10)*Y+\n                   2.15971131403034E-09 )*Y-7.95045680685193E-08 )*Y+\n                 5.15021914287057E-07 )*Y+1.11788717230514E-05 )*Y-\n               3.33739312603632E-04 )*Y+5.30601428208358E-03 )*Y-\n             5.93483267268959E-02 )*Y+4.31180523260239E-01;\n      RT3 = ((((((((-3.92833750584041E-10*Y-4.16423229782280E-09)*Y+\n                   4.42413039572867E-08 )*Y+6.40574545989551E-07 )*Y-\n                 3.05512456576552E-06 )*Y-1.05296443527943E-04 )*Y-\n               6.14120969315617E-04 )*Y+4.89665802767005E-02 )*Y-\n             6.24498381002855E-01 )*Y+3.36412312243724E+00;\n      F2 = ((((((((((-2.36788772599074E-11*Y+2.89147476459092E-10 )*Y-\n                    3.18111322308846E-09 )*Y+3.25336816562485E-08 )*Y-\n                  3.00873821471489E-07 )*Y+2.48749160874431E-06 )*Y-\n                1.81353179793672E-05 )*Y+1.14504948737066E-04 )*Y-\n              6.10614987696677E-04 )*Y+2.64584212770942E-03 )*Y-\n            8.66415899015349E-03 )*Y+1.75257821619922E-02;\n      E = exp(-X);\n      F1 = ((X+X)*F2+E)/3.0E+00;\n      WW1 = (X+X)*F1+E;\n      T1 = RT1/(RT1+1.0E+00);\n      T2 = RT2/(RT2+1.0E+00);\n      T3 = RT3/(RT3+1.0E+00);\n      A2 = F2-T1*F1;\n      A1 = F1-T1*WW1;\n      WW3 = (A2-T2*A1)/((T3-T2)*(T3-T1));\n      WW2 = (T3*A1-A2)/((T3-T2)*(T2-T1));\n      WW1 = WW1-WW2-WW3;\n    }\n  } else if (X < 10) {\n    E = exp(-X);\n    WW1 = (((((( 4.6897511375022E-01/X-6.9955602298985E-01)/X +\n               5.3689283271887E-01)/X-3.2883030418398E-01)/X +\n             2.4645596956002E-01)/X-4.9984072848436E-01)/X -\n           3.1501078774085E-06)*E + sqrt(PIE4/X);\n    F1 = (WW1-E)/(X+X);\n    if (n == 1)\n      RT1 = F1/(WW1-F1);\n    else if (n == 2){\n      Y = X-7.5E+00;\n      RT1 = (((((((((((((-1.43632730148572E-16*Y+2.38198922570405E-16)*\n                        Y+1.358319618800E-14)*Y-7.064522786879E-14)*Y-\n                      7.719300212748E-13)*Y+7.802544789997E-12)*Y+\n                    6.628721099436E-11)*Y-1.775564159743E-09)*Y+\n                  1.713828823990E-08)*Y-1.497500187053E-07)*Y+\n                2.283485114279E-06)*Y-3.76953869614706E-05 )*Y+\n              4.74791204651451E-04 )*Y-4.60448960876139E-03 )*Y+\n        3.72458587837249E-02;\n      RT2 = (((((((((((( 2.48791622798900E-14*Y-1.36113510175724E-13)*Y-\n                       2.224334349799E-12)*Y+4.190559455515E-11)*Y-\n                     2.222722579924E-10)*Y-2.624183464275E-09)*Y+\n                   6.128153450169E-08)*Y-4.383376014528E-07)*Y-\n                 2.49952200232910E-06 )*Y+1.03236647888320E-04 )*Y-\n               1.44614664924989E-03 )*Y+1.35094294917224E-02 )*Y-\n             9.53478510453887E-02 )*Y+5.44765245686790E-01;\n      WW2 = ((F1-WW1)*RT1+F1)*(1.0E+00+RT2)/(RT2-RT1);\n      WW1 = WW1-WW2;\n    } else if (n == 3) {\n      F2 = (F1+F1+F1-E)/(X+X);\n      Y = X-7.5E+00;\n      RT1 = ((((((((((( 5.74429401360115E-16*Y+7.11884203790984E-16)*Y-\n                      6.736701449826E-14)*Y-6.264613873998E-13)*Y+\n                    1.315418927040E-11)*Y-4.23879635610964E-11 )*Y+\n                  1.39032379769474E-09 )*Y-4.65449552856856E-08 )*Y+\n                7.34609900170759E-07 )*Y-1.08656008854077E-05 )*Y+\n              1.77930381549953E-04 )*Y-2.39864911618015E-03 )*Y+\n        2.39112249488821E-02;\n      RT2 = ((((((((((( 1.13464096209120E-14*Y+6.99375313934242E-15)*Y-\n                      8.595618132088E-13)*Y-5.293620408757E-12)*Y-\n                    2.492175211635E-11)*Y+2.73681574882729E-09 )*Y-\n                  1.06656985608482E-08 )*Y-4.40252529648056E-07 )*Y+\n                9.68100917793911E-06 )*Y-1.68211091755327E-04 )*Y+\n              2.69443611274173E-03 )*Y-3.23845035189063E-02 )*Y+\n        2.75969447451882E-01;\n      RT3 = (((((((((((( 6.66339416996191E-15*Y+1.84955640200794E-13)*Y-\n                       1.985141104444E-12)*Y-2.309293727603E-11)*Y+\n                     3.917984522103E-10)*Y+1.663165279876E-09)*Y-\n                   6.205591993923E-08)*Y+8.769581622041E-09)*Y+\n                 8.97224398620038E-06 )*Y-3.14232666170796E-05 )*Y-\n               1.83917335649633E-03 )*Y+3.51246831672571E-02 )*Y-\n             3.22335051270860E-01 )*Y+1.73582831755430E+00;\n      T1 = RT1/(RT1+1.0E+00);\n      T2 = RT2/(RT2+1.0E+00);\n      T3 = RT3/(RT3+1.0E+00);\n      A2 = F2-T1*F1;\n      A1 = F1-T1*WW1;\n      WW3 = (A2-T2*A1)/((T3-T2)*(T3-T1));\n      WW2 = (T3*A1-A2)/((T3-T2)*(T2-T1));\n      WW1 = WW1-WW2-WW3;\n    }\n  } else if (X < 15) {\n    E = exp(-X);\n    WW1 = (((-1.8784686463512E-01/X+2.2991849164985E-01)/X -\n            4.9893752514047E-01)/X-2.1916512131607E-05)*E \n      + sqrt(PIE4/X);\n    F1 = (WW1-E)/(X+X);\n    if (n == 1)\n      RT1 = F1/(WW1-F1);\n    else if (n == 2) {\n      RT1 = ((((-1.01041157064226E-05*X+1.19483054115173E-03)*X -\n               6.73760231824074E-02)*X+1.25705571069895E+00)*X +\n             (((-8.57609422987199E+03/X+5.91005939591842E+03)/X -\n               1.70807677109425E+03)/X+2.64536689959503E+02)/X -\n             2.38570496490846E+01)*E + R12/(X-R12);\n      RT2 = ((( 3.39024225137123E-04*X-9.34976436343509E-02)*X -\n              4.22216483306320E+00)*X +\n             (((-2.08457050986847E+03/X -\n                1.04999071905664E+03)/X+3.39891508992661E+02)/X -\n              1.56184800325063E+02)/X+8.00839033297501E+00)*E + R22/(X-R22);\n      WW2 = ((F1-WW1)*RT1+F1)*(1.0E+00+RT2)/(RT2-RT1);\n      WW1 = WW1-WW2;\n    } else if (n == 3) {\n      F2 = (F1+F1+F1-E)/(X+X);\n      Y = X-12.5E+00;\n      RT1 = ((((((((((( 4.42133001283090E-16*Y-2.77189767070441E-15)*Y-\n                      4.084026087887E-14)*Y+5.379885121517E-13)*Y+\n                    1.882093066702E-12)*Y-8.67286219861085E-11 )*Y+\n                  7.11372337079797E-10 )*Y-3.55578027040563E-09 )*Y+\n                1.29454702851936E-07 )*Y-4.14222202791434E-06 )*Y+\n              8.04427643593792E-05 )*Y-1.18587782909876E-03 )*Y+\n        1.53435577063174E-02;\n      RT2 = ((((((((((( 6.85146742119357E-15*Y-1.08257654410279E-14)*Y-\n                      8.579165965128E-13)*Y+6.642452485783E-12)*Y+\n                    4.798806828724E-11)*Y-1.13413908163831E-09 )*Y+\n                  7.08558457182751E-09 )*Y-5.59678576054633E-08 )*Y+\n                2.51020389884249E-06 )*Y-6.63678914608681E-05 )*Y+\n              1.11888323089714E-03 )*Y-1.45361636398178E-02 )*Y+\n        1.65077877454402E-01;\n      RT3 = (((((((((((( 3.20622388697743E-15*Y-2.73458804864628E-14)*Y-\n                       3.157134329361E-13)*Y+8.654129268056E-12)*Y-\n                     5.625235879301E-11)*Y-7.718080513708E-10)*Y+\n                   2.064664199164E-08)*Y-1.567725007761E-07)*Y-\n                 1.57938204115055E-06 )*Y+6.27436306915967E-05 )*Y-\n               1.01308723606946E-03 )*Y+1.13901881430697E-02 )*Y-\n             1.01449652899450E-01 )*Y+7.77203937334739E-01;\n      T1 = RT1/(RT1+1.0E+00);\n      T2 = RT2/(RT2+1.0E+00);\n      T3 = RT3/(RT3+1.0E+00);\n      A2 = F2-T1*F1;\n      A1 = F1-T1*WW1;\n      WW3 = (A2-T2*A1)/((T3-T2)*(T3-T1));\n      WW2 = (T3*A1-A2)/((T3-T2)*(T2-T1));\n      WW1 = WW1-WW2-WW3;\n    }\n  } else if (X < 33) {\n    E = exp(-X);\n    WW1 = (( 1.9623264149430E-01/X-4.9695241464490E-01)/X -\n           6.0156581186481E-05)*E + sqrt(PIE4/X);\n    F1 = (WW1-E)/(X+X);\n    if (n == 1)\n      RT1 = F1/(WW1-F1);\n    else if (n == 2){\n      RT1 = ((((-1.14906395546354E-06*X+1.76003409708332E-04)*X -\n               1.71984023644904E-02)*X-1.37292644149838E-01)*X +\n             (-4.75742064274859E+01/X+9.21005186542857E+00)/X -\n             2.31080873898939E-02)*E + R12/(X-R12);\n      RT2 = ((( 3.64921633404158E-04*X-9.71850973831558E-02)*X -\n              4.02886174850252E+00)*X +\n             (-1.35831002139173E+02/X -\n              8.66891724287962E+01)/X+2.98011277766958E+00)*E + R22/(X-R22);\n      WW2 = ((F1-WW1)*RT1+F1)*(1.0E+00+RT2)/(RT2-RT1);\n      WW1 = WW1-WW2;\n    } else if (n == 3) {\n      F2 = (F1+F1+F1-E)/(X+X);\n      if (X < 20) {\n        RT1 = ((((((-2.43270989903742E-06*X+3.57901398988359E-04)*X -\n                   2.34112415981143E-02)*X+7.81425144913975E-01)*X -\n                 1.73209218219175E+01)*X+2.43517435690398E+02)*X +\n               (-1.97611541576986E+04/X+9.82441363463929E+03)/X -\n               2.07970687843258E+03)*E + R13/(X-R13);\n        RT2 = (((((-2.62627010965435E-04*X+3.49187925428138E-02)*X -\n                  3.09337618731880E+00)*X+1.07037141010778E+02)*X -\n                2.36659637247087E+03)*X +\n               ((-2.91669113681020E+06/X +\n                 1.41129505262758E+06)/X-2.91532335433779E+05)/X +\n               3.35202872835409E+04)*E + R23/(X-R23);\n        RT3 = ((((( 9.31856404738601E-05*X-2.87029400759565E-02)*X -\n                  7.83503697918455E-01)*X-1.84338896480695E+01)*X +\n                4.04996712650414E+02)*X +\n               (-1.89829509315154E+05/X +\n                5.11498390849158E+04)/X-6.88145821789955E+03)*E \n          + R33/(X-R33);\n      } else {\n        RT1 = ((((-4.97561537069643E-04*X-5.00929599665316E-02)*X +\n                 1.31099142238996E+00)*X-1.88336409225481E+01)*X -\n               6.60344754467191E+02 /X+1.64931462413877E+02)*E \n          + R13/(X-R13);\n        RT2 = ((((-4.48218898474906E-03*X-5.17373211334924E-01)*X +\n                 1.13691058739678E+01)*X-1.65426392885291E+02)*X -\n               6.30909125686731E+03 /X+1.52231757709236E+03)*E \n          + R23/(X-R23);\n        RT3 = ((((-1.38368602394293E-02*X-1.77293428863008E+00)*X +\n                 1.73639054044562E+01)*X-3.57615122086961E+02)*X -\n               1.45734701095912E+04 /X+2.69831813951849E+03)*E \n          + R33/(X-R33);\n      }\n      T1 = RT1/(RT1+1.0E+00);\n      T2 = RT2/(RT2+1.0E+00);\n      T3 = RT3/(RT3+1.0E+00);\n      A2 = F2-T1*F1;\n      A1 = F1-T1*WW1;\n      WW3 = (A2-T2*A1)/((T3-T2)*(T3-T1));\n      WW2 = (T3*A1-A2)/((T3-T2)*(T2-T1));\n      WW1 = WW1-WW2-WW3; \n    }\n  } else {\n    WW1 = sqrt(PIE4/X);\n    if (n == 1)\n      RT1 = 0.5E+00/(X-0.5E+00);\n    else if (n == 2) {\n      if (X < 40) {\n        E = exp(-X);\n        RT1 = (-8.78947307498880E-01*X+1.09243702330261E+01)*E \n          + R12/(X-R12);\n        RT2 = (-9.28903924275977E+00*X+8.10642367843811E+01)*E \n          + R22/(X-R22);\n        WW2 = ( 4.46857389308400E+00*X-7.79250653461045E+01)*E + W22*WW1;\n        WW1 = WW1-WW2;\n      } else {\n        RT1 = R12/(X-R12);\n        RT2 = R22/(X-R22);\n        WW2 = W22*WW1;\n        WW1 = WW1-WW2;\n      }\n    } else if (n == 3) {\n      if (X < 47) {\n        E = exp(-X);\n        RT1 = ((-7.39058467995275E+00*X+3.21318352526305E+02)*X -\n               3.99433696473658E+03)*E + R13/(X-R13);\n        RT2 = ((-7.38726243906513E+01*X+3.13569966333873E+03)*X -\n               3.86862867311321E+04)*E + R23/(X-R23);\n        RT3 = ((-2.63750565461336E+02*X+1.04412168692352E+04)*X -\n               1.28094577915394E+05)*E + R33/(X-R33);\n        WW3 = ((( 1.52258947224714E-01*X-8.30661900042651E+00)*X +\n                1.92977367967984E+02)*X-1.67787926005344E+03)*E \n          + W33*WW1;\n        WW2 = (( 6.15072615497811E+01*X-2.91980647450269E+03)*X +\n               3.80794303087338E+04)*E + W23*WW1;\n        WW1 = WW1-WW2-WW3;\n      } else {\n        RT1 = R13/(X-R13);\n        RT2 = R23/(X-R23);\n        RT3 = R33/(X-R33);\n        WW2 = W23*WW1;\n        WW3 = W33*WW1;\n        WW1 = WW1-WW2-WW3;\n      }\n    }\n  }\n  roots[0] = RT1;\n  weights[0] = WW1;\n  if (n > 1){\n    roots[1] = RT2;\n    weights[1] = WW2;\n  }\n  if (n > 2) {\n    roots[2] = RT3;\n    weights[2] = WW3;\n  }\n  return;\n}\n\n__device__ void cuda_Root4_dp(double X, double roots[], double weights[]){\n  double R14,PIE4,R24,W24,R34,W34,R44,W44;\n  double RT1=0,RT2=0,RT3=0,RT4=0,WW1=0,WW2=0,WW3=0,WW4=0;\n  double Y,E;\n  \n  R14 = 1.45303521503316E-01;\n  PIE4 = 7.85398163397448E-01;\n  R24 = 1.33909728812636E+00;\n  W24 = 2.34479815323517E-01;\n  R34 = 3.92696350135829E+00;\n  W34 = 1.92704402415764E-02;\n  R44 = 8.58863568901199E+00;\n  W44 = 2.25229076750736E-04;\n\n  if (X <= 3.0E-7) {\n    RT1 = 3.48198973061471E-02 -4.09645850660395E-03 *X;\n    RT2 = 3.81567185080042E-01 -4.48902570656719E-02 *X;\n    RT3 = 1.73730726945891E+00 -2.04389090547327E-01 *X;\n    RT4 = 1.18463056481549E+01 -1.39368301742312E+00 *X;\n    WW1 = 3.62683783378362E-01 -3.13844305713928E-02 *X;\n    WW2 = 3.13706645877886E-01 -8.98046242557724E-02 *X;\n    WW3 = 2.22381034453372E-01 -1.29314370958973E-01 *X;\n    WW4 = 1.01228536290376E-01 -8.28299075414321E-02 *X;\n  } else if (X <= 1.0) {\n    RT1 = ((((((-1.95309614628539E-10*X+5.19765728707592E-09)*X-\n               1.01756452250573E-07 )*X+1.72365935872131E-06 )*X-\n             2.61203523522184E-05 )*X+3.52921308769880E-04 )*X-\n           4.09645850658433E-03 )*X+3.48198973061469E-02;\n    RT2 = (((((-1.89554881382342E-08*X+3.07583114342365E-07)*X+\n              1.270981734393E-06)*X-1.417298563884E-04)*X+\n            3.226979163176E-03)*X-4.48902570678178E-02 )*X+\n      3.81567185080039E-01;\n    RT3 = (((((( 1.77280535300416E-09*X+3.36524958870615E-08)*X-\n               2.58341529013893E-07 )*X-1.13644895662320E-05 )*X-\n             7.91549618884063E-05 )*X+1.03825827346828E-02 )*X-\n           2.04389090525137E-01 )*X+1.73730726945889E+00;\n    RT4 = (((((-5.61188882415248E-08*X-2.49480733072460E-07)*X+\n              3.428685057114E-06)*X+1.679007454539E-04)*X+\n            4.722855585715E-02)*X-1.39368301737828E+00 )*X+\n      1.18463056481543E+01;\n    WW1 = ((((((-1.14649303201279E-08*X+1.88015570196787E-07)*X-\n               2.33305875372323E-06 )*X+2.68880044371597E-05 )*X-\n             2.94268428977387E-04 )*X+3.06548909776613E-03 )*X-\n           3.13844305680096E-02 )*X+3.62683783378335E-01;\n    WW2 = ((((((((-4.11720483772634E-09*X+6.54963481852134E-08)*X-\n                 7.20045285129626E-07 )*X+6.93779646721723E-06 )*X-\n               6.05367572016373E-05 )*X+4.74241566251899E-04 )*X-\n             3.26956188125316E-03 )*X+1.91883866626681E-02 )*X-\n           8.98046242565811E-02 )*X+3.13706645877886E-01;\n    WW3 = ((((((((-3.41688436990215E-08*X+5.07238960340773E-07)*X-\n                 5.01675628408220E-06 )*X+4.20363420922845E-05 )*X-\n               3.08040221166823E-04 )*X+1.94431864731239E-03 )*X-\n             1.02477820460278E-02 )*X+4.28670143840073E-02 )*X-\n           1.29314370962569E-01 )*X+2.22381034453369E-01;\n    WW4 = ((((((((( 4.99660550769508E-09*X-7.94585963310120E-08)*X+\n                  8.359072409485E-07)*X-7.422369210610E-06)*X+\n                5.763374308160E-05)*X-3.86645606718233E-04 )*X+\n              2.18417516259781E-03 )*X-9.99791027771119E-03 )*X+\n            3.48791097377370E-02 )*X-8.28299075413889E-02 )*X+\n      1.01228536290376E-01;\n  } else if (X <= 5.0) {\n    Y = X-3.0E+00;\n    RT1 = (((((((((-1.48570633747284E-15*Y-1.33273068108777E-13)*Y+\n                  4.068543696670E-12)*Y-9.163164161821E-11)*Y+\n                2.046819017845E-09)*Y-4.03076426299031E-08 )*Y+\n              7.29407420660149E-07 )*Y-1.23118059980833E-05 )*Y+\n            1.88796581246938E-04 )*Y-2.53262912046853E-03 )*Y+\n      2.51198234505021E-02;\n    RT2 = ((((((((( 1.35830583483312E-13*Y-2.29772605964836E-12)*Y-\n                  3.821500128045E-12)*Y+6.844424214735E-10)*Y-\n                1.048063352259E-08)*Y+1.50083186233363E-08 )*Y+\n              3.48848942324454E-06 )*Y-1.08694174399193E-04 )*Y+\n            2.08048885251999E-03 )*Y-2.91205805373793E-02 )*Y+\n      2.72276489515713E-01;\n    RT3 = ((((((((( 5.02799392850289E-13*Y+1.07461812944084E-11)*Y-\n                  1.482277886411E-10)*Y-2.153585661215E-09)*Y+\n                3.654087802817E-08)*Y+5.15929575830120E-07 )*Y-\n              9.52388379435709E-06 )*Y-2.16552440036426E-04 )*Y+\n            9.03551469568320E-03 )*Y-1.45505469175613E-01 )*Y+\n      1.21449092319186E+00;\n    RT4 = (((((((((-1.08510370291979E-12*Y+6.41492397277798E-11)*Y+\n                  7.542387436125E-10)*Y-2.213111836647E-09)*Y-\n                1.448228963549E-07)*Y-1.95670833237101E-06 )*Y-\n              1.07481314670844E-05 )*Y+1.49335941252765E-04 )*Y+\n            4.87791531990593E-02 )*Y-1.10559909038653E+00 )*Y+\n      8.09502028611780E+00;\n    WW1 = ((((((((((-4.65801912689961E-14*Y+7.58669507106800E-13)*Y-\n                   1.186387548048E-11)*Y+1.862334710665E-10)*Y-\n                 2.799399389539E-09)*Y+4.148972684255E-08)*Y-\n               5.933568079600E-07)*Y+8.168349266115E-06)*Y-\n             1.08989176177409E-04 )*Y+1.41357961729531E-03 )*Y-\n           1.87588361833659E-02 )*Y+2.89898651436026E-01;\n    WW2 = ((((((((((((-1.46345073267549E-14*Y+2.25644205432182E-13)*Y-\n                     3.116258693847E-12)*Y+4.321908756610E-11)*Y-\n                   5.673270062669E-10)*Y+7.006295962960E-09)*Y-\n                 8.120186517000E-08)*Y+8.775294645770E-07)*Y-\n               8.77829235749024E-06 )*Y+8.04372147732379E-05 )*Y-\n             6.64149238804153E-04 )*Y+4.81181506827225E-03 )*Y-\n           2.88982669486183E-02 )*Y+1.56247249979288E-01;\n    WW3 = ((((((((((((( 9.06812118895365E-15*Y-1.40541322766087E-13)*\n                      Y+1.919270015269E-12)*Y-2.605135739010E-11)*Y+\n                    3.299685839012E-10)*Y-3.86354139348735E-09 )*Y+\n                  4.16265847927498E-08 )*Y-4.09462835471470E-07 )*Y+\n                3.64018881086111E-06 )*Y-2.88665153269386E-05 )*Y+\n              2.00515819789028E-04 )*Y-1.18791896897934E-03 )*Y+\n            5.75223633388589E-03 )*Y-2.09400418772687E-02 )*Y+\n      4.85368861938873E-02;\n    WW4 = ((((((((((((((-9.74835552342257E-16*Y+1.57857099317175E-14)*\n                       Y-2.249993780112E-13)*Y+3.173422008953E-12)*Y-\n                     4.161159459680E-11)*Y+5.021343560166E-10)*Y-\n                   5.545047534808E-09)*Y+5.554146993491E-08)*Y-\n                 4.99048696190133E-07 )*Y+3.96650392371311E-06 )*Y-\n               2.73816413291214E-05 )*Y+1.60106988333186E-04 )*Y-\n             7.64560567879592E-04 )*Y+2.81330044426892E-03 )*Y-\n           7.16227030134947E-03 )*Y+9.66077262223353E-03;\n  } else if (X <= 10.0) {\n    Y = X-7.5E+00;\n    RT1 = ((((((((( 4.64217329776215E-15*Y-6.27892383644164E-15)*Y+\n                  3.462236347446E-13)*Y-2.927229355350E-11)*Y+\n                5.090355371676E-10)*Y-9.97272656345253E-09 )*Y+\n              2.37835295639281E-07 )*Y-4.60301761310921E-06 )*Y+\n            8.42824204233222E-05 )*Y-1.37983082233081E-03 )*Y+\n      1.66630865869375E-02;\n    RT2 = ((((((((( 2.93981127919047E-14*Y+8.47635639065744E-13)*Y-\n                  1.446314544774E-11)*Y-6.149155555753E-12)*Y+\n                8.484275604612E-10)*Y-6.10898827887652E-08 )*Y+\n              2.39156093611106E-06 )*Y-5.35837089462592E-05 )*Y+\n            1.00967602595557E-03 )*Y-1.57769317127372E-02 )*Y+\n      1.74853819464285E-01;\n    RT3 = (((((((((( 2.93523563363000E-14*Y-6.40041776667020E-14)*Y-\n                   2.695740446312E-12)*Y+1.027082960169E-10)*Y-\n                 5.822038656780E-10)*Y-3.159991002539E-08)*Y+\n               4.327249251331E-07)*Y+4.856768455119E-06)*Y-\n             2.54617989427762E-04 )*Y+5.54843378106589E-03 )*Y-\n           7.95013029486684E-02 )*Y+7.20206142703162E-01;\n    RT4 = (((((((((((-1.62212382394553E-14*Y+7.68943641360593E-13)*Y+\n                    5.764015756615E-12)*Y-1.380635298784E-10)*Y-\n                  1.476849808675E-09)*Y+1.84347052385605E-08 )*Y+\n                3.34382940759405E-07 )*Y-1.39428366421645E-06 )*Y-\n              7.50249313713996E-05 )*Y-6.26495899187507E-04 )*Y+\n            4.69716410901162E-02 )*Y-6.66871297428209E-01 )*Y+\n      4.11207530217806E+00;\n    WW1 = ((((((((((-1.65995045235997E-15*Y+6.91838935879598E-14)*Y-\n                   9.131223418888E-13)*Y+1.403341829454E-11)*Y-\n                 3.672235069444E-10)*Y+6.366962546990E-09)*Y-\n               1.039220021671E-07)*Y+1.959098751715E-06)*Y-\n             3.33474893152939E-05 )*Y+5.72164211151013E-04 )*Y-\n           1.05583210553392E-02 )*Y+2.26696066029591E-01;\n    WW2 = ((((((((((((-3.57248951192047E-16*Y+6.25708409149331E-15)*Y-\n                     9.657033089714E-14)*Y+1.507864898748E-12)*Y-\n                   2.332522256110E-11)*Y+3.428545616603E-10)*Y-\n                 4.698730937661E-09)*Y+6.219977635130E-08)*Y-\n               7.83008889613661E-07 )*Y+9.08621687041567E-06 )*Y-\n             9.86368311253873E-05 )*Y+9.69632496710088E-04 )*Y-\n           8.14594214284187E-03 )*Y+8.50218447733457E-02;\n    WW3 = ((((((((((((( 1.64742458534277E-16*Y-2.68512265928410E-15)*\n                      Y+3.788890667676E-14)*Y-5.508918529823E-13)*Y+\n                    7.555896810069E-12)*Y-9.69039768312637E-11 )*Y+\n                  1.16034263529672E-09 )*Y-1.28771698573873E-08 )*Y+\n                1.31949431805798E-07 )*Y-1.23673915616005E-06 )*Y+\n              1.04189803544936E-05 )*Y-7.79566003744742E-05 )*Y+\n            5.03162624754434E-04 )*Y-2.55138844587555E-03 )*Y+\n      1.13250730954014E-02;\n    WW4 = ((((((((((((((-1.55714130075679E-17*Y+2.57193722698891E-16)*\n                       Y-3.626606654097E-15)*Y+5.234734676175E-14)*Y-\n                     7.067105402134E-13)*Y+8.793512664890E-12)*Y-\n                   1.006088923498E-10)*Y+1.050565098393E-09)*Y-\n                 9.91517881772662E-09 )*Y+8.35835975882941E-08 )*Y-\n               6.19785782240693E-07 )*Y+3.95841149373135E-06 )*Y-\n             2.11366761402403E-05 )*Y+9.00474771229507E-05 )*Y-\n           2.78777909813289E-04 )*Y+5.26543779837487E-04;\n  } else if (X <= 15.0) {\n    Y = X-12.5E+00;\n    RT1 = ((((((((((( 4.94869622744119E-17*Y+8.03568805739160E-16)*Y-\n                    5.599125915431E-15)*Y-1.378685560217E-13)*Y+\n                  7.006511663249E-13)*Y+1.30391406991118E-11 )*Y+\n                8.06987313467541E-11 )*Y-5.20644072732933E-09 )*Y+\n              7.72794187755457E-08 )*Y-1.61512612564194E-06 )*Y+\n            4.15083811185831E-05 )*Y-7.87855975560199E-04 )*Y+\n      1.14189319050009E-02;\n    RT2 = ((((((((((( 4.89224285522336E-16*Y+1.06390248099712E-14)*Y-\n                    5.446260182933E-14)*Y-1.613630106295E-12)*Y+\n                  3.910179118937E-12)*Y+1.90712434258806E-10 )*Y+\n                8.78470199094761E-10 )*Y-5.97332993206797E-08 )*Y+\n              9.25750831481589E-07 )*Y-2.02362185197088E-05 )*Y+\n            4.92341968336776E-04 )*Y-8.68438439874703E-03 )*Y+\n      1.15825965127958E-01;\n    RT3 = (((((((((( 6.12419396208408E-14*Y+1.12328861406073E-13)*Y-\n                   9.051094103059E-12)*Y-4.781797525341E-11)*Y+\n                 1.660828868694E-09)*Y+4.499058798868E-10)*Y-\n               2.519549641933E-07)*Y+4.977444040180E-06)*Y-\n             1.25858350034589E-04 )*Y+2.70279176970044E-03 )*Y-\n           3.99327850801083E-02 )*Y+4.33467200855434E-01;\n    RT4 = ((((((((((( 4.63414725924048E-14*Y-4.72757262693062E-14)*Y-\n                    1.001926833832E-11)*Y+6.074107718414E-11)*Y+\n                  1.576976911942E-09)*Y-2.01186401974027E-08 )*Y-\n                1.84530195217118E-07 )*Y+5.02333087806827E-06 )*Y+\n              9.66961790843006E-06 )*Y-1.58522208889528E-03 )*Y+\n            2.80539673938339E-02 )*Y-2.78953904330072E-01 )*Y+\n      1.82835655238235E+00;\n    WW4 = ((((((((((((( 2.90401781000996E-18*Y-4.63389683098251E-17)*\n                      Y+6.274018198326E-16)*Y-8.936002188168E-15)*Y+\n                    1.194719074934E-13)*Y-1.45501321259466E-12 )*Y+\n                  1.64090830181013E-11 )*Y-1.71987745310181E-10 )*Y+\n                1.63738403295718E-09 )*Y-1.39237504892842E-08 )*Y+\n              1.06527318142151E-07 )*Y-7.27634957230524E-07 )*Y+\n            4.12159381310339E-06 )*Y-1.74648169719173E-05 )*Y+\n      8.50290130067818E-05;\n    WW3 = ((((((((((((-4.19569145459480E-17*Y+5.94344180261644E-16)*Y-\n                     1.148797566469E-14)*Y+1.881303962576E-13)*Y-\n                   2.413554618391E-12)*Y+3.372127423047E-11)*Y-\n                 4.933988617784E-10)*Y+6.116545396281E-09)*Y-\n               6.69965691739299E-08 )*Y+7.52380085447161E-07 )*Y-\n             8.08708393262321E-06 )*Y+6.88603417296672E-05 )*Y-\n           4.67067112993427E-04 )*Y+5.42313365864597E-03;\n    WW2 = ((((((((((-6.22272689880615E-15*Y+1.04126809657554E-13)*Y-\n                   6.842418230913E-13)*Y+1.576841731919E-11)*Y-\n                 4.203948834175E-10)*Y+6.287255934781E-09)*Y-\n               8.307159819228E-08)*Y+1.356478091922E-06)*Y-\n             2.08065576105639E-05 )*Y+2.52396730332340E-04 )*Y-\n           2.94484050194539E-03 )*Y+6.01396183129168E-02;\n    WW1 = (((-1.8784686463512E-01/X+2.2991849164985E-01)/X -\n            4.9893752514047E-01)/X-2.1916512131607E-05)*exp(-X) +\n      sqrt(PIE4/X)-WW4-WW3-WW2;\n  } else if (X <= 20.0) {\n    WW1 = sqrt(PIE4/X);\n    Y = X-17.5E+00;\n    RT1 = ((((((((((( 4.36701759531398E-17*Y-1.12860600219889E-16)*Y-\n                    6.149849164164E-15)*Y+5.820231579541E-14)*Y+\n                  4.396602872143E-13)*Y-1.24330365320172E-11 )*Y+\n                6.71083474044549E-11 )*Y+2.43865205376067E-10 )*Y+\n              1.67559587099969E-08 )*Y-9.32738632357572E-07 )*Y+\n            2.39030487004977E-05 )*Y-4.68648206591515E-04 )*Y+\n      8.34977776583956E-03;\n    RT2 = ((((((((((( 4.98913142288158E-16*Y-2.60732537093612E-16)*Y-\n                    7.775156445127E-14)*Y+5.766105220086E-13)*Y+\n                  6.432696729600E-12)*Y-1.39571683725792E-10 )*Y+\n                5.95451479522191E-10 )*Y+2.42471442836205E-09 )*Y+\n              2.47485710143120E-07 )*Y-1.14710398652091E-05 )*Y+\n            2.71252453754519E-04 )*Y-4.96812745851408E-03 )*Y+\n      8.26020602026780E-02;\n    RT3 = ((((((((((( 1.91498302509009E-15*Y+1.48840394311115E-14)*Y-\n                    4.316925145767E-13)*Y+1.186495793471E-12)*Y+\n                  4.615806713055E-11)*Y-5.54336148667141E-10 )*Y+\n                3.48789978951367E-10 )*Y-2.79188977451042E-09 )*Y+\n              2.09563208958551E-06 )*Y-6.76512715080324E-05 )*Y+\n            1.32129867629062E-03 )*Y-2.05062147771513E-02 )*Y+\n      2.88068671894324E-01;\n    RT4 = (((((((((((-5.43697691672942E-15*Y-1.12483395714468E-13)*Y+\n                    2.826607936174E-12)*Y-1.266734493280E-11)*Y-\n                  4.258722866437E-10)*Y+9.45486578503261E-09 )*Y-\n                5.86635622821309E-08 )*Y-1.28835028104639E-06 )*Y+\n              4.41413815691885E-05 )*Y-7.61738385590776E-04 )*Y+\n            9.66090902985550E-03 )*Y-1.01410568057649E-01 )*Y+\n      9.54714798156712E-01;\n    WW4 = ((((((((((((-7.56882223582704E-19*Y+7.53541779268175E-18)*Y-\n                     1.157318032236E-16)*Y+2.411195002314E-15)*Y-\n                   3.601794386996E-14)*Y+4.082150659615E-13)*Y-\n                 4.289542980767E-12)*Y+5.086829642731E-11)*Y-\n               6.35435561050807E-10 )*Y+6.82309323251123E-09 )*Y-\n             5.63374555753167E-08 )*Y+3.57005361100431E-07 )*Y-\n           2.40050045173721E-06 )*Y+4.94171300536397E-05;\n    WW3 = (((((((((((-5.54451040921657E-17*Y+2.68748367250999E-16)*Y+\n                    1.349020069254E-14)*Y-2.507452792892E-13)*Y+\n                  1.944339743818E-12)*Y-1.29816917658823E-11 )*Y+\n                3.49977768819641E-10 )*Y-8.67270669346398E-09 )*Y+\n              1.31381116840118E-07 )*Y-1.36790720600822E-06 )*Y+\n            1.19210697673160E-05 )*Y-1.42181943986587E-04 )*Y+\n      4.12615396191829E-03;\n    WW2 = (((((((((((-1.86506057729700E-16*Y+1.16661114435809E-15)*Y+\n                    2.563712856363E-14)*Y-4.498350984631E-13)*Y+\n                  1.765194089338E-12)*Y+9.04483676345625E-12 )*Y+\n                4.98930345609785E-10 )*Y-2.11964170928181E-08 )*Y+\n              3.98295476005614E-07 )*Y-5.49390160829409E-06 )*Y+\n            7.74065155353262E-05 )*Y-1.48201933009105E-03 )*Y+\n      4.97836392625268E-02;\n    WW1 = (( 1.9623264149430E-01/X-4.9695241464490E-01)/X -\n           6.0156581186481E-05)*exp(-X)+WW1-WW2-WW3-WW4;\n  } else if (X <= 35.0) {\n    WW1 = sqrt(PIE4/X);\n    E = exp(-X);\n    RT1 = ((((((-4.45711399441838E-05*X+1.27267770241379E-03)*X -\n               2.36954961381262E-01)*X+1.54330657903756E+01)*X -\n             5.22799159267808E+02)*X+1.05951216669313E+04)*X +\n           (-2.51177235556236E+06/X+8.72975373557709E+05)/X -\n           1.29194382386499E+05)*E + R14/(X-R14);\n    RT2 = (((((-7.85617372254488E-02*X+6.35653573484868E+00)*X -\n              3.38296938763990E+02)*X+1.25120495802096E+04)*X -\n            3.16847570511637E+05)*X +\n           ((-1.02427466127427E+09/X +\n             3.70104713293016E+08)/X-5.87119005093822E+07)/X +\n           5.38614211391604E+06)*E + R24/(X-R24);\n    RT3 = (((((-2.37900485051067E-01*X+1.84122184400896E+01)*X -\n              1.00200731304146E+03)*X+3.75151841595736E+04)*X -\n            9.50626663390130E+05)*X +\n           ((-2.88139014651985E+09/X +\n             1.06625915044526E+09)/X-1.72465289687396E+08)/X +\n           1.60419390230055E+07)*E + R34/(X-R34);\n    RT4 = ((((((-6.00691586407385E-04*X-3.64479545338439E-01)*X +\n               1.57496131755179E+01)*X-6.54944248734901E+02)*X +\n             1.70830039597097E+04)*X-2.90517939780207E+05)*X +\n           (3.49059698304732E+07/X-1.64944522586065E+07)/X +\n           2.96817940164703E+06)*E + R44/(X-R44);\n    if (X <= 25.0) \n      WW4 = ((((((( 2.33766206773151E-07*X-\n                    3.81542906607063E-05)*X +3.51416601267000E-03)*X-\n                 1.66538571864728E-01)*X +4.80006136831847E+00)*X-\n               8.73165934223603E+01)*X +9.77683627474638E+02)*X +\n             1.66000945117640E+04/X -6.14479071209961E+03)*E + W44*WW1;\n    else\n      WW4 = (((((( 5.74245945342286E-06*X-\n                   7.58735928102351E-05)*X +2.35072857922892E-04)*X-\n                3.78812134013125E-03)*X +3.09871652785805E-01)*X-\n              7.11108633061306E+00)*X +5.55297573149528E+01)*E + W44*WW1;\n    WW3 = (((((( 2.36392855180768E-04*X-9.16785337967013E-03)*X +\n               4.62186525041313E-01)*X-1.96943786006540E+01)*X +\n             4.99169195295559E+02)*X-6.21419845845090E+03)*X +\n           ((+5.21445053212414E+07/X-1.34113464389309E+07)/X +\n            1.13673298305631E+06)/X-2.81501182042707E+03)*E + W34*WW1;\n    WW2 = (((((( 7.29841848989391E-04*X-3.53899555749875E-02)*X +\n               2.07797425718513E+00)*X-1.00464709786287E+02)*X +\n             3.15206108877819E+03)*X-6.27054715090012E+04)*X +\n           (+1.54721246264919E+07/X-5.26074391316381E+06)/X +\n           7.67135400969617E+05)*E + W24*WW1;\n    WW1 = (( 1.9623264149430E-01/X-4.9695241464490E-01)/X -\n           6.0156581186481E-05)*E + WW1-WW2-WW3-WW4;\n  } else if (X <= 53.0) {\n    WW1 = sqrt(PIE4/X);\n    E = exp(-X)*pow(X,4.0);\n    RT4 = ((-2.19135070169653E-03*X-1.19108256987623E-01)*X -\n           7.50238795695573E-01)*E + R44/(X-R44);\n    RT3 = ((-9.65842534508637E-04*X-4.49822013469279E-02)*X +\n           6.08784033347757E-01)*E + R34/(X-R34);\n    RT2 = ((-3.62569791162153E-04*X-9.09231717268466E-03)*X +\n           1.84336760556262E-01)*E + R24/(X-R24);\n    RT1 = ((-4.07557525914600E-05*X-6.88846864931685E-04)*X +\n           1.74725309199384E-02)*E + R14/(X-R14);\n    WW4 = (( 5.76631982000990E-06*X-7.89187283804890E-05)*X +\n           3.28297971853126E-04)*E + W44*WW1;\n    WW3 = (( 2.08294969857230E-04*X-3.77489954837361E-03)*X +\n           2.09857151617436E-02)*E + W34*WW1;\n    WW2 = (( 6.16374517326469E-04*X-1.26711744680092E-02)*X +\n           8.14504890732155E-02)*E + W24*WW1;\n    WW1 = WW1-WW2-WW3-WW4;\n  } else {\n    WW1 = sqrt(PIE4/X);\n    RT1 = R14/(X-R14);\n    RT2 = R24/(X-R24);\n    RT3 = R34/(X-R34);\n    RT4 = R44/(X-R44);\n    WW4 = W44*WW1;\n    WW3 = W34*WW1;\n    WW2 = W24*WW1;\n    WW1 = WW1-WW2-WW3-WW4;\n  }\n  roots[0] = RT1;\n  weights[0] = WW1;\n  roots[1] = RT2;\n  weights[1] = WW2;\n  roots[2] = RT3;\n  weights[2] = WW3;\n  roots[3] = RT4;\n  weights[3] = WW4;\n  return;\n}\n\n__device__ void cuda_Root5_dp(double X, double roots[], double weights[]){\n  double R15,PIE4,R25,W25,R35,W35,R45,W45,R55,W55;\n  double RT1=0,RT2=0,RT3=0,RT4=0,RT5=0,\n    WW1=0,WW2=0,WW3=0,WW4=0,WW5=0;\n  double Y,E=0,XXX;\n\n  R15 = 1.17581320211778E-01;\n  PIE4 = 7.85398163397448E-01;\n  R25 = 1.07456201243690E+00;\n  W25 = 2.70967405960535E-01;\n  R35 = 3.08593744371754E+00;\n  W35 = 3.82231610015404E-02;\n  R45 = 6.41472973366203E+00;\n  W45 = 1.51614186862443E-03;\n  R55 = 1.18071894899717E+01;\n  W55 = 8.62130526143657E-06;\n\n  if (X < 3.e-7){\n    RT1 = 2.26659266316985E-02 -2.15865967920897E-03 *X;\n    RT2 = 2.31271692140903E-01 -2.20258754389745E-02 *X;\n    RT3 = 8.57346024118836E-01 -8.16520023025515E-02 *X;\n    RT4 = 2.97353038120346E+00 -2.83193369647137E-01 *X;\n    RT5 = 1.84151859759051E+01 -1.75382723579439E+00 *X;\n    WW1 = 2.95524224714752E-01 -1.96867576909777E-02 *X;\n    WW2 = 2.69266719309995E-01 -5.61737590184721E-02 *X;\n    WW3 = 2.19086362515981E-01 -9.71152726793658E-02 *X;\n    WW4 = 1.49451349150580E-01 -1.02979262193565E-01 *X;\n    WW5 = 6.66713443086877E-02 -5.73782817488315E-02 *X;\n  } else if (X < 1.0){\n    RT1 = ((((((-4.46679165328413E-11*X+1.21879111988031E-09)*X-\n               2.62975022612104E-08 )*X+5.15106194905897E-07 )*X-\n             9.27933625824749E-06 )*X+1.51794097682482E-04 )*X-\n           2.15865967920301E-03 )*X+2.26659266316985E-02;\n    RT2 = (((((( 1.93117331714174E-10*X-4.57267589660699E-09)*X+\n               2.48339908218932E-08 )*X+1.50716729438474E-06 )*X-\n             6.07268757707381E-05 )*X+1.37506939145643E-03 )*X-\n           2.20258754419939E-02 )*X+2.31271692140905E-01;\n    RT3 = ((((( 4.84989776180094E-09*X+1.31538893944284E-07)*X-\n              2.766753852879E-06)*X-7.651163510626E-05)*X+\n            4.033058545972E-03)*X-8.16520022916145E-02 )*X+\n      8.57346024118779E-01;\n    RT4 = ((((-2.48581772214623E-07*X-4.34482635782585E-06)*X-\n             7.46018257987630E-07 )*X+1.01210776517279E-02 )*X-\n           2.83193369640005E-01 )*X+2.97353038120345E+00;\n    RT5 = (((((-8.92432153868554E-09*X+1.77288899268988E-08)*X+\n              3.040754680666E-06)*X+1.058229325071E-04)*X+\n            4.596379534985E-02)*X-1.75382723579114E+00 )*X+\n      1.84151859759049E+01;\n    WW1 = ((((((-2.03822632771791E-09*X+3.89110229133810E-08)*X-\n               5.84914787904823E-07 )*X+8.30316168666696E-06 )*X-\n             1.13218402310546E-04 )*X+1.49128888586790E-03 )*X-\n           1.96867576904816E-02 )*X+2.95524224714749E-01;\n    WW2 = ((((((( 8.62848118397570E-09*X-1.38975551148989E-07)*X+\n                1.602894068228E-06)*X-1.646364300836E-05)*X+\n              1.538445806778E-04)*X-1.28848868034502E-03 )*X+\n            9.38866933338584E-03 )*X-5.61737590178812E-02 )*X+\n      2.69266719309991E-01;\n    WW3 = ((((((((-9.41953204205665E-09*X+1.47452251067755E-07)*X-\n                 1.57456991199322E-06 )*X+1.45098401798393E-05 )*X-\n               1.18858834181513E-04 )*X+8.53697675984210E-04 )*X-\n             5.22877807397165E-03 )*X+2.60854524809786E-02 )*X-\n           9.71152726809059E-02 )*X+2.19086362515979E-01;\n    WW4 = ((((((((-3.84961617022042E-08*X+5.66595396544470E-07)*X-\n                 5.52351805403748E-06 )*X+4.53160377546073E-05 )*X-\n               3.22542784865557E-04 )*X+1.95682017370967E-03 )*X-\n             9.77232537679229E-03 )*X+3.79455945268632E-02 )*X-\n           1.02979262192227E-01 )*X+1.49451349150573E-01;\n    WW5 = ((((((((( 4.09594812521430E-09*X-6.47097874264417E-08)*X+\n                  6.743541482689E-07)*X-5.917993920224E-06)*X+\n                4.531969237381E-05)*X-2.99102856679638E-04 )*X+\n              1.65695765202643E-03 )*X-7.40671222520653E-03 )*X+\n            2.50889946832192E-02 )*X-5.73782817487958E-02 )*X+\n      6.66713443086877E-02;\n  } else if (X < 5.0) {\n    Y = X-3.0E+00;\n    RT1 = ((((((((-2.58163897135138E-14*Y+8.14127461488273E-13)*Y-\n                 2.11414838976129E-11 )*Y+5.09822003260014E-10 )*Y-\n               1.16002134438663E-08 )*Y+2.46810694414540E-07 )*Y-\n             4.92556826124502E-06 )*Y+9.02580687971053E-05 )*Y-\n           1.45190025120726E-03 )*Y+1.73416786387475E-02;\n    RT2 = ((((((((( 1.04525287289788E-14*Y+5.44611782010773E-14)*Y-\n                  4.831059411392E-12)*Y+1.136643908832E-10)*Y-\n                1.104373076913E-09)*Y-2.35346740649916E-08 )*Y+\n              1.43772622028764E-06 )*Y-4.23405023015273E-05 )*Y+\n            9.12034574793379E-04 )*Y-1.52479441718739E-02 )*Y+\n      1.76055265928744E-01;\n    RT3 = (((((((((-6.89693150857911E-14*Y+5.92064260918861E-13)*Y+\n                  1.847170956043E-11)*Y-3.390752744265E-10)*Y-\n                2.995532064116E-09)*Y+1.57456141058535E-07 )*Y-\n              3.95859409711346E-07 )*Y-9.58924580919747E-05 )*Y+\n            3.23551502557785E-03 )*Y-5.97587007636479E-02 )*Y+\n      6.46432853383057E-01;\n    RT4 = ((((((((-3.61293809667763E-12*Y-2.70803518291085E-11)*Y+\n                 8.83758848468769E-10 )*Y+1.59166632851267E-08 )*Y-\n               1.32581997983422E-07 )*Y-7.60223407443995E-06 )*Y-\n             7.41019244900952E-05 )*Y+9.81432631743423E-03 )*Y-\n           2.23055570487771E-01 )*Y+2.21460798080643E+00;\n    RT5 = ((((((((( 7.12332088345321E-13*Y+3.16578501501894E-12)*Y-\n                  8.776668218053E-11)*Y-2.342817613343E-09)*Y-\n                3.496962018025E-08)*Y-3.03172870136802E-07 )*Y+\n              1.50511293969805E-06 )*Y+1.37704919387696E-04 )*Y+\n            4.70723869619745E-02 )*Y-1.47486623003693E+00 )*Y+\n      1.35704792175847E+01;\n    WW1 = ((((((((( 1.04348658616398E-13*Y-1.94147461891055E-12)*Y+\n                  3.485512360993E-11)*Y-6.277497362235E-10)*Y+\n                1.100758247388E-08)*Y-1.88329804969573E-07 )*Y+\n              3.12338120839468E-06 )*Y-5.04404167403568E-05 )*Y+\n            8.00338056610995E-04 )*Y-1.30892406559521E-02 )*Y+\n      2.47383140241103E-01;\n    WW2 = ((((((((((( 3.23496149760478E-14*Y-5.24314473469311E-13)*Y+\n                    7.743219385056E-12)*Y-1.146022750992E-10)*Y+\n                  1.615238462197E-09)*Y-2.15479017572233E-08 )*Y+\n                2.70933462557631E-07 )*Y-3.18750295288531E-06 )*Y+\n              3.47425221210099E-05 )*Y-3.45558237388223E-04 )*Y+\n            3.05779768191621E-03 )*Y-2.29118251223003E-02 )*Y+\n      1.59834227924213E-01;\n    WW3 = ((((((((((((-3.42790561802876E-14*Y+5.26475736681542E-13)*Y-\n                     7.184330797139E-12)*Y+9.763932908544E-11)*Y-\n                   1.244014559219E-09)*Y+1.472744068942E-08)*Y-\n                 1.611749975234E-07)*Y+1.616487851917E-06)*Y-\n               1.46852359124154E-05 )*Y+1.18900349101069E-04 )*Y-\n             8.37562373221756E-04 )*Y+4.93752683045845E-03 )*Y-\n           2.25514728915673E-02 )*Y+6.95211812453929E-02;\n    WW4 = ((((((((((((( 1.04072340345039E-14*Y-1.60808044529211E-13)*\n                      Y+2.183534866798E-12)*Y-2.939403008391E-11)*Y+\n                    3.679254029085E-10)*Y-4.23775673047899E-09 )*Y+\n                  4.46559231067006E-08 )*Y-4.26488836563267E-07 )*Y+\n                3.64721335274973E-06 )*Y-2.74868382777722E-05 )*Y+\n              1.78586118867488E-04 )*Y-9.68428981886534E-04 )*Y+\n            4.16002324339929E-03 )*Y-1.28290192663141E-02 )*Y+\n      2.22353727685016E-02;\n    WW5 = ((((((((((((((-8.16770412525963E-16*Y+1.31376515047977E-14)*\n                       Y-1.856950818865E-13)*Y+2.596836515749E-12)*Y-\n                     3.372639523006E-11)*Y+4.025371849467E-10)*Y-\n                   4.389453269417E-09)*Y+4.332753856271E-08)*Y-\n                 3.82673275931962E-07 )*Y+2.98006900751543E-06 )*Y-\n               2.00718990300052E-05 )*Y+1.13876001386361E-04 )*Y-\n             5.23627942443563E-04 )*Y+1.83524565118203E-03 )*Y-\n           4.37785737450783E-03 )*Y+5.36963805223095E-03;\n  } else if (X < 10.0) {\n    Y = X-7.5E+00;\n    RT1 = ((((((((-1.13825201010775E-14*Y+1.89737681670375E-13)*Y-\n                 4.81561201185876E-12 )*Y+1.56666512163407E-10 )*Y-\n               3.73782213255083E-09 )*Y+9.15858355075147E-08 )*Y-\n             2.13775073585629E-06 )*Y+4.56547356365536E-05 )*Y-\n           8.68003909323740E-04 )*Y+1.22703754069176E-02;\n    RT2 = (((((((((-3.67160504428358E-15*Y+1.27876280158297E-14)*Y-\n                  1.296476623788E-12)*Y+1.477175434354E-11)*Y+\n                5.464102147892E-10)*Y-2.42538340602723E-08 )*Y+\n              8.20460740637617E-07 )*Y-2.20379304598661E-05 )*Y+\n            4.90295372978785E-04 )*Y-9.14294111576119E-03 )*Y+\n      1.22590403403690E-01;\n    RT3 = ((((((((( 1.39017367502123E-14*Y-6.96391385426890E-13)*Y+\n                  1.176946020731E-12)*Y+1.725627235645E-10)*Y-\n                3.686383856300E-09)*Y+2.87495324207095E-08 )*Y+\n              1.71307311000282E-06 )*Y-7.94273603184629E-05 )*Y+\n            2.00938064965897E-03 )*Y-3.63329491677178E-02 )*Y+\n      4.34393683888443E-01;\n    RT4 = ((((((((((-1.27815158195209E-14*Y+1.99910415869821E-14)*Y+\n                   3.753542914426E-12)*Y-2.708018219579E-11)*Y-\n                 1.190574776587E-09)*Y+1.106696436509E-08)*Y+\n               3.954955671326E-07)*Y-4.398596059588E-06)*Y-\n             2.01087998907735E-04 )*Y+7.89092425542937E-03 )*Y-\n           1.42056749162695E-01 )*Y+1.39964149420683E+00;\n    RT5 = ((((((((((-1.19442341030461E-13*Y-2.34074833275956E-12)*Y+\n                   6.861649627426E-12)*Y+6.082671496226E-10)*Y+\n                 5.381160105420E-09)*Y-6.253297138700E-08)*Y-\n               2.135966835050E-06)*Y-2.373394341886E-05)*Y+\n             2.88711171412814E-06 )*Y+4.85221195290753E-02 )*Y-\n           1.04346091985269E+00 )*Y+7.89901551676692E+00;\n    WW1 = ((((((((( 7.95526040108997E-15*Y-2.48593096128045E-13)*Y+\n                  4.761246208720E-12)*Y-9.535763686605E-11)*Y+\n                2.225273630974E-09)*Y-4.49796778054865E-08 )*Y+\n              9.17812870287386E-07 )*Y-1.86764236490502E-05 )*Y+\n            3.76807779068053E-04 )*Y-8.10456360143408E-03 )*Y+\n      2.01097936411496E-01;\n    WW2 = ((((((((((( 1.25678686624734E-15*Y-2.34266248891173E-14)*Y+\n                    3.973252415832E-13)*Y-6.830539401049E-12)*Y+\n                  1.140771033372E-10)*Y-1.82546185762009E-09 )*Y+\n                2.77209637550134E-08 )*Y-4.01726946190383E-07 )*Y+\n              5.48227244014763E-06 )*Y-6.95676245982121E-05 )*Y+\n            8.05193921815776E-04 )*Y-8.15528438784469E-03 )*Y+\n      9.71769901268114E-02;\n    WW3 = ((((((((((((-8.20929494859896E-16*Y+1.37356038393016E-14)*Y-\n                     2.022863065220E-13)*Y+3.058055403795E-12)*Y-\n                   4.387890955243E-11)*Y+5.923946274445E-10)*Y-\n                 7.503659964159E-09)*Y+8.851599803902E-08)*Y-\n               9.65561998415038E-07 )*Y+9.60884622778092E-06 )*Y-\n             8.56551787594404E-05 )*Y+6.66057194311179E-04 )*Y-\n           4.17753183902198E-03 )*Y+2.25443826852447E-02;\n    WW4 = ((((((((((((((-1.08764612488790E-17*Y+1.85299909689937E-16)*\n                       Y-2.730195628655E-15)*Y+4.127368817265E-14)*Y-\n                     5.881379088074E-13)*Y+7.805245193391E-12)*Y-\n                   9.632707991704E-11)*Y+1.099047050624E-09)*Y-\n                 1.15042731790748E-08 )*Y+1.09415155268932E-07 )*Y-\n               9.33687124875935E-07 )*Y+7.02338477986218E-06 )*Y-\n             4.53759748787756E-05 )*Y+2.41722511389146E-04 )*Y-\n           9.75935943447037E-04 )*Y+2.57520532789644E-03;\n    WW5 = ((((((((((((((( 7.28996979748849E-19*Y-1.26518146195173E-17)\n                        *Y+1.886145834486E-16)*Y-2.876728287383E-15)*Y+\n                      4.114588668138E-14)*Y-5.44436631413933E-13 )*Y+\n                    6.64976446790959E-12 )*Y-7.44560069974940E-11 )*Y+\n                  7.57553198166848E-10 )*Y-6.92956101109829E-09 )*Y+\n                5.62222859033624E-08 )*Y-3.97500114084351E-07 )*Y+\n              2.39039126138140E-06 )*Y-1.18023950002105E-05 )*Y+\n            4.52254031046244E-05 )*Y-1.21113782150370E-04 )*Y+\n      1.75013126731224E-04;\n  } else if (X < 15.0) {\n    Y = X-12.5E+00;\n    RT1 = ((((((((((-4.16387977337393E-17*Y+7.20872997373860E-16)*Y+\n                   1.395993802064E-14)*Y+3.660484641252E-14)*Y-\n                 4.154857548139E-12)*Y+2.301379846544E-11)*Y-\n               1.033307012866E-09)*Y+3.997777641049E-08)*Y-\n             9.35118186333939E-07 )*Y+2.38589932752937E-05 )*Y-\n           5.35185183652937E-04 )*Y+8.85218988709735E-03;\n    RT2 = ((((((((((-4.56279214732217E-16*Y+6.24941647247927E-15)*Y+\n                   1.737896339191E-13)*Y+8.964205979517E-14)*Y-\n                 3.538906780633E-11)*Y+9.561341254948E-11)*Y-\n               9.772831891310E-09)*Y+4.240340194620E-07)*Y-\n             1.02384302866534E-05 )*Y+2.57987709704822E-04 )*Y-\n           5.54735977651677E-03 )*Y+8.68245143991948E-02;\n    RT3 = ((((((((((-2.52879337929239E-15*Y+2.13925810087833E-14)*Y+\n                   7.884307667104E-13)*Y-9.023398159510E-13)*Y-\n                 5.814101544957E-11)*Y-1.333480437968E-09)*Y-\n               2.217064940373E-08)*Y+1.643290788086E-06)*Y-\n             4.39602147345028E-05 )*Y+1.08648982748911E-03 )*Y-\n           2.13014521653498E-02 )*Y+2.94150684465425E-01;\n    RT4 = ((((((((((-6.42391438038888E-15*Y+5.37848223438815E-15)*Y+\n                   8.960828117859E-13)*Y+5.214153461337E-11)*Y-\n                 1.106601744067E-10)*Y-2.007890743962E-08)*Y+\n               1.543764346501E-07)*Y+4.520749076914E-06)*Y-\n             1.88893338587047E-04 )*Y+4.73264487389288E-03 )*Y-\n           7.91197893350253E-02 )*Y+8.60057928514554E-01;\n    RT5 = (((((((((((-2.24366166957225E-14*Y+4.87224967526081E-14)*Y+\n                    5.587369053655E-12)*Y-3.045253104617E-12)*Y-\n                  1.223983883080E-09)*Y-2.05603889396319E-09 )*Y+\n                2.58604071603561E-07 )*Y+1.34240904266268E-06 )*Y-\n              5.72877569731162E-05 )*Y-9.56275105032191E-04 )*Y+\n            4.23367010370921E-02 )*Y-5.76800927133412E-01 )*Y+\n      3.87328263873381E+00;\n    WW1 = ((((((((( 8.98007931950169E-15*Y+7.25673623859497E-14)*Y+\n                  5.851494250405E-14)*Y-4.234204823846E-11)*Y+\n                3.911507312679E-10)*Y-9.65094802088511E-09 )*Y+\n              3.42197444235714E-07 )*Y-7.51821178144509E-06 )*Y+\n            1.94218051498662E-04 )*Y-5.38533819142287E-03 )*Y+\n      1.68122596736809E-01;\n    WW2 = ((((((((((-1.05490525395105E-15*Y+1.96855386549388E-14)*Y-\n                   5.500330153548E-13)*Y+1.003849567976E-11)*Y-\n                 1.720997242621E-10)*Y+3.533277061402E-09)*Y-\n               6.389171736029E-08)*Y+1.046236652393E-06)*Y-\n             1.73148206795827E-05 )*Y+2.57820531617185E-04 )*Y-\n           3.46188265338350E-03 )*Y+7.03302497508176E-02;\n    WW3 = ((((((((((( 3.60020423754545E-16*Y-6.24245825017148E-15)*Y+\n                    9.945311467434E-14)*Y-1.749051512721E-12)*Y+\n                  2.768503957853E-11)*Y-4.08688551136506E-10 )*Y+\n                6.04189063303610E-09 )*Y-8.23540111024147E-08 )*Y+\n              1.01503783870262E-06 )*Y-1.20490761741576E-05 )*Y+\n            1.26928442448148E-04 )*Y-1.05539461930597E-03 )*Y+\n      1.15543698537013E-02;\n    WW4 = ((((((((((((( 2.51163533058925E-18*Y-4.31723745510697E-17)*\n                      Y+6.557620865832E-16)*Y-1.016528519495E-14)*Y+\n                    1.491302084832E-13)*Y-2.06638666222265E-12 )*Y+\n                  2.67958697789258E-11 )*Y-3.23322654638336E-10 )*Y+\n                3.63722952167779E-09 )*Y-3.75484943783021E-08 )*Y+\n              3.49164261987184E-07 )*Y-2.92658670674908E-06 )*Y+\n            2.12937256719543E-05 )*Y-1.19434130620929E-04 )*Y+\n      6.45524336158384E-04;\n    WW5 = ((((((((((((((-1.29043630202811E-19*Y+2.16234952241296E-18)*\n                       Y-3.107631557965E-17)*Y+4.570804313173E-16)*Y-\n                     6.301348858104E-15)*Y+8.031304476153E-14)*Y-\n                   9.446196472547E-13)*Y+1.018245804339E-11)*Y-\n                 9.96995451348129E-11 )*Y+8.77489010276305E-10 )*Y-\n               6.84655877575364E-09 )*Y+4.64460857084983E-08 )*Y-\n             2.66924538268397E-07 )*Y+1.24621276265907E-06 )*Y-\n           4.30868944351523E-06 )*Y+9.94307982432868E-06;\n  } else if (X < 20.0){\n    Y = X-17.5E+00;\n    RT1 = (((((((((( 1.91875764545740E-16*Y+7.8357401095707E-16)*Y-\n                   3.260875931644E-14)*Y-1.186752035569E-13)*Y+\n                 4.275180095653E-12)*Y+3.357056136731E-11)*Y-\n               1.123776903884E-09)*Y+1.231203269887E-08)*Y-\n             3.99851421361031E-07 )*Y+1.45418822817771E-05 )*Y-\n           3.49912254976317E-04 )*Y+6.67768703938812E-03;\n    RT2 = (((((((((( 2.02778478673555E-15*Y+1.01640716785099E-14)*Y-\n                   3.385363492036E-13)*Y-1.615655871159E-12)*Y+\n                 4.527419140333E-11)*Y+3.853670706486E-10)*Y-\n               1.184607130107E-08)*Y+1.347873288827E-07)*Y-\n             4.47788241748377E-06 )*Y+1.54942754358273E-04 )*Y-\n           3.55524254280266E-03 )*Y+6.44912219301603E-02;\n    RT3 = (((((((((( 7.79850771456444E-15*Y+6.00464406395001E-14)*Y-\n                   1.249779730869E-12)*Y-1.020720636353E-11)*Y+\n                 1.814709816693E-10)*Y+1.766397336977E-09)*Y-\n               4.603559449010E-08)*Y+5.863956443581E-07)*Y-\n             2.03797212506691E-05 )*Y+6.31405161185185E-04 )*Y-\n           1.30102750145071E-02 )*Y+2.10244289044705E-01;\n    RT4 = (((((((((((-2.92397030777912E-15*Y+1.94152129078465E-14)*Y+\n                    4.859447665850E-13)*Y-3.217227223463E-12)*Y-\n                  7.484522135512E-11)*Y+7.19101516047753E-10 )*Y+\n                6.88409355245582E-09 )*Y-1.44374545515769E-07 )*Y+\n              2.74941013315834E-06 )*Y-1.02790452049013E-04 )*Y+\n            2.59924221372643E-03 )*Y-4.35712368303551E-02 )*Y+\n      5.62170709585029E-01;\n    RT5 = ((((((((((( 1.17976126840060E-14*Y+1.24156229350669E-13)*Y-\n                    3.892741622280E-12)*Y-7.755793199043E-12)*Y+\n                  9.492190032313E-10)*Y-4.98680128123353E-09 )*Y-\n                1.81502268782664E-07 )*Y+2.69463269394888E-06 )*Y+\n              2.50032154421640E-05 )*Y-1.33684303917681E-03 )*Y+\n            2.29121951862538E-02 )*Y-2.45653725061323E-01 )*Y+\n      1.89999883453047E+00;\n    WW1 = (((((((((( 1.74841995087592E-15*Y-6.95671892641256E-16)*Y-\n                   3.000659497257E-13)*Y+2.021279817961E-13)*Y+\n                 3.853596935400E-11)*Y+1.461418533652E-10)*Y-\n               1.014517563435E-08)*Y+1.132736008979E-07)*Y-\n             2.86605475073259E-06 )*Y+1.21958354908768E-04 )*Y-\n           3.86293751153466E-03 )*Y+1.45298342081522E-01;\n    WW2 = ((((((((((-1.11199320525573E-15*Y+1.85007587796671E-15)*Y+\n                   1.220613939709E-13)*Y+1.275068098526E-12)*Y-\n                 5.341838883262E-11)*Y+6.161037256669E-10)*Y-\n               1.009147879750E-08)*Y+2.907862965346E-07)*Y-\n             6.12300038720919E-06 )*Y+1.00104454489518E-04 )*Y-\n           1.80677298502757E-03 )*Y+5.78009914536630E-02;\n    WW3 = ((((((((((-9.49816486853687E-16*Y+6.67922080354234E-15)*Y+\n                   2.606163540537E-15)*Y+1.983799950150E-12)*Y-\n                 5.400548574357E-11)*Y+6.638043374114E-10)*Y-\n               8.799518866802E-09)*Y+1.791418482685E-07)*Y-\n             2.96075397351101E-06 )*Y+3.38028206156144E-05 )*Y-\n           3.58426847857878E-04 )*Y+8.39213709428516E-03;\n    WW4 = ((((((((((( 1.33829971060180E-17*Y-3.44841877844140E-16)*Y+\n                    4.745009557656E-15)*Y-6.033814209875E-14)*Y+\n                  1.049256040808E-12)*Y-1.70859789556117E-11 )*Y+\n                2.15219425727959E-10 )*Y-2.52746574206884E-09 )*Y+\n              3.27761714422960E-08 )*Y-3.90387662925193E-07 )*Y+\n            3.46340204593870E-06 )*Y-2.43236345136782E-05 )*Y+\n      3.54846978585226E-04;\n    WW5 = ((((((((((((( 2.69412277020887E-20*Y-4.24837886165685E-19)*\n                      Y+6.030500065438E-18)*Y-9.069722758289E-17)*Y+\n                    1.246599177672E-15)*Y-1.56872999797549E-14 )*Y+\n                  1.87305099552692E-13 )*Y-2.09498886675861E-12 )*Y+\n                2.11630022068394E-11 )*Y-1.92566242323525E-10 )*Y+\n              1.62012436344069E-09 )*Y-1.23621614171556E-08 )*Y+\n            7.72165684563049E-08 )*Y-3.59858901591047E-07 )*Y+\n      2.43682618601000E-06;\n  } else if (X < 25.0) {\n    Y = X-22.5E+00;\n    RT1 = (((((((((-1.13927848238726E-15*Y+7.39404133595713E-15)*Y+\n                  1.445982921243E-13)*Y-2.676703245252E-12)*Y+\n                5.823521627177E-12)*Y+2.17264723874381E-10 )*Y+\n              3.56242145897468E-09 )*Y-3.03763737404491E-07 )*Y+\n            9.46859114120901E-06 )*Y-2.30896753853196E-04 )*Y+\n      5.24663913001114E-03;\n    RT2 = (((((((((( 2.89872355524581E-16*Y-1.22296292045864E-14)*Y+\n                   6.184065097200E-14)*Y+1.649846591230E-12)*Y-\n                 2.729713905266E-11)*Y+3.709913790650E-11)*Y+\n               2.216486288382E-09)*Y+4.616160236414E-08)*Y-\n             3.32380270861364E-06 )*Y+9.84635072633776E-05 )*Y-\n           2.30092118015697E-03 )*Y+5.00845183695073E-02;\n    RT3 = (((((((((( 1.97068646590923E-15*Y-4.89419270626800E-14)*Y+\n                   1.136466605916E-13)*Y+7.546203883874E-12)*Y-\n                 9.635646767455E-11)*Y-8.295965491209E-11)*Y+\n               7.534109114453E-09)*Y+2.699970652707E-07)*Y-\n             1.42982334217081E-05 )*Y+3.78290946669264E-04 )*Y-\n           8.03133015084373E-03 )*Y+1.58689469640791E-01;\n    RT4 = (((((((((( 1.33642069941389E-14*Y-1.55850612605745E-13)*Y-\n                   7.522712577474E-13)*Y+3.209520801187E-11)*Y-\n                 2.075594313618E-10)*Y-2.070575894402E-09)*Y+\n               7.323046997451E-09)*Y+1.851491550417E-06)*Y-\n             6.37524802411383E-05 )*Y+1.36795464918785E-03 )*Y-\n           2.42051126993146E-02 )*Y+3.97847167557815E-01;\n    RT5 = ((((((((((-6.07053986130526E-14*Y+1.04447493138843E-12)*Y-\n                   4.286617818951E-13)*Y-2.632066100073E-10)*Y+\n                 4.804518986559E-09)*Y-1.835675889421E-08)*Y-\n               1.068175391334E-06)*Y+3.292234974141E-05)*Y-\n             5.94805357558251E-04 )*Y+8.29382168612791E-03 )*Y-\n           9.93122509049447E-02 )*Y+1.09857804755042E+00;\n    WW1 = (((((((((-9.10338640266542E-15*Y+1.00438927627833E-13)*Y+\n                  7.817349237071E-13)*Y-2.547619474232E-11)*Y+\n                1.479321506529E-10)*Y+1.52314028857627E-09 )*Y+\n              9.20072040917242E-09 )*Y-2.19427111221848E-06 )*Y+\n            8.65797782880311E-05 )*Y-2.82718629312875E-03 )*Y+\n      1.28718310443295E-01;\n    WW2 = ((((((((( 5.52380927618760E-15*Y-6.43424400204124E-14)*Y-\n                  2.358734508092E-13)*Y+8.261326648131E-12)*Y+\n                9.229645304956E-11)*Y-5.68108973828949E-09 )*Y+\n              1.22477891136278E-07 )*Y-2.11919643127927E-06 )*Y+\n            4.23605032368922E-05 )*Y-1.14423444576221E-03 )*Y+\n      5.06607252890186E-02;\n    WW3 = ((((((((( 3.99457454087556E-15*Y-5.11826702824182E-14)*Y-\n                  4.157593182747E-14)*Y+4.214670817758E-12)*Y+\n                6.705582751532E-11)*Y-3.36086411698418E-09 )*Y+\n              6.07453633298986E-08 )*Y-7.40736211041247E-07 )*Y+\n            8.84176371665149E-06 )*Y-1.72559275066834E-04 )*Y+\n      7.16639814253567E-03;\n    WW4 = (((((((((((-2.14649508112234E-18*Y-2.45525846412281E-18)*Y+\n                    6.126212599772E-16)*Y-8.526651626939E-15)*Y+\n                  4.826636065733E-14)*Y-3.39554163649740E-13 )*Y+\n                1.67070784862985E-11 )*Y-4.42671979311163E-10 )*Y+\n              6.77368055908400E-09 )*Y-7.03520999708859E-08 )*Y+\n            6.04993294708874E-07 )*Y-7.80555094280483E-06 )*Y+\n      2.85954806605017E-04;\n    WW5 = ((((((((((((-5.63938733073804E-21*Y+6.92182516324628E-20)*Y-\n                     1.586937691507E-18)*Y+3.357639744582E-17)*Y-\n                   4.810285046442E-16)*Y+5.386312669975E-15)*Y-\n                 6.117895297439E-14)*Y+8.441808227634E-13)*Y-\n               1.18527596836592E-11 )*Y+1.36296870441445E-10 )*Y-\n             1.17842611094141E-09 )*Y+7.80430641995926E-09 )*Y-\n           5.97767417400540E-08 )*Y+1.65186146094969E-06;\n  } else if (X < 40) {\n    WW1 = sqrt(PIE4/X);\n    E = exp(-X);\n    RT1 = ((((((((-1.73363958895356E-06*X+1.19921331441483E-04)*X -\n                 1.59437614121125E-02)*X+1.13467897349442E+00)*X -\n               4.47216460864586E+01)*X+1.06251216612604E+03)*X -\n             1.52073917378512E+04)*X+1.20662887111273E+05)*X -\n           4.07186366852475E+05)*E + R15/(X-R15);\n    RT2 = ((((((((-1.60102542621710E-05*X+1.10331262112395E-03)*X -\n                 1.50043662589017E-01)*X+1.05563640866077E+01)*X -\n               4.10468817024806E+02)*X+9.62604416506819E+03)*X -\n             1.35888069838270E+05)*X+1.06107577038340E+06)*X -\n           3.51190792816119E+06)*E + R25/(X-R25);\n    RT3 = ((((((((-4.48880032128422E-05*X+2.69025112122177E-03)*X -\n                 4.01048115525954E-01)*X+2.78360021977405E+01)*X -\n               1.04891729356965E+03)*X+2.36985942687423E+04)*X -\n             3.19504627257548E+05)*X+2.34879693563358E+06)*X -\n           7.16341568174085E+06)*E + R35/(X-R35);\n    RT4 = ((((((((-6.38526371092582E-05*X-2.29263585792626E-03)*X -\n                 7.65735935499627E-02)*X+9.12692349152792E+00)*X -\n               2.32077034386717E+02)*X+2.81839578728845E+02)*X +\n             9.59529683876419E+04)*X-1.77638956809518E+06)*X +\n           1.02489759645410E+07)*E + R45/(X-R45);\n    RT5 = ((((((((-3.59049364231569E-05*X-2.25963977930044E-02)*X +\n                 1.12594870794668E+00)*X-4.56752462103909E+01)*X +\n               1.05804526830637E+03)*X-1.16003199605875E+04)*X -\n             4.07297627297272E+04)*X+2.22215528319857E+06)*X -\n           1.61196455032613E+07)*E + R55/(X-R55);\n    WW5 = (((((((((-4.61100906133970E-10*X+1.43069932644286E-07)*X -\n                  1.63960915431080E-05)*X+1.15791154612838E-03)*X -\n                5.30573476742071E-02)*X+1.61156533367153E+00)*X -\n              3.23248143316007E+01)*X+4.12007318109157E+02)*X -\n            3.02260070158372E+03)*X+9.71575094154768E+03)*E + W55*WW1;\n    WW4 = (((((((((-2.40799435809950E-08*X+8.12621667601546E-06)*X -\n                  9.04491430884113E-04)*X+6.37686375770059E-02)*X -\n                2.96135703135647E+00)*X+9.15142356996330E+01)*X -\n              1.86971865249111E+03)*X+2.42945528916947E+04)*X -\n            1.81852473229081E+05)*X+5.96854758661427E+05)*E + W45*WW1;\n    WW3 = (((((((( 1.83574464457207E-05*X-1.54837969489927E-03)*X +\n                 1.18520453711586E-01)*X-6.69649981309161E+00)*X +\n               2.44789386487321E+02)*X-5.68832664556359E+03)*X +\n             8.14507604229357E+04)*X-6.55181056671474E+05)*X +\n           2.26410896607237E+06)*E + W35*WW1;\n    WW2 = (((((((( 2.77778345870650E-05*X-2.22835017655890E-03)*X +\n                 1.61077633475573E-01)*X-8.96743743396132E+00)*X +\n               3.28062687293374E+02)*X-7.65722701219557E+03)*X +\n             1.10255055017664E+05)*X-8.92528122219324E+05)*X +\n           3.10638627744347E+06)*E + W25*WW1;\n    WW1 = WW1-0.01962E+00*E-WW2-WW3-WW4-WW5;\n  } else if (X < 59.0) {\n    WW1 = sqrt(PIE4/X);\n    XXX = pow(X,3.0);\n    E = XXX*exp(-X);\n    RT1 = (((-2.43758528330205E-02*X+2.07301567989771E+00)*X -\n            6.45964225381113E+01)*X+7.14160088655470E+02)*E + R15/(X-R15);\n    RT2 = (((-2.28861955413636E-01*X+1.93190784733691E+01)*X -\n            5.99774730340912E+02)*X+6.61844165304871E+03)*E + R25/(X-R25);\n    RT3 = (((-6.95053039285586E-01*X+5.76874090316016E+01)*X -\n            1.77704143225520E+03)*X+1.95366082947811E+04)*E + R35/(X-R35);\n    RT4 = (((-1.58072809087018E+00*X+1.27050801091948E+02)*X -\n            3.86687350914280E+03)*X+4.23024828121420E+04)*E + R45/(X-R45);\n    RT5 = (((-3.33963830405396E+00*X+2.51830424600204E+02)*X -\n            7.57728527654961E+03)*X+8.21966816595690E+04)*E + R55/(X-R55);\n    E = XXX*E;\n    WW5 = (( 1.35482430510942E-08*X-3.27722199212781E-07)*X +\n           2.41522703684296E-06)*E + W55*WW1;\n    WW4 = (( 1.23464092261605E-06*X-3.55224564275590E-05)*X +\n           3.03274662192286E-04)*E + W45*WW1;\n    WW3 = (( 1.34547929260279E-05*X-4.19389884772726E-04)*X +\n           3.87706687610809E-03)*E + W35*WW1;\n    WW2 = (( 2.09539509123135E-05*X-6.87646614786982E-04)*X +\n           6.68743788585688E-03)*E + W25*WW1;\n    WW1 = WW1-WW2-WW3-WW4-WW5;\n  } else {\n    WW1 = sqrt(PIE4/X);\n    RT1 = R15/(X-R15);\n    RT2 = R25/(X-R25);\n    RT3 = R35/(X-R35);\n    RT4 = R45/(X-R45);\n    RT5 = R55/(X-R55);\n    WW2 = W25*WW1;\n    WW3 = W35*WW1;\n    WW4 = W45*WW1;\n    WW5 = W55*WW1;\n    WW1 = WW1-WW2-WW3-WW4-WW5;\n  }\n  roots[0] = RT1;\n  weights[0] = WW1;\n  roots[1] = RT2;\n  weights[1] = WW2;\n  roots[2] = RT3;\n  weights[2] = WW3;\n  roots[3] = RT4;\n  weights[3] = WW4;\n  roots[4] = RT5;\n  weights[4] = WW5;\n  return;\n}\n\n__device__ void cuda_Root6_dp(int n,double X, double roots[], double weights[]){\n  // Root6 not implemented yet\n  return;\n}\n\n__device__ double cuda_Int1d_dp(int i, int j, int k, int l,\n                                double xi, double xj, double xk, double xl,\n                                double alpha_ij_A, double alpha_kl_B, double sqrt_AB,\n                                double A, double B, double Px, double Qx,\n                                double inv_t1, double B00, double B1, double B1p, \n                                double G[][MAXROOTS])\n{\n  // Form G(n,m)=I(n,0,m,0) intermediate values for a Rys polynomial \n  int n = i+j;\n  int m = k+l;\n\n  double xij = xi-xj;\n  double xkl = xk-xl;\n\n  // RecurFactorsGamess\n  double C  = (Px-xi) * inv_t1 + (B*(Qx-xi)+A*(Px-xi))*B00*2.0;\n  double Cp = (Qx-xk) * inv_t1 + (B*(Qx-xk)+A*(Px-xk))*B00*2.0;\n\n  // ABD eq 11. \n  G[0][0] = M_PI * exp(-alpha_ij_A*xij*xij -alpha_kl_B*xkl*xkl) / sqrt_AB;\n\n  if (n > 0) { G[1][0] = C *G[0][0]; } // ABD eq 15 \n  if (m > 0) { G[0][1] = Cp*G[0][0]; } // ABD eq 16 \n\n  for (int a = 2; a < n+1; ++ a) { G[a][0] = B1 *(a-1)*G[a-2][0] + C *G[a-1][0]; } \n  for (int b = 2; b < m+1; ++ b) { G[0][b] = B1p*(b-1)*G[0][b-2] + Cp*G[0][b-1]; } \n\n  if ((m>0) && (n>0)){\n    for (int a=1; a<n+1; ++a){\n      G[a][1] = a*B00*G[a-1][0] + Cp*G[a][0];\n      for (int b=2; b<m+1; ++b)\n        G[a][b] = B1p*(b-1)*G[a][b-2] + a*B00*G[a-1][b-1] + Cp*G[a][b-1];\n    }\n  }\n\n  // Compute and output I(i,j,k,l) from I(i+j,0,k+l,0) (G) \n  double ijkl = 0.0;\n  for (int m=0; m<l+1; ++m){\n    double ijm0 = 0.0;\n    for (int n=0; n<j+1; ++n) // I(i,j,m,0)<-I(n,0,m,0)  \n      ijm0 += cuda_binomial(j,n)*pow(xij,(double)(j-n))*G[n+i][m+k];\n    ijkl += cuda_binomial(l,m)*pow(xkl,(double)(l-m))*ijm0; // I(i,j,k,l)<-I(i,j,m,0) \n  }\n\n  return ijkl;\n}\n\n__device__ void cuda_Roots_dp(int n, double X, double roots[], double weights[]){\n  if (n <= 3)\n    cuda_Root123_dp(n,X, roots,weights);\n  else if (n==4) \n    cuda_Root4_dp(X, roots,weights);\n  else if (n==5)\n    cuda_Root5_dp(X, roots,weights);\n  else\n    cuda_Root6_dp(n,X, roots,weights);\n  return;\n}\n\ninline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\ninline __host__ __device__ float4 fabs(float4 v)\n{\n    return make_float4(fabs(v.x), fabs(v.y), fabs(v.z), fabs(v.w));\n}\n\n__device__ int cuda_ij2intindex(int i, int j)\n{\n  if (i < j) {\n    int t = i; i = j; j = t;\n  }\n  return i * (i + 1) / 2 + j;\n}\n\n__device__ double cuda_rys_pbf_dp(const double *ptr_i, const double *ptr_j, \n                                  const double *ptr_k, const double *ptr_l)\n{\n  // download xyz, lmn, expon, and coef*norm\n  double xa = ptr_i[0];\n  double ya = ptr_i[1];\n  double za = ptr_i[2];\n  int    la = (int)ptr_i[3];\n  int    ma = (int)ptr_i[4];\n  int    na = (int)ptr_i[5];\n  double alphaa = ptr_i[6];\n  double norma  = ptr_i[7];\n\n  double xb = ptr_j[0];\n  double yb = ptr_j[1];\n  double zb = ptr_j[2];\n  int    lb = (int)ptr_j[3];\n  int    mb = (int)ptr_j[4];\n  int    nb = (int)ptr_j[5];\n  double alphab = ptr_j[6];\n  double normb  = ptr_j[7];\n\n  double xc = ptr_k[0];\n  double yc = ptr_k[1];\n  double zc = ptr_k[2];\n  int    lc = (int)ptr_k[3];\n  int    mc = (int)ptr_k[4];\n  int    nc = (int)ptr_k[5];\n  double alphac = ptr_k[6];\n  double normc  = ptr_k[7];\n\n  double xd = ptr_l[0];\n  double yd = ptr_l[1];\n  double zd = ptr_l[2];\n  int    ld = (int)ptr_l[3];\n  int    md = (int)ptr_l[4];\n  int    nd = (int)ptr_l[5];\n  double alphad = ptr_l[6];\n  double normd  = ptr_l[7];\n\n  // calculate primitive integral [ij|kl]\n  int norder,i;\n  double A,B,xp,yp,zp,xq,yq,zq,X,rho,sum,t,Ix,Iy,Iz;\n  \n  norder = (la+ma+na+lb+nb+mb+lc+mc+nc+ld+md+nd)/2 + 1;\n  A = alphaa+alphab; \n  B = alphac+alphad;\n\n  xp = (alphaa*xa+alphab*xb)/A;\n  yp = (alphaa*ya+alphab*yb)/A;\n  zp = (alphaa*za+alphab*zb)/A;\n\n  xq = (alphac*xc+alphad*xd)/B;\n  yq = (alphac*yc+alphad*yd)/B;\n  zq = (alphac*zc+alphad*zd)/B;\n\n  rho = A*B/(A+B);\n  X = rho * ((xp-xq)*(xp-xq)+(yp-yq)*(yp-yq)+(zp-zq)*(zp-zq));\n\n  double alpha_ab_A = alphaa * alphab / A;\n  double alpha_cd_B = alphac * alphad / B;\n  double sqrt_AB = sqrt(A * B);\n\n  double roots[MAXROOTS],weights[MAXROOTS];\n  double G[MAXROOTS][MAXROOTS];\n\n  cuda_Roots_dp(norder,X, roots,weights); // get currect roots/weights\n\n  sum = 0.;\n  for (i=0; i<norder; ++i){\n    t = roots[i];\n\n    double inv_t1, B00, B1, B1p;\n    inv_t1 = 1.0 / (1 + t);\n    B00 = 0.5 * t/(A+B) * inv_t1;\n    B1  = 0.5 / A * inv_t1 + B00;\n    B1p = 0.5 / B * inv_t1 + B00;\n\n    Ix = cuda_Int1d_dp(la,lb,lc,ld, xa,xb,xc,xd,\n           alpha_ab_A,alpha_cd_B,sqrt_AB, A,B,xp,xq, inv_t1,B00,B1,B1p, G);\n    Iy = cuda_Int1d_dp(ma,mb,mc,md, ya,yb,yc,yd,\n           alpha_ab_A,alpha_cd_B,sqrt_AB, A,B,yp,yq, inv_t1,B00,B1,B1p, G);\n    Iz = cuda_Int1d_dp(na,nb,nc,nd, za,zb,zc,zd,\n           alpha_ab_A,alpha_cd_B,sqrt_AB, A,B,zp,zq, inv_t1,B00,B1,B1p, G);\n    sum = sum + Ix*Iy*Iz*weights[i]; /* ABD eq 5 & 9 */\n  }\n\n  // inv_sqrt_pi_2: 2.0*sqrt(1.0/M_PI) = 1.12837916709551255856\n  return 1.12837916709551255856 * sqrt(rho)*norma*normb*normc*normd*sum; /* ABD eq 5 & 9 */\n}\n\n__global__ void cuda_mat_J_PI_dp(\n  const double *__restrict pbf_xlec,\n  const int *__restrict pbf_to_cbf,\n  int n_pbf,\n  const double *__restrict mat_D,\n  double *__restrict mat_J_PI,\n  const double *__restrict mat_Q)\n{\n    __shared__ double elem_J_PI[BLOCKSIZE * BLOCKSIZE];\n\n    // each block scans over [ij|??] and sum up to a primitive J matrix element\n    int i = blockIdx.x;\n    int j = blockIdx.y;\n\n    // avoid accessing out of bounds elements and make use of i<=>j symmetry\n    if (i >= n_pbf || j > i) { return; }\n\n    int ij = cuda_ij2intindex(i,j);\n\n    const double *ptr_i = &pbf_xlec[i * 8];\n    const double *ptr_j = &pbf_xlec[j * 8];\n\n    int a = pbf_to_cbf[i];\n    int b = pbf_to_cbf[j];\n    int ab = cuda_ij2intindex(a,b);\n\n    // initialize shared array\n    elem_J_PI[threadIdx.x * BLOCKSIZE + threadIdx.y] = 0.0;\n\n    for (int k = threadIdx.x; k < n_pbf; k += BLOCKSIZE)\n    {\n        int c = pbf_to_cbf[k];\n        const double *ptr_k = &pbf_xlec[k * 8];\n\n        // NOTE: make use of k<=>l symmetry\n        for (int l = threadIdx.y; l <= k; l += BLOCKSIZE)\n        {\n            int d = pbf_to_cbf[l];\n            int cd = cuda_ij2intindex(c,d);\n\n            // Schwartz screening\n            if (fabs(mat_Q[ab] * mat_Q[cd] * mat_D[cd]) < SCREEN_THR) { continue; }\n\n            const double *ptr_l = &pbf_xlec[l * 8];\n\n            // calculate ERI\n            double this_eri = cuda_rys_pbf_dp(ptr_i, ptr_j, ptr_k, ptr_l);\n\n            // NOTE: doubling for off-diagonal elements of D due to k<=>l symmetry\n            elem_J_PI[threadIdx.x * BLOCKSIZE + threadIdx.y] += this_eri * mat_D[cd] * (k == l ? 1.0 : 2.0);\n        }\n    }\n\n    __syncthreads();\n\n    // only update mat_J_PI on one thread of the block\n    if (0 == threadIdx.x && 0 == threadIdx.y)\n    {\n        mat_J_PI[ij] = 0.0; \n        for (int t1 = 0; t1 < BLOCKSIZE; ++ t1) {\n            for (int t2 = 0; t2 < BLOCKSIZE; ++ t2) {\n                mat_J_PI[ij] += elem_J_PI[t1 * BLOCKSIZE + t2];\n            }\n        }\n    }\n}",
            "__device__ int cuda_fact(int n){\n  int result = 1;\n  for (int i = 2; i <= n; i++) result *= i;\n  return result;\n}\n\n__device__ int cuda_binomial(int a, int b){\n  return cuda_fact(a)/(cuda_fact(b)*cuda_fact(a-b));\n}\n\n__device__ void cuda_Root123_dp(int n, double X, double roots[], double weights[]){\n\n  double R12, PIE4, R22, W22, R13, R23, W23, R33, W33;\n  double RT1=0,RT2=0,RT3=0,WW1=0,WW2=0,WW3=0;\n  double F1,F2,E,T1,T2,T3,A1,A2,Y;\n\n  R12 = 2.75255128608411E-01;\n  PIE4 = 7.85398163397448E-01;\n  R22 =  2.72474487139158E+00;\n  W22 = 9.17517095361369E-02;\n  R13 = 1.90163509193487E-01;\n  R23 = 1.78449274854325E+00;\n  W23 = 1.77231492083829E-01;\n  R33 = 5.52534374226326E+00;\n  W33 = 5.11156880411248E-03;\n    \n  if (X < 3.e-7){\n    if (n == 1){\n      RT1 = 0.5E+00 -X/5.0E+00;\n      WW1 = 1.0E+00 -X/3.0E+00;\n    } else if (n == 2) {\n      RT1 = 1.30693606237085E-01 -2.90430236082028E-02 *X;\n      RT2 = 2.86930639376291E+00 -6.37623643058102E-01 *X;\n      WW1 = 6.52145154862545E-01 -1.22713621927067E-01 *X;\n      WW2 = 3.47854845137453E-01 -2.10619711404725E-01 *X;\n    } else if (n == 3) {\n      RT1 = 6.03769246832797E-02 -9.28875764357368E-03 *X;\n      RT2 = 7.76823355931043E-01 -1.19511285527878E-01 *X;\n      RT3 = 6.66279971938567E+00 -1.02504611068957E+00 *X;\n      WW1 = 4.67913934572691E-01 -5.64876917232519E-02 *X;\n      WW2 = 3.60761573048137E-01 -1.49077186455208E-01 *X;\n      WW3 = 1.71324492379169E-01 -1.27768455150979E-01 *X;\n    }\n  } else if (X < 1.) {\n    if (n == 1){\n      F1 = ((((((((-8.36313918003957E-08*X+1.21222603512827E-06 )*X-\n                  1.15662609053481E-05 )*X+9.25197374512647E-05 )*X-\n                6.40994113129432E-04 )*X+3.78787044215009E-03 )*X-\n              1.85185172458485E-02 )*X+7.14285713298222E-02 )*X-\n            1.99999999997023E-01 )*X+3.33333333333318E-01;\n      WW1 = (X+X)*F1+exp(-X);\n      RT1 = F1/(WW1-F1);\n    } else if (n == 2) {\n      F1 = ((((((((-8.36313918003957E-08*X+1.21222603512827E-06 )*X-\n                  1.15662609053481E-05 )*X+9.25197374512647E-05 )*X-\n                6.40994113129432E-04 )*X+3.78787044215009E-03 )*X-\n              1.85185172458485E-02 )*X+7.14285713298222E-02 )*X-\n            1.99999999997023E-01 )*X+3.33333333333318E-01;\n      WW1 = (X+X)*F1+exp(-X);\n      RT1 = (((((((-2.35234358048491E-09*X+2.49173650389842E-08)*X-\n                  4.558315364581E-08)*X-2.447252174587E-06)*X+\n                4.743292959463E-05)*X-5.33184749432408E-04 )*X+\n              4.44654947116579E-03 )*X-2.90430236084697E-02 )*X+\n        1.30693606237085E-01;\n      RT2 = (((((((-2.47404902329170E-08*X+2.36809910635906E-07)*X+\n                  1.835367736310E-06)*X-2.066168802076E-05)*X-\n                1.345693393936E-04)*X-5.88154362858038E-05 )*X+\n              5.32735082098139E-02 )*X-6.37623643056745E-01 )*X+\n        2.86930639376289E+00;\n      WW2 = ((F1-WW1)*RT1+F1)*(1.0E+00+RT2)/(RT2-RT1);\n      WW1 = WW1-WW2;\n    } else if (n==3){\n      RT1 = ((((((-5.10186691538870E-10*X+2.40134415703450E-08)*X-\n                 5.01081057744427E-07 )*X+7.58291285499256E-06 )*X-\n               9.55085533670919E-05 )*X+1.02893039315878E-03 )*X-\n             9.28875764374337E-03 )*X+6.03769246832810E-02;\n      RT2 = ((((((-1.29646524960555E-08*X+7.74602292865683E-08)*X+\n                 1.56022811158727E-06 )*X-1.58051990661661E-05 )*X-\n               3.30447806384059E-04 )*X+9.74266885190267E-03 )*X-\n             1.19511285526388E-01 )*X+7.76823355931033E-01;\n      RT3 = ((((((-9.28536484109606E-09*X-3.02786290067014E-07)*X-\n                 2.50734477064200E-06 )*X-7.32728109752881E-06 )*X+\n               2.44217481700129E-04 )*X+4.94758452357327E-02 )*X-\n             1.02504611065774E+00 )*X+6.66279971938553E+00;\n      F2 = ((((((((-7.60911486098850E-08*X+1.09552870123182E-06 )*X-\n                  1.03463270693454E-05 )*X+8.16324851790106E-05 )*X-\n                5.55526624875562E-04 )*X+3.20512054753924E-03 )*X-\n              1.51515139838540E-02 )*X+5.55555554649585E-02 )*X-\n            1.42857142854412E-01 )*X+1.99999999999986E-01;\n      E = exp(-X);\n      F1 = ((X+X)*F2+E)/3.0E+00;\n      WW1 = (X+X)*F1+E;\n      T1 = RT1/(RT1+1.0E+00);\n      T2 = RT2/(RT2+1.0E+00);\n      T3 = RT3/(RT3+1.0E+00);\n      A2 = F2-T1*F1;\n      A1 = F1-T1*WW1;\n      WW3 = (A2-T2*A1)/((T3-T2)*(T3-T1));\n      WW2 = (T3*A1-A2)/((T3-T2)*(T2-T1));\n      WW1 = WW1-WW2-WW3;\n    }\n  } else if (X < 3.) {\n    Y = X-2.0E+00;\n    if (n == 1) {\n      F1 = ((((((((((-1.61702782425558E-10*Y+1.96215250865776E-09 )*Y-\n                    2.14234468198419E-08 )*Y+2.17216556336318E-07 )*Y-\n                  1.98850171329371E-06 )*Y+1.62429321438911E-05 )*Y-\n                1.16740298039895E-04 )*Y+7.24888732052332E-04 )*Y-\n              3.79490003707156E-03 )*Y+1.61723488664661E-02 )*Y-\n            5.29428148329736E-02 )*Y+1.15702180856167E-01;\n      WW1 = (X+X)*F1+exp(-X);\n      RT1 = F1/(WW1-F1);\n    } else if (n == 2) {\n      F1 = ((((((((((-1.61702782425558E-10*Y+1.96215250865776E-09 )*Y-\n                    2.14234468198419E-08 )*Y+2.17216556336318E-07 )*Y-\n                  1.98850171329371E-06 )*Y+1.62429321438911E-05 )*Y-\n                1.16740298039895E-04 )*Y+7.24888732052332E-04 )*Y-\n              3.79490003707156E-03 )*Y+1.61723488664661E-02 )*Y-\n            5.29428148329736E-02 )*Y+1.15702180856167E-01;\n      WW1 = (X+X)*F1+exp(-X);\n      RT1 = (((((((((-6.36859636616415E-12*Y+8.47417064776270E-11)*Y-\n                    5.152207846962E-10)*Y-3.846389873308E-10)*Y+\n                  8.472253388380E-08)*Y-1.85306035634293E-06 )*Y+\n                2.47191693238413E-05 )*Y-2.49018321709815E-04 )*Y+\n              2.19173220020161E-03 )*Y-1.63329339286794E-02 )*Y+\n        8.68085688285261E-02;\n      RT2 = ((((((((( 1.45331350488343E-10*Y+2.07111465297976E-09)*Y-\n                    1.878920917404E-08)*Y-1.725838516261E-07)*Y+\n                  2.247389642339E-06)*Y+9.76783813082564E-06 )*Y-\n                1.93160765581969E-04 )*Y-1.58064140671893E-03 )*Y+\n              4.85928174507904E-02 )*Y-4.30761584997596E-01 )*Y+\n        1.80400974537950E+00;\n      WW2 = ((F1-WW1)*RT1+F1)*(1.0E+00+RT2)/(RT2-RT1);\n      WW1 = WW1-WW2;\n    } else if (n == 3) {\n      RT1 = (((((((( 1.44687969563318E-12*Y+4.85300143926755E-12)*Y-\n                   6.55098264095516E-10 )*Y+1.56592951656828E-08 )*Y-\n                 2.60122498274734E-07 )*Y+3.86118485517386E-06 )*Y-\n               5.13430986707889E-05 )*Y+6.03194524398109E-04 )*Y-\n             6.11219349825090E-03 )*Y+4.52578254679079E-02;\n      RT2 = ((((((( 6.95964248788138E-10*Y-5.35281831445517E-09)*Y-\n                  6.745205954533E-08)*Y+1.502366784525E-06)*Y+\n                9.923326947376E-07)*Y-3.89147469249594E-04 )*Y+\n              7.51549330892401E-03 )*Y-8.48778120363400E-02 )*Y+\n        5.73928229597613E-01;\n      RT3 = ((((((((-2.81496588401439E-10*Y+3.61058041895031E-09)*Y+\n                   4.53631789436255E-08 )*Y-1.40971837780847E-07 )*Y-\n                 6.05865557561067E-06 )*Y-5.15964042227127E-05 )*Y+\n               3.34761560498171E-05 )*Y+5.04871005319119E-02 )*Y-\n             8.24708946991557E-01 )*Y+4.81234667357205E+00;\n      F2 = ((((((((((-1.48044231072140E-10*Y+1.78157031325097E-09 )*Y-\n                    1.92514145088973E-08 )*Y+1.92804632038796E-07 )*Y-\n                  1.73806555021045E-06 )*Y+1.39195169625425E-05 )*Y-\n                9.74574633246452E-05 )*Y+5.83701488646511E-04 )*Y-\n              2.89955494844975E-03 )*Y+1.13847001113810E-02 )*Y-\n            3.23446977320647E-02 )*Y+5.29428148329709E-02;\n      E = exp(-X);\n      F1 = ((X+X)*F2+E)/3.0E+00;\n      WW1 = (X+X)*F1+E;\n      T1 = RT1/(RT1+1.0E+00);\n      T2 = RT2/(RT2+1.0E+00);\n      T3 = RT3/(RT3+1.0E+00);\n      A2 = F2-T1*F1;\n      A1 = F1-T1*WW1;\n      WW3 = (A2-T2*A1)/((T3-T2)*(T3-T1));\n      WW2 = (T3*A1-A2)/((T3-T2)*(T2-T1));\n      WW1 = WW1-WW2-WW3;\n    }\n  } else if (X < 5.){\n    Y = X-4.0E+00;\n    if (n == 1){\n      F1 = ((((((((((-2.62453564772299E-11*Y+3.24031041623823E-10 )*Y-\n                    3.614965656163E-09)*Y+3.760256799971E-08)*Y-\n                  3.553558319675E-07)*Y+3.022556449731E-06)*Y-\n                2.290098979647E-05)*Y+1.526537461148E-04)*Y-\n              8.81947375894379E-04 )*Y+4.33207949514611E-03 )*Y-\n            1.75257821619926E-02 )*Y+5.28406320615584E-02;\n      WW1 = (X+X)*F1+exp(-X);\n      RT1 = F1/(WW1-F1);\n    } else if (n == 2) {\n      F1 = ((((((((((-2.62453564772299E-11*Y+3.24031041623823E-10 )*Y-\n                    3.614965656163E-09)*Y+3.760256799971E-08)*Y-\n                  3.553558319675E-07)*Y+3.022556449731E-06)*Y-\n                2.290098979647E-05)*Y+1.526537461148E-04)*Y-\n              8.81947375894379E-04 )*Y+4.33207949514611E-03 )*Y-\n            1.75257821619926E-02 )*Y+5.28406320615584E-02;\n      WW1 = (X+X)*F1+exp(-X);\n      RT1 = ((((((((-4.11560117487296E-12*Y+7.10910223886747E-11)*Y-\n                   1.73508862390291E-09 )*Y+5.93066856324744E-08 )*Y-\n                 9.76085576741771E-07 )*Y+1.08484384385679E-05 )*Y-\n               1.12608004981982E-04 )*Y+1.16210907653515E-03 )*Y-\n             9.89572595720351E-03 )*Y+6.12589701086408E-02;\n      RT2 = (((((((((-1.80555625241001E-10*Y+5.44072475994123E-10)*Y+\n                    1.603498045240E-08)*Y-1.497986283037E-07)*Y-\n                  7.017002532106E-07)*Y+1.85882653064034E-05 )*Y-\n                2.04685420150802E-05 )*Y-2.49327728643089E-03 )*Y+\n              3.56550690684281E-02 )*Y-2.60417417692375E-01 )*Y+\n        1.12155283108289E+00;\n      WW2 = ((F1-WW1)*RT1+F1)*(1.0E+00+RT2)/(RT2-RT1);\n      WW1 = WW1-WW2;\n    } else if (n == 3) {\n      RT1 = ((((((( 1.44265709189601E-11*Y-4.66622033006074E-10)*Y+\n                  7.649155832025E-09)*Y-1.229940017368E-07)*Y+\n                2.026002142457E-06)*Y-2.87048671521677E-05 )*Y+\n              3.70326938096287E-04 )*Y-4.21006346373634E-03 )*Y+\n        3.50898470729044E-02;\n      RT2 = ((((((((-2.65526039155651E-11*Y+1.97549041402552E-10)*Y+\n                   2.15971131403034E-09 )*Y-7.95045680685193E-08 )*Y+\n                 5.15021914287057E-07 )*Y+1.11788717230514E-05 )*Y-\n               3.33739312603632E-04 )*Y+5.30601428208358E-03 )*Y-\n             5.93483267268959E-02 )*Y+4.31180523260239E-01;\n      RT3 = ((((((((-3.92833750584041E-10*Y-4.16423229782280E-09)*Y+\n                   4.42413039572867E-08 )*Y+6.40574545989551E-07 )*Y-\n                 3.05512456576552E-06 )*Y-1.05296443527943E-04 )*Y-\n               6.14120969315617E-04 )*Y+4.89665802767005E-02 )*Y-\n             6.24498381002855E-01 )*Y+3.36412312243724E+00;\n      F2 = ((((((((((-2.36788772599074E-11*Y+2.89147476459092E-10 )*Y-\n                    3.18111322308846E-09 )*Y+3.25336816562485E-08 )*Y-\n                  3.00873821471489E-07 )*Y+2.48749160874431E-06 )*Y-\n                1.81353179793672E-05 )*Y+1.14504948737066E-04 )*Y-\n              6.10614987696677E-04 )*Y+2.64584212770942E-03 )*Y-\n            8.66415899015349E-03 )*Y+1.75257821619922E-02;\n      E = exp(-X);\n      F1 = ((X+X)*F2+E)/3.0E+00;\n      WW1 = (X+X)*F1+E;\n      T1 = RT1/(RT1+1.0E+00);\n      T2 = RT2/(RT2+1.0E+00);\n      T3 = RT3/(RT3+1.0E+00);\n      A2 = F2-T1*F1;\n      A1 = F1-T1*WW1;\n      WW3 = (A2-T2*A1)/((T3-T2)*(T3-T1));\n      WW2 = (T3*A1-A2)/((T3-T2)*(T2-T1));\n      WW1 = WW1-WW2-WW3;\n    }\n  } else if (X < 10) {\n    E = exp(-X);\n    WW1 = (((((( 4.6897511375022E-01/X-6.9955602298985E-01)/X +\n               5.3689283271887E-01)/X-3.2883030418398E-01)/X +\n             2.4645596956002E-01)/X-4.9984072848436E-01)/X -\n           3.1501078774085E-06)*E + sqrt(PIE4/X);\n    F1 = (WW1-E)/(X+X);\n    if (n == 1)\n      RT1 = F1/(WW1-F1);\n    else if (n == 2){\n      Y = X-7.5E+00;\n      RT1 = (((((((((((((-1.43632730148572E-16*Y+2.38198922570405E-16)*\n                        Y+1.358319618800E-14)*Y-7.064522786879E-14)*Y-\n                      7.719300212748E-13)*Y+7.802544789997E-12)*Y+\n                    6.628721099436E-11)*Y-1.775564159743E-09)*Y+\n                  1.713828823990E-08)*Y-1.497500187053E-07)*Y+\n                2.283485114279E-06)*Y-3.76953869614706E-05 )*Y+\n              4.74791204651451E-04 )*Y-4.60448960876139E-03 )*Y+\n        3.72458587837249E-02;\n      RT2 = (((((((((((( 2.48791622798900E-14*Y-1.36113510175724E-13)*Y-\n                       2.224334349799E-12)*Y+4.190559455515E-11)*Y-\n                     2.222722579924E-10)*Y-2.624183464275E-09)*Y+\n                   6.128153450169E-08)*Y-4.383376014528E-07)*Y-\n                 2.49952200232910E-06 )*Y+1.03236647888320E-04 )*Y-\n               1.44614664924989E-03 )*Y+1.35094294917224E-02 )*Y-\n             9.53478510453887E-02 )*Y+5.44765245686790E-01;\n      WW2 = ((F1-WW1)*RT1+F1)*(1.0E+00+RT2)/(RT2-RT1);\n      WW1 = WW1-WW2;\n    } else if (n == 3) {\n      F2 = (F1+F1+F1-E)/(X+X);\n      Y = X-7.5E+00;\n      RT1 = ((((((((((( 5.74429401360115E-16*Y+7.11884203790984E-16)*Y-\n                      6.736701449826E-14)*Y-6.264613873998E-13)*Y+\n                    1.315418927040E-11)*Y-4.23879635610964E-11 )*Y+\n                  1.39032379769474E-09 )*Y-4.65449552856856E-08 )*Y+\n                7.34609900170759E-07 )*Y-1.08656008854077E-05 )*Y+\n              1.77930381549953E-04 )*Y-2.39864911618015E-03 )*Y+\n        2.39112249488821E-02;\n      RT2 = ((((((((((( 1.13464096209120E-14*Y+6.99375313934242E-15)*Y-\n                      8.595618132088E-13)*Y-5.293620408757E-12)*Y-\n                    2.492175211635E-11)*Y+2.73681574882729E-09 )*Y-\n                  1.06656985608482E-08 )*Y-4.40252529648056E-07 )*Y+\n                9.68100917793911E-06 )*Y-1.68211091755327E-04 )*Y+\n              2.69443611274173E-03 )*Y-3.23845035189063E-02 )*Y+\n        2.75969447451882E-01;\n      RT3 = (((((((((((( 6.66339416996191E-15*Y+1.84955640200794E-13)*Y-\n                       1.985141104444E-12)*Y-2.309293727603E-11)*Y+\n                     3.917984522103E-10)*Y+1.663165279876E-09)*Y-\n                   6.205591993923E-08)*Y+8.769581622041E-09)*Y+\n                 8.97224398620038E-06 )*Y-3.14232666170796E-05 )*Y-\n               1.83917335649633E-03 )*Y+3.51246831672571E-02 )*Y-\n             3.22335051270860E-01 )*Y+1.73582831755430E+00;\n      T1 = RT1/(RT1+1.0E+00);\n      T2 = RT2/(RT2+1.0E+00);\n      T3 = RT3/(RT3+1.0E+00);\n      A2 = F2-T1*F1;\n      A1 = F1-T1*WW1;\n      WW3 = (A2-T2*A1)/((T3-T2)*(T3-T1));\n      WW2 = (T3*A1-A2)/((T3-T2)*(T2-T1));\n      WW1 = WW1-WW2-WW3;\n    }\n  } else if (X < 15) {\n    E = exp(-X);\n    WW1 = (((-1.8784686463512E-01/X+2.2991849164985E-01)/X -\n            4.9893752514047E-01)/X-2.1916512131607E-05)*E \n      + sqrt(PIE4/X);\n    F1 = (WW1-E)/(X+X);\n    if (n == 1)\n      RT1 = F1/(WW1-F1);\n    else if (n == 2) {\n      RT1 = ((((-1.01041157064226E-05*X+1.19483054115173E-03)*X -\n               6.73760231824074E-02)*X+1.25705571069895E+00)*X +\n             (((-8.57609422987199E+03/X+5.91005939591842E+03)/X -\n               1.70807677109425E+03)/X+2.64536689959503E+02)/X -\n             2.38570496490846E+01)*E + R12/(X-R12);\n      RT2 = ((( 3.39024225137123E-04*X-9.34976436343509E-02)*X -\n              4.22216483306320E+00)*X +\n             (((-2.08457050986847E+03/X -\n                1.04999071905664E+03)/X+3.39891508992661E+02)/X -\n              1.56184800325063E+02)/X+8.00839033297501E+00)*E + R22/(X-R22);\n      WW2 = ((F1-WW1)*RT1+F1)*(1.0E+00+RT2)/(RT2-RT1);\n      WW1 = WW1-WW2;\n    } else if (n == 3) {\n      F2 = (F1+F1+F1-E)/(X+X);\n      Y = X-12.5E+00;\n      RT1 = ((((((((((( 4.42133001283090E-16*Y-2.77189767070441E-15)*Y-\n                      4.084026087887E-14)*Y+5.379885121517E-13)*Y+\n                    1.882093066702E-12)*Y-8.67286219861085E-11 )*Y+\n                  7.11372337079797E-10 )*Y-3.55578027040563E-09 )*Y+\n                1.29454702851936E-07 )*Y-4.14222202791434E-06 )*Y+\n              8.04427643593792E-05 )*Y-1.18587782909876E-03 )*Y+\n        1.53435577063174E-02;\n      RT2 = ((((((((((( 6.85146742119357E-15*Y-1.08257654410279E-14)*Y-\n                      8.579165965128E-13)*Y+6.642452485783E-12)*Y+\n                    4.798806828724E-11)*Y-1.13413908163831E-09 )*Y+\n                  7.08558457182751E-09 )*Y-5.59678576054633E-08 )*Y+\n                2.51020389884249E-06 )*Y-6.63678914608681E-05 )*Y+\n              1.11888323089714E-03 )*Y-1.45361636398178E-02 )*Y+\n        1.65077877454402E-01;\n      RT3 = (((((((((((( 3.20622388697743E-15*Y-2.73458804864628E-14)*Y-\n                       3.157134329361E-13)*Y+8.654129268056E-12)*Y-\n                     5.625235879301E-11)*Y-7.718080513708E-10)*Y+\n                   2.064664199164E-08)*Y-1.567725007761E-07)*Y-\n                 1.57938204115055E-06 )*Y+6.27436306915967E-05 )*Y-\n               1.01308723606946E-03 )*Y+1.13901881430697E-02 )*Y-\n             1.01449652899450E-01 )*Y+7.77203937334739E-01;\n      T1 = RT1/(RT1+1.0E+00);\n      T2 = RT2/(RT2+1.0E+00);\n      T3 = RT3/(RT3+1.0E+00);\n      A2 = F2-T1*F1;\n      A1 = F1-T1*WW1;\n      WW3 = (A2-T2*A1)/((T3-T2)*(T3-T1));\n      WW2 = (T3*A1-A2)/((T3-T2)*(T2-T1));\n      WW1 = WW1-WW2-WW3;\n    }\n  } else if (X < 33) {\n    E = exp(-X);\n    WW1 = (( 1.9623264149430E-01/X-4.9695241464490E-01)/X -\n           6.0156581186481E-05)*E + sqrt(PIE4/X);\n    F1 = (WW1-E)/(X+X);\n    if (n == 1)\n      RT1 = F1/(WW1-F1);\n    else if (n == 2){\n      RT1 = ((((-1.14906395546354E-06*X+1.76003409708332E-04)*X -\n               1.71984023644904E-02)*X-1.37292644149838E-01)*X +\n             (-4.75742064274859E+01/X+9.21005186542857E+00)/X -\n             2.31080873898939E-02)*E + R12/(X-R12);\n      RT2 = ((( 3.64921633404158E-04*X-9.71850973831558E-02)*X -\n              4.02886174850252E+00)*X +\n             (-1.35831002139173E+02/X -\n              8.66891724287962E+01)/X+2.98011277766958E+00)*E + R22/(X-R22);\n      WW2 = ((F1-WW1)*RT1+F1)*(1.0E+00+RT2)/(RT2-RT1);\n      WW1 = WW1-WW2;\n    } else if (n == 3) {\n      F2 = (F1+F1+F1-E)/(X+X);\n      if (X < 20) {\n        RT1 = ((((((-2.43270989903742E-06*X+3.57901398988359E-04)*X -\n                   2.34112415981143E-02)*X+7.81425144913975E-01)*X -\n                 1.73209218219175E+01)*X+2.43517435690398E+02)*X +\n               (-1.97611541576986E+04/X+9.82441363463929E+03)/X -\n               2.07970687843258E+03)*E + R13/(X-R13);\n        RT2 = (((((-2.62627010965435E-04*X+3.49187925428138E-02)*X -\n                  3.09337618731880E+00)*X+1.07037141010778E+02)*X -\n                2.36659637247087E+03)*X +\n               ((-2.91669113681020E+06/X +\n                 1.41129505262758E+06)/X-2.91532335433779E+05)/X +\n               3.35202872835409E+04)*E + R23/(X-R23);\n        RT3 = ((((( 9.31856404738601E-05*X-2.87029400759565E-02)*X -\n                  7.83503697918455E-01)*X-1.84338896480695E+01)*X +\n                4.04996712650414E+02)*X +\n               (-1.89829509315154E+05/X +\n                5.11498390849158E+04)/X-6.88145821789955E+03)*E \n          + R33/(X-R33);\n      } else {\n        RT1 = ((((-4.97561537069643E-04*X-5.00929599665316E-02)*X +\n                 1.31099142238996E+00)*X-1.88336409225481E+01)*X -\n               6.60344754467191E+02 /X+1.64931462413877E+02)*E \n          + R13/(X-R13);\n        RT2 = ((((-4.48218898474906E-03*X-5.17373211334924E-01)*X +\n                 1.13691058739678E+01)*X-1.65426392885291E+02)*X -\n               6.30909125686731E+03 /X+1.52231757709236E+03)*E \n          + R23/(X-R23);\n        RT3 = ((((-1.38368602394293E-02*X-1.77293428863008E+00)*X +\n                 1.73639054044562E+01)*X-3.57615122086961E+02)*X -\n               1.45734701095912E+04 /X+2.69831813951849E+03)*E \n          + R33/(X-R33);\n      }\n      T1 = RT1/(RT1+1.0E+00);\n      T2 = RT2/(RT2+1.0E+00);\n      T3 = RT3/(RT3+1.0E+00);\n      A2 = F2-T1*F1;\n      A1 = F1-T1*WW1;\n      WW3 = (A2-T2*A1)/((T3-T2)*(T3-T1));\n      WW2 = (T3*A1-A2)/((T3-T2)*(T2-T1));\n      WW1 = WW1-WW2-WW3; \n    }\n  } else {\n    WW1 = sqrt(PIE4/X);\n    if (n == 1)\n      RT1 = 0.5E+00/(X-0.5E+00);\n    else if (n == 2) {\n      if (X < 40) {\n        E = exp(-X);\n        RT1 = (-8.78947307498880E-01*X+1.09243702330261E+01)*E \n          + R12/(X-R12);\n        RT2 = (-9.28903924275977E+00*X+8.10642367843811E+01)*E \n          + R22/(X-R22);\n        WW2 = ( 4.46857389308400E+00*X-7.79250653461045E+01)*E + W22*WW1;\n        WW1 = WW1-WW2;\n      } else {\n        RT1 = R12/(X-R12);\n        RT2 = R22/(X-R22);\n        WW2 = W22*WW1;\n        WW1 = WW1-WW2;\n      }\n    } else if (n == 3) {\n      if (X < 47) {\n        E = exp(-X);\n        RT1 = ((-7.39058467995275E+00*X+3.21318352526305E+02)*X -\n               3.99433696473658E+03)*E + R13/(X-R13);\n        RT2 = ((-7.38726243906513E+01*X+3.13569966333873E+03)*X -\n               3.86862867311321E+04)*E + R23/(X-R23);\n        RT3 = ((-2.63750565461336E+02*X+1.04412168692352E+04)*X -\n               1.28094577915394E+05)*E + R33/(X-R33);\n        WW3 = ((( 1.52258947224714E-01*X-8.30661900042651E+00)*X +\n                1.92977367967984E+02)*X-1.67787926005344E+03)*E \n          + W33*WW1;\n        WW2 = (( 6.15072615497811E+01*X-2.91980647450269E+03)*X +\n               3.80794303087338E+04)*E + W23*WW1;\n        WW1 = WW1-WW2-WW3;\n      } else {\n        RT1 = R13/(X-R13);\n        RT2 = R23/(X-R23);\n        RT3 = R33/(X-R33);\n        WW2 = W23*WW1;\n        WW3 = W33*WW1;\n        WW1 = WW1-WW2-WW3;\n      }\n    }\n  }\n  roots[0] = RT1;\n  weights[0] = WW1;\n  if (n > 1){\n    roots[1] = RT2;\n    weights[1] = WW2;\n  }\n  if (n > 2) {\n    roots[2] = RT3;\n    weights[2] = WW3;\n  }\n  return;\n}\n\n__device__ void cuda_Root4_dp(double X, double roots[], double weights[]){\n  double R14,PIE4,R24,W24,R34,W34,R44,W44;\n  double RT1=0,RT2=0,RT3=0,RT4=0,WW1=0,WW2=0,WW3=0,WW4=0;\n  double Y,E;\n  \n  R14 = 1.45303521503316E-01;\n  PIE4 = 7.85398163397448E-01;\n  R24 = 1.33909728812636E+00;\n  W24 = 2.34479815323517E-01;\n  R34 = 3.92696350135829E+00;\n  W34 = 1.92704402415764E-02;\n  R44 = 8.58863568901199E+00;\n  W44 = 2.25229076750736E-04;\n\n  if (X <= 3.0E-7) {\n    RT1 = 3.48198973061471E-02 -4.09645850660395E-03 *X;\n    RT2 = 3.81567185080042E-01 -4.48902570656719E-02 *X;\n    RT3 = 1.73730726945891E+00 -2.04389090547327E-01 *X;\n    RT4 = 1.18463056481549E+01 -1.39368301742312E+00 *X;\n    WW1 = 3.62683783378362E-01 -3.13844305713928E-02 *X;\n    WW2 = 3.13706645877886E-01 -8.98046242557724E-02 *X;\n    WW3 = 2.22381034453372E-01 -1.29314370958973E-01 *X;\n    WW4 = 1.01228536290376E-01 -8.28299075414321E-02 *X;\n  } else if (X <= 1.0) {\n    RT1 = ((((((-1.95309614628539E-10*X+5.19765728707592E-09)*X-\n               1.01756452250573E-07 )*X+1.72365935872131E-06 )*X-\n             2.61203523522184E-05 )*X+3.52921308769880E-04 )*X-\n           4.09645850658433E-03 )*X+3.48198973061469E-02;\n    RT2 = (((((-1.89554881382342E-08*X+3.07583114342365E-07)*X+\n              1.270981734393E-06)*X-1.417298563884E-04)*X+\n            3.226979163176E-03)*X-4.48902570678178E-02 )*X+\n      3.81567185080039E-01;\n    RT3 = (((((( 1.77280535300416E-09*X+3.36524958870615E-08)*X-\n               2.58341529013893E-07 )*X-1.13644895662320E-05 )*X-\n             7.91549618884063E-05 )*X+1.03825827346828E-02 )*X-\n           2.04389090525137E-01 )*X+1.73730726945889E+00;\n    RT4 = (((((-5.61188882415248E-08*X-2.49480733072460E-07)*X+\n              3.428685057114E-06)*X+1.679007454539E-04)*X+\n            4.722855585715E-02)*X-1.39368301737828E+00 )*X+\n      1.18463056481543E+01;\n    WW1 = ((((((-1.14649303201279E-08*X+1.88015570196787E-07)*X-\n               2.33305875372323E-06 )*X+2.68880044371597E-05 )*X-\n             2.94268428977387E-04 )*X+3.06548909776613E-03 )*X-\n           3.13844305680096E-02 )*X+3.62683783378335E-01;\n    WW2 = ((((((((-4.11720483772634E-09*X+6.54963481852134E-08)*X-\n                 7.20045285129626E-07 )*X+6.93779646721723E-06 )*X-\n               6.05367572016373E-05 )*X+4.74241566251899E-04 )*X-\n             3.26956188125316E-03 )*X+1.91883866626681E-02 )*X-\n           8.98046242565811E-02 )*X+3.13706645877886E-01;\n    WW3 = ((((((((-3.41688436990215E-08*X+5.07238960340773E-07)*X-\n                 5.01675628408220E-06 )*X+4.20363420922845E-05 )*X-\n               3.08040221166823E-04 )*X+1.94431864731239E-03 )*X-\n             1.02477820460278E-02 )*X+4.28670143840073E-02 )*X-\n           1.29314370962569E-01 )*X+2.22381034453369E-01;\n    WW4 = ((((((((( 4.99660550769508E-09*X-7.94585963310120E-08)*X+\n                  8.359072409485E-07)*X-7.422369210610E-06)*X+\n                5.763374308160E-05)*X-3.86645606718233E-04 )*X+\n              2.18417516259781E-03 )*X-9.99791027771119E-03 )*X+\n            3.48791097377370E-02 )*X-8.28299075413889E-02 )*X+\n      1.01228536290376E-01;\n  } else if (X <= 5.0) {\n    Y = X-3.0E+00;\n    RT1 = (((((((((-1.48570633747284E-15*Y-1.33273068108777E-13)*Y+\n                  4.068543696670E-12)*Y-9.163164161821E-11)*Y+\n                2.046819017845E-09)*Y-4.03076426299031E-08 )*Y+\n              7.29407420660149E-07 )*Y-1.23118059980833E-05 )*Y+\n            1.88796581246938E-04 )*Y-2.53262912046853E-03 )*Y+\n      2.51198234505021E-02;\n    RT2 = ((((((((( 1.35830583483312E-13*Y-2.29772605964836E-12)*Y-\n                  3.821500128045E-12)*Y+6.844424214735E-10)*Y-\n                1.048063352259E-08)*Y+1.50083186233363E-08 )*Y+\n              3.48848942324454E-06 )*Y-1.08694174399193E-04 )*Y+\n            2.08048885251999E-03 )*Y-2.91205805373793E-02 )*Y+\n      2.72276489515713E-01;\n    RT3 = ((((((((( 5.02799392850289E-13*Y+1.07461812944084E-11)*Y-\n                  1.482277886411E-10)*Y-2.153585661215E-09)*Y+\n                3.654087802817E-08)*Y+5.15929575830120E-07 )*Y-\n              9.52388379435709E-06 )*Y-2.16552440036426E-04 )*Y+\n            9.03551469568320E-03 )*Y-1.45505469175613E-01 )*Y+\n      1.21449092319186E+00;\n    RT4 = (((((((((-1.08510370291979E-12*Y+6.41492397277798E-11)*Y+\n                  7.542387436125E-10)*Y-2.213111836647E-09)*Y-\n                1.448228963549E-07)*Y-1.95670833237101E-06 )*Y-\n              1.07481314670844E-05 )*Y+1.49335941252765E-04 )*Y+\n            4.87791531990593E-02 )*Y-1.10559909038653E+00 )*Y+\n      8.09502028611780E+00;\n    WW1 = ((((((((((-4.65801912689961E-14*Y+7.58669507106800E-13)*Y-\n                   1.186387548048E-11)*Y+1.862334710665E-10)*Y-\n                 2.799399389539E-09)*Y+4.148972684255E-08)*Y-\n               5.933568079600E-07)*Y+8.168349266115E-06)*Y-\n             1.08989176177409E-04 )*Y+1.41357961729531E-03 )*Y-\n           1.87588361833659E-02 )*Y+2.89898651436026E-01;\n    WW2 = ((((((((((((-1.46345073267549E-14*Y+2.25644205432182E-13)*Y-\n                     3.116258693847E-12)*Y+4.321908756610E-11)*Y-\n                   5.673270062669E-10)*Y+7.006295962960E-09)*Y-\n                 8.120186517000E-08)*Y+8.775294645770E-07)*Y-\n               8.77829235749024E-06 )*Y+8.04372147732379E-05 )*Y-\n             6.64149238804153E-04 )*Y+4.81181506827225E-03 )*Y-\n           2.88982669486183E-02 )*Y+1.56247249979288E-01;\n    WW3 = ((((((((((((( 9.06812118895365E-15*Y-1.40541322766087E-13)*\n                      Y+1.919270015269E-12)*Y-2.605135739010E-11)*Y+\n                    3.299685839012E-10)*Y-3.86354139348735E-09 )*Y+\n                  4.16265847927498E-08 )*Y-4.09462835471470E-07 )*Y+\n                3.64018881086111E-06 )*Y-2.88665153269386E-05 )*Y+\n              2.00515819789028E-04 )*Y-1.18791896897934E-03 )*Y+\n            5.75223633388589E-03 )*Y-2.09400418772687E-02 )*Y+\n      4.85368861938873E-02;\n    WW4 = ((((((((((((((-9.74835552342257E-16*Y+1.57857099317175E-14)*\n                       Y-2.249993780112E-13)*Y+3.173422008953E-12)*Y-\n                     4.161159459680E-11)*Y+5.021343560166E-10)*Y-\n                   5.545047534808E-09)*Y+5.554146993491E-08)*Y-\n                 4.99048696190133E-07 )*Y+3.96650392371311E-06 )*Y-\n               2.73816413291214E-05 )*Y+1.60106988333186E-04 )*Y-\n             7.64560567879592E-04 )*Y+2.81330044426892E-03 )*Y-\n           7.16227030134947E-03 )*Y+9.66077262223353E-03;\n  } else if (X <= 10.0) {\n    Y = X-7.5E+00;\n    RT1 = ((((((((( 4.64217329776215E-15*Y-6.27892383644164E-15)*Y+\n                  3.462236347446E-13)*Y-2.927229355350E-11)*Y+\n                5.090355371676E-10)*Y-9.97272656345253E-09 )*Y+\n              2.37835295639281E-07 )*Y-4.60301761310921E-06 )*Y+\n            8.42824204233222E-05 )*Y-1.37983082233081E-03 )*Y+\n      1.66630865869375E-02;\n    RT2 = ((((((((( 2.93981127919047E-14*Y+8.47635639065744E-13)*Y-\n                  1.446314544774E-11)*Y-6.149155555753E-12)*Y+\n                8.484275604612E-10)*Y-6.10898827887652E-08 )*Y+\n              2.39156093611106E-06 )*Y-5.35837089462592E-05 )*Y+\n            1.00967602595557E-03 )*Y-1.57769317127372E-02 )*Y+\n      1.74853819464285E-01;\n    RT3 = (((((((((( 2.93523563363000E-14*Y-6.40041776667020E-14)*Y-\n                   2.695740446312E-12)*Y+1.027082960169E-10)*Y-\n                 5.822038656780E-10)*Y-3.159991002539E-08)*Y+\n               4.327249251331E-07)*Y+4.856768455119E-06)*Y-\n             2.54617989427762E-04 )*Y+5.54843378106589E-03 )*Y-\n           7.95013029486684E-02 )*Y+7.20206142703162E-01;\n    RT4 = (((((((((((-1.62212382394553E-14*Y+7.68943641360593E-13)*Y+\n                    5.764015756615E-12)*Y-1.380635298784E-10)*Y-\n                  1.476849808675E-09)*Y+1.84347052385605E-08 )*Y+\n                3.34382940759405E-07 )*Y-1.39428366421645E-06 )*Y-\n              7.50249313713996E-05 )*Y-6.26495899187507E-04 )*Y+\n            4.69716410901162E-02 )*Y-6.66871297428209E-01 )*Y+\n      4.11207530217806E+00;\n    WW1 = ((((((((((-1.65995045235997E-15*Y+6.91838935879598E-14)*Y-\n                   9.131223418888E-13)*Y+1.403341829454E-11)*Y-\n                 3.672235069444E-10)*Y+6.366962546990E-09)*Y-\n               1.039220021671E-07)*Y+1.959098751715E-06)*Y-\n             3.33474893152939E-05 )*Y+5.72164211151013E-04 )*Y-\n           1.05583210553392E-02 )*Y+2.26696066029591E-01;\n    WW2 = ((((((((((((-3.57248951192047E-16*Y+6.25708409149331E-15)*Y-\n                     9.657033089714E-14)*Y+1.507864898748E-12)*Y-\n                   2.332522256110E-11)*Y+3.428545616603E-10)*Y-\n                 4.698730937661E-09)*Y+6.219977635130E-08)*Y-\n               7.83008889613661E-07 )*Y+9.08621687041567E-06 )*Y-\n             9.86368311253873E-05 )*Y+9.69632496710088E-04 )*Y-\n           8.14594214284187E-03 )*Y+8.50218447733457E-02;\n    WW3 = ((((((((((((( 1.64742458534277E-16*Y-2.68512265928410E-15)*\n                      Y+3.788890667676E-14)*Y-5.508918529823E-13)*Y+\n                    7.555896810069E-12)*Y-9.69039768312637E-11 )*Y+\n                  1.16034263529672E-09 )*Y-1.28771698573873E-08 )*Y+\n                1.31949431805798E-07 )*Y-1.23673915616005E-06 )*Y+\n              1.04189803544936E-05 )*Y-7.79566003744742E-05 )*Y+\n            5.03162624754434E-04 )*Y-2.55138844587555E-03 )*Y+\n      1.13250730954014E-02;\n    WW4 = ((((((((((((((-1.55714130075679E-17*Y+2.57193722698891E-16)*\n                       Y-3.626606654097E-15)*Y+5.234734676175E-14)*Y-\n                     7.067105402134E-13)*Y+8.793512664890E-12)*Y-\n                   1.006088923498E-10)*Y+1.050565098393E-09)*Y-\n                 9.91517881772662E-09 )*Y+8.35835975882941E-08 )*Y-\n               6.19785782240693E-07 )*Y+3.95841149373135E-06 )*Y-\n             2.11366761402403E-05 )*Y+9.00474771229507E-05 )*Y-\n           2.78777909813289E-04 )*Y+5.26543779837487E-04;\n  } else if (X <= 15.0) {\n    Y = X-12.5E+00;\n    RT1 = ((((((((((( 4.94869622744119E-17*Y+8.03568805739160E-16)*Y-\n                    5.599125915431E-15)*Y-1.378685560217E-13)*Y+\n                  7.006511663249E-13)*Y+1.30391406991118E-11 )*Y+\n                8.06987313467541E-11 )*Y-5.20644072732933E-09 )*Y+\n              7.72794187755457E-08 )*Y-1.61512612564194E-06 )*Y+\n            4.15083811185831E-05 )*Y-7.87855975560199E-04 )*Y+\n      1.14189319050009E-02;\n    RT2 = ((((((((((( 4.89224285522336E-16*Y+1.06390248099712E-14)*Y-\n                    5.446260182933E-14)*Y-1.613630106295E-12)*Y+\n                  3.910179118937E-12)*Y+1.90712434258806E-10 )*Y+\n                8.78470199094761E-10 )*Y-5.97332993206797E-08 )*Y+\n              9.25750831481589E-07 )*Y-2.02362185197088E-05 )*Y+\n            4.92341968336776E-04 )*Y-8.68438439874703E-03 )*Y+\n      1.15825965127958E-01;\n    RT3 = (((((((((( 6.12419396208408E-14*Y+1.12328861406073E-13)*Y-\n                   9.051094103059E-12)*Y-4.781797525341E-11)*Y+\n                 1.660828868694E-09)*Y+4.499058798868E-10)*Y-\n               2.519549641933E-07)*Y+4.977444040180E-06)*Y-\n             1.25858350034589E-04 )*Y+2.70279176970044E-03 )*Y-\n           3.99327850801083E-02 )*Y+4.33467200855434E-01;\n    RT4 = ((((((((((( 4.63414725924048E-14*Y-4.72757262693062E-14)*Y-\n                    1.001926833832E-11)*Y+6.074107718414E-11)*Y+\n                  1.576976911942E-09)*Y-2.01186401974027E-08 )*Y-\n                1.84530195217118E-07 )*Y+5.02333087806827E-06 )*Y+\n              9.66961790843006E-06 )*Y-1.58522208889528E-03 )*Y+\n            2.80539673938339E-02 )*Y-2.78953904330072E-01 )*Y+\n      1.82835655238235E+00;\n    WW4 = ((((((((((((( 2.90401781000996E-18*Y-4.63389683098251E-17)*\n                      Y+6.274018198326E-16)*Y-8.936002188168E-15)*Y+\n                    1.194719074934E-13)*Y-1.45501321259466E-12 )*Y+\n                  1.64090830181013E-11 )*Y-1.71987745310181E-10 )*Y+\n                1.63738403295718E-09 )*Y-1.39237504892842E-08 )*Y+\n              1.06527318142151E-07 )*Y-7.27634957230524E-07 )*Y+\n            4.12159381310339E-06 )*Y-1.74648169719173E-05 )*Y+\n      8.50290130067818E-05;\n    WW3 = ((((((((((((-4.19569145459480E-17*Y+5.94344180261644E-16)*Y-\n                     1.148797566469E-14)*Y+1.881303962576E-13)*Y-\n                   2.413554618391E-12)*Y+3.372127423047E-11)*Y-\n                 4.933988617784E-10)*Y+6.116545396281E-09)*Y-\n               6.69965691739299E-08 )*Y+7.52380085447161E-07 )*Y-\n             8.08708393262321E-06 )*Y+6.88603417296672E-05 )*Y-\n           4.67067112993427E-04 )*Y+5.42313365864597E-03;\n    WW2 = ((((((((((-6.22272689880615E-15*Y+1.04126809657554E-13)*Y-\n                   6.842418230913E-13)*Y+1.576841731919E-11)*Y-\n                 4.203948834175E-10)*Y+6.287255934781E-09)*Y-\n               8.307159819228E-08)*Y+1.356478091922E-06)*Y-\n             2.08065576105639E-05 )*Y+2.52396730332340E-04 )*Y-\n           2.94484050194539E-03 )*Y+6.01396183129168E-02;\n    WW1 = (((-1.8784686463512E-01/X+2.2991849164985E-01)/X -\n            4.9893752514047E-01)/X-2.1916512131607E-05)*exp(-X) +\n      sqrt(PIE4/X)-WW4-WW3-WW2;\n  } else if (X <= 20.0) {\n    WW1 = sqrt(PIE4/X);\n    Y = X-17.5E+00;\n    RT1 = ((((((((((( 4.36701759531398E-17*Y-1.12860600219889E-16)*Y-\n                    6.149849164164E-15)*Y+5.820231579541E-14)*Y+\n                  4.396602872143E-13)*Y-1.24330365320172E-11 )*Y+\n                6.71083474044549E-11 )*Y+2.43865205376067E-10 )*Y+\n              1.67559587099969E-08 )*Y-9.32738632357572E-07 )*Y+\n            2.39030487004977E-05 )*Y-4.68648206591515E-04 )*Y+\n      8.34977776583956E-03;\n    RT2 = ((((((((((( 4.98913142288158E-16*Y-2.60732537093612E-16)*Y-\n                    7.775156445127E-14)*Y+5.766105220086E-13)*Y+\n                  6.432696729600E-12)*Y-1.39571683725792E-10 )*Y+\n                5.95451479522191E-10 )*Y+2.42471442836205E-09 )*Y+\n              2.47485710143120E-07 )*Y-1.14710398652091E-05 )*Y+\n            2.71252453754519E-04 )*Y-4.96812745851408E-03 )*Y+\n      8.26020602026780E-02;\n    RT3 = ((((((((((( 1.91498302509009E-15*Y+1.48840394311115E-14)*Y-\n                    4.316925145767E-13)*Y+1.186495793471E-12)*Y+\n                  4.615806713055E-11)*Y-5.54336148667141E-10 )*Y+\n                3.48789978951367E-10 )*Y-2.79188977451042E-09 )*Y+\n              2.09563208958551E-06 )*Y-6.76512715080324E-05 )*Y+\n            1.32129867629062E-03 )*Y-2.05062147771513E-02 )*Y+\n      2.88068671894324E-01;\n    RT4 = (((((((((((-5.43697691672942E-15*Y-1.12483395714468E-13)*Y+\n                    2.826607936174E-12)*Y-1.266734493280E-11)*Y-\n                  4.258722866437E-10)*Y+9.45486578503261E-09 )*Y-\n                5.86635622821309E-08 )*Y-1.28835028104639E-06 )*Y+\n              4.41413815691885E-05 )*Y-7.61738385590776E-04 )*Y+\n            9.66090902985550E-03 )*Y-1.01410568057649E-01 )*Y+\n      9.54714798156712E-01;\n    WW4 = ((((((((((((-7.56882223582704E-19*Y+7.53541779268175E-18)*Y-\n                     1.157318032236E-16)*Y+2.411195002314E-15)*Y-\n                   3.601794386996E-14)*Y+4.082150659615E-13)*Y-\n                 4.289542980767E-12)*Y+5.086829642731E-11)*Y-\n               6.35435561050807E-10 )*Y+6.82309323251123E-09 )*Y-\n             5.63374555753167E-08 )*Y+3.57005361100431E-07 )*Y-\n           2.40050045173721E-06 )*Y+4.94171300536397E-05;\n    WW3 = (((((((((((-5.54451040921657E-17*Y+2.68748367250999E-16)*Y+\n                    1.349020069254E-14)*Y-2.507452792892E-13)*Y+\n                  1.944339743818E-12)*Y-1.29816917658823E-11 )*Y+\n                3.49977768819641E-10 )*Y-8.67270669346398E-09 )*Y+\n              1.31381116840118E-07 )*Y-1.36790720600822E-06 )*Y+\n            1.19210697673160E-05 )*Y-1.42181943986587E-04 )*Y+\n      4.12615396191829E-03;\n    WW2 = (((((((((((-1.86506057729700E-16*Y+1.16661114435809E-15)*Y+\n                    2.563712856363E-14)*Y-4.498350984631E-13)*Y+\n                  1.765194089338E-12)*Y+9.04483676345625E-12 )*Y+\n                4.98930345609785E-10 )*Y-2.11964170928181E-08 )*Y+\n              3.98295476005614E-07 )*Y-5.49390160829409E-06 )*Y+\n            7.74065155353262E-05 )*Y-1.48201933009105E-03 )*Y+\n      4.97836392625268E-02;\n    WW1 = (( 1.9623264149430E-01/X-4.9695241464490E-01)/X -\n           6.0156581186481E-05)*exp(-X)+WW1-WW2-WW3-WW4;\n  } else if (X <= 35.0) {\n    WW1 = sqrt(PIE4/X);\n    E = exp(-X);\n    RT1 = ((((((-4.45711399441838E-05*X+1.27267770241379E-03)*X -\n               2.36954961381262E-01)*X+1.54330657903756E+01)*X -\n             5.22799159267808E+02)*X+1.05951216669313E+04)*X +\n           (-2.51177235556236E+06/X+8.72975373557709E+05)/X -\n           1.29194382386499E+05)*E + R14/(X-R14);\n    RT2 = (((((-7.85617372254488E-02*X+6.35653573484868E+00)*X -\n              3.38296938763990E+02)*X+1.25120495802096E+04)*X -\n            3.16847570511637E+05)*X +\n           ((-1.02427466127427E+09/X +\n             3.70104713293016E+08)/X-5.87119005093822E+07)/X +\n           5.38614211391604E+06)*E + R24/(X-R24);\n    RT3 = (((((-2.37900485051067E-01*X+1.84122184400896E+01)*X -\n              1.00200731304146E+03)*X+3.75151841595736E+04)*X -\n            9.50626663390130E+05)*X +\n           ((-2.88139014651985E+09/X +\n             1.06625915044526E+09)/X-1.72465289687396E+08)/X +\n           1.60419390230055E+07)*E + R34/(X-R34);\n    RT4 = ((((((-6.00691586407385E-04*X-3.64479545338439E-01)*X +\n               1.57496131755179E+01)*X-6.54944248734901E+02)*X +\n             1.70830039597097E+04)*X-2.90517939780207E+05)*X +\n           (3.49059698304732E+07/X-1.64944522586065E+07)/X +\n           2.96817940164703E+06)*E + R44/(X-R44);\n    if (X <= 25.0) \n      WW4 = ((((((( 2.33766206773151E-07*X-\n                    3.81542906607063E-05)*X +3.51416601267000E-03)*X-\n                 1.66538571864728E-01)*X +4.80006136831847E+00)*X-\n               8.73165934223603E+01)*X +9.77683627474638E+02)*X +\n             1.66000945117640E+04/X -6.14479071209961E+03)*E + W44*WW1;\n    else\n      WW4 = (((((( 5.74245945342286E-06*X-\n                   7.58735928102351E-05)*X +2.35072857922892E-04)*X-\n                3.78812134013125E-03)*X +3.09871652785805E-01)*X-\n              7.11108633061306E+00)*X +5.55297573149528E+01)*E + W44*WW1;\n    WW3 = (((((( 2.36392855180768E-04*X-9.16785337967013E-03)*X +\n               4.62186525041313E-01)*X-1.96943786006540E+01)*X +\n             4.99169195295559E+02)*X-6.21419845845090E+03)*X +\n           ((+5.21445053212414E+07/X-1.34113464389309E+07)/X +\n            1.13673298305631E+06)/X-2.81501182042707E+03)*E + W34*WW1;\n    WW2 = (((((( 7.29841848989391E-04*X-3.53899555749875E-02)*X +\n               2.07797425718513E+00)*X-1.00464709786287E+02)*X +\n             3.15206108877819E+03)*X-6.27054715090012E+04)*X +\n           (+1.54721246264919E+07/X-5.26074391316381E+06)/X +\n           7.67135400969617E+05)*E + W24*WW1;\n    WW1 = (( 1.9623264149430E-01/X-4.9695241464490E-01)/X -\n           6.0156581186481E-05)*E + WW1-WW2-WW3-WW4;\n  } else if (X <= 53.0) {\n    WW1 = sqrt(PIE4/X);\n    E = exp(-X)*pow(X,4.0);\n    RT4 = ((-2.19135070169653E-03*X-1.19108256987623E-01)*X -\n           7.50238795695573E-01)*E + R44/(X-R44);\n    RT3 = ((-9.65842534508637E-04*X-4.49822013469279E-02)*X +\n           6.08784033347757E-01)*E + R34/(X-R34);\n    RT2 = ((-3.62569791162153E-04*X-9.09231717268466E-03)*X +\n           1.84336760556262E-01)*E + R24/(X-R24);\n    RT1 = ((-4.07557525914600E-05*X-6.88846864931685E-04)*X +\n           1.74725309199384E-02)*E + R14/(X-R14);\n    WW4 = (( 5.76631982000990E-06*X-7.89187283804890E-05)*X +\n           3.28297971853126E-04)*E + W44*WW1;\n    WW3 = (( 2.08294969857230E-04*X-3.77489954837361E-03)*X +\n           2.09857151617436E-02)*E + W34*WW1;\n    WW2 = (( 6.16374517326469E-04*X-1.26711744680092E-02)*X +\n           8.14504890732155E-02)*E + W24*WW1;\n    WW1 = WW1-WW2-WW3-WW4;\n  } else {\n    WW1 = sqrt(PIE4/X);\n    RT1 = R14/(X-R14);\n    RT2 = R24/(X-R24);\n    RT3 = R34/(X-R34);\n    RT4 = R44/(X-R44);\n    WW4 = W44*WW1;\n    WW3 = W34*WW1;\n    WW2 = W24*WW1;\n    WW1 = WW1-WW2-WW3-WW4;\n  }\n  roots[0] = RT1;\n  weights[0] = WW1;\n  roots[1] = RT2;\n  weights[1] = WW2;\n  roots[2] = RT3;\n  weights[2] = WW3;\n  roots[3] = RT4;\n  weights[3] = WW4;\n  return;\n}\n\n__device__ void cuda_Root5_dp(double X, double roots[], double weights[]){\n  double R15,PIE4,R25,W25,R35,W35,R45,W45,R55,W55;\n  double RT1=0,RT2=0,RT3=0,RT4=0,RT5=0,\n    WW1=0,WW2=0,WW3=0,WW4=0,WW5=0;\n  double Y,E=0,XXX;\n\n  R15 = 1.17581320211778E-01;\n  PIE4 = 7.85398163397448E-01;\n  R25 = 1.07456201243690E+00;\n  W25 = 2.70967405960535E-01;\n  R35 = 3.08593744371754E+00;\n  W35 = 3.82231610015404E-02;\n  R45 = 6.41472973366203E+00;\n  W45 = 1.51614186862443E-03;\n  R55 = 1.18071894899717E+01;\n  W55 = 8.62130526143657E-06;\n\n  if (X < 3.e-7){\n    RT1 = 2.26659266316985E-02 -2.15865967920897E-03 *X;\n    RT2 = 2.31271692140903E-01 -2.20258754389745E-02 *X;\n    RT3 = 8.57346024118836E-01 -8.16520023025515E-02 *X;\n    RT4 = 2.97353038120346E+00 -2.83193369647137E-01 *X;\n    RT5 = 1.84151859759051E+01 -1.75382723579439E+00 *X;\n    WW1 = 2.95524224714752E-01 -1.96867576909777E-02 *X;\n    WW2 = 2.69266719309995E-01 -5.61737590184721E-02 *X;\n    WW3 = 2.19086362515981E-01 -9.71152726793658E-02 *X;\n    WW4 = 1.49451349150580E-01 -1.02979262193565E-01 *X;\n    WW5 = 6.66713443086877E-02 -5.73782817488315E-02 *X;\n  } else if (X < 1.0){\n    RT1 = ((((((-4.46679165328413E-11*X+1.21879111988031E-09)*X-\n               2.62975022612104E-08 )*X+5.15106194905897E-07 )*X-\n             9.27933625824749E-06 )*X+1.51794097682482E-04 )*X-\n           2.15865967920301E-03 )*X+2.26659266316985E-02;\n    RT2 = (((((( 1.93117331714174E-10*X-4.57267589660699E-09)*X+\n               2.48339908218932E-08 )*X+1.50716729438474E-06 )*X-\n             6.07268757707381E-05 )*X+1.37506939145643E-03 )*X-\n           2.20258754419939E-02 )*X+2.31271692140905E-01;\n    RT3 = ((((( 4.84989776180094E-09*X+1.31538893944284E-07)*X-\n              2.766753852879E-06)*X-7.651163510626E-05)*X+\n            4.033058545972E-03)*X-8.16520022916145E-02 )*X+\n      8.57346024118779E-01;\n    RT4 = ((((-2.48581772214623E-07*X-4.34482635782585E-06)*X-\n             7.46018257987630E-07 )*X+1.01210776517279E-02 )*X-\n           2.83193369640005E-01 )*X+2.97353038120345E+00;\n    RT5 = (((((-8.92432153868554E-09*X+1.77288899268988E-08)*X+\n              3.040754680666E-06)*X+1.058229325071E-04)*X+\n            4.596379534985E-02)*X-1.75382723579114E+00 )*X+\n      1.84151859759049E+01;\n    WW1 = ((((((-2.03822632771791E-09*X+3.89110229133810E-08)*X-\n               5.84914787904823E-07 )*X+8.30316168666696E-06 )*X-\n             1.13218402310546E-04 )*X+1.49128888586790E-03 )*X-\n           1.96867576904816E-02 )*X+2.95524224714749E-01;\n    WW2 = ((((((( 8.62848118397570E-09*X-1.38975551148989E-07)*X+\n                1.602894068228E-06)*X-1.646364300836E-05)*X+\n              1.538445806778E-04)*X-1.28848868034502E-03 )*X+\n            9.38866933338584E-03 )*X-5.61737590178812E-02 )*X+\n      2.69266719309991E-01;\n    WW3 = ((((((((-9.41953204205665E-09*X+1.47452251067755E-07)*X-\n                 1.57456991199322E-06 )*X+1.45098401798393E-05 )*X-\n               1.18858834181513E-04 )*X+8.53697675984210E-04 )*X-\n             5.22877807397165E-03 )*X+2.60854524809786E-02 )*X-\n           9.71152726809059E-02 )*X+2.19086362515979E-01;\n    WW4 = ((((((((-3.84961617022042E-08*X+5.66595396544470E-07)*X-\n                 5.52351805403748E-06 )*X+4.53160377546073E-05 )*X-\n               3.22542784865557E-04 )*X+1.95682017370967E-03 )*X-\n             9.77232537679229E-03 )*X+3.79455945268632E-02 )*X-\n           1.02979262192227E-01 )*X+1.49451349150573E-01;\n    WW5 = ((((((((( 4.09594812521430E-09*X-6.47097874264417E-08)*X+\n                  6.743541482689E-07)*X-5.917993920224E-06)*X+\n                4.531969237381E-05)*X-2.99102856679638E-04 )*X+\n              1.65695765202643E-03 )*X-7.40671222520653E-03 )*X+\n            2.50889946832192E-02 )*X-5.73782817487958E-02 )*X+\n      6.66713443086877E-02;\n  } else if (X < 5.0) {\n    Y = X-3.0E+00;\n    RT1 = ((((((((-2.58163897135138E-14*Y+8.14127461488273E-13)*Y-\n                 2.11414838976129E-11 )*Y+5.09822003260014E-10 )*Y-\n               1.16002134438663E-08 )*Y+2.46810694414540E-07 )*Y-\n             4.92556826124502E-06 )*Y+9.02580687971053E-05 )*Y-\n           1.45190025120726E-03 )*Y+1.73416786387475E-02;\n    RT2 = ((((((((( 1.04525287289788E-14*Y+5.44611782010773E-14)*Y-\n                  4.831059411392E-12)*Y+1.136643908832E-10)*Y-\n                1.104373076913E-09)*Y-2.35346740649916E-08 )*Y+\n              1.43772622028764E-06 )*Y-4.23405023015273E-05 )*Y+\n            9.12034574793379E-04 )*Y-1.52479441718739E-02 )*Y+\n      1.76055265928744E-01;\n    RT3 = (((((((((-6.89693150857911E-14*Y+5.92064260918861E-13)*Y+\n                  1.847170956043E-11)*Y-3.390752744265E-10)*Y-\n                2.995532064116E-09)*Y+1.57456141058535E-07 )*Y-\n              3.95859409711346E-07 )*Y-9.58924580919747E-05 )*Y+\n            3.23551502557785E-03 )*Y-5.97587007636479E-02 )*Y+\n      6.46432853383057E-01;\n    RT4 = ((((((((-3.61293809667763E-12*Y-2.70803518291085E-11)*Y+\n                 8.83758848468769E-10 )*Y+1.59166632851267E-08 )*Y-\n               1.32581997983422E-07 )*Y-7.60223407443995E-06 )*Y-\n             7.41019244900952E-05 )*Y+9.81432631743423E-03 )*Y-\n           2.23055570487771E-01 )*Y+2.21460798080643E+00;\n    RT5 = ((((((((( 7.12332088345321E-13*Y+3.16578501501894E-12)*Y-\n                  8.776668218053E-11)*Y-2.342817613343E-09)*Y-\n                3.496962018025E-08)*Y-3.03172870136802E-07 )*Y+\n              1.50511293969805E-06 )*Y+1.37704919387696E-04 )*Y+\n            4.70723869619745E-02 )*Y-1.47486623003693E+00 )*Y+\n      1.35704792175847E+01;\n    WW1 = ((((((((( 1.04348658616398E-13*Y-1.94147461891055E-12)*Y+\n                  3.485512360993E-11)*Y-6.277497362235E-10)*Y+\n                1.100758247388E-08)*Y-1.88329804969573E-07 )*Y+\n              3.12338120839468E-06 )*Y-5.04404167403568E-05 )*Y+\n            8.00338056610995E-04 )*Y-1.30892406559521E-02 )*Y+\n      2.47383140241103E-01;\n    WW2 = ((((((((((( 3.23496149760478E-14*Y-5.24314473469311E-13)*Y+\n                    7.743219385056E-12)*Y-1.146022750992E-10)*Y+\n                  1.615238462197E-09)*Y-2.15479017572233E-08 )*Y+\n                2.70933462557631E-07 )*Y-3.18750295288531E-06 )*Y+\n              3.47425221210099E-05 )*Y-3.45558237388223E-04 )*Y+\n            3.05779768191621E-03 )*Y-2.29118251223003E-02 )*Y+\n      1.59834227924213E-01;\n    WW3 = ((((((((((((-3.42790561802876E-14*Y+5.26475736681542E-13)*Y-\n                     7.184330797139E-12)*Y+9.763932908544E-11)*Y-\n                   1.244014559219E-09)*Y+1.472744068942E-08)*Y-\n                 1.611749975234E-07)*Y+1.616487851917E-06)*Y-\n               1.46852359124154E-05 )*Y+1.18900349101069E-04 )*Y-\n             8.37562373221756E-04 )*Y+4.93752683045845E-03 )*Y-\n           2.25514728915673E-02 )*Y+6.95211812453929E-02;\n    WW4 = ((((((((((((( 1.04072340345039E-14*Y-1.60808044529211E-13)*\n                      Y+2.183534866798E-12)*Y-2.939403008391E-11)*Y+\n                    3.679254029085E-10)*Y-4.23775673047899E-09 )*Y+\n                  4.46559231067006E-08 )*Y-4.26488836563267E-07 )*Y+\n                3.64721335274973E-06 )*Y-2.74868382777722E-05 )*Y+\n              1.78586118867488E-04 )*Y-9.68428981886534E-04 )*Y+\n            4.16002324339929E-03 )*Y-1.28290192663141E-02 )*Y+\n      2.22353727685016E-02;\n    WW5 = ((((((((((((((-8.16770412525963E-16*Y+1.31376515047977E-14)*\n                       Y-1.856950818865E-13)*Y+2.596836515749E-12)*Y-\n                     3.372639523006E-11)*Y+4.025371849467E-10)*Y-\n                   4.389453269417E-09)*Y+4.332753856271E-08)*Y-\n                 3.82673275931962E-07 )*Y+2.98006900751543E-06 )*Y-\n               2.00718990300052E-05 )*Y+1.13876001386361E-04 )*Y-\n             5.23627942443563E-04 )*Y+1.83524565118203E-03 )*Y-\n           4.37785737450783E-03 )*Y+5.36963805223095E-03;\n  } else if (X < 10.0) {\n    Y = X-7.5E+00;\n    RT1 = ((((((((-1.13825201010775E-14*Y+1.89737681670375E-13)*Y-\n                 4.81561201185876E-12 )*Y+1.56666512163407E-10 )*Y-\n               3.73782213255083E-09 )*Y+9.15858355075147E-08 )*Y-\n             2.13775073585629E-06 )*Y+4.56547356365536E-05 )*Y-\n           8.68003909323740E-04 )*Y+1.22703754069176E-02;\n    RT2 = (((((((((-3.67160504428358E-15*Y+1.27876280158297E-14)*Y-\n                  1.296476623788E-12)*Y+1.477175434354E-11)*Y+\n                5.464102147892E-10)*Y-2.42538340602723E-08 )*Y+\n              8.20460740637617E-07 )*Y-2.20379304598661E-05 )*Y+\n            4.90295372978785E-04 )*Y-9.14294111576119E-03 )*Y+\n      1.22590403403690E-01;\n    RT3 = ((((((((( 1.39017367502123E-14*Y-6.96391385426890E-13)*Y+\n                  1.176946020731E-12)*Y+1.725627235645E-10)*Y-\n                3.686383856300E-09)*Y+2.87495324207095E-08 )*Y+\n              1.71307311000282E-06 )*Y-7.94273603184629E-05 )*Y+\n            2.00938064965897E-03 )*Y-3.63329491677178E-02 )*Y+\n      4.34393683888443E-01;\n    RT4 = ((((((((((-1.27815158195209E-14*Y+1.99910415869821E-14)*Y+\n                   3.753542914426E-12)*Y-2.708018219579E-11)*Y-\n                 1.190574776587E-09)*Y+1.106696436509E-08)*Y+\n               3.954955671326E-07)*Y-4.398596059588E-06)*Y-\n             2.01087998907735E-04 )*Y+7.89092425542937E-03 )*Y-\n           1.42056749162695E-01 )*Y+1.39964149420683E+00;\n    RT5 = ((((((((((-1.19442341030461E-13*Y-2.34074833275956E-12)*Y+\n                   6.861649627426E-12)*Y+6.082671496226E-10)*Y+\n                 5.381160105420E-09)*Y-6.253297138700E-08)*Y-\n               2.135966835050E-06)*Y-2.373394341886E-05)*Y+\n             2.88711171412814E-06 )*Y+4.85221195290753E-02 )*Y-\n           1.04346091985269E+00 )*Y+7.89901551676692E+00;\n    WW1 = ((((((((( 7.95526040108997E-15*Y-2.48593096128045E-13)*Y+\n                  4.761246208720E-12)*Y-9.535763686605E-11)*Y+\n                2.225273630974E-09)*Y-4.49796778054865E-08 )*Y+\n              9.17812870287386E-07 )*Y-1.86764236490502E-05 )*Y+\n            3.76807779068053E-04 )*Y-8.10456360143408E-03 )*Y+\n      2.01097936411496E-01;\n    WW2 = ((((((((((( 1.25678686624734E-15*Y-2.34266248891173E-14)*Y+\n                    3.973252415832E-13)*Y-6.830539401049E-12)*Y+\n                  1.140771033372E-10)*Y-1.82546185762009E-09 )*Y+\n                2.77209637550134E-08 )*Y-4.01726946190383E-07 )*Y+\n              5.48227244014763E-06 )*Y-6.95676245982121E-05 )*Y+\n            8.05193921815776E-04 )*Y-8.15528438784469E-03 )*Y+\n      9.71769901268114E-02;\n    WW3 = ((((((((((((-8.20929494859896E-16*Y+1.37356038393016E-14)*Y-\n                     2.022863065220E-13)*Y+3.058055403795E-12)*Y-\n                   4.387890955243E-11)*Y+5.923946274445E-10)*Y-\n                 7.503659964159E-09)*Y+8.851599803902E-08)*Y-\n               9.65561998415038E-07 )*Y+9.60884622778092E-06 )*Y-\n             8.56551787594404E-05 )*Y+6.66057194311179E-04 )*Y-\n           4.17753183902198E-03 )*Y+2.25443826852447E-02;\n    WW4 = ((((((((((((((-1.08764612488790E-17*Y+1.85299909689937E-16)*\n                       Y-2.730195628655E-15)*Y+4.127368817265E-14)*Y-\n                     5.881379088074E-13)*Y+7.805245193391E-12)*Y-\n                   9.632707991704E-11)*Y+1.099047050624E-09)*Y-\n                 1.15042731790748E-08 )*Y+1.09415155268932E-07 )*Y-\n               9.33687124875935E-07 )*Y+7.02338477986218E-06 )*Y-\n             4.53759748787756E-05 )*Y+2.41722511389146E-04 )*Y-\n           9.75935943447037E-04 )*Y+2.57520532789644E-03;\n    WW5 = ((((((((((((((( 7.28996979748849E-19*Y-1.26518146195173E-17)\n                        *Y+1.886145834486E-16)*Y-2.876728287383E-15)*Y+\n                      4.114588668138E-14)*Y-5.44436631413933E-13 )*Y+\n                    6.64976446790959E-12 )*Y-7.44560069974940E-11 )*Y+\n                  7.57553198166848E-10 )*Y-6.92956101109829E-09 )*Y+\n                5.62222859033624E-08 )*Y-3.97500114084351E-07 )*Y+\n              2.39039126138140E-06 )*Y-1.18023950002105E-05 )*Y+\n            4.52254031046244E-05 )*Y-1.21113782150370E-04 )*Y+\n      1.75013126731224E-04;\n  } else if (X < 15.0) {\n    Y = X-12.5E+00;\n    RT1 = ((((((((((-4.16387977337393E-17*Y+7.20872997373860E-16)*Y+\n                   1.395993802064E-14)*Y+3.660484641252E-14)*Y-\n                 4.154857548139E-12)*Y+2.301379846544E-11)*Y-\n               1.033307012866E-09)*Y+3.997777641049E-08)*Y-\n             9.35118186333939E-07 )*Y+2.38589932752937E-05 )*Y-\n           5.35185183652937E-04 )*Y+8.85218988709735E-03;\n    RT2 = ((((((((((-4.56279214732217E-16*Y+6.24941647247927E-15)*Y+\n                   1.737896339191E-13)*Y+8.964205979517E-14)*Y-\n                 3.538906780633E-11)*Y+9.561341254948E-11)*Y-\n               9.772831891310E-09)*Y+4.240340194620E-07)*Y-\n             1.02384302866534E-05 )*Y+2.57987709704822E-04 )*Y-\n           5.54735977651677E-03 )*Y+8.68245143991948E-02;\n    RT3 = ((((((((((-2.52879337929239E-15*Y+2.13925810087833E-14)*Y+\n                   7.884307667104E-13)*Y-9.023398159510E-13)*Y-\n                 5.814101544957E-11)*Y-1.333480437968E-09)*Y-\n               2.217064940373E-08)*Y+1.643290788086E-06)*Y-\n             4.39602147345028E-05 )*Y+1.08648982748911E-03 )*Y-\n           2.13014521653498E-02 )*Y+2.94150684465425E-01;\n    RT4 = ((((((((((-6.42391438038888E-15*Y+5.37848223438815E-15)*Y+\n                   8.960828117859E-13)*Y+5.214153461337E-11)*Y-\n                 1.106601744067E-10)*Y-2.007890743962E-08)*Y+\n               1.543764346501E-07)*Y+4.520749076914E-06)*Y-\n             1.88893338587047E-04 )*Y+4.73264487389288E-03 )*Y-\n           7.91197893350253E-02 )*Y+8.60057928514554E-01;\n    RT5 = (((((((((((-2.24366166957225E-14*Y+4.87224967526081E-14)*Y+\n                    5.587369053655E-12)*Y-3.045253104617E-12)*Y-\n                  1.223983883080E-09)*Y-2.05603889396319E-09 )*Y+\n                2.58604071603561E-07 )*Y+1.34240904266268E-06 )*Y-\n              5.72877569731162E-05 )*Y-9.56275105032191E-04 )*Y+\n            4.23367010370921E-02 )*Y-5.76800927133412E-01 )*Y+\n      3.87328263873381E+00;\n    WW1 = ((((((((( 8.98007931950169E-15*Y+7.25673623859497E-14)*Y+\n                  5.851494250405E-14)*Y-4.234204823846E-11)*Y+\n                3.911507312679E-10)*Y-9.65094802088511E-09 )*Y+\n              3.42197444235714E-07 )*Y-7.51821178144509E-06 )*Y+\n            1.94218051498662E-04 )*Y-5.38533819142287E-03 )*Y+\n      1.68122596736809E-01;\n    WW2 = ((((((((((-1.05490525395105E-15*Y+1.96855386549388E-14)*Y-\n                   5.500330153548E-13)*Y+1.003849567976E-11)*Y-\n                 1.720997242621E-10)*Y+3.533277061402E-09)*Y-\n               6.389171736029E-08)*Y+1.046236652393E-06)*Y-\n             1.73148206795827E-05 )*Y+2.57820531617185E-04 )*Y-\n           3.46188265338350E-03 )*Y+7.03302497508176E-02;\n    WW3 = ((((((((((( 3.60020423754545E-16*Y-6.24245825017148E-15)*Y+\n                    9.945311467434E-14)*Y-1.749051512721E-12)*Y+\n                  2.768503957853E-11)*Y-4.08688551136506E-10 )*Y+\n                6.04189063303610E-09 )*Y-8.23540111024147E-08 )*Y+\n              1.01503783870262E-06 )*Y-1.20490761741576E-05 )*Y+\n            1.26928442448148E-04 )*Y-1.05539461930597E-03 )*Y+\n      1.15543698537013E-02;\n    WW4 = ((((((((((((( 2.51163533058925E-18*Y-4.31723745510697E-17)*\n                      Y+6.557620865832E-16)*Y-1.016528519495E-14)*Y+\n                    1.491302084832E-13)*Y-2.06638666222265E-12 )*Y+\n                  2.67958697789258E-11 )*Y-3.23322654638336E-10 )*Y+\n                3.63722952167779E-09 )*Y-3.75484943783021E-08 )*Y+\n              3.49164261987184E-07 )*Y-2.92658670674908E-06 )*Y+\n            2.12937256719543E-05 )*Y-1.19434130620929E-04 )*Y+\n      6.45524336158384E-04;\n    WW5 = ((((((((((((((-1.29043630202811E-19*Y+2.16234952241296E-18)*\n                       Y-3.107631557965E-17)*Y+4.570804313173E-16)*Y-\n                     6.301348858104E-15)*Y+8.031304476153E-14)*Y-\n                   9.446196472547E-13)*Y+1.018245804339E-11)*Y-\n                 9.96995451348129E-11 )*Y+8.77489010276305E-10 )*Y-\n               6.84655877575364E-09 )*Y+4.64460857084983E-08 )*Y-\n             2.66924538268397E-07 )*Y+1.24621276265907E-06 )*Y-\n           4.30868944351523E-06 )*Y+9.94307982432868E-06;\n  } else if (X < 20.0){\n    Y = X-17.5E+00;\n    RT1 = (((((((((( 1.91875764545740E-16*Y+7.8357401095707E-16)*Y-\n                   3.260875931644E-14)*Y-1.186752035569E-13)*Y+\n                 4.275180095653E-12)*Y+3.357056136731E-11)*Y-\n               1.123776903884E-09)*Y+1.231203269887E-08)*Y-\n             3.99851421361031E-07 )*Y+1.45418822817771E-05 )*Y-\n           3.49912254976317E-04 )*Y+6.67768703938812E-03;\n    RT2 = (((((((((( 2.02778478673555E-15*Y+1.01640716785099E-14)*Y-\n                   3.385363492036E-13)*Y-1.615655871159E-12)*Y+\n                 4.527419140333E-11)*Y+3.853670706486E-10)*Y-\n               1.184607130107E-08)*Y+1.347873288827E-07)*Y-\n             4.47788241748377E-06 )*Y+1.54942754358273E-04 )*Y-\n           3.55524254280266E-03 )*Y+6.44912219301603E-02;\n    RT3 = (((((((((( 7.79850771456444E-15*Y+6.00464406395001E-14)*Y-\n                   1.249779730869E-12)*Y-1.020720636353E-11)*Y+\n                 1.814709816693E-10)*Y+1.766397336977E-09)*Y-\n               4.603559449010E-08)*Y+5.863956443581E-07)*Y-\n             2.03797212506691E-05 )*Y+6.31405161185185E-04 )*Y-\n           1.30102750145071E-02 )*Y+2.10244289044705E-01;\n    RT4 = (((((((((((-2.92397030777912E-15*Y+1.94152129078465E-14)*Y+\n                    4.859447665850E-13)*Y-3.217227223463E-12)*Y-\n                  7.484522135512E-11)*Y+7.19101516047753E-10 )*Y+\n                6.88409355245582E-09 )*Y-1.44374545515769E-07 )*Y+\n              2.74941013315834E-06 )*Y-1.02790452049013E-04 )*Y+\n            2.59924221372643E-03 )*Y-4.35712368303551E-02 )*Y+\n      5.62170709585029E-01;\n    RT5 = ((((((((((( 1.17976126840060E-14*Y+1.24156229350669E-13)*Y-\n                    3.892741622280E-12)*Y-7.755793199043E-12)*Y+\n                  9.492190032313E-10)*Y-4.98680128123353E-09 )*Y-\n                1.81502268782664E-07 )*Y+2.69463269394888E-06 )*Y+\n              2.50032154421640E-05 )*Y-1.33684303917681E-03 )*Y+\n            2.29121951862538E-02 )*Y-2.45653725061323E-01 )*Y+\n      1.89999883453047E+00;\n    WW1 = (((((((((( 1.74841995087592E-15*Y-6.95671892641256E-16)*Y-\n                   3.000659497257E-13)*Y+2.021279817961E-13)*Y+\n                 3.853596935400E-11)*Y+1.461418533652E-10)*Y-\n               1.014517563435E-08)*Y+1.132736008979E-07)*Y-\n             2.86605475073259E-06 )*Y+1.21958354908768E-04 )*Y-\n           3.86293751153466E-03 )*Y+1.45298342081522E-01;\n    WW2 = ((((((((((-1.11199320525573E-15*Y+1.85007587796671E-15)*Y+\n                   1.220613939709E-13)*Y+1.275068098526E-12)*Y-\n                 5.341838883262E-11)*Y+6.161037256669E-10)*Y-\n               1.009147879750E-08)*Y+2.907862965346E-07)*Y-\n             6.12300038720919E-06 )*Y+1.00104454489518E-04 )*Y-\n           1.80677298502757E-03 )*Y+5.78009914536630E-02;\n    WW3 = ((((((((((-9.49816486853687E-16*Y+6.67922080354234E-15)*Y+\n                   2.606163540537E-15)*Y+1.983799950150E-12)*Y-\n                 5.400548574357E-11)*Y+6.638043374114E-10)*Y-\n               8.799518866802E-09)*Y+1.791418482685E-07)*Y-\n             2.96075397351101E-06 )*Y+3.38028206156144E-05 )*Y-\n           3.58426847857878E-04 )*Y+8.39213709428516E-03;\n    WW4 = ((((((((((( 1.33829971060180E-17*Y-3.44841877844140E-16)*Y+\n                    4.745009557656E-15)*Y-6.033814209875E-14)*Y+\n                  1.049256040808E-12)*Y-1.70859789556117E-11 )*Y+\n                2.15219425727959E-10 )*Y-2.52746574206884E-09 )*Y+\n              3.27761714422960E-08 )*Y-3.90387662925193E-07 )*Y+\n            3.46340204593870E-06 )*Y-2.43236345136782E-05 )*Y+\n      3.54846978585226E-04;\n    WW5 = ((((((((((((( 2.69412277020887E-20*Y-4.24837886165685E-19)*\n                      Y+6.030500065438E-18)*Y-9.069722758289E-17)*Y+\n                    1.246599177672E-15)*Y-1.56872999797549E-14 )*Y+\n                  1.87305099552692E-13 )*Y-2.09498886675861E-12 )*Y+\n                2.11630022068394E-11 )*Y-1.92566242323525E-10 )*Y+\n              1.62012436344069E-09 )*Y-1.23621614171556E-08 )*Y+\n            7.72165684563049E-08 )*Y-3.59858901591047E-07 )*Y+\n      2.43682618601000E-06;\n  } else if (X < 25.0) {\n    Y = X-22.5E+00;\n    RT1 = (((((((((-1.13927848238726E-15*Y+7.39404133595713E-15)*Y+\n                  1.445982921243E-13)*Y-2.676703245252E-12)*Y+\n                5.823521627177E-12)*Y+2.17264723874381E-10 )*Y+\n              3.56242145897468E-09 )*Y-3.03763737404491E-07 )*Y+\n            9.46859114120901E-06 )*Y-2.30896753853196E-04 )*Y+\n      5.24663913001114E-03;\n    RT2 = (((((((((( 2.89872355524581E-16*Y-1.22296292045864E-14)*Y+\n                   6.184065097200E-14)*Y+1.649846591230E-12)*Y-\n                 2.729713905266E-11)*Y+3.709913790650E-11)*Y+\n               2.216486288382E-09)*Y+4.616160236414E-08)*Y-\n             3.32380270861364E-06 )*Y+9.84635072633776E-05 )*Y-\n           2.30092118015697E-03 )*Y+5.00845183695073E-02;\n    RT3 = (((((((((( 1.97068646590923E-15*Y-4.89419270626800E-14)*Y+\n                   1.136466605916E-13)*Y+7.546203883874E-12)*Y-\n                 9.635646767455E-11)*Y-8.295965491209E-11)*Y+\n               7.534109114453E-09)*Y+2.699970652707E-07)*Y-\n             1.42982334217081E-05 )*Y+3.78290946669264E-04 )*Y-\n           8.03133015084373E-03 )*Y+1.58689469640791E-01;\n    RT4 = (((((((((( 1.33642069941389E-14*Y-1.55850612605745E-13)*Y-\n                   7.522712577474E-13)*Y+3.209520801187E-11)*Y-\n                 2.075594313618E-10)*Y-2.070575894402E-09)*Y+\n               7.323046997451E-09)*Y+1.851491550417E-06)*Y-\n             6.37524802411383E-05 )*Y+1.36795464918785E-03 )*Y-\n           2.42051126993146E-02 )*Y+3.97847167557815E-01;\n    RT5 = ((((((((((-6.07053986130526E-14*Y+1.04447493138843E-12)*Y-\n                   4.286617818951E-13)*Y-2.632066100073E-10)*Y+\n                 4.804518986559E-09)*Y-1.835675889421E-08)*Y-\n               1.068175391334E-06)*Y+3.292234974141E-05)*Y-\n             5.94805357558251E-04 )*Y+8.29382168612791E-03 )*Y-\n           9.93122509049447E-02 )*Y+1.09857804755042E+00;\n    WW1 = (((((((((-9.10338640266542E-15*Y+1.00438927627833E-13)*Y+\n                  7.817349237071E-13)*Y-2.547619474232E-11)*Y+\n                1.479321506529E-10)*Y+1.52314028857627E-09 )*Y+\n              9.20072040917242E-09 )*Y-2.19427111221848E-06 )*Y+\n            8.65797782880311E-05 )*Y-2.82718629312875E-03 )*Y+\n      1.28718310443295E-01;\n    WW2 = ((((((((( 5.52380927618760E-15*Y-6.43424400204124E-14)*Y-\n                  2.358734508092E-13)*Y+8.261326648131E-12)*Y+\n                9.229645304956E-11)*Y-5.68108973828949E-09 )*Y+\n              1.22477891136278E-07 )*Y-2.11919643127927E-06 )*Y+\n            4.23605032368922E-05 )*Y-1.14423444576221E-03 )*Y+\n      5.06607252890186E-02;\n    WW3 = ((((((((( 3.99457454087556E-15*Y-5.11826702824182E-14)*Y-\n                  4.157593182747E-14)*Y+4.214670817758E-12)*Y+\n                6.705582751532E-11)*Y-3.36086411698418E-09 )*Y+\n              6.07453633298986E-08 )*Y-7.40736211041247E-07 )*Y+\n            8.84176371665149E-06 )*Y-1.72559275066834E-04 )*Y+\n      7.16639814253567E-03;\n    WW4 = (((((((((((-2.14649508112234E-18*Y-2.45525846412281E-18)*Y+\n                    6.126212599772E-16)*Y-8.526651626939E-15)*Y+\n                  4.826636065733E-14)*Y-3.39554163649740E-13 )*Y+\n                1.67070784862985E-11 )*Y-4.42671979311163E-10 )*Y+\n              6.77368055908400E-09 )*Y-7.03520999708859E-08 )*Y+\n            6.04993294708874E-07 )*Y-7.80555094280483E-06 )*Y+\n      2.85954806605017E-04;\n    WW5 = ((((((((((((-5.63938733073804E-21*Y+6.92182516324628E-20)*Y-\n                     1.586937691507E-18)*Y+3.357639744582E-17)*Y-\n                   4.810285046442E-16)*Y+5.386312669975E-15)*Y-\n                 6.117895297439E-14)*Y+8.441808227634E-13)*Y-\n               1.18527596836592E-11 )*Y+1.36296870441445E-10 )*Y-\n             1.17842611094141E-09 )*Y+7.80430641995926E-09 )*Y-\n           5.97767417400540E-08 )*Y+1.65186146094969E-06;\n  } else if (X < 40) {\n    WW1 = sqrt(PIE4/X);\n    E = exp(-X);\n    RT1 = ((((((((-1.73363958895356E-06*X+1.19921331441483E-04)*X -\n                 1.59437614121125E-02)*X+1.13467897349442E+00)*X -\n               4.47216460864586E+01)*X+1.06251216612604E+03)*X -\n             1.52073917378512E+04)*X+1.20662887111273E+05)*X -\n           4.07186366852475E+05)*E + R15/(X-R15);\n    RT2 = ((((((((-1.60102542621710E-05*X+1.10331262112395E-03)*X -\n                 1.50043662589017E-01)*X+1.05563640866077E+01)*X -\n               4.10468817024806E+02)*X+9.62604416506819E+03)*X -\n             1.35888069838270E+05)*X+1.06107577038340E+06)*X -\n           3.51190792816119E+06)*E + R25/(X-R25);\n    RT3 = ((((((((-4.48880032128422E-05*X+2.69025112122177E-03)*X -\n                 4.01048115525954E-01)*X+2.78360021977405E+01)*X -\n               1.04891729356965E+03)*X+2.36985942687423E+04)*X -\n             3.19504627257548E+05)*X+2.34879693563358E+06)*X -\n           7.16341568174085E+06)*E + R35/(X-R35);\n    RT4 = ((((((((-6.38526371092582E-05*X-2.29263585792626E-03)*X -\n                 7.65735935499627E-02)*X+9.12692349152792E+00)*X -\n               2.32077034386717E+02)*X+2.81839578728845E+02)*X +\n             9.59529683876419E+04)*X-1.77638956809518E+06)*X +\n           1.02489759645410E+07)*E + R45/(X-R45);\n    RT5 = ((((((((-3.59049364231569E-05*X-2.25963977930044E-02)*X +\n                 1.12594870794668E+00)*X-4.56752462103909E+01)*X +\n               1.05804526830637E+03)*X-1.16003199605875E+04)*X -\n             4.07297627297272E+04)*X+2.22215528319857E+06)*X -\n           1.61196455032613E+07)*E + R55/(X-R55);\n    WW5 = (((((((((-4.61100906133970E-10*X+1.43069932644286E-07)*X -\n                  1.63960915431080E-05)*X+1.15791154612838E-03)*X -\n                5.30573476742071E-02)*X+1.61156533367153E+00)*X -\n              3.23248143316007E+01)*X+4.12007318109157E+02)*X -\n            3.02260070158372E+03)*X+9.71575094154768E+03)*E + W55*WW1;\n    WW4 = (((((((((-2.40799435809950E-08*X+8.12621667601546E-06)*X -\n                  9.04491430884113E-04)*X+6.37686375770059E-02)*X -\n                2.96135703135647E+00)*X+9.15142356996330E+01)*X -\n              1.86971865249111E+03)*X+2.42945528916947E+04)*X -\n            1.81852473229081E+05)*X+5.96854758661427E+05)*E + W45*WW1;\n    WW3 = (((((((( 1.83574464457207E-05*X-1.54837969489927E-03)*X +\n                 1.18520453711586E-01)*X-6.69649981309161E+00)*X +\n               2.44789386487321E+02)*X-5.68832664556359E+03)*X +\n             8.14507604229357E+04)*X-6.55181056671474E+05)*X +\n           2.26410896607237E+06)*E + W35*WW1;\n    WW2 = (((((((( 2.77778345870650E-05*X-2.22835017655890E-03)*X +\n                 1.61077633475573E-01)*X-8.96743743396132E+00)*X +\n               3.28062687293374E+02)*X-7.65722701219557E+03)*X +\n             1.10255055017664E+05)*X-8.92528122219324E+05)*X +\n           3.10638627744347E+06)*E + W25*WW1;\n    WW1 = WW1-0.01962E+00*E-WW2-WW3-WW4-WW5;\n  } else if (X < 59.0) {\n    WW1 = sqrt(PIE4/X);\n    XXX = pow(X,3.0);\n    E = XXX*exp(-X);\n    RT1 = (((-2.43758528330205E-02*X+2.07301567989771E+00)*X -\n            6.45964225381113E+01)*X+7.14160088655470E+02)*E + R15/(X-R15);\n    RT2 = (((-2.28861955413636E-01*X+1.93190784733691E+01)*X -\n            5.99774730340912E+02)*X+6.61844165304871E+03)*E + R25/(X-R25);\n    RT3 = (((-6.95053039285586E-01*X+5.76874090316016E+01)*X -\n            1.77704143225520E+03)*X+1.95366082947811E+04)*E + R35/(X-R35);\n    RT4 = (((-1.58072809087018E+00*X+1.27050801091948E+02)*X -\n            3.86687350914280E+03)*X+4.23024828121420E+04)*E + R45/(X-R45);\n    RT5 = (((-3.33963830405396E+00*X+2.51830424600204E+02)*X -\n            7.57728527654961E+03)*X+8.21966816595690E+04)*E + R55/(X-R55);\n    E = XXX*E;\n    WW5 = (( 1.35482430510942E-08*X-3.27722199212781E-07)*X +\n           2.41522703684296E-06)*E + W55*WW1;\n    WW4 = (( 1.23464092261605E-06*X-3.55224564275590E-05)*X +\n           3.03274662192286E-04)*E + W45*WW1;\n    WW3 = (( 1.34547929260279E-05*X-4.19389884772726E-04)*X +\n           3.87706687610809E-03)*E + W35*WW1;\n    WW2 = (( 2.09539509123135E-05*X-6.87646614786982E-04)*X +\n           6.68743788585688E-03)*E + W25*WW1;\n    WW1 = WW1-WW2-WW3-WW4-WW5;\n  } else {\n    WW1 = sqrt(PIE4/X);\n    RT1 = R15/(X-R15);\n    RT2 = R25/(X-R25);\n    RT3 = R35/(X-R35);\n    RT4 = R45/(X-R45);\n    RT5 = R55/(X-R55);\n    WW2 = W25*WW1;\n    WW3 = W35*WW1;\n    WW4 = W45*WW1;\n    WW5 = W55*WW1;\n    WW1 = WW1-WW2-WW3-WW4-WW5;\n  }\n  roots[0] = RT1;\n  weights[0] = WW1;\n  roots[1] = RT2;\n  weights[1] = WW2;\n  roots[2] = RT3;\n  weights[2] = WW3;\n  roots[3] = RT4;\n  weights[3] = WW4;\n  roots[4] = RT5;\n  weights[4] = WW5;\n  return;\n}\n\n__device__ void cuda_Root6_dp(int n,double X, double roots[], double weights[]){\n  // Root6 not implemented yet\n  return;\n}\n\n__device__ double cuda_Int1d_dp(int i, int j, int k, int l,\n                                double xi, double xj, double xk, double xl,\n                                double alpha_ij_A, double alpha_kl_B, double sqrt_AB,\n                                double A, double B, double Px, double Qx,\n                                double inv_t1, double B00, double B1, double B1p, \n                                double G[][MAXROOTS])\n{\n  // Form G(n,m)=I(n,0,m,0) intermediate values for a Rys polynomial \n  int n = i+j;\n  int m = k+l;\n\n  double xij = xi-xj;\n  double xkl = xk-xl;\n\n  // RecurFactorsGamess\n  double C  = (Px-xi) * inv_t1 + (B*(Qx-xi)+A*(Px-xi))*B00*2.0;\n  double Cp = (Qx-xk) * inv_t1 + (B*(Qx-xk)+A*(Px-xk))*B00*2.0;\n\n  // ABD eq 11. \n  G[0][0] = M_PI * exp(-alpha_ij_A*xij*xij -alpha_kl_B*xkl*xkl) / sqrt_AB;\n\n  if (n > 0) { G[1][0] = C *G[0][0]; } // ABD eq 15 \n  if (m > 0) { G[0][1] = Cp*G[0][0]; } // ABD eq 16 \n\n  for (int a = 2; a < n+1; ++ a) { G[a][0] = B1 *(a-1)*G[a-2][0] + C *G[a-1][0]; } \n  for (int b = 2; b < m+1; ++ b) { G[0][b] = B1p*(b-1)*G[0][b-2] + Cp*G[0][b-1]; } \n\n  if ((m>0) && (n>0)){\n    for (int a=1; a<n+1; ++a){\n      G[a][1] = a*B00*G[a-1][0] + Cp*G[a][0];\n      for (int b=2; b<m+1; ++b)\n        G[a][b] = B1p*(b-1)*G[a][b-2] + a*B00*G[a-1][b-1] + Cp*G[a][b-1];\n    }\n  }\n\n  // Compute and output I(i,j,k,l) from I(i+j,0,k+l,0) (G) \n  double ijkl = 0.0;\n  for (int m=0; m<l+1; ++m){\n    double ijm0 = 0.0;\n    for (int n=0; n<j+1; ++n) // I(i,j,m,0)<-I(n,0,m,0)  \n      ijm0 += cuda_binomial(j,n)*pow(xij,(double)(j-n))*G[n+i][m+k];\n    ijkl += cuda_binomial(l,m)*pow(xkl,(double)(l-m))*ijm0; // I(i,j,k,l)<-I(i,j,m,0) \n  }\n\n  return ijkl;\n}\n\n__device__ void cuda_Roots_dp(int n, double X, double roots[], double weights[]){\n  if (n <= 3)\n    cuda_Root123_dp(n,X, roots,weights);\n  else if (n==4) \n    cuda_Root4_dp(X, roots,weights);\n  else if (n==5)\n    cuda_Root5_dp(X, roots,weights);\n  else\n    cuda_Root6_dp(n,X, roots,weights);\n  return;\n}\n\ninline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\ninline __host__ __device__ float4 fabs(float4 v)\n{\n    return make_float4(fabs(v.x), fabs(v.y), fabs(v.z), fabs(v.w));\n}\n\n__device__ int cuda_ij2intindex(int i, int j)\n{\n  if (i < j) {\n    int t = i; i = j; j = t;\n  }\n  return i * (i + 1) / 2 + j;\n}\n\n__device__ double cuda_rys_pbf_dp(const double *ptr_i, const double *ptr_j, \n                                  const double *ptr_k, const double *ptr_l)\n{\n  // download xyz, lmn, expon, and coef*norm\n  double xa = ptr_i[0];\n  double ya = ptr_i[1];\n  double za = ptr_i[2];\n  int    la = (int)ptr_i[3];\n  int    ma = (int)ptr_i[4];\n  int    na = (int)ptr_i[5];\n  double alphaa = ptr_i[6];\n  double norma  = ptr_i[7];\n\n  double xb = ptr_j[0];\n  double yb = ptr_j[1];\n  double zb = ptr_j[2];\n  int    lb = (int)ptr_j[3];\n  int    mb = (int)ptr_j[4];\n  int    nb = (int)ptr_j[5];\n  double alphab = ptr_j[6];\n  double normb  = ptr_j[7];\n\n  double xc = ptr_k[0];\n  double yc = ptr_k[1];\n  double zc = ptr_k[2];\n  int    lc = (int)ptr_k[3];\n  int    mc = (int)ptr_k[4];\n  int    nc = (int)ptr_k[5];\n  double alphac = ptr_k[6];\n  double normc  = ptr_k[7];\n\n  double xd = ptr_l[0];\n  double yd = ptr_l[1];\n  double zd = ptr_l[2];\n  int    ld = (int)ptr_l[3];\n  int    md = (int)ptr_l[4];\n  int    nd = (int)ptr_l[5];\n  double alphad = ptr_l[6];\n  double normd  = ptr_l[7];\n\n  // calculate primitive integral [ij|kl]\n  int norder,i;\n  double A,B,xp,yp,zp,xq,yq,zq,X,rho,sum,t,Ix,Iy,Iz;\n  \n  norder = (la+ma+na+lb+nb+mb+lc+mc+nc+ld+md+nd)/2 + 1;\n  A = alphaa+alphab; \n  B = alphac+alphad;\n\n  xp = (alphaa*xa+alphab*xb)/A;\n  yp = (alphaa*ya+alphab*yb)/A;\n  zp = (alphaa*za+alphab*zb)/A;\n\n  xq = (alphac*xc+alphad*xd)/B;\n  yq = (alphac*yc+alphad*yd)/B;\n  zq = (alphac*zc+alphad*zd)/B;\n\n  rho = A*B/(A+B);\n  X = rho * ((xp-xq)*(xp-xq)+(yp-yq)*(yp-yq)+(zp-zq)*(zp-zq));\n\n  double alpha_ab_A = alphaa * alphab / A;\n  double alpha_cd_B = alphac * alphad / B;\n  double sqrt_AB = sqrt(A * B);\n\n  double roots[MAXROOTS],weights[MAXROOTS];\n  double G[MAXROOTS][MAXROOTS];\n\n  cuda_Roots_dp(norder,X, roots,weights); // get currect roots/weights\n\n  sum = 0.;\n  for (i=0; i<norder; ++i){\n    t = roots[i];\n\n    double inv_t1, B00, B1, B1p;\n    inv_t1 = 1.0 / (1 + t);\n    B00 = 0.5 * t/(A+B) * inv_t1;\n    B1  = 0.5 / A * inv_t1 + B00;\n    B1p = 0.5 / B * inv_t1 + B00;\n\n    Ix = cuda_Int1d_dp(la,lb,lc,ld, xa,xb,xc,xd,\n           alpha_ab_A,alpha_cd_B,sqrt_AB, A,B,xp,xq, inv_t1,B00,B1,B1p, G);\n    Iy = cuda_Int1d_dp(ma,mb,mc,md, ya,yb,yc,yd,\n           alpha_ab_A,alpha_cd_B,sqrt_AB, A,B,yp,yq, inv_t1,B00,B1,B1p, G);\n    Iz = cuda_Int1d_dp(na,nb,nc,nd, za,zb,zc,zd,\n           alpha_ab_A,alpha_cd_B,sqrt_AB, A,B,zp,zq, inv_t1,B00,B1,B1p, G);\n    sum = sum + Ix*Iy*Iz*weights[i]; /* ABD eq 5 & 9 */\n  }\n\n  // inv_sqrt_pi_2: 2.0*sqrt(1.0/M_PI) = 1.12837916709551255856\n  return 1.12837916709551255856 * sqrt(rho)*norma*normb*normc*normd*sum; /* ABD eq 5 & 9 */\n}\n\n__global__ void cuda_mat_K_PI_dp(\n  const double *__restrict pbf_xlec, \n  const int *__restrict pbf_to_cbf, \n  int n_pbf,\n  const double *__restrict mat_D,\n  double *__restrict mat_K_PI,\n  const double *__restrict mat_Q)\n{\n    __shared__ double elem_K_PI[BLOCKSIZE * BLOCKSIZE];\n\n    // each block scans over [i?|k?] and sum up to a primitive K matrix element\n    int i = blockIdx.x;\n    int k = blockIdx.y;\n\n    // avoid accessing out of bounds elements and make use of ij<=>kl symmetry\n    if (i >= n_pbf || k > i) { return; }\n\n    int ik = cuda_ij2intindex(i,k);\n\n    const double *ptr_i = &pbf_xlec[i * 8];\n    const double *ptr_k = &pbf_xlec[k * 8];\n\n    int a = pbf_to_cbf[i];\n    int c = pbf_to_cbf[k];\n\n    // initialize shared array\n    elem_K_PI[threadIdx.x * BLOCKSIZE + threadIdx.y] = 0.0;\n\n    for (int j = threadIdx.x; j < n_pbf; j += BLOCKSIZE)\n    {\n        int b = pbf_to_cbf[j];\n        int ab = cuda_ij2intindex(a,b);\n        const double *ptr_j = &pbf_xlec[j * 8];\n\n        for (int l = threadIdx.y; l < n_pbf; l += BLOCKSIZE)\n        {\n            int d = pbf_to_cbf[l];\n            int cd = cuda_ij2intindex(c,d);\n            int bd = cuda_ij2intindex(b,d);\n\n            // Schwartz screening\n            if (fabs(mat_Q[ab] * mat_Q[cd] * mat_D[bd]) < SCREEN_THR) { continue; }\n\n            const double *ptr_l = &pbf_xlec[l * 8];\n\n            // calculate ERI\n            double this_eri = cuda_rys_pbf_dp(ptr_i, ptr_j, ptr_k, ptr_l);\n\n            // NOTE: no doubling for off-diagonal elements of D\n            elem_K_PI[threadIdx.x * BLOCKSIZE + threadIdx.y] += this_eri * mat_D[bd];\n        }\n    }\n\n    __syncthreads();\n\n    // only update mat_K_PI on one thread of the block\n    if (0 == threadIdx.x && 0 == threadIdx.y)\n    {\n        mat_K_PI[ik] = 0.0; \n        for (int t1 = 0; t1 < BLOCKSIZE; ++ t1) {\n            for (int t2 = 0; t2 < BLOCKSIZE; ++ t2) {\n                mat_K_PI[ik] += elem_K_PI[t1 * BLOCKSIZE + t2];\n            }\n        }\n    }\n}"
        ],
        "/Users/gbolet/hecbench-roofline/src/xlqc-cuda/cuda_rys_sp.cu": [
            "__device__ int cuda_fact(int n){\n  int result = 1;\n  for (int i = 2; i <= n; i++) result *= i;\n  return result;\n}\n\ninline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\ninline __host__ __device__ float4 expf(float4 v)\n{\n    return make_float4(expf(v.x), expf(v.y), expf(v.z), expf(v.w));\n}\n\n__device__ int cuda_binomial(int a, int b){\n  return cuda_fact(a)/(cuda_fact(b)*cuda_fact(a-b));\n}\n\ninline __host__ __device__ float4 sqrtf(float4 v)\n{\n    return make_float4(sqrtf(v.x), sqrtf(v.y), sqrtf(v.z), sqrtf(v.w));\n}\n\n__device__ void cuda_Root123(int n, float X, float roots[], float weights[]){\n\n  float R12, PIE4, R22, W22, R13, R23, W23, R33, W33;\n  float RT1=0,RT2=0,RT3=0,WW1=0,WW2=0,WW3=0;\n  float F1,F2,E,T1,T2,T3,A1,A2,Y;\n\n  R12  = 2.75255128608411E-01f;\n  PIE4 = 7.85398163397448E-01f;\n  R22  =  2.72474487139158E+00f;\n  W22  = 9.17517095361369E-02f;\n  R13  = 1.90163509193487E-01f;\n  R23  = 1.78449274854325E+00f;\n  W23  = 1.77231492083829E-01f;\n  R33  = 5.52534374226326E+00f;\n  W33  = 5.11156880411248E-03f;\n\n  if (X < 3.e-7f){\n    if (n == 1){\n      RT1 = 0.5E+00f -X/5.0E+00f;\n      WW1 = 1.0E+00f -X/3.0E+00f;\n    } else if (n == 2) {\n      RT1 = 1.30693606237085E-01f -2.90430236082028E-02f *X;\n      RT2 = 2.86930639376291E+00f -6.37623643058102E-01f *X;\n      WW1 = 6.52145154862545E-01f -1.22713621927067E-01f *X;\n      WW2 = 3.47854845137453E-01f -2.10619711404725E-01f *X;\n    } else if (n == 3) {\n      RT1 = 6.03769246832797E-02f -9.28875764357368E-03f *X;\n      RT2 = 7.76823355931043E-01f -1.19511285527878E-01f *X;\n      RT3 = 6.66279971938567E+00f -1.02504611068957E+00f *X;\n      WW1 = 4.67913934572691E-01f -5.64876917232519E-02f *X;\n      WW2 = 3.60761573048137E-01f -1.49077186455208E-01f *X;\n      WW3 = 1.71324492379169E-01f -1.27768455150979E-01f *X;\n    }\n  } else if (X < 1.f) {\n    if (n == 1){\n      F1 = ((((((((-8.36313918003957E-08f*X+1.21222603512827E-06f )*X-\n                      1.15662609053481E-05f )*X+9.25197374512647E-05f )*X-\n                  6.40994113129432E-04f )*X+3.78787044215009E-03f )*X-\n              1.85185172458485E-02f )*X+7.14285713298222E-02f )*X-\n          1.99999999997023E-01f )*X+3.33333333333318E-01f;\n      WW1 = (X+X)*F1+expf(-X);\n      RT1 = F1/(WW1-F1);\n    } else if (n == 2) {\n      F1 = ((((((((-8.36313918003957E-08f*X+1.21222603512827E-06f )*X-\n                      1.15662609053481E-05f )*X+9.25197374512647E-05f )*X-\n                  6.40994113129432E-04f )*X+3.78787044215009E-03f )*X-\n              1.85185172458485E-02f )*X+7.14285713298222E-02f )*X-\n          1.99999999997023E-01f )*X+3.33333333333318E-01f;\n      WW1 = (X+X)*F1+expf(-X);\n      RT1 = (((((((-2.35234358048491E-09f*X+2.49173650389842E-08f)*X-\n                    4.558315364581E-08f)*X-2.447252174587E-06f)*X+\n                4.743292959463E-05f)*X-5.33184749432408E-04f )*X+\n            4.44654947116579E-03f )*X-2.90430236084697E-02f )*X+\n        1.30693606237085E-01f;\n      RT2 = (((((((-2.47404902329170E-08f*X+2.36809910635906E-07f)*X+\n                    1.835367736310E-06f)*X-2.066168802076E-05f)*X-\n                1.345693393936E-04f)*X-5.88154362858038E-05f )*X+\n            5.32735082098139E-02f )*X-6.37623643056745E-01f )*X+\n        2.86930639376289E+00f;\n      WW2 = ((F1-WW1)*RT1+F1)*(1.0E+00f+RT2)/(RT2-RT1);\n      WW1 = WW1-WW2;\n    } else if (n==3){\n      RT1 = ((((((-5.10186691538870E-10f*X+2.40134415703450E-08f)*X-\n                  5.01081057744427E-07f )*X+7.58291285499256E-06f )*X-\n              9.55085533670919E-05f )*X+1.02893039315878E-03f )*X-\n          9.28875764374337E-03f )*X+6.03769246832810E-02f;\n      RT2 = ((((((-1.29646524960555E-08f*X+7.74602292865683E-08f)*X+\n                  1.56022811158727E-06f )*X-1.58051990661661E-05f )*X-\n              3.30447806384059E-04f )*X+9.74266885190267E-03f )*X-\n          1.19511285526388E-01f )*X+7.76823355931033E-01f;\n      RT3 = ((((((-9.28536484109606E-09f*X-3.02786290067014E-07f)*X-\n                  2.50734477064200E-06f )*X-7.32728109752881E-06f )*X+\n              2.44217481700129E-04f )*X+4.94758452357327E-02f )*X-\n          1.02504611065774E+00f )*X+6.66279971938553E+00f;\n      F2 = ((((((((-7.60911486098850E-08f*X+1.09552870123182E-06f )*X-\n                      1.03463270693454E-05f )*X+8.16324851790106E-05f )*X-\n                  5.55526624875562E-04f )*X+3.20512054753924E-03f )*X-\n              1.51515139838540E-02f )*X+5.55555554649585E-02f )*X-\n          1.42857142854412E-01f )*X+1.99999999999986E-01f;\n      E = expf(-X);\n      F1 = ((X+X)*F2+E)/3.0E+00f;\n      WW1 = (X+X)*F1+E;\n      T1 = RT1/(RT1+1.0E+00f);\n      T2 = RT2/(RT2+1.0E+00f);\n      T3 = RT3/(RT3+1.0E+00f);\n      A2 = F2-T1*F1;\n      A1 = F1-T1*WW1;\n      WW3 = (A2-T2*A1)/((T3-T2)*(T3-T1));\n      WW2 = (T3*A1-A2)/((T3-T2)*(T2-T1));\n      WW1 = WW1-WW2-WW3;\n    }\n  } else if (X < 3.f) {\n    Y = X-2.0E+00f;\n    if (n == 1) {\n      F1 = ((((((((((-1.61702782425558E-10f*Y+1.96215250865776E-09f )*Y-\n                          2.14234468198419E-08f )*Y+2.17216556336318E-07f )*Y-\n                      1.98850171329371E-06f )*Y+1.62429321438911E-05f )*Y-\n                  1.16740298039895E-04f )*Y+7.24888732052332E-04f )*Y-\n              3.79490003707156E-03f )*Y+1.61723488664661E-02f )*Y-\n          5.29428148329736E-02f )*Y+1.15702180856167E-01f;\n      WW1 = (X+X)*F1+expf(-X);\n      RT1 = F1/(WW1-F1);\n    } else if (n == 2) {\n      F1 = ((((((((((-1.61702782425558E-10f*Y+1.96215250865776E-09f )*Y-\n                          2.14234468198419E-08f )*Y+2.17216556336318E-07f )*Y-\n                      1.98850171329371E-06f )*Y+1.62429321438911E-05f )*Y-\n                  1.16740298039895E-04f )*Y+7.24888732052332E-04f )*Y-\n              3.79490003707156E-03f )*Y+1.61723488664661E-02f )*Y-\n          5.29428148329736E-02f )*Y+1.15702180856167E-01f;\n      WW1 = (X+X)*F1+expf(-X);\n      RT1 = (((((((((-6.36859636616415E-12f*Y+8.47417064776270E-11f)*Y-\n                        5.152207846962E-10f)*Y-3.846389873308E-10f)*Y+\n                    8.472253388380E-08f)*Y-1.85306035634293E-06f )*Y+\n                2.47191693238413E-05f )*Y-2.49018321709815E-04f )*Y+\n            2.19173220020161E-03f )*Y-1.63329339286794E-02f )*Y+\n        8.68085688285261E-02f;\n      RT2 = ((((((((( 1.45331350488343E-10f*Y+2.07111465297976E-09f)*Y-\n                        1.878920917404E-08f)*Y-1.725838516261E-07f)*Y+\n                    2.247389642339E-06f)*Y+9.76783813082564E-06f )*Y-\n                1.93160765581969E-04f )*Y-1.58064140671893E-03f )*Y+\n            4.85928174507904E-02f )*Y-4.30761584997596E-01f )*Y+\n        1.80400974537950E+00f;\n      WW2 = ((F1-WW1)*RT1+F1)*(1.0E+00f+RT2)/(RT2-RT1);\n      WW1 = WW1-WW2;\n    } else if (n == 3) {\n      RT1 = (((((((( 1.44687969563318E-12f*Y+4.85300143926755E-12f)*Y-\n                      6.55098264095516E-10f )*Y+1.56592951656828E-08f )*Y-\n                  2.60122498274734E-07f )*Y+3.86118485517386E-06f )*Y-\n              5.13430986707889E-05f )*Y+6.03194524398109E-04f )*Y-\n          6.11219349825090E-03f )*Y+4.52578254679079E-02f;\n      RT2 = ((((((( 6.95964248788138E-10f*Y-5.35281831445517E-09f)*Y-\n                    6.745205954533E-08f)*Y+1.502366784525E-06f)*Y+\n                9.923326947376E-07f)*Y-3.89147469249594E-04f )*Y+\n            7.51549330892401E-03f )*Y-8.48778120363400E-02f )*Y+\n        5.73928229597613E-01f;\n      RT3 = ((((((((-2.81496588401439E-10f*Y+3.61058041895031E-09f)*Y+\n                      4.53631789436255E-08f )*Y-1.40971837780847E-07f )*Y-\n                  6.05865557561067E-06f )*Y-5.15964042227127E-05f )*Y+\n              3.34761560498171E-05f )*Y+5.04871005319119E-02f )*Y-\n          8.24708946991557E-01f )*Y+4.81234667357205E+00f;\n      F2 = ((((((((((-1.48044231072140E-10f*Y+1.78157031325097E-09f )*Y-\n                          1.92514145088973E-08f )*Y+1.92804632038796E-07f )*Y-\n                      1.73806555021045E-06f )*Y+1.39195169625425E-05f )*Y-\n                  9.74574633246452E-05f )*Y+5.83701488646511E-04f )*Y-\n              2.89955494844975E-03f )*Y+1.13847001113810E-02f )*Y-\n          3.23446977320647E-02f )*Y+5.29428148329709E-02f;\n      E = expf(-X);\n      F1 = ((X+X)*F2+E)/3.0E+00f;\n      WW1 = (X+X)*F1+E;\n      T1 = RT1/(RT1+1.0E+00f);\n      T2 = RT2/(RT2+1.0E+00f);\n      T3 = RT3/(RT3+1.0E+00f);\n      A2 = F2-T1*F1;\n      A1 = F1-T1*WW1;\n      WW3 = (A2-T2*A1)/((T3-T2)*(T3-T1));\n      WW2 = (T3*A1-A2)/((T3-T2)*(T2-T1));\n      WW1 = WW1-WW2-WW3;\n    }\n  } else if (X < 5.f){\n    Y = X-4.0E+00f;\n    if (n == 1){\n      F1 = ((((((((((-2.62453564772299E-11f*Y+3.24031041623823E-10f )*Y-\n                          3.614965656163E-09f)*Y+3.760256799971E-08f)*Y-\n                      3.553558319675E-07f)*Y+3.022556449731E-06f)*Y-\n                  2.290098979647E-05f)*Y+1.526537461148E-04f)*Y-\n              8.81947375894379E-04f )*Y+4.33207949514611E-03f )*Y-\n          1.75257821619926E-02f )*Y+5.28406320615584E-02f;\n      WW1 = (X+X)*F1+expf(-X);\n      RT1 = F1/(WW1-F1);\n    } else if (n == 2) {\n      F1 = ((((((((((-2.62453564772299E-11f*Y+3.24031041623823E-10f )*Y-\n                          3.614965656163E-09f)*Y+3.760256799971E-08f)*Y-\n                      3.553558319675E-07f)*Y+3.022556449731E-06f)*Y-\n                  2.290098979647E-05f)*Y+1.526537461148E-04f)*Y-\n              8.81947375894379E-04f )*Y+4.33207949514611E-03f )*Y-\n          1.75257821619926E-02f )*Y+5.28406320615584E-02f;\n      WW1 = (X+X)*F1+expf(-X);\n      RT1 = ((((((((-4.11560117487296E-12f*Y+7.10910223886747E-11f)*Y-\n                      1.73508862390291E-09f )*Y+5.93066856324744E-08f )*Y-\n                  9.76085576741771E-07f )*Y+1.08484384385679E-05f )*Y-\n              1.12608004981982E-04f )*Y+1.16210907653515E-03f )*Y-\n          9.89572595720351E-03f )*Y+6.12589701086408E-02f;\n      RT2 = (((((((((-1.80555625241001E-10f*Y+5.44072475994123E-10f)*Y+\n                        1.603498045240E-08f)*Y-1.497986283037E-07f)*Y-\n                    7.017002532106E-07f)*Y+1.85882653064034E-05f )*Y-\n                2.04685420150802E-05f )*Y-2.49327728643089E-03f )*Y+\n            3.56550690684281E-02f )*Y-2.60417417692375E-01f )*Y+\n        1.12155283108289E+00f;\n      WW2 = ((F1-WW1)*RT1+F1)*(1.0E+00f+RT2)/(RT2-RT1);\n      WW1 = WW1-WW2;\n    } else if (n == 3) {\n      RT1 = ((((((( 1.44265709189601E-11f*Y-4.66622033006074E-10f)*Y+\n                    7.649155832025E-09f)*Y-1.229940017368E-07f)*Y+\n                2.026002142457E-06f)*Y-2.87048671521677E-05f )*Y+\n            3.70326938096287E-04f )*Y-4.21006346373634E-03f )*Y+\n        3.50898470729044E-02f;\n      RT2 = ((((((((-2.65526039155651E-11f*Y+1.97549041402552E-10f)*Y+\n                      2.15971131403034E-09f )*Y-7.95045680685193E-08f )*Y+\n                  5.15021914287057E-07f )*Y+1.11788717230514E-05f )*Y-\n              3.33739312603632E-04f )*Y+5.30601428208358E-03f )*Y-\n          5.93483267268959E-02f )*Y+4.31180523260239E-01f;\n      RT3 = ((((((((-3.92833750584041E-10f*Y-4.16423229782280E-09f)*Y+\n                      4.42413039572867E-08f )*Y+6.40574545989551E-07f )*Y-\n                  3.05512456576552E-06f )*Y-1.05296443527943E-04f )*Y-\n              6.14120969315617E-04f )*Y+4.89665802767005E-02f )*Y-\n          6.24498381002855E-01f )*Y+3.36412312243724E+00f;\n      F2 = ((((((((((-2.36788772599074E-11f*Y+2.89147476459092E-10f )*Y-\n                          3.18111322308846E-09f )*Y+3.25336816562485E-08f )*Y-\n                      3.00873821471489E-07f )*Y+2.48749160874431E-06f )*Y-\n                  1.81353179793672E-05f )*Y+1.14504948737066E-04f )*Y-\n              6.10614987696677E-04f )*Y+2.64584212770942E-03f )*Y-\n          8.66415899015349E-03f )*Y+1.75257821619922E-02f;\n      E = expf(-X);\n      F1 = ((X+X)*F2+E)/3.0E+00f;\n      WW1 = (X+X)*F1+E;\n      T1 = RT1/(RT1+1.0E+00f);\n      T2 = RT2/(RT2+1.0E+00f);\n      T3 = RT3/(RT3+1.0E+00f);\n      A2 = F2-T1*F1;\n      A1 = F1-T1*WW1;\n      WW3 = (A2-T2*A1)/((T3-T2)*(T3-T1));\n      WW2 = (T3*A1-A2)/((T3-T2)*(T2-T1));\n      WW1 = WW1-WW2-WW3;\n    }\n  } else if (X < 10.f) {\n    E = expf(-X);\n    WW1 = (((((( 4.6897511375022E-01f/X-6.9955602298985E-01f)/X +\n                5.3689283271887E-01f)/X-3.2883030418398E-01f)/X +\n            2.4645596956002E-01f)/X-4.9984072848436E-01f)/X -\n        3.1501078774085E-06f)*E + sqrtf(PIE4/X);\n    F1 = (WW1-E)/(X+X);\n    if (n == 1)\n      RT1 = F1/(WW1-F1);\n    else if (n == 2){\n      Y = X-7.5E+00f;\n      RT1 = (((((((((((((-1.43632730148572E-16f*Y+2.38198922570405E-16f)*\n                                Y+1.358319618800E-14f)*Y-7.064522786879E-14f)*Y-\n                            7.719300212748E-13f)*Y+7.802544789997E-12f)*Y+\n                        6.628721099436E-11f)*Y-1.775564159743E-09f)*Y+\n                    1.713828823990E-08f)*Y-1.497500187053E-07f)*Y+\n                2.283485114279E-06f)*Y-3.76953869614706E-05f )*Y+\n            4.74791204651451E-04f )*Y-4.60448960876139E-03f )*Y+\n        3.72458587837249E-02f;\n      RT2 = (((((((((((( 2.48791622798900E-14f*Y-1.36113510175724E-13f)*Y-\n                              2.224334349799E-12f)*Y+4.190559455515E-11f)*Y-\n                          2.222722579924E-10f)*Y-2.624183464275E-09f)*Y+\n                      6.128153450169E-08f)*Y-4.383376014528E-07f)*Y-\n                  2.49952200232910E-06f )*Y+1.03236647888320E-04f )*Y-\n              1.44614664924989E-03f )*Y+1.35094294917224E-02f )*Y-\n          9.53478510453887E-02f )*Y+5.44765245686790E-01f;\n      WW2 = ((F1-WW1)*RT1+F1)*(1.0E+00f+RT2)/(RT2-RT1);\n      WW1 = WW1-WW2;\n    } else if (n == 3) {\n      F2 = (F1+F1+F1-E)/(X+X);\n      Y = X-7.5E+00f;\n      RT1 = ((((((((((( 5.74429401360115E-16f*Y+7.11884203790984E-16f)*Y-\n                            6.736701449826E-14f)*Y-6.264613873998E-13f)*Y+\n                        1.315418927040E-11f)*Y-4.23879635610964E-11f )*Y+\n                    1.39032379769474E-09f )*Y-4.65449552856856E-08f )*Y+\n                7.34609900170759E-07f )*Y-1.08656008854077E-05f )*Y+\n            1.77930381549953E-04f )*Y-2.39864911618015E-03f )*Y+\n        2.39112249488821E-02f;\n      RT2 = ((((((((((( 1.13464096209120E-14f*Y+6.99375313934242E-15f)*Y-\n                            8.595618132088E-13f)*Y-5.293620408757E-12f)*Y-\n                        2.492175211635E-11f)*Y+2.73681574882729E-09f )*Y-\n                    1.06656985608482E-08f )*Y-4.40252529648056E-07f )*Y+\n                9.68100917793911E-06f )*Y-1.68211091755327E-04f )*Y+\n            2.69443611274173E-03f )*Y-3.23845035189063E-02f )*Y+\n        2.75969447451882E-01f;\n      RT3 = (((((((((((( 6.66339416996191E-15f*Y+1.84955640200794E-13f)*Y-\n                              1.985141104444E-12f)*Y-2.309293727603E-11f)*Y+\n                          3.917984522103E-10f)*Y+1.663165279876E-09f)*Y-\n                      6.205591993923E-08f)*Y+8.769581622041E-09f)*Y+\n                  8.97224398620038E-06f )*Y-3.14232666170796E-05f )*Y-\n              1.83917335649633E-03f )*Y+3.51246831672571E-02f )*Y-\n          3.22335051270860E-01f )*Y+1.73582831755430E+00f;\n      T1 = RT1/(RT1+1.0E+00f);\n      T2 = RT2/(RT2+1.0E+00f);\n      T3 = RT3/(RT3+1.0E+00f);\n      A2 = F2-T1*F1;\n      A1 = F1-T1*WW1;\n      WW3 = (A2-T2*A1)/((T3-T2)*(T3-T1));\n      WW2 = (T3*A1-A2)/((T3-T2)*(T2-T1));\n      WW1 = WW1-WW2-WW3;\n    }\n  } else if (X < 15.f) {\n    E = expf(-X);\n    WW1 = (((-1.8784686463512E-01f/X+2.2991849164985E-01f)/X -\n          4.9893752514047E-01f)/X-2.1916512131607E-05f)*E \n      + sqrtf(PIE4/X);\n    F1 = (WW1-E)/(X+X);\n    if (n == 1)\n      RT1 = F1/(WW1-F1);\n    else if (n == 2) {\n      RT1 = ((((-1.01041157064226E-05f*X+1.19483054115173E-03f)*X -\n              6.73760231824074E-02f)*X+1.25705571069895E+00f)*X +\n          (((-8.57609422987199E+03f/X+5.91005939591842E+03f)/X -\n            1.70807677109425E+03f)/X+2.64536689959503E+02f)/X -\n          2.38570496490846E+01f)*E + R12/(X-R12);\n      RT2 = ((( 3.39024225137123E-04f*X-9.34976436343509E-02f)*X -\n            4.22216483306320E+00f)*X +\n          (((-2.08457050986847E+03f/X -\n             1.04999071905664E+03f)/X+3.39891508992661E+02f)/X -\n           1.56184800325063E+02f)/X+8.00839033297501E+00f)*E + R22/(X-R22);\n      WW2 = ((F1-WW1)*RT1+F1)*(1.0E+00f+RT2)/(RT2-RT1);\n      WW1 = WW1-WW2;\n    } else if (n == 3) {\n      F2 = (F1+F1+F1-E)/(X+X);\n      Y = X-12.5E+00f;\n      RT1 = ((((((((((( 4.42133001283090E-16f*Y-2.77189767070441E-15f)*Y-\n                            4.084026087887E-14f)*Y+5.379885121517E-13f)*Y+\n                        1.882093066702E-12f)*Y-8.67286219861085E-11f )*Y+\n                    7.11372337079797E-10f )*Y-3.55578027040563E-09f )*Y+\n                1.29454702851936E-07f )*Y-4.14222202791434E-06f )*Y+\n            8.04427643593792E-05f )*Y-1.18587782909876E-03f )*Y+\n        1.53435577063174E-02f;\n      RT2 = ((((((((((( 6.85146742119357E-15f*Y-1.08257654410279E-14f)*Y-\n                            8.579165965128E-13f)*Y+6.642452485783E-12f)*Y+\n                        4.798806828724E-11f)*Y-1.13413908163831E-09f )*Y+\n                    7.08558457182751E-09f )*Y-5.59678576054633E-08f )*Y+\n                2.51020389884249E-06f )*Y-6.63678914608681E-05f )*Y+\n            1.11888323089714E-03f )*Y-1.45361636398178E-02f )*Y+\n        1.65077877454402E-01f;\n      RT3 = (((((((((((( 3.20622388697743E-15f*Y-2.73458804864628E-14f)*Y-\n                              3.157134329361E-13f)*Y+8.654129268056E-12f)*Y-\n                          5.625235879301E-11f)*Y-7.718080513708E-10f)*Y+\n                      2.064664199164E-08f)*Y-1.567725007761E-07f)*Y-\n                  1.57938204115055E-06f )*Y+6.27436306915967E-05f )*Y-\n              1.01308723606946E-03f )*Y+1.13901881430697E-02f )*Y-\n          1.01449652899450E-01f )*Y+7.77203937334739E-01f;\n      T1 = RT1/(RT1+1.0E+00f);\n      T2 = RT2/(RT2+1.0E+00f);\n      T3 = RT3/(RT3+1.0E+00f);\n      A2 = F2-T1*F1;\n      A1 = F1-T1*WW1;\n      WW3 = (A2-T2*A1)/((T3-T2)*(T3-T1));\n      WW2 = (T3*A1-A2)/((T3-T2)*(T2-T1));\n      WW1 = WW1-WW2-WW3;\n    }\n  } else if (X < 33.f) {\n    E = expf(-X);\n    WW1 = (( 1.9623264149430E-01f/X-4.9695241464490E-01f)/X -\n        6.0156581186481E-05f)*E + sqrtf(PIE4/X);\n    F1 = (WW1-E)/(X+X);\n    if (n == 1)\n      RT1 = F1/(WW1-F1);\n    else if (n == 2){\n      RT1 = ((((-1.14906395546354E-06f*X+1.76003409708332E-04f)*X -\n              1.71984023644904E-02f)*X-1.37292644149838E-01f)*X +\n          (-4.75742064274859E+01f/X+9.21005186542857E+00f)/X -\n          2.31080873898939E-02f)*E + R12/(X-R12);\n      RT2 = ((( 3.64921633404158E-04f*X-9.71850973831558E-02f)*X -\n            4.02886174850252E+00f)*X +\n          (-1.35831002139173E+02f/X -\n           8.66891724287962E+01f)/X+2.98011277766958E+00f)*E + R22/(X-R22);\n      WW2 = ((F1-WW1)*RT1+F1)*(1.0E+00f+RT2)/(RT2-RT1);\n      WW1 = WW1-WW2;\n    } else if (n == 3) {\n      F2 = (F1+F1+F1-E)/(X+X);\n      if (X < 20.f) {\n        RT1 = ((((((-2.43270989903742E-06f*X+3.57901398988359E-04f)*X -\n                    2.34112415981143E-02f)*X+7.81425144913975E-01f)*X -\n                1.73209218219175E+01f)*X+2.43517435690398E+02f)*X +\n            (-1.97611541576986E+04f/X+9.82441363463929E+03f)/X -\n            2.07970687843258E+03f)*E + R13/(X-R13);\n        RT2 = (((((-2.62627010965435E-04f*X+3.49187925428138E-02f)*X -\n                  3.09337618731880E+00f)*X+1.07037141010778E+02f)*X -\n              2.36659637247087E+03f)*X +\n            ((-2.91669113681020E+06f/X +\n              1.41129505262758E+06f)/X-2.91532335433779E+05f)/X +\n            3.35202872835409E+04f)*E + R23/(X-R23);\n        RT3 = ((((( 9.31856404738601E-05f*X-2.87029400759565E-02f)*X -\n                  7.83503697918455E-01f)*X-1.84338896480695E+01f)*X +\n              4.04996712650414E+02f)*X +\n            (-1.89829509315154E+05f/X +\n             5.11498390849158E+04f)/X-6.88145821789955E+03f)*E \n          + R33/(X-R33);\n      } else {\n        RT1 = ((((-4.97561537069643E-04f*X-5.00929599665316E-02f)*X +\n                1.31099142238996E+00f)*X-1.88336409225481E+01f)*X -\n            6.60344754467191E+02f /X+1.64931462413877E+02f)*E \n          + R13/(X-R13);\n        RT2 = ((((-4.48218898474906E-03f*X-5.17373211334924E-01f)*X +\n                1.13691058739678E+01f)*X-1.65426392885291E+02f)*X -\n            6.30909125686731E+03f /X+1.52231757709236E+03f)*E \n          + R23/(X-R23);\n        RT3 = ((((-1.38368602394293E-02f*X-1.77293428863008E+00f)*X +\n                1.73639054044562E+01f)*X-3.57615122086961E+02f)*X -\n            1.45734701095912E+04f /X+2.69831813951849E+03f)*E \n          + R33/(X-R33);\n      }\n      T1 = RT1/(RT1+1.0E+00f);\n      T2 = RT2/(RT2+1.0E+00f);\n      T3 = RT3/(RT3+1.0E+00f);\n      A2 = F2-T1*F1;\n      A1 = F1-T1*WW1;\n      WW3 = (A2-T2*A1)/((T3-T2)*(T3-T1));\n      WW2 = (T3*A1-A2)/((T3-T2)*(T2-T1));\n      WW1 = WW1-WW2-WW3; \n    }\n  } else {\n    WW1 = sqrtf(PIE4/X);\n    if (n == 1)\n      RT1 = 0.5E+00f/(X-0.5E+00f);\n    else if (n == 2) {\n      if (X < 40.f) {\n        E = expf(-X);\n        RT1 = (-8.78947307498880E-01f*X+1.09243702330261E+01f)*E \n          + R12/(X-R12);\n        RT2 = (-9.28903924275977E+00f*X+8.10642367843811E+01f)*E \n          + R22/(X-R22);\n        WW2 = ( 4.46857389308400E+00f*X-7.79250653461045E+01f)*E + W22*WW1;\n        WW1 = WW1-WW2;\n      } else {\n        RT1 = R12/(X-R12);\n        RT2 = R22/(X-R22);\n        WW2 = W22*WW1;\n        WW1 = WW1-WW2;\n      }\n    } else if (n == 3) {\n      if (X < 47.f) {\n        E = expf(-X);\n        RT1 = ((-7.39058467995275E+00f*X+3.21318352526305E+02f)*X -\n            3.99433696473658E+03f)*E + R13/(X-R13);\n        RT2 = ((-7.38726243906513E+01f*X+3.13569966333873E+03f)*X -\n            3.86862867311321E+04f)*E + R23/(X-R23);\n        RT3 = ((-2.63750565461336E+02f*X+1.04412168692352E+04f)*X -\n            1.28094577915394E+05f)*E + R33/(X-R33);\n        WW3 = ((( 1.52258947224714E-01f*X-8.30661900042651E+00f)*X +\n              1.92977367967984E+02f)*X-1.67787926005344E+03f)*E \n          + W33*WW1;\n        WW2 = (( 6.15072615497811E+01f*X-2.91980647450269E+03f)*X +\n            3.80794303087338E+04f)*E + W23*WW1;\n        WW1 = WW1-WW2-WW3;\n      } else {\n        RT1 = R13/(X-R13);\n        RT2 = R23/(X-R23);\n        RT3 = R33/(X-R33);\n        WW2 = W23*WW1;\n        WW3 = W33*WW1;\n        WW1 = WW1-WW2-WW3;\n      }\n    }\n  }\n  roots[0] = RT1;\n  weights[0] = WW1;\n  if (n > 1){\n    roots[1] = RT2;\n    weights[1] = WW2;\n  }\n  if (n > 2) {\n    roots[2] = RT3;\n    weights[2] = WW3;\n  }\n  return;\n}\n\n__device__ void cuda_Root4(float X, float roots[], float weights[]){\n  float R14,PIE4,R24,W24,R34,W34,R44,W44;\n  float RT1=0,RT2=0,RT3=0,RT4=0,WW1=0,WW2=0,WW3=0,WW4=0;\n  float Y,E;\n\n  R14 = 1.45303521503316E-01f;\n  PIE4 = 7.85398163397448E-01f;\n  R24 = 1.33909728812636E+00f;\n  W24 = 2.34479815323517E-01f;\n  R34 = 3.92696350135829E+00f;\n  W34 = 1.92704402415764E-02f;\n  R44 = 8.58863568901199E+00f;\n  W44 = 2.25229076750736E-04f;\n\n  if (X <= 3.0E-7f) {\n    RT1 = 3.48198973061471E-02f -4.09645850660395E-03f *X;\n    RT2 = 3.81567185080042E-01f -4.48902570656719E-02f *X;\n    RT3 = 1.73730726945891E+00f -2.04389090547327E-01f *X;\n    RT4 = 1.18463056481549E+01f -1.39368301742312E+00f *X;\n    WW1 = 3.62683783378362E-01f -3.13844305713928E-02f *X;\n    WW2 = 3.13706645877886E-01f -8.98046242557724E-02f *X;\n    WW3 = 2.22381034453372E-01f -1.29314370958973E-01f *X;\n    WW4 = 1.01228536290376E-01f -8.28299075414321E-02f *X;\n  } else if (X <= 1.f) {\n    RT1 = ((((((-1.95309614628539E-10f*X+5.19765728707592E-09f)*X-\n                1.01756452250573E-07f )*X+1.72365935872131E-06f )*X-\n            2.61203523522184E-05f )*X+3.52921308769880E-04f )*X-\n        4.09645850658433E-03f )*X+3.48198973061469E-02f;\n    RT2 = (((((-1.89554881382342E-08f*X+3.07583114342365E-07f)*X+\n              1.270981734393E-06f)*X-1.417298563884E-04f)*X+\n          3.226979163176E-03f)*X-4.48902570678178E-02f )*X+\n      3.81567185080039E-01f;\n    RT3 = (((((( 1.77280535300416E-09f*X+3.36524958870615E-08f)*X-\n                2.58341529013893E-07f )*X-1.13644895662320E-05f )*X-\n            7.91549618884063E-05f )*X+1.03825827346828E-02f )*X-\n        2.04389090525137E-01f )*X+1.73730726945889E+00f;\n    RT4 = (((((-5.61188882415248E-08f*X-2.49480733072460E-07f)*X+\n              3.428685057114E-06f)*X+1.679007454539E-04f)*X+\n          4.722855585715E-02f)*X-1.39368301737828E+00f )*X+\n      1.18463056481543E+01f;\n    WW1 = ((((((-1.14649303201279E-08f*X+1.88015570196787E-07f)*X-\n                2.33305875372323E-06f )*X+2.68880044371597E-05f )*X-\n            2.94268428977387E-04f )*X+3.06548909776613E-03f )*X-\n        3.13844305680096E-02f )*X+3.62683783378335E-01f;\n    WW2 = ((((((((-4.11720483772634E-09f*X+6.54963481852134E-08f)*X-\n                    7.20045285129626E-07f )*X+6.93779646721723E-06f )*X-\n                6.05367572016373E-05f )*X+4.74241566251899E-04f )*X-\n            3.26956188125316E-03f )*X+1.91883866626681E-02f )*X-\n        8.98046242565811E-02f )*X+3.13706645877886E-01f;\n    WW3 = ((((((((-3.41688436990215E-08f*X+5.07238960340773E-07f)*X-\n                    5.01675628408220E-06f )*X+4.20363420922845E-05f )*X-\n                3.08040221166823E-04f )*X+1.94431864731239E-03f )*X-\n            1.02477820460278E-02f )*X+4.28670143840073E-02f )*X-\n        1.29314370962569E-01f )*X+2.22381034453369E-01f;\n    WW4 = ((((((((( 4.99660550769508E-09f*X-7.94585963310120E-08f)*X+\n                      8.359072409485E-07f)*X-7.422369210610E-06f)*X+\n                  5.763374308160E-05f)*X-3.86645606718233E-04f )*X+\n              2.18417516259781E-03f )*X-9.99791027771119E-03f )*X+\n          3.48791097377370E-02f )*X-8.28299075413889E-02f )*X+\n      1.01228536290376E-01f;\n  } else if (X <= 5.f) {\n    Y = X-3.0E+00f;\n    RT1 = (((((((((-1.48570633747284E-15f*Y-1.33273068108777E-13f)*Y+\n                      4.068543696670E-12f)*Y-9.163164161821E-11f)*Y+\n                  2.046819017845E-09f)*Y-4.03076426299031E-08f )*Y+\n              7.29407420660149E-07f )*Y-1.23118059980833E-05f )*Y+\n          1.88796581246938E-04f )*Y-2.53262912046853E-03f )*Y+\n      2.51198234505021E-02f;\n    RT2 = ((((((((( 1.35830583483312E-13f*Y-2.29772605964836E-12f)*Y-\n                      3.821500128045E-12f)*Y+6.844424214735E-10f)*Y-\n                  1.048063352259E-08f)*Y+1.50083186233363E-08f )*Y+\n              3.48848942324454E-06f )*Y-1.08694174399193E-04f )*Y+\n          2.08048885251999E-03f )*Y-2.91205805373793E-02f )*Y+\n      2.72276489515713E-01f;\n    RT3 = ((((((((( 5.02799392850289E-13f*Y+1.07461812944084E-11f)*Y-\n                      1.482277886411E-10f)*Y-2.153585661215E-09f)*Y+\n                  3.654087802817E-08f)*Y+5.15929575830120E-07f )*Y-\n              9.52388379435709E-06f )*Y-2.16552440036426E-04f )*Y+\n          9.03551469568320E-03f )*Y-1.45505469175613E-01f )*Y+\n      1.21449092319186E+00f;\n    RT4 = (((((((((-1.08510370291979E-12f*Y+6.41492397277798E-11f)*Y+\n                      7.542387436125E-10f)*Y-2.213111836647E-09f)*Y-\n                  1.448228963549E-07f)*Y-1.95670833237101E-06f )*Y-\n              1.07481314670844E-05f )*Y+1.49335941252765E-04f )*Y+\n          4.87791531990593E-02f )*Y-1.10559909038653E+00f )*Y+\n      8.09502028611780E+00f;\n    WW1 = ((((((((((-4.65801912689961E-14f*Y+7.58669507106800E-13f)*Y-\n                        1.186387548048E-11f)*Y+1.862334710665E-10f)*Y-\n                    2.799399389539E-09f)*Y+4.148972684255E-08f)*Y-\n                5.933568079600E-07f)*Y+8.168349266115E-06f)*Y-\n            1.08989176177409E-04f )*Y+1.41357961729531E-03f )*Y-\n        1.87588361833659E-02f )*Y+2.89898651436026E-01f;\n    WW2 = ((((((((((((-1.46345073267549E-14f*Y+2.25644205432182E-13f)*Y-\n                            3.116258693847E-12f)*Y+4.321908756610E-11f)*Y-\n                        5.673270062669E-10f)*Y+7.006295962960E-09f)*Y-\n                    8.120186517000E-08f)*Y+8.775294645770E-07f)*Y-\n                8.77829235749024E-06f )*Y+8.04372147732379E-05f )*Y-\n            6.64149238804153E-04f )*Y+4.81181506827225E-03f )*Y-\n        2.88982669486183E-02f )*Y+1.56247249979288E-01f;\n    WW3 = ((((((((((((( 9.06812118895365E-15f*Y-1.40541322766087E-13f)*\n                              Y+1.919270015269E-12f)*Y-2.605135739010E-11f)*Y+\n                          3.299685839012E-10f)*Y-3.86354139348735E-09f )*Y+\n                      4.16265847927498E-08f )*Y-4.09462835471470E-07f )*Y+\n                  3.64018881086111E-06f )*Y-2.88665153269386E-05f )*Y+\n              2.00515819789028E-04f )*Y-1.18791896897934E-03f )*Y+\n          5.75223633388589E-03f )*Y-2.09400418772687E-02f )*Y+\n      4.85368861938873E-02f;\n    WW4 = ((((((((((((((-9.74835552342257E-16f*Y+1.57857099317175E-14f)*\n                                Y-2.249993780112E-13f)*Y+3.173422008953E-12f)*Y-\n                            4.161159459680E-11f)*Y+5.021343560166E-10f)*Y-\n                        5.545047534808E-09f)*Y+5.554146993491E-08f)*Y-\n                    4.99048696190133E-07f )*Y+3.96650392371311E-06f )*Y-\n                2.73816413291214E-05f )*Y+1.60106988333186E-04f )*Y-\n            7.64560567879592E-04f )*Y+2.81330044426892E-03f )*Y-\n        7.16227030134947E-03f )*Y+9.66077262223353E-03f;\n  } else if (X <= 10.f) {\n    Y = X-7.5E+00f;\n    RT1 = ((((((((( 4.64217329776215E-15f*Y-6.27892383644164E-15f)*Y+\n                      3.462236347446E-13f)*Y-2.927229355350E-11f)*Y+\n                  5.090355371676E-10f)*Y-9.97272656345253E-09f )*Y+\n              2.37835295639281E-07f )*Y-4.60301761310921E-06f )*Y+\n          8.42824204233222E-05f )*Y-1.37983082233081E-03f )*Y+\n      1.66630865869375E-02f;\n    RT2 = ((((((((( 2.93981127919047E-14f*Y+8.47635639065744E-13f)*Y-\n                      1.446314544774E-11f)*Y-6.149155555753E-12f)*Y+\n                  8.484275604612E-10f)*Y-6.10898827887652E-08f )*Y+\n              2.39156093611106E-06f )*Y-5.35837089462592E-05f )*Y+\n          1.00967602595557E-03f )*Y-1.57769317127372E-02f )*Y+\n      1.74853819464285E-01f;\n    RT3 = (((((((((( 2.93523563363000E-14f*Y-6.40041776667020E-14f)*Y-\n                        2.695740446312E-12f)*Y+1.027082960169E-10f)*Y-\n                    5.822038656780E-10f)*Y-3.159991002539E-08f)*Y+\n                4.327249251331E-07f)*Y+4.856768455119E-06f)*Y-\n            2.54617989427762E-04f )*Y+5.54843378106589E-03f )*Y-\n        7.95013029486684E-02f )*Y+7.20206142703162E-01f;\n    RT4 = (((((((((((-1.62212382394553E-14f*Y+7.68943641360593E-13f)*Y+\n                          5.764015756615E-12f)*Y-1.380635298784E-10f)*Y-\n                      1.476849808675E-09f)*Y+1.84347052385605E-08f )*Y+\n                  3.34382940759405E-07f )*Y-1.39428366421645E-06f )*Y-\n              7.50249313713996E-05f )*Y-6.26495899187507E-04f )*Y+\n          4.69716410901162E-02f )*Y-6.66871297428209E-01f )*Y+\n      4.11207530217806E+00f;\n    WW1 = ((((((((((-1.65995045235997E-15f*Y+6.91838935879598E-14f)*Y-\n                        9.131223418888E-13f)*Y+1.403341829454E-11f)*Y-\n                    3.672235069444E-10f)*Y+6.366962546990E-09f)*Y-\n                1.039220021671E-07f)*Y+1.959098751715E-06f)*Y-\n            3.33474893152939E-05f )*Y+5.72164211151013E-04f )*Y-\n        1.05583210553392E-02f )*Y+2.26696066029591E-01f;\n    WW2 = ((((((((((((-3.57248951192047E-16f*Y+6.25708409149331E-15f)*Y-\n                            9.657033089714E-14f)*Y+1.507864898748E-12f)*Y-\n                        2.332522256110E-11f)*Y+3.428545616603E-10f)*Y-\n                    4.698730937661E-09f)*Y+6.219977635130E-08f)*Y-\n                7.83008889613661E-07f )*Y+9.08621687041567E-06f )*Y-\n            9.86368311253873E-05f )*Y+9.69632496710088E-04f )*Y-\n        8.14594214284187E-03f )*Y+8.50218447733457E-02f;\n    WW3 = ((((((((((((( 1.64742458534277E-16f*Y-2.68512265928410E-15f)*\n                              Y+3.788890667676E-14f)*Y-5.508918529823E-13f)*Y+\n                          7.555896810069E-12f)*Y-9.69039768312637E-11f )*Y+\n                      1.16034263529672E-09f )*Y-1.28771698573873E-08f )*Y+\n                  1.31949431805798E-07f )*Y-1.23673915616005E-06f )*Y+\n              1.04189803544936E-05f )*Y-7.79566003744742E-05f )*Y+\n          5.03162624754434E-04f )*Y-2.55138844587555E-03f )*Y+\n      1.13250730954014E-02f;\n    WW4 = ((((((((((((((-1.55714130075679E-17f*Y+2.57193722698891E-16f)*\n                                Y-3.626606654097E-15f)*Y+5.234734676175E-14f)*Y-\n                            7.067105402134E-13f)*Y+8.793512664890E-12f)*Y-\n                        1.006088923498E-10f)*Y+1.050565098393E-09f)*Y-\n                    9.91517881772662E-09f )*Y+8.35835975882941E-08f )*Y-\n                6.19785782240693E-07f )*Y+3.95841149373135E-06f )*Y-\n            2.11366761402403E-05f )*Y+9.00474771229507E-05f )*Y-\n        2.78777909813289E-04f )*Y+5.26543779837487E-04f;\n  } else if (X <= 15.f) {\n    Y = X-12.5E+00f;\n    RT1 = ((((((((((( 4.94869622744119E-17f*Y+8.03568805739160E-16f)*Y-\n                          5.599125915431E-15f)*Y-1.378685560217E-13f)*Y+\n                      7.006511663249E-13f)*Y+1.30391406991118E-11f )*Y+\n                  8.06987313467541E-11f )*Y-5.20644072732933E-09f )*Y+\n              7.72794187755457E-08f )*Y-1.61512612564194E-06f )*Y+\n          4.15083811185831E-05f )*Y-7.87855975560199E-04f )*Y+\n      1.14189319050009E-02f;\n    RT2 = ((((((((((( 4.89224285522336E-16f*Y+1.06390248099712E-14f)*Y-\n                          5.446260182933E-14f)*Y-1.613630106295E-12f)*Y+\n                      3.910179118937E-12f)*Y+1.90712434258806E-10f )*Y+\n                  8.78470199094761E-10f )*Y-5.97332993206797E-08f )*Y+\n              9.25750831481589E-07f )*Y-2.02362185197088E-05f )*Y+\n          4.92341968336776E-04f )*Y-8.68438439874703E-03f )*Y+\n      1.15825965127958E-01f;\n    RT3 = (((((((((( 6.12419396208408E-14f*Y+1.12328861406073E-13f)*Y-\n                        9.051094103059E-12f)*Y-4.781797525341E-11f)*Y+\n                    1.660828868694E-09f)*Y+4.499058798868E-10f)*Y-\n                2.519549641933E-07f)*Y+4.977444040180E-06f)*Y-\n            1.25858350034589E-04f )*Y+2.70279176970044E-03f )*Y-\n        3.99327850801083E-02f )*Y+4.33467200855434E-01f;\n    RT4 = ((((((((((( 4.63414725924048E-14f*Y-4.72757262693062E-14f)*Y-\n                          1.001926833832E-11f)*Y+6.074107718414E-11f)*Y+\n                      1.576976911942E-09f)*Y-2.01186401974027E-08f )*Y-\n                  1.84530195217118E-07f )*Y+5.02333087806827E-06f )*Y+\n              9.66961790843006E-06f )*Y-1.58522208889528E-03f )*Y+\n          2.80539673938339E-02f )*Y-2.78953904330072E-01f )*Y+\n      1.82835655238235E+00f;\n    WW4 = ((((((((((((( 2.90401781000996E-18f*Y-4.63389683098251E-17f)*\n                              Y+6.274018198326E-16f)*Y-8.936002188168E-15f)*Y+\n                          1.194719074934E-13f)*Y-1.45501321259466E-12f )*Y+\n                      1.64090830181013E-11f )*Y-1.71987745310181E-10f )*Y+\n                  1.63738403295718E-09f )*Y-1.39237504892842E-08f )*Y+\n              1.06527318142151E-07f )*Y-7.27634957230524E-07f )*Y+\n          4.12159381310339E-06f )*Y-1.74648169719173E-05f )*Y+\n      8.50290130067818E-05f;\n    WW3 = ((((((((((((-4.19569145459480E-17f*Y+5.94344180261644E-16f)*Y-\n                            1.148797566469E-14f)*Y+1.881303962576E-13f)*Y-\n                        2.413554618391E-12f)*Y+3.372127423047E-11f)*Y-\n                    4.933988617784E-10f)*Y+6.116545396281E-09f)*Y-\n                6.69965691739299E-08f )*Y+7.52380085447161E-07f )*Y-\n            8.08708393262321E-06f )*Y+6.88603417296672E-05f )*Y-\n        4.67067112993427E-04f )*Y+5.42313365864597E-03f;\n    WW2 = ((((((((((-6.22272689880615E-15f*Y+1.04126809657554E-13f)*Y-\n                        6.842418230913E-13f)*Y+1.576841731919E-11f)*Y-\n                    4.203948834175E-10f)*Y+6.287255934781E-09f)*Y-\n                8.307159819228E-08f)*Y+1.356478091922E-06f)*Y-\n            2.08065576105639E-05f )*Y+2.52396730332340E-04f )*Y-\n        2.94484050194539E-03f )*Y+6.01396183129168E-02f;\n    WW1 = (((-1.8784686463512E-01f/X+2.2991849164985E-01f)/X -\n          4.9893752514047E-01f)/X-2.1916512131607E-05f)*expf(-X) +\n      sqrtf(PIE4/X)-WW4-WW3-WW2;\n  } else if (X <= 20.f) {\n    WW1 = sqrtf(PIE4/X);\n    Y = X-17.5E+00f;\n    RT1 = ((((((((((( 4.36701759531398E-17f*Y-1.12860600219889E-16f)*Y-\n                          6.149849164164E-15f)*Y+5.820231579541E-14f)*Y+\n                      4.396602872143E-13f)*Y-1.24330365320172E-11f )*Y+\n                  6.71083474044549E-11f )*Y+2.43865205376067E-10f )*Y+\n              1.67559587099969E-08f )*Y-9.32738632357572E-07f )*Y+\n          2.39030487004977E-05f )*Y-4.68648206591515E-04f )*Y+\n      8.34977776583956E-03f;\n    RT2 = ((((((((((( 4.98913142288158E-16f*Y-2.60732537093612E-16f)*Y-\n                          7.775156445127E-14f)*Y+5.766105220086E-13f)*Y+\n                      6.432696729600E-12f)*Y-1.39571683725792E-10f )*Y+\n                  5.95451479522191E-10f )*Y+2.42471442836205E-09f )*Y+\n              2.47485710143120E-07f )*Y-1.14710398652091E-05f )*Y+\n          2.71252453754519E-04f )*Y-4.96812745851408E-03f )*Y+\n      8.26020602026780E-02f;\n    RT3 = ((((((((((( 1.91498302509009E-15f*Y+1.48840394311115E-14f)*Y-\n                          4.316925145767E-13f)*Y+1.186495793471E-12f)*Y+\n                      4.615806713055E-11f)*Y-5.54336148667141E-10f )*Y+\n                  3.48789978951367E-10f )*Y-2.79188977451042E-09f )*Y+\n              2.09563208958551E-06f )*Y-6.76512715080324E-05f )*Y+\n          1.32129867629062E-03f )*Y-2.05062147771513E-02f )*Y+\n      2.88068671894324E-01f;\n    RT4 = (((((((((((-5.43697691672942E-15f*Y-1.12483395714468E-13f)*Y+\n                          2.826607936174E-12f)*Y-1.266734493280E-11f)*Y-\n                      4.258722866437E-10f)*Y+9.45486578503261E-09f )*Y-\n                  5.86635622821309E-08f )*Y-1.28835028104639E-06f )*Y+\n              4.41413815691885E-05f )*Y-7.61738385590776E-04f )*Y+\n          9.66090902985550E-03f )*Y-1.01410568057649E-01f )*Y+\n      9.54714798156712E-01f;\n    WW4 = ((((((((((((-7.56882223582704E-19f*Y+7.53541779268175E-18f)*Y-\n                            1.157318032236E-16f)*Y+2.411195002314E-15f)*Y-\n                        3.601794386996E-14f)*Y+4.082150659615E-13f)*Y-\n                    4.289542980767E-12f)*Y+5.086829642731E-11f)*Y-\n                6.35435561050807E-10f )*Y+6.82309323251123E-09f )*Y-\n            5.63374555753167E-08f )*Y+3.57005361100431E-07f )*Y-\n        2.40050045173721E-06f )*Y+4.94171300536397E-05f;\n    WW3 = (((((((((((-5.54451040921657E-17f*Y+2.68748367250999E-16f)*Y+\n                          1.349020069254E-14f)*Y-2.507452792892E-13f)*Y+\n                      1.944339743818E-12f)*Y-1.29816917658823E-11f )*Y+\n                  3.49977768819641E-10f )*Y-8.67270669346398E-09f )*Y+\n              1.31381116840118E-07f )*Y-1.36790720600822E-06f )*Y+\n          1.19210697673160E-05f )*Y-1.42181943986587E-04f )*Y+\n      4.12615396191829E-03f;\n    WW2 = (((((((((((-1.86506057729700E-16f*Y+1.16661114435809E-15f)*Y+\n                          2.563712856363E-14f)*Y-4.498350984631E-13f)*Y+\n                      1.765194089338E-12f)*Y+9.04483676345625E-12f )*Y+\n                  4.98930345609785E-10f )*Y-2.11964170928181E-08f )*Y+\n              3.98295476005614E-07f )*Y-5.49390160829409E-06f )*Y+\n          7.74065155353262E-05f )*Y-1.48201933009105E-03f )*Y+\n      4.97836392625268E-02f;\n    WW1 = (( 1.9623264149430E-01f/X-4.9695241464490E-01f)/X -\n        6.0156581186481E-05f)*expf(-X)+WW1-WW2-WW3-WW4;\n  } else if (X <= 35.f) {\n    WW1 = sqrtf(PIE4/X);\n    E = expf(-X);\n    RT1 = ((((((-4.45711399441838E-05f*X+1.27267770241379E-03f)*X -\n                2.36954961381262E-01f)*X+1.54330657903756E+01f)*X -\n            5.22799159267808E+02f)*X+1.05951216669313E+04f)*X +\n        (-2.51177235556236E+06f/X+8.72975373557709E+05f)/X -\n        1.29194382386499E+05f)*E + R14/(X-R14);\n    RT2 = (((((-7.85617372254488E-02f*X+6.35653573484868E+00f)*X -\n              3.38296938763990E+02f)*X+1.25120495802096E+04f)*X -\n          3.16847570511637E+05f)*X +\n        ((-1.02427466127427E+09f/X +\n          3.70104713293016E+08f)/X-5.87119005093822E+07f)/X +\n        5.38614211391604E+06f)*E + R24/(X-R24);\n    RT3 = (((((-2.37900485051067E-01f*X+1.84122184400896E+01f)*X -\n              1.00200731304146E+03f)*X+3.75151841595736E+04f)*X -\n          9.50626663390130E+05f)*X +\n        ((-2.88139014651985E+09f/X +\n          1.06625915044526E+09f)/X-1.72465289687396E+08f)/X +\n        1.60419390230055E+07f)*E + R34/(X-R34);\n    RT4 = ((((((-6.00691586407385E-04f*X-3.64479545338439E-01f)*X +\n                1.57496131755179E+01f)*X-6.54944248734901E+02f)*X +\n            1.70830039597097E+04f)*X-2.90517939780207E+05f)*X +\n        (3.49059698304732E+07f/X-1.64944522586065E+07f)/X +\n        2.96817940164703E+06f)*E + R44/(X-R44);\n    if (X <= 25.f) \n      WW4 = ((((((( 2.33766206773151E-07f*X-\n                      3.81542906607063E-05f)*X +3.51416601267000E-03f)*X-\n                  1.66538571864728E-01f)*X +4.80006136831847E+00f)*X-\n              8.73165934223603E+01f)*X +9.77683627474638E+02f)*X +\n          1.66000945117640E+04f/X -6.14479071209961E+03f)*E + W44*WW1;\n    else\n      WW4 = (((((( 5.74245945342286E-06f*X-\n                    7.58735928102351E-05f)*X +2.35072857922892E-04f)*X-\n                3.78812134013125E-03f)*X +3.09871652785805E-01f)*X-\n            7.11108633061306E+00f)*X +5.55297573149528E+01f)*E + W44*WW1;\n    WW3 = (((((( 2.36392855180768E-04f*X-9.16785337967013E-03f)*X +\n                4.62186525041313E-01f)*X-1.96943786006540E+01f)*X +\n            4.99169195295559E+02f)*X-6.21419845845090E+03f)*X +\n        ((+5.21445053212414E+07f/X-1.34113464389309E+07f)/X +\n         1.13673298305631E+06f)/X-2.81501182042707E+03f)*E + W34*WW1;\n    WW2 = (((((( 7.29841848989391E-04f*X-3.53899555749875E-02f)*X +\n                2.07797425718513E+00f)*X-1.00464709786287E+02f)*X +\n            3.15206108877819E+03f)*X-6.27054715090012E+04f)*X +\n        (+1.54721246264919E+07f/X-5.26074391316381E+06f)/X +\n        7.67135400969617E+05f)*E + W24*WW1;\n    WW1 = (( 1.9623264149430E-01f/X-4.9695241464490E-01f)/X -\n        6.0156581186481E-05f)*E + WW1-WW2-WW3-WW4;\n  } else if (X <= 53.f) {\n    WW1 = sqrtf(PIE4/X);\n    E = expf(-X)*powf(X,4.f);\n    RT4 = ((-2.19135070169653E-03f*X-1.19108256987623E-01f)*X -\n        7.50238795695573E-01f)*E + R44/(X-R44);\n    RT3 = ((-9.65842534508637E-04f*X-4.49822013469279E-02f)*X +\n        6.08784033347757E-01f)*E + R34/(X-R34);\n    RT2 = ((-3.62569791162153E-04f*X-9.09231717268466E-03f)*X +\n        1.84336760556262E-01f)*E + R24/(X-R24);\n    RT1 = ((-4.07557525914600E-05f*X-6.88846864931685E-04f)*X +\n        1.74725309199384E-02f)*E + R14/(X-R14);\n    WW4 = (( 5.76631982000990E-06f*X-7.89187283804890E-05f)*X +\n        3.28297971853126E-04f)*E + W44*WW1;\n    WW3 = (( 2.08294969857230E-04f*X-3.77489954837361E-03f)*X +\n        2.09857151617436E-02f)*E + W34*WW1;\n    WW2 = (( 6.16374517326469E-04f*X-1.26711744680092E-02f)*X +\n        8.14504890732155E-02f)*E + W24*WW1;\n    WW1 = WW1-WW2-WW3-WW4;\n  } else {\n    WW1 = sqrtf(PIE4/X);\n    RT1 = R14/(X-R14);\n    RT2 = R24/(X-R24);\n    RT3 = R34/(X-R34);\n    RT4 = R44/(X-R44);\n    WW4 = W44*WW1;\n    WW3 = W34*WW1;\n    WW2 = W24*WW1;\n    WW1 = WW1-WW2-WW3-WW4;\n  }\n  roots[0] = RT1;\n  weights[0] = WW1;\n  roots[1] = RT2;\n  weights[1] = WW2;\n  roots[2] = RT3;\n  weights[2] = WW3;\n  roots[3] = RT4;\n  weights[3] = WW4;\n  return;\n}\n\n__device__ void cuda_Root5(float X, float roots[], float weights[]){\n  float R15,PIE4,R25,W25,R35,W35,R45,W45,R55,W55;\n  float RT1=0,RT2=0,RT3=0,RT4=0,RT5=0,\n        WW1=0,WW2=0,WW3=0,WW4=0,WW5=0;\n  float Y,E=0,XXX;\n\n  R15 = 1.17581320211778E-01f;\n  PIE4 = 7.85398163397448E-01f;\n  R25 = 1.07456201243690E+00f;\n  W25 = 2.70967405960535E-01f;\n  R35 = 3.08593744371754E+00f;\n  W35 = 3.82231610015404E-02f;\n  R45 = 6.41472973366203E+00f;\n  W45 = 1.51614186862443E-03f;\n  R55 = 1.18071894899717E+01f;\n  W55 = 8.62130526143657E-06f;\n\n  if (X < 3.e-7f){\n    RT1 = 2.26659266316985E-02f -2.15865967920897E-03f *X;\n    RT2 = 2.31271692140903E-01f -2.20258754389745E-02f *X;\n    RT3 = 8.57346024118836E-01f -8.16520023025515E-02f *X;\n    RT4 = 2.97353038120346E+00f -2.83193369647137E-01f *X;\n    RT5 = 1.84151859759051E+01f -1.75382723579439E+00f *X;\n    WW1 = 2.95524224714752E-01f -1.96867576909777E-02f *X;\n    WW2 = 2.69266719309995E-01f -5.61737590184721E-02f *X;\n    WW3 = 2.19086362515981E-01f -9.71152726793658E-02f *X;\n    WW4 = 1.49451349150580E-01f -1.02979262193565E-01f *X;\n    WW5 = 6.66713443086877E-02f -5.73782817488315E-02f *X;\n  } else if (X < 1.f){\n    RT1 = ((((((-4.46679165328413E-11f*X+1.21879111988031E-09f)*X-\n                2.62975022612104E-08f )*X+5.15106194905897E-07f )*X-\n            9.27933625824749E-06f )*X+1.51794097682482E-04f )*X-\n        2.15865967920301E-03f )*X+2.26659266316985E-02f;\n    RT2 = (((((( 1.93117331714174E-10f*X-4.57267589660699E-09f)*X+\n                2.48339908218932E-08f )*X+1.50716729438474E-06f )*X-\n            6.07268757707381E-05f )*X+1.37506939145643E-03f )*X-\n        2.20258754419939E-02f )*X+2.31271692140905E-01f;\n    RT3 = ((((( 4.84989776180094E-09f*X+1.31538893944284E-07f)*X-\n              2.766753852879E-06f)*X-7.651163510626E-05f)*X+\n          4.033058545972E-03f)*X-8.16520022916145E-02f )*X+\n      8.57346024118779E-01f;\n    RT4 = ((((-2.48581772214623E-07f*X-4.34482635782585E-06f)*X-\n            7.46018257987630E-07f )*X+1.01210776517279E-02f )*X-\n        2.83193369640005E-01f )*X+2.97353038120345E+00f;\n    RT5 = (((((-8.92432153868554E-09f*X+1.77288899268988E-08f)*X+\n              3.040754680666E-06f)*X+1.058229325071E-04f)*X+\n          4.596379534985E-02f)*X-1.75382723579114E+00f )*X+\n      1.84151859759049E+01f;\n    WW1 = ((((((-2.03822632771791E-09f*X+3.89110229133810E-08f)*X-\n                5.84914787904823E-07f )*X+8.30316168666696E-06f )*X-\n            1.13218402310546E-04f )*X+1.49128888586790E-03f )*X-\n        1.96867576904816E-02f )*X+2.95524224714749E-01f;\n    WW2 = ((((((( 8.62848118397570E-09f*X-1.38975551148989E-07f)*X+\n                  1.602894068228E-06f)*X-1.646364300836E-05f)*X+\n              1.538445806778E-04f)*X-1.28848868034502E-03f )*X+\n          9.38866933338584E-03f )*X-5.61737590178812E-02f )*X+\n      2.69266719309991E-01f;\n    WW3 = ((((((((-9.41953204205665E-09f*X+1.47452251067755E-07f)*X-\n                    1.57456991199322E-06f )*X+1.45098401798393E-05f )*X-\n                1.18858834181513E-04f )*X+8.53697675984210E-04f )*X-\n            5.22877807397165E-03f )*X+2.60854524809786E-02f )*X-\n        9.71152726809059E-02f )*X+2.19086362515979E-01f;\n    WW4 = ((((((((-3.84961617022042E-08f*X+5.66595396544470E-07f)*X-\n                    5.52351805403748E-06f )*X+4.53160377546073E-05f )*X-\n                3.22542784865557E-04f )*X+1.95682017370967E-03f )*X-\n            9.77232537679229E-03f )*X+3.79455945268632E-02f )*X-\n        1.02979262192227E-01f )*X+1.49451349150573E-01f;\n    WW5 = ((((((((( 4.09594812521430E-09f*X-6.47097874264417E-08f)*X+\n                      6.743541482689E-07f)*X-5.917993920224E-06f)*X+\n                  4.531969237381E-05f)*X-2.99102856679638E-04f )*X+\n              1.65695765202643E-03f )*X-7.40671222520653E-03f )*X+\n          2.50889946832192E-02f )*X-5.73782817487958E-02f )*X+\n      6.66713443086877E-02f;\n  } else if (X < 5.f) {\n    Y = X-3.0E+00f;\n    RT1 = ((((((((-2.58163897135138E-14f*Y+8.14127461488273E-13f)*Y-\n                    2.11414838976129E-11f )*Y+5.09822003260014E-10f )*Y-\n                1.16002134438663E-08f )*Y+2.46810694414540E-07f )*Y-\n            4.92556826124502E-06f )*Y+9.02580687971053E-05f )*Y-\n        1.45190025120726E-03f )*Y+1.73416786387475E-02f;\n    RT2 = ((((((((( 1.04525287289788E-14f*Y+5.44611782010773E-14f)*Y-\n                      4.831059411392E-12f)*Y+1.136643908832E-10f)*Y-\n                  1.104373076913E-09f)*Y-2.35346740649916E-08f )*Y+\n              1.43772622028764E-06f )*Y-4.23405023015273E-05f )*Y+\n          9.12034574793379E-04f )*Y-1.52479441718739E-02f )*Y+\n      1.76055265928744E-01f;\n    RT3 = (((((((((-6.89693150857911E-14f*Y+5.92064260918861E-13f)*Y+\n                      1.847170956043E-11f)*Y-3.390752744265E-10f)*Y-\n                  2.995532064116E-09f)*Y+1.57456141058535E-07f )*Y-\n              3.95859409711346E-07f )*Y-9.58924580919747E-05f )*Y+\n          3.23551502557785E-03f )*Y-5.97587007636479E-02f )*Y+\n      6.46432853383057E-01f;\n    RT4 = ((((((((-3.61293809667763E-12f*Y-2.70803518291085E-11f)*Y+\n                    8.83758848468769E-10f )*Y+1.59166632851267E-08f )*Y-\n                1.32581997983422E-07f )*Y-7.60223407443995E-06f )*Y-\n            7.41019244900952E-05f )*Y+9.81432631743423E-03f )*Y-\n        2.23055570487771E-01f )*Y+2.21460798080643E+00f;\n    RT5 = ((((((((( 7.12332088345321E-13f*Y+3.16578501501894E-12f)*Y-\n                      8.776668218053E-11f)*Y-2.342817613343E-09f)*Y-\n                  3.496962018025E-08f)*Y-3.03172870136802E-07f )*Y+\n              1.50511293969805E-06f )*Y+1.37704919387696E-04f )*Y+\n          4.70723869619745E-02f )*Y-1.47486623003693E+00f )*Y+\n      1.35704792175847E+01f;\n    WW1 = ((((((((( 1.04348658616398E-13f*Y-1.94147461891055E-12f)*Y+\n                      3.485512360993E-11f)*Y-6.277497362235E-10f)*Y+\n                  1.100758247388E-08f)*Y-1.88329804969573E-07f )*Y+\n              3.12338120839468E-06f )*Y-5.04404167403568E-05f )*Y+\n          8.00338056610995E-04f )*Y-1.30892406559521E-02f )*Y+\n      2.47383140241103E-01f;\n    WW2 = ((((((((((( 3.23496149760478E-14f*Y-5.24314473469311E-13f)*Y+\n                          7.743219385056E-12f)*Y-1.146022750992E-10f)*Y+\n                      1.615238462197E-09f)*Y-2.15479017572233E-08f )*Y+\n                  2.70933462557631E-07f )*Y-3.18750295288531E-06f )*Y+\n              3.47425221210099E-05f )*Y-3.45558237388223E-04f )*Y+\n          3.05779768191621E-03f )*Y-2.29118251223003E-02f )*Y+\n      1.59834227924213E-01f;\n    WW3 = ((((((((((((-3.42790561802876E-14f*Y+5.26475736681542E-13f)*Y-\n                            7.184330797139E-12f)*Y+9.763932908544E-11f)*Y-\n                        1.244014559219E-09f)*Y+1.472744068942E-08f)*Y-\n                    1.611749975234E-07f)*Y+1.616487851917E-06f)*Y-\n                1.46852359124154E-05f )*Y+1.18900349101069E-04f )*Y-\n            8.37562373221756E-04f )*Y+4.93752683045845E-03f )*Y-\n        2.25514728915673E-02f )*Y+6.95211812453929E-02f;\n    WW4 = ((((((((((((( 1.04072340345039E-14f*Y-1.60808044529211E-13f)*\n                              Y+2.183534866798E-12f)*Y-2.939403008391E-11f)*Y+\n                          3.679254029085E-10f)*Y-4.23775673047899E-09f )*Y+\n                      4.46559231067006E-08f )*Y-4.26488836563267E-07f )*Y+\n                  3.64721335274973E-06f )*Y-2.74868382777722E-05f )*Y+\n              1.78586118867488E-04f )*Y-9.68428981886534E-04f )*Y+\n          4.16002324339929E-03f )*Y-1.28290192663141E-02f )*Y+\n      2.22353727685016E-02f;\n    WW5 = ((((((((((((((-8.16770412525963E-16f*Y+1.31376515047977E-14f)*\n                                Y-1.856950818865E-13f)*Y+2.596836515749E-12f)*Y-\n                            3.372639523006E-11f)*Y+4.025371849467E-10f)*Y-\n                        4.389453269417E-09f)*Y+4.332753856271E-08f)*Y-\n                    3.82673275931962E-07f )*Y+2.98006900751543E-06f )*Y-\n                2.00718990300052E-05f )*Y+1.13876001386361E-04f )*Y-\n            5.23627942443563E-04f )*Y+1.83524565118203E-03f )*Y-\n        4.37785737450783E-03f )*Y+5.36963805223095E-03f;\n  } else if (X < 10.f) {\n    Y = X-7.5E+00f;\n    RT1 = ((((((((-1.13825201010775E-14f*Y+1.89737681670375E-13f)*Y-\n                    4.81561201185876E-12f )*Y+1.56666512163407E-10f )*Y-\n                3.73782213255083E-09f )*Y+9.15858355075147E-08f )*Y-\n            2.13775073585629E-06f )*Y+4.56547356365536E-05f )*Y-\n        8.68003909323740E-04f )*Y+1.22703754069176E-02f;\n    RT2 = (((((((((-3.67160504428358E-15f*Y+1.27876280158297E-14f)*Y-\n                      1.296476623788E-12f)*Y+1.477175434354E-11f)*Y+\n                  5.464102147892E-10f)*Y-2.42538340602723E-08f )*Y+\n              8.20460740637617E-07f )*Y-2.20379304598661E-05f )*Y+\n          4.90295372978785E-04f )*Y-9.14294111576119E-03f )*Y+\n      1.22590403403690E-01f;\n    RT3 = ((((((((( 1.39017367502123E-14f*Y-6.96391385426890E-13f)*Y+\n                      1.176946020731E-12f)*Y+1.725627235645E-10f)*Y-\n                  3.686383856300E-09f)*Y+2.87495324207095E-08f )*Y+\n              1.71307311000282E-06f )*Y-7.94273603184629E-05f )*Y+\n          2.00938064965897E-03f )*Y-3.63329491677178E-02f )*Y+\n      4.34393683888443E-01f;\n    RT4 = ((((((((((-1.27815158195209E-14f*Y+1.99910415869821E-14f)*Y+\n                        3.753542914426E-12f)*Y-2.708018219579E-11f)*Y-\n                    1.190574776587E-09f)*Y+1.106696436509E-08f)*Y+\n                3.954955671326E-07f)*Y-4.398596059588E-06f)*Y-\n            2.01087998907735E-04f )*Y+7.89092425542937E-03f )*Y-\n        1.42056749162695E-01f )*Y+1.39964149420683E+00f;\n    RT5 = ((((((((((-1.19442341030461E-13f*Y-2.34074833275956E-12f)*Y+\n                        6.861649627426E-12f)*Y+6.082671496226E-10f)*Y+\n                    5.381160105420E-09f)*Y-6.253297138700E-08f)*Y-\n                2.135966835050E-06f)*Y-2.373394341886E-05f)*Y+\n            2.88711171412814E-06f )*Y+4.85221195290753E-02f )*Y-\n        1.04346091985269E+00f )*Y+7.89901551676692E+00f;\n    WW1 = ((((((((( 7.95526040108997E-15f*Y-2.48593096128045E-13f)*Y+\n                      4.761246208720E-12f)*Y-9.535763686605E-11f)*Y+\n                  2.225273630974E-09f)*Y-4.49796778054865E-08f )*Y+\n              9.17812870287386E-07f )*Y-1.86764236490502E-05f )*Y+\n          3.76807779068053E-04f )*Y-8.10456360143408E-03f )*Y+\n      2.01097936411496E-01f;\n    WW2 = ((((((((((( 1.25678686624734E-15f*Y-2.34266248891173E-14f)*Y+\n                          3.973252415832E-13f)*Y-6.830539401049E-12f)*Y+\n                      1.140771033372E-10f)*Y-1.82546185762009E-09f )*Y+\n                  2.77209637550134E-08f )*Y-4.01726946190383E-07f )*Y+\n              5.48227244014763E-06f )*Y-6.95676245982121E-05f )*Y+\n          8.05193921815776E-04f )*Y-8.15528438784469E-03f )*Y+\n      9.71769901268114E-02f;\n    WW3 = ((((((((((((-8.20929494859896E-16f*Y+1.37356038393016E-14f)*Y-\n                            2.022863065220E-13f)*Y+3.058055403795E-12f)*Y-\n                        4.387890955243E-11f)*Y+5.923946274445E-10f)*Y-\n                    7.503659964159E-09f)*Y+8.851599803902E-08f)*Y-\n                9.65561998415038E-07f )*Y+9.60884622778092E-06f )*Y-\n            8.56551787594404E-05f )*Y+6.66057194311179E-04f )*Y-\n        4.17753183902198E-03f )*Y+2.25443826852447E-02f;\n    WW4 = ((((((((((((((-1.08764612488790E-17f*Y+1.85299909689937E-16f)*\n                                Y-2.730195628655E-15f)*Y+4.127368817265E-14f)*Y-\n                            5.881379088074E-13f)*Y+7.805245193391E-12f)*Y-\n                        9.632707991704E-11f)*Y+1.099047050624E-09f)*Y-\n                    1.15042731790748E-08f )*Y+1.09415155268932E-07f )*Y-\n                9.33687124875935E-07f )*Y+7.02338477986218E-06f )*Y-\n            4.53759748787756E-05f )*Y+2.41722511389146E-04f )*Y-\n        9.75935943447037E-04f )*Y+2.57520532789644E-03f;\n    WW5 = ((((((((((((((( 7.28996979748849E-19f*Y-1.26518146195173E-17f)\n                                  *Y+1.886145834486E-16f)*Y-2.876728287383E-15f)*Y+\n                              4.114588668138E-14f)*Y-5.44436631413933E-13f )*Y+\n                          6.64976446790959E-12f )*Y-7.44560069974940E-11f )*Y+\n                      7.57553198166848E-10f )*Y-6.92956101109829E-09f )*Y+\n                  5.62222859033624E-08f )*Y-3.97500114084351E-07f )*Y+\n              2.39039126138140E-06f )*Y-1.18023950002105E-05f )*Y+\n          4.52254031046244E-05f )*Y-1.21113782150370E-04f )*Y+\n      1.75013126731224E-04f;\n  } else if (X < 15.f) {\n    Y = X-12.5E+00f;\n    RT1 = ((((((((((-4.16387977337393E-17f*Y+7.20872997373860E-16f)*Y+\n                        1.395993802064E-14f)*Y+3.660484641252E-14f)*Y-\n                    4.154857548139E-12f)*Y+2.301379846544E-11f)*Y-\n                1.033307012866E-09f)*Y+3.997777641049E-08f)*Y-\n            9.35118186333939E-07f )*Y+2.38589932752937E-05f )*Y-\n        5.35185183652937E-04f )*Y+8.85218988709735E-03f;\n    RT2 = ((((((((((-4.56279214732217E-16f*Y+6.24941647247927E-15f)*Y+\n                        1.737896339191E-13f)*Y+8.964205979517E-14f)*Y-\n                    3.538906780633E-11f)*Y+9.561341254948E-11f)*Y-\n                9.772831891310E-09f)*Y+4.240340194620E-07f)*Y-\n            1.02384302866534E-05f )*Y+2.57987709704822E-04f )*Y-\n        5.54735977651677E-03f )*Y+8.68245143991948E-02f;\n    RT3 = ((((((((((-2.52879337929239E-15f*Y+2.13925810087833E-14f)*Y+\n                        7.884307667104E-13f)*Y-9.023398159510E-13f)*Y-\n                    5.814101544957E-11f)*Y-1.333480437968E-09f)*Y-\n                2.217064940373E-08f)*Y+1.643290788086E-06f)*Y-\n            4.39602147345028E-05f )*Y+1.08648982748911E-03f )*Y-\n        2.13014521653498E-02f )*Y+2.94150684465425E-01f;\n    RT4 = ((((((((((-6.42391438038888E-15f*Y+5.37848223438815E-15f)*Y+\n                        8.960828117859E-13f)*Y+5.214153461337E-11f)*Y-\n                    1.106601744067E-10f)*Y-2.007890743962E-08f)*Y+\n                1.543764346501E-07f)*Y+4.520749076914E-06f)*Y-\n            1.88893338587047E-04f )*Y+4.73264487389288E-03f )*Y-\n        7.91197893350253E-02f )*Y+8.60057928514554E-01f;\n    RT5 = (((((((((((-2.24366166957225E-14f*Y+4.87224967526081E-14f)*Y+\n                          5.587369053655E-12f)*Y-3.045253104617E-12f)*Y-\n                      1.223983883080E-09f)*Y-2.05603889396319E-09f )*Y+\n                  2.58604071603561E-07f )*Y+1.34240904266268E-06f )*Y-\n              5.72877569731162E-05f )*Y-9.56275105032191E-04f )*Y+\n          4.23367010370921E-02f )*Y-5.76800927133412E-01f )*Y+\n      3.87328263873381E+00f;\n    WW1 = ((((((((( 8.98007931950169E-15f*Y+7.25673623859497E-14f)*Y+\n                      5.851494250405E-14f)*Y-4.234204823846E-11f)*Y+\n                  3.911507312679E-10f)*Y-9.65094802088511E-09f )*Y+\n              3.42197444235714E-07f )*Y-7.51821178144509E-06f )*Y+\n          1.94218051498662E-04f )*Y-5.38533819142287E-03f )*Y+\n      1.68122596736809E-01f;\n    WW2 = ((((((((((-1.05490525395105E-15f*Y+1.96855386549388E-14f)*Y-\n                        5.500330153548E-13f)*Y+1.003849567976E-11f)*Y-\n                    1.720997242621E-10f)*Y+3.533277061402E-09f)*Y-\n                6.389171736029E-08f)*Y+1.046236652393E-06f)*Y-\n            1.73148206795827E-05f )*Y+2.57820531617185E-04f )*Y-\n        3.46188265338350E-03f )*Y+7.03302497508176E-02f;\n    WW3 = ((((((((((( 3.60020423754545E-16f*Y-6.24245825017148E-15f)*Y+\n                          9.945311467434E-14f)*Y-1.749051512721E-12f)*Y+\n                      2.768503957853E-11f)*Y-4.08688551136506E-10f )*Y+\n                  6.04189063303610E-09f )*Y-8.23540111024147E-08f )*Y+\n              1.01503783870262E-06f )*Y-1.20490761741576E-05f )*Y+\n          1.26928442448148E-04f )*Y-1.05539461930597E-03f )*Y+\n      1.15543698537013E-02f;\n    WW4 = ((((((((((((( 2.51163533058925E-18f*Y-4.31723745510697E-17f)*\n                              Y+6.557620865832E-16f)*Y-1.016528519495E-14f)*Y+\n                          1.491302084832E-13f)*Y-2.06638666222265E-12f )*Y+\n                      2.67958697789258E-11f )*Y-3.23322654638336E-10f )*Y+\n                  3.63722952167779E-09f )*Y-3.75484943783021E-08f )*Y+\n              3.49164261987184E-07f )*Y-2.92658670674908E-06f )*Y+\n          2.12937256719543E-05f )*Y-1.19434130620929E-04f )*Y+\n      6.45524336158384E-04f;\n    WW5 = ((((((((((((((-1.29043630202811E-19f*Y+2.16234952241296E-18f)*\n                                Y-3.107631557965E-17f)*Y+4.570804313173E-16f)*Y-\n                            6.301348858104E-15f)*Y+8.031304476153E-14f)*Y-\n                        9.446196472547E-13f)*Y+1.018245804339E-11f)*Y-\n                    9.96995451348129E-11f )*Y+8.77489010276305E-10f )*Y-\n                6.84655877575364E-09f )*Y+4.64460857084983E-08f )*Y-\n            2.66924538268397E-07f )*Y+1.24621276265907E-06f )*Y-\n        4.30868944351523E-06f )*Y+9.94307982432868E-06f;\n  } else if (X < 20.f){\n    Y = X-17.5E+00f;\n    RT1 = (((((((((( 1.91875764545740E-16f*Y+7.8357401095707E-16f)*Y-\n                        3.260875931644E-14f)*Y-1.186752035569E-13f)*Y+\n                    4.275180095653E-12f)*Y+3.357056136731E-11f)*Y-\n                1.123776903884E-09f)*Y+1.231203269887E-08f)*Y-\n            3.99851421361031E-07f )*Y+1.45418822817771E-05f )*Y-\n        3.49912254976317E-04f )*Y+6.67768703938812E-03f;\n    RT2 = (((((((((( 2.02778478673555E-15f*Y+1.01640716785099E-14f)*Y-\n                        3.385363492036E-13f)*Y-1.615655871159E-12f)*Y+\n                    4.527419140333E-11f)*Y+3.853670706486E-10f)*Y-\n                1.184607130107E-08f)*Y+1.347873288827E-07f)*Y-\n            4.47788241748377E-06f )*Y+1.54942754358273E-04f )*Y-\n        3.55524254280266E-03f )*Y+6.44912219301603E-02f;\n    RT3 = (((((((((( 7.79850771456444E-15f*Y+6.00464406395001E-14f)*Y-\n                        1.249779730869E-12f)*Y-1.020720636353E-11f)*Y+\n                    1.814709816693E-10f)*Y+1.766397336977E-09f)*Y-\n                4.603559449010E-08f)*Y+5.863956443581E-07f)*Y-\n            2.03797212506691E-05f )*Y+6.31405161185185E-04f )*Y-\n        1.30102750145071E-02f )*Y+2.10244289044705E-01f;\n    RT4 = (((((((((((-2.92397030777912E-15f*Y+1.94152129078465E-14f)*Y+\n                          4.859447665850E-13f)*Y-3.217227223463E-12f)*Y-\n                      7.484522135512E-11f)*Y+7.19101516047753E-10f )*Y+\n                  6.88409355245582E-09f )*Y-1.44374545515769E-07f )*Y+\n              2.74941013315834E-06f )*Y-1.02790452049013E-04f )*Y+\n          2.59924221372643E-03f )*Y-4.35712368303551E-02f )*Y+\n      5.62170709585029E-01f;\n    RT5 = ((((((((((( 1.17976126840060E-14f*Y+1.24156229350669E-13f)*Y-\n                          3.892741622280E-12f)*Y-7.755793199043E-12f)*Y+\n                      9.492190032313E-10f)*Y-4.98680128123353E-09f )*Y-\n                  1.81502268782664E-07f )*Y+2.69463269394888E-06f )*Y+\n              2.50032154421640E-05f )*Y-1.33684303917681E-03f )*Y+\n          2.29121951862538E-02f )*Y-2.45653725061323E-01f )*Y+\n      1.89999883453047E+00f;\n    WW1 = (((((((((( 1.74841995087592E-15f*Y-6.95671892641256E-16f)*Y-\n                        3.000659497257E-13f)*Y+2.021279817961E-13f)*Y+\n                    3.853596935400E-11f)*Y+1.461418533652E-10f)*Y-\n                1.014517563435E-08f)*Y+1.132736008979E-07f)*Y-\n            2.86605475073259E-06f )*Y+1.21958354908768E-04f )*Y-\n        3.86293751153466E-03f )*Y+1.45298342081522E-01f;\n    WW2 = ((((((((((-1.11199320525573E-15f*Y+1.85007587796671E-15f)*Y+\n                        1.220613939709E-13f)*Y+1.275068098526E-12f)*Y-\n                    5.341838883262E-11f)*Y+6.161037256669E-10f)*Y-\n                1.009147879750E-08f)*Y+2.907862965346E-07f)*Y-\n            6.12300038720919E-06f )*Y+1.00104454489518E-04f )*Y-\n        1.80677298502757E-03f )*Y+5.78009914536630E-02f;\n    WW3 = ((((((((((-9.49816486853687E-16f*Y+6.67922080354234E-15f)*Y+\n                        2.606163540537E-15f)*Y+1.983799950150E-12f)*Y-\n                    5.400548574357E-11f)*Y+6.638043374114E-10f)*Y-\n                8.799518866802E-09f)*Y+1.791418482685E-07f)*Y-\n            2.96075397351101E-06f )*Y+3.38028206156144E-05f )*Y-\n        3.58426847857878E-04f )*Y+8.39213709428516E-03f;\n    WW4 = ((((((((((( 1.33829971060180E-17f*Y-3.44841877844140E-16f)*Y+\n                          4.745009557656E-15f)*Y-6.033814209875E-14f)*Y+\n                      1.049256040808E-12f)*Y-1.70859789556117E-11f )*Y+\n                  2.15219425727959E-10f )*Y-2.52746574206884E-09f )*Y+\n              3.27761714422960E-08f )*Y-3.90387662925193E-07f )*Y+\n          3.46340204593870E-06f )*Y-2.43236345136782E-05f )*Y+\n      3.54846978585226E-04f;\n    WW5 = ((((((((((((( 2.69412277020887E-20f*Y-4.24837886165685E-19f)*\n                              Y+6.030500065438E-18f)*Y-9.069722758289E-17f)*Y+\n                          1.246599177672E-15f)*Y-1.56872999797549E-14f )*Y+\n                      1.87305099552692E-13f )*Y-2.09498886675861E-12f )*Y+\n                  2.11630022068394E-11f )*Y-1.92566242323525E-10f )*Y+\n              1.62012436344069E-09f )*Y-1.23621614171556E-08f )*Y+\n          7.72165684563049E-08f )*Y-3.59858901591047E-07f )*Y+\n      2.43682618601000E-06f;\n  } else if (X < 25.f) {\n    Y = X-22.5E+00f;\n    RT1 = (((((((((-1.13927848238726E-15f*Y+7.39404133595713E-15f)*Y+\n                      1.445982921243E-13f)*Y-2.676703245252E-12f)*Y+\n                  5.823521627177E-12f)*Y+2.17264723874381E-10f )*Y+\n              3.56242145897468E-09f )*Y-3.03763737404491E-07f )*Y+\n          9.46859114120901E-06f )*Y-2.30896753853196E-04f )*Y+\n      5.24663913001114E-03f;\n    RT2 = (((((((((( 2.89872355524581E-16f*Y-1.22296292045864E-14f)*Y+\n                        6.184065097200E-14f)*Y+1.649846591230E-12f)*Y-\n                    2.729713905266E-11f)*Y+3.709913790650E-11f)*Y+\n                2.216486288382E-09f)*Y+4.616160236414E-08f)*Y-\n            3.32380270861364E-06f )*Y+9.84635072633776E-05f )*Y-\n        2.30092118015697E-03f )*Y+5.00845183695073E-02f;\n    RT3 = (((((((((( 1.97068646590923E-15f*Y-4.89419270626800E-14f)*Y+\n                        1.136466605916E-13f)*Y+7.546203883874E-12f)*Y-\n                    9.635646767455E-11f)*Y-8.295965491209E-11f)*Y+\n                7.534109114453E-09f)*Y+2.699970652707E-07f)*Y-\n            1.42982334217081E-05f )*Y+3.78290946669264E-04f )*Y-\n        8.03133015084373E-03f )*Y+1.58689469640791E-01f;\n    RT4 = (((((((((( 1.33642069941389E-14f*Y-1.55850612605745E-13f)*Y-\n                        7.522712577474E-13f)*Y+3.209520801187E-11f)*Y-\n                    2.075594313618E-10f)*Y-2.070575894402E-09f)*Y+\n                7.323046997451E-09f)*Y+1.851491550417E-06f)*Y-\n            6.37524802411383E-05f )*Y+1.36795464918785E-03f )*Y-\n        2.42051126993146E-02f )*Y+3.97847167557815E-01f;\n    RT5 = ((((((((((-6.07053986130526E-14f*Y+1.04447493138843E-12f)*Y-\n                        4.286617818951E-13f)*Y-2.632066100073E-10f)*Y+\n                    4.804518986559E-09f)*Y-1.835675889421E-08f)*Y-\n                1.068175391334E-06f)*Y+3.292234974141E-05f)*Y-\n            5.94805357558251E-04f )*Y+8.29382168612791E-03f )*Y-\n        9.93122509049447E-02f )*Y+1.09857804755042E+00f;\n    WW1 = (((((((((-9.10338640266542E-15f*Y+1.00438927627833E-13f)*Y+\n                      7.817349237071E-13f)*Y-2.547619474232E-11f)*Y+\n                  1.479321506529E-10f)*Y+1.52314028857627E-09f )*Y+\n              9.20072040917242E-09f )*Y-2.19427111221848E-06f )*Y+\n          8.65797782880311E-05f )*Y-2.82718629312875E-03f )*Y+\n      1.28718310443295E-01f;\n    WW2 = ((((((((( 5.52380927618760E-15f*Y-6.43424400204124E-14f)*Y-\n                      2.358734508092E-13f)*Y+8.261326648131E-12f)*Y+\n                  9.229645304956E-11f)*Y-5.68108973828949E-09f )*Y+\n              1.22477891136278E-07f )*Y-2.11919643127927E-06f )*Y+\n          4.23605032368922E-05f )*Y-1.14423444576221E-03f )*Y+\n      5.06607252890186E-02f;\n    WW3 = ((((((((( 3.99457454087556E-15f*Y-5.11826702824182E-14f)*Y-\n                      4.157593182747E-14f)*Y+4.214670817758E-12f)*Y+\n                  6.705582751532E-11f)*Y-3.36086411698418E-09f )*Y+\n              6.07453633298986E-08f )*Y-7.40736211041247E-07f )*Y+\n          8.84176371665149E-06f )*Y-1.72559275066834E-04f )*Y+\n      7.16639814253567E-03f;\n    WW4 = (((((((((((-2.14649508112234E-18f*Y-2.45525846412281E-18f)*Y+\n                          6.126212599772E-16f)*Y-8.526651626939E-15f)*Y+\n                      4.826636065733E-14f)*Y-3.39554163649740E-13f )*Y+\n                  1.67070784862985E-11f )*Y-4.42671979311163E-10f )*Y+\n              6.77368055908400E-09f )*Y-7.03520999708859E-08f )*Y+\n          6.04993294708874E-07f )*Y-7.80555094280483E-06f )*Y+\n      2.85954806605017E-04f;\n    WW5 = ((((((((((((-5.63938733073804E-21f*Y+6.92182516324628E-20f)*Y-\n                            1.586937691507E-18f)*Y+3.357639744582E-17f)*Y-\n                        4.810285046442E-16f)*Y+5.386312669975E-15f)*Y-\n                    6.117895297439E-14f)*Y+8.441808227634E-13f)*Y-\n                1.18527596836592E-11f )*Y+1.36296870441445E-10f )*Y-\n            1.17842611094141E-09f )*Y+7.80430641995926E-09f )*Y-\n        5.97767417400540E-08f )*Y+1.65186146094969E-06f;\n  } else if (X < 40.f) {\n    WW1 = sqrtf(PIE4/X);\n    E = expf(-X);\n    RT1 = ((((((((-1.73363958895356E-06f*X+1.19921331441483E-04f)*X -\n                    1.59437614121125E-02f)*X+1.13467897349442E+00f)*X -\n                4.47216460864586E+01f)*X+1.06251216612604E+03f)*X -\n            1.52073917378512E+04f)*X+1.20662887111273E+05f)*X -\n        4.07186366852475E+05f)*E + R15/(X-R15);\n    RT2 = ((((((((-1.60102542621710E-05f*X+1.10331262112395E-03f)*X -\n                    1.50043662589017E-01f)*X+1.05563640866077E+01f)*X -\n                4.10468817024806E+02f)*X+9.62604416506819E+03f)*X -\n            1.35888069838270E+05f)*X+1.06107577038340E+06f)*X -\n        3.51190792816119E+06f)*E + R25/(X-R25);\n    RT3 = ((((((((-4.48880032128422E-05f*X+2.69025112122177E-03f)*X -\n                    4.01048115525954E-01f)*X+2.78360021977405E+01f)*X -\n                1.04891729356965E+03f)*X+2.36985942687423E+04f)*X -\n            3.19504627257548E+05f)*X+2.34879693563358E+06f)*X -\n        7.16341568174085E+06f)*E + R35/(X-R35);\n    RT4 = ((((((((-6.38526371092582E-05f*X-2.29263585792626E-03f)*X -\n                    7.65735935499627E-02f)*X+9.12692349152792E+00f)*X -\n                2.32077034386717E+02f)*X+2.81839578728845E+02f)*X +\n            9.59529683876419E+04f)*X-1.77638956809518E+06f)*X +\n        1.02489759645410E+07f)*E + R45/(X-R45);\n    RT5 = ((((((((-3.59049364231569E-05f*X-2.25963977930044E-02f)*X +\n                    1.12594870794668E+00f)*X-4.56752462103909E+01f)*X +\n                1.05804526830637E+03f)*X-1.16003199605875E+04f)*X -\n            4.07297627297272E+04f)*X+2.22215528319857E+06f)*X -\n        1.61196455032613E+07f)*E + R55/(X-R55);\n    WW5 = (((((((((-4.61100906133970E-10f*X+1.43069932644286E-07f)*X -\n                      1.63960915431080E-05f)*X+1.15791154612838E-03f)*X -\n                  5.30573476742071E-02f)*X+1.61156533367153E+00f)*X -\n              3.23248143316007E+01f)*X+4.12007318109157E+02f)*X -\n          3.02260070158372E+03f)*X+9.71575094154768E+03f)*E + W55*WW1;\n    WW4 = (((((((((-2.40799435809950E-08f*X+8.12621667601546E-06f)*X -\n                      9.04491430884113E-04f)*X+6.37686375770059E-02f)*X -\n                  2.96135703135647E+00f)*X+9.15142356996330E+01f)*X -\n              1.86971865249111E+03f)*X+2.42945528916947E+04f)*X -\n          1.81852473229081E+05f)*X+5.96854758661427E+05f)*E + W45*WW1;\n    WW3 = (((((((( 1.83574464457207E-05f*X-1.54837969489927E-03f)*X +\n                    1.18520453711586E-01f)*X-6.69649981309161E+00f)*X +\n                2.44789386487321E+02f)*X-5.68832664556359E+03f)*X +\n            8.14507604229357E+04f)*X-6.55181056671474E+05f)*X +\n        2.26410896607237E+06f)*E + W35*WW1;\n    WW2 = (((((((( 2.77778345870650E-05f*X-2.22835017655890E-03f)*X +\n                    1.61077633475573E-01f)*X-8.96743743396132E+00f)*X +\n                3.28062687293374E+02f)*X-7.65722701219557E+03f)*X +\n            1.10255055017664E+05f)*X-8.92528122219324E+05f)*X +\n        3.10638627744347E+06f)*E + W25*WW1;\n    WW1 = WW1-0.01962E+00f*E-WW2-WW3-WW4-WW5;\n  } else if (X < 59.f) {\n    WW1 = sqrtf(PIE4/X);\n    XXX = powf(X,3.f);\n    E = XXX*expf(-X);\n    RT1 = (((-2.43758528330205E-02f*X+2.07301567989771E+00f)*X -\n          6.45964225381113E+01f)*X+7.14160088655470E+02f)*E + R15/(X-R15);\n    RT2 = (((-2.28861955413636E-01f*X+1.93190784733691E+01f)*X -\n          5.99774730340912E+02f)*X+6.61844165304871E+03f)*E + R25/(X-R25);\n    RT3 = (((-6.95053039285586E-01f*X+5.76874090316016E+01f)*X -\n          1.77704143225520E+03f)*X+1.95366082947811E+04f)*E + R35/(X-R35);\n    RT4 = (((-1.58072809087018E+00f*X+1.27050801091948E+02f)*X -\n          3.86687350914280E+03f)*X+4.23024828121420E+04f)*E + R45/(X-R45);\n    RT5 = (((-3.33963830405396E+00f*X+2.51830424600204E+02f)*X -\n          7.57728527654961E+03f)*X+8.21966816595690E+04f)*E + R55/(X-R55);\n    E = XXX*E;\n    WW5 = (( 1.35482430510942E-08f*X-3.27722199212781E-07f)*X +\n        2.41522703684296E-06f)*E + W55*WW1;\n    WW4 = (( 1.23464092261605E-06f*X-3.55224564275590E-05f)*X +\n        3.03274662192286E-04f)*E + W45*WW1;\n    WW3 = (( 1.34547929260279E-05f*X-4.19389884772726E-04f)*X +\n        3.87706687610809E-03f)*E + W35*WW1;\n    WW2 = (( 2.09539509123135E-05f*X-6.87646614786982E-04f)*X +\n        6.68743788585688E-03f)*E + W25*WW1;\n    WW1 = WW1-WW2-WW3-WW4-WW5;\n  } else {\n    WW1 = sqrtf(PIE4/X);\n    RT1 = R15/(X-R15);\n    RT2 = R25/(X-R25);\n    RT3 = R35/(X-R35);\n    RT4 = R45/(X-R45);\n    RT5 = R55/(X-R55);\n    WW2 = W25*WW1;\n    WW3 = W35*WW1;\n    WW4 = W45*WW1;\n    WW5 = W55*WW1;\n    WW1 = WW1-WW2-WW3-WW4-WW5;\n  }\n  roots[0] = RT1;\n  weights[0] = WW1;\n  roots[1] = RT2;\n  weights[1] = WW2;\n  roots[2] = RT3;\n  weights[2] = WW3;\n  roots[3] = RT4;\n  weights[3] = WW4;\n  roots[4] = RT5;\n  weights[4] = WW5;\n  return;\n}\n\n__device__ void cuda_Root6(int n,float X, float roots[], float weights[]){\n  // Root6 not implemented yet\n  return;\n}\n\n__device__ float cuda_Int1d(int i, int j, int k, int l,\n    float xi, float xj, float xk, float xl,\n    float alpha_ij_A, float alpha_kl_B, float sqrt_AB,\n    float A, float B, float Px, float Qx,\n    float inv_t1, float B00, float B1, float B1p, \n    float G[][MAXROOTS])\n{\n  // Form G(n,m)=I(n,0,m,0) intermediate values for a Rys polynomial \n  int n = i+j;\n  int m = k+l;\n\n  float xij = xi-xj;\n  float xkl = xk-xl;\n\n  // RecurFactorsGamess\n  float C  = (Px-xi) * inv_t1 + (B*(Qx-xi)+A*(Px-xi))*B00*2.0;\n  float Cp = (Qx-xk) * inv_t1 + (B*(Qx-xk)+A*(Px-xk))*B00*2.0;\n\n  // ABD eq 11. \n  G[0][0] = (float)M_PI * expf(-alpha_ij_A*xij*xij -alpha_kl_B*xkl*xkl) / sqrt_AB;\n\n  if (n > 0) { G[1][0] = C *G[0][0]; } // ABD eq 15 \n  if (m > 0) { G[0][1] = Cp*G[0][0]; } // ABD eq 16 \n\n  for (int a = 2; a < n+1; ++ a) { G[a][0] = B1 *(a-1)*G[a-2][0] + C *G[a-1][0]; } \n  for (int b = 2; b < m+1; ++ b) { G[0][b] = B1p*(b-1)*G[0][b-2] + Cp*G[0][b-1]; } \n\n  if ((m>0) && (n>0)){\n    for (int a=1; a<n+1; ++a){\n      G[a][1] = a*B00*G[a-1][0] + Cp*G[a][0];\n      for (int b=2; b<m+1; ++b)\n        G[a][b] = B1p*(b-1)*G[a][b-2] + a*B00*G[a-1][b-1] + Cp*G[a][b-1];\n    }\n  }\n\n  // Compute and output I(i,j,k,l) from I(i+j,0,k+l,0) (G) \n  float ijkl = 0.0;\n  for (int m=0; m<l+1; ++m){\n    float ijm0 = 0.0;\n    for (int n=0; n<j+1; ++n) // I(i,j,m,0)<-I(n,0,m,0)  \n      ijm0 += cuda_binomial(j,n)*powf(xij,(float)(j-n))*G[n+i][m+k];\n    ijkl += cuda_binomial(l,m)*powf(xkl,(float)(l-m))*ijm0; // I(i,j,k,l)<-I(i,j,m,0) \n  }\n\n  return ijkl;\n}\n\n__device__ void cuda_Roots(int n, float X, float roots[], float weights[]){\n  if (n <= 3)\n    cuda_Root123(n,X, roots,weights);\n  else if (n==4) \n    cuda_Root4(X, roots,weights);\n  else if (n==5)\n    cuda_Root5(X, roots,weights);\n  else\n    cuda_Root6(n,X, roots,weights);\n  return;\n}\n\ninline __host__ __device__ float4 fabs(float4 v)\n{\n    return make_float4(fabs(v.x), fabs(v.y), fabs(v.z), fabs(v.w));\n}\n\n__device__ int cuda_ij2intindex(int i, int j)\n{\n  if (i < j) {\n    int t = i; i = j; j = t;\n  }\n  return i * (i + 1) / 2 + j;\n}\n\n__device__ float cuda_rys_pbf(const double *ptr_i, const double *ptr_j, \n                              const double *ptr_k, const double *ptr_l)\n{\n  // download xyz, lmn, expon, and coef*norm\n  float xa = (float)ptr_i[0];\n  float ya = (float)ptr_i[1];\n  float za = (float)ptr_i[2];\n  int   la = (int)ptr_i[3];\n  int   ma = (int)ptr_i[4];\n  int   na = (int)ptr_i[5];\n  float alphaa = (float)ptr_i[6];\n  float norma  = (float)ptr_i[7];\n\n  float xb = (float)ptr_j[0];\n  float yb = (float)ptr_j[1];\n  float zb = (float)ptr_j[2];\n  int   lb = (int)ptr_j[3];\n  int   mb = (int)ptr_j[4];\n  int   nb = (int)ptr_j[5];\n  float alphab = (float)ptr_j[6];\n  float normb  = (float)ptr_j[7];\n\n  float xc = (float)ptr_k[0];\n  float yc = (float)ptr_k[1];\n  float zc = (float)ptr_k[2];\n  int   lc = (int)ptr_k[3];\n  int   mc = (int)ptr_k[4];\n  int   nc = (int)ptr_k[5];\n  float alphac = (float)ptr_k[6];\n  float normc  = (float)ptr_k[7];\n\n  float xd = (float)ptr_l[0];\n  float yd = (float)ptr_l[1];\n  float zd = (float)ptr_l[2];\n  int   ld = (int)ptr_l[3];\n  int   md = (int)ptr_l[4];\n  int   nd = (int)ptr_l[5];\n  float alphad = (float)ptr_l[6];\n  float normd  = (float)ptr_l[7];\n\n  // calculate primitive integral [ij|kl]\n  int norder,i;\n  float A,B,xp,yp,zp,xq,yq,zq,X,rho,sum,t,Ix,Iy,Iz;\n\n  norder = (la+ma+na+lb+nb+mb+lc+mc+nc+ld+md+nd)/2 + 1;\n  A = alphaa+alphab; \n  B = alphac+alphad;\n\n  xp = (alphaa*xa+alphab*xb)/A;\n  yp = (alphaa*ya+alphab*yb)/A;\n  zp = (alphaa*za+alphab*zb)/A;\n\n  xq = (alphac*xc+alphad*xd)/B;\n  yq = (alphac*yc+alphad*yd)/B;\n  zq = (alphac*zc+alphad*zd)/B;\n\n  rho = A*B/(A+B);\n  X = rho * ((xp-xq)*(xp-xq)+(yp-yq)*(yp-yq)+(zp-zq)*(zp-zq));\n\n  float alpha_ab_A = alphaa * alphab / A;\n  float alpha_cd_B = alphac * alphad / B;\n  float sqrt_AB = sqrtf(A * B);\n\n  float roots[MAXROOTS],weights[MAXROOTS];\n  float G[MAXROOTS][MAXROOTS];\n\n  cuda_Roots(norder,X,roots,weights); // get currect roots/weights\n\n  sum = 0.;\n  for (i=0; i<norder; ++i){\n    t = roots[i];\n\n    float inv_t1, B00, B1, B1p;\n    inv_t1 = 1.f / (1.f + t);\n    B00 = 0.5f * t/(A+B) * inv_t1;\n    B1  = 0.5f / A * inv_t1 + B00;\n    B1p = 0.5f / B * inv_t1 + B00;\n\n    Ix = cuda_Int1d(la,lb,lc,ld, xa,xb,xc,xd,\n        alpha_ab_A,alpha_cd_B,sqrt_AB, A,B,xp,xq, inv_t1,B00,B1,B1p, G);\n    Iy = cuda_Int1d(ma,mb,mc,md, ya,yb,yc,yd,\n        alpha_ab_A,alpha_cd_B,sqrt_AB, A,B,yp,yq, inv_t1,B00,B1,B1p, G);\n    Iz = cuda_Int1d(na,nb,nc,nd, za,zb,zc,zd,\n        alpha_ab_A,alpha_cd_B,sqrt_AB, A,B,zp,zq, inv_t1,B00,B1,B1p, G);\n    sum = sum + Ix*Iy*Iz*weights[i]; /* ABD eq 5 & 9 */\n  }\n\n  // inv_sqrt_pi_2: 2.0*sqrtf(1.0/M_PI) = 1.12837916709551255856\n  return 1.12837916709551255856f * sqrtf(rho)*norma*normb*normc*normd*sum; /* ABD eq 5 & 9 */\n}\n\n__global__ void cuda_mat_J_PI(\n  const double *__restrict pbf_xlec,\n  const int *__restrict pbf_to_cbf,\n  int n_pbf,\n  const double *__restrict mat_D,\n  double *__restrict mat_J_PI,\n  const double *__restrict mat_Q)\n{\n  __shared__ double elem_J_PI[BLOCKSIZE * BLOCKSIZE];\n\n  // each block scans over [ij|??] and sum up to a primitive J matrix element\n  int i = blockIdx.x;\n  int j = blockIdx.y;\n\n  // avoid accessing out of bounds elements and make use of i<=>j symmetry\n  if (i >= n_pbf || j > i) { return; }\n\n  int ij = cuda_ij2intindex(i,j);\n\n  const double *ptr_i = &pbf_xlec[i * 8];\n  const double *ptr_j = &pbf_xlec[j * 8];\n\n  int a = pbf_to_cbf[i];\n  int b = pbf_to_cbf[j];\n  int ab = cuda_ij2intindex(a,b);\n\n  // initialize shared array\n  elem_J_PI[threadIdx.x * BLOCKSIZE + threadIdx.y] = 0.0;\n\n  for (int k = threadIdx.x; k < n_pbf; k += BLOCKSIZE)\n  {\n    int c = pbf_to_cbf[k];\n    const double *ptr_k = &pbf_xlec[k * 8];\n\n    // NOTE: make use of k<=>l symmetry\n    for (int l = threadIdx.y; l <= k; l += BLOCKSIZE)\n    {\n      int d = pbf_to_cbf[l];\n      int cd = cuda_ij2intindex(c,d);\n\n      // Schwartz screening\n      if (fabs(mat_Q[ab] * mat_Q[cd] * mat_D[cd]) < SCREEN_THR) { continue; }\n\n      const double *ptr_l = &pbf_xlec[l * 8];\n\n      // calculate ERI\n      double this_eri = cuda_rys_pbf(ptr_i, ptr_j, ptr_k, ptr_l);\n\n      // NOTE: doubling for off-diagonal elements of D due to k<=>l symmetry\n      elem_J_PI[threadIdx.x *BLOCKSIZE + threadIdx.y] += this_eri * mat_D[cd] * (k == l ? 1.0 : 2.0);\n    }\n  }\n\n  __syncthreads();\n\n  // only update mat_J_PI on one thread of the block\n  if (0 == threadIdx.x && 0 == threadIdx.y)\n  {\n    mat_J_PI[ij] = 0.0; \n    for (int t1 = 0; t1 < BLOCKSIZE; ++ t1) {\n      for (int t2 = 0; t2 < BLOCKSIZE; ++ t2) {\n        mat_J_PI[ij] += elem_J_PI[t1 * BLOCKSIZE + t2];\n      }\n    }\n  }\n}",
            "__device__ int cuda_fact(int n){\n  int result = 1;\n  for (int i = 2; i <= n; i++) result *= i;\n  return result;\n}\n\ninline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\ninline __host__ __device__ float4 expf(float4 v)\n{\n    return make_float4(expf(v.x), expf(v.y), expf(v.z), expf(v.w));\n}\n\n__device__ int cuda_binomial(int a, int b){\n  return cuda_fact(a)/(cuda_fact(b)*cuda_fact(a-b));\n}\n\ninline __host__ __device__ float4 sqrtf(float4 v)\n{\n    return make_float4(sqrtf(v.x), sqrtf(v.y), sqrtf(v.z), sqrtf(v.w));\n}\n\n__device__ void cuda_Root123(int n, float X, float roots[], float weights[]){\n\n  float R12, PIE4, R22, W22, R13, R23, W23, R33, W33;\n  float RT1=0,RT2=0,RT3=0,WW1=0,WW2=0,WW3=0;\n  float F1,F2,E,T1,T2,T3,A1,A2,Y;\n\n  R12  = 2.75255128608411E-01f;\n  PIE4 = 7.85398163397448E-01f;\n  R22  =  2.72474487139158E+00f;\n  W22  = 9.17517095361369E-02f;\n  R13  = 1.90163509193487E-01f;\n  R23  = 1.78449274854325E+00f;\n  W23  = 1.77231492083829E-01f;\n  R33  = 5.52534374226326E+00f;\n  W33  = 5.11156880411248E-03f;\n\n  if (X < 3.e-7f){\n    if (n == 1){\n      RT1 = 0.5E+00f -X/5.0E+00f;\n      WW1 = 1.0E+00f -X/3.0E+00f;\n    } else if (n == 2) {\n      RT1 = 1.30693606237085E-01f -2.90430236082028E-02f *X;\n      RT2 = 2.86930639376291E+00f -6.37623643058102E-01f *X;\n      WW1 = 6.52145154862545E-01f -1.22713621927067E-01f *X;\n      WW2 = 3.47854845137453E-01f -2.10619711404725E-01f *X;\n    } else if (n == 3) {\n      RT1 = 6.03769246832797E-02f -9.28875764357368E-03f *X;\n      RT2 = 7.76823355931043E-01f -1.19511285527878E-01f *X;\n      RT3 = 6.66279971938567E+00f -1.02504611068957E+00f *X;\n      WW1 = 4.67913934572691E-01f -5.64876917232519E-02f *X;\n      WW2 = 3.60761573048137E-01f -1.49077186455208E-01f *X;\n      WW3 = 1.71324492379169E-01f -1.27768455150979E-01f *X;\n    }\n  } else if (X < 1.f) {\n    if (n == 1){\n      F1 = ((((((((-8.36313918003957E-08f*X+1.21222603512827E-06f )*X-\n                      1.15662609053481E-05f )*X+9.25197374512647E-05f )*X-\n                  6.40994113129432E-04f )*X+3.78787044215009E-03f )*X-\n              1.85185172458485E-02f )*X+7.14285713298222E-02f )*X-\n          1.99999999997023E-01f )*X+3.33333333333318E-01f;\n      WW1 = (X+X)*F1+expf(-X);\n      RT1 = F1/(WW1-F1);\n    } else if (n == 2) {\n      F1 = ((((((((-8.36313918003957E-08f*X+1.21222603512827E-06f )*X-\n                      1.15662609053481E-05f )*X+9.25197374512647E-05f )*X-\n                  6.40994113129432E-04f )*X+3.78787044215009E-03f )*X-\n              1.85185172458485E-02f )*X+7.14285713298222E-02f )*X-\n          1.99999999997023E-01f )*X+3.33333333333318E-01f;\n      WW1 = (X+X)*F1+expf(-X);\n      RT1 = (((((((-2.35234358048491E-09f*X+2.49173650389842E-08f)*X-\n                    4.558315364581E-08f)*X-2.447252174587E-06f)*X+\n                4.743292959463E-05f)*X-5.33184749432408E-04f )*X+\n            4.44654947116579E-03f )*X-2.90430236084697E-02f )*X+\n        1.30693606237085E-01f;\n      RT2 = (((((((-2.47404902329170E-08f*X+2.36809910635906E-07f)*X+\n                    1.835367736310E-06f)*X-2.066168802076E-05f)*X-\n                1.345693393936E-04f)*X-5.88154362858038E-05f )*X+\n            5.32735082098139E-02f )*X-6.37623643056745E-01f )*X+\n        2.86930639376289E+00f;\n      WW2 = ((F1-WW1)*RT1+F1)*(1.0E+00f+RT2)/(RT2-RT1);\n      WW1 = WW1-WW2;\n    } else if (n==3){\n      RT1 = ((((((-5.10186691538870E-10f*X+2.40134415703450E-08f)*X-\n                  5.01081057744427E-07f )*X+7.58291285499256E-06f )*X-\n              9.55085533670919E-05f )*X+1.02893039315878E-03f )*X-\n          9.28875764374337E-03f )*X+6.03769246832810E-02f;\n      RT2 = ((((((-1.29646524960555E-08f*X+7.74602292865683E-08f)*X+\n                  1.56022811158727E-06f )*X-1.58051990661661E-05f )*X-\n              3.30447806384059E-04f )*X+9.74266885190267E-03f )*X-\n          1.19511285526388E-01f )*X+7.76823355931033E-01f;\n      RT3 = ((((((-9.28536484109606E-09f*X-3.02786290067014E-07f)*X-\n                  2.50734477064200E-06f )*X-7.32728109752881E-06f )*X+\n              2.44217481700129E-04f )*X+4.94758452357327E-02f )*X-\n          1.02504611065774E+00f )*X+6.66279971938553E+00f;\n      F2 = ((((((((-7.60911486098850E-08f*X+1.09552870123182E-06f )*X-\n                      1.03463270693454E-05f )*X+8.16324851790106E-05f )*X-\n                  5.55526624875562E-04f )*X+3.20512054753924E-03f )*X-\n              1.51515139838540E-02f )*X+5.55555554649585E-02f )*X-\n          1.42857142854412E-01f )*X+1.99999999999986E-01f;\n      E = expf(-X);\n      F1 = ((X+X)*F2+E)/3.0E+00f;\n      WW1 = (X+X)*F1+E;\n      T1 = RT1/(RT1+1.0E+00f);\n      T2 = RT2/(RT2+1.0E+00f);\n      T3 = RT3/(RT3+1.0E+00f);\n      A2 = F2-T1*F1;\n      A1 = F1-T1*WW1;\n      WW3 = (A2-T2*A1)/((T3-T2)*(T3-T1));\n      WW2 = (T3*A1-A2)/((T3-T2)*(T2-T1));\n      WW1 = WW1-WW2-WW3;\n    }\n  } else if (X < 3.f) {\n    Y = X-2.0E+00f;\n    if (n == 1) {\n      F1 = ((((((((((-1.61702782425558E-10f*Y+1.96215250865776E-09f )*Y-\n                          2.14234468198419E-08f )*Y+2.17216556336318E-07f )*Y-\n                      1.98850171329371E-06f )*Y+1.62429321438911E-05f )*Y-\n                  1.16740298039895E-04f )*Y+7.24888732052332E-04f )*Y-\n              3.79490003707156E-03f )*Y+1.61723488664661E-02f )*Y-\n          5.29428148329736E-02f )*Y+1.15702180856167E-01f;\n      WW1 = (X+X)*F1+expf(-X);\n      RT1 = F1/(WW1-F1);\n    } else if (n == 2) {\n      F1 = ((((((((((-1.61702782425558E-10f*Y+1.96215250865776E-09f )*Y-\n                          2.14234468198419E-08f )*Y+2.17216556336318E-07f )*Y-\n                      1.98850171329371E-06f )*Y+1.62429321438911E-05f )*Y-\n                  1.16740298039895E-04f )*Y+7.24888732052332E-04f )*Y-\n              3.79490003707156E-03f )*Y+1.61723488664661E-02f )*Y-\n          5.29428148329736E-02f )*Y+1.15702180856167E-01f;\n      WW1 = (X+X)*F1+expf(-X);\n      RT1 = (((((((((-6.36859636616415E-12f*Y+8.47417064776270E-11f)*Y-\n                        5.152207846962E-10f)*Y-3.846389873308E-10f)*Y+\n                    8.472253388380E-08f)*Y-1.85306035634293E-06f )*Y+\n                2.47191693238413E-05f )*Y-2.49018321709815E-04f )*Y+\n            2.19173220020161E-03f )*Y-1.63329339286794E-02f )*Y+\n        8.68085688285261E-02f;\n      RT2 = ((((((((( 1.45331350488343E-10f*Y+2.07111465297976E-09f)*Y-\n                        1.878920917404E-08f)*Y-1.725838516261E-07f)*Y+\n                    2.247389642339E-06f)*Y+9.76783813082564E-06f )*Y-\n                1.93160765581969E-04f )*Y-1.58064140671893E-03f )*Y+\n            4.85928174507904E-02f )*Y-4.30761584997596E-01f )*Y+\n        1.80400974537950E+00f;\n      WW2 = ((F1-WW1)*RT1+F1)*(1.0E+00f+RT2)/(RT2-RT1);\n      WW1 = WW1-WW2;\n    } else if (n == 3) {\n      RT1 = (((((((( 1.44687969563318E-12f*Y+4.85300143926755E-12f)*Y-\n                      6.55098264095516E-10f )*Y+1.56592951656828E-08f )*Y-\n                  2.60122498274734E-07f )*Y+3.86118485517386E-06f )*Y-\n              5.13430986707889E-05f )*Y+6.03194524398109E-04f )*Y-\n          6.11219349825090E-03f )*Y+4.52578254679079E-02f;\n      RT2 = ((((((( 6.95964248788138E-10f*Y-5.35281831445517E-09f)*Y-\n                    6.745205954533E-08f)*Y+1.502366784525E-06f)*Y+\n                9.923326947376E-07f)*Y-3.89147469249594E-04f )*Y+\n            7.51549330892401E-03f )*Y-8.48778120363400E-02f )*Y+\n        5.73928229597613E-01f;\n      RT3 = ((((((((-2.81496588401439E-10f*Y+3.61058041895031E-09f)*Y+\n                      4.53631789436255E-08f )*Y-1.40971837780847E-07f )*Y-\n                  6.05865557561067E-06f )*Y-5.15964042227127E-05f )*Y+\n              3.34761560498171E-05f )*Y+5.04871005319119E-02f )*Y-\n          8.24708946991557E-01f )*Y+4.81234667357205E+00f;\n      F2 = ((((((((((-1.48044231072140E-10f*Y+1.78157031325097E-09f )*Y-\n                          1.92514145088973E-08f )*Y+1.92804632038796E-07f )*Y-\n                      1.73806555021045E-06f )*Y+1.39195169625425E-05f )*Y-\n                  9.74574633246452E-05f )*Y+5.83701488646511E-04f )*Y-\n              2.89955494844975E-03f )*Y+1.13847001113810E-02f )*Y-\n          3.23446977320647E-02f )*Y+5.29428148329709E-02f;\n      E = expf(-X);\n      F1 = ((X+X)*F2+E)/3.0E+00f;\n      WW1 = (X+X)*F1+E;\n      T1 = RT1/(RT1+1.0E+00f);\n      T2 = RT2/(RT2+1.0E+00f);\n      T3 = RT3/(RT3+1.0E+00f);\n      A2 = F2-T1*F1;\n      A1 = F1-T1*WW1;\n      WW3 = (A2-T2*A1)/((T3-T2)*(T3-T1));\n      WW2 = (T3*A1-A2)/((T3-T2)*(T2-T1));\n      WW1 = WW1-WW2-WW3;\n    }\n  } else if (X < 5.f){\n    Y = X-4.0E+00f;\n    if (n == 1){\n      F1 = ((((((((((-2.62453564772299E-11f*Y+3.24031041623823E-10f )*Y-\n                          3.614965656163E-09f)*Y+3.760256799971E-08f)*Y-\n                      3.553558319675E-07f)*Y+3.022556449731E-06f)*Y-\n                  2.290098979647E-05f)*Y+1.526537461148E-04f)*Y-\n              8.81947375894379E-04f )*Y+4.33207949514611E-03f )*Y-\n          1.75257821619926E-02f )*Y+5.28406320615584E-02f;\n      WW1 = (X+X)*F1+expf(-X);\n      RT1 = F1/(WW1-F1);\n    } else if (n == 2) {\n      F1 = ((((((((((-2.62453564772299E-11f*Y+3.24031041623823E-10f )*Y-\n                          3.614965656163E-09f)*Y+3.760256799971E-08f)*Y-\n                      3.553558319675E-07f)*Y+3.022556449731E-06f)*Y-\n                  2.290098979647E-05f)*Y+1.526537461148E-04f)*Y-\n              8.81947375894379E-04f )*Y+4.33207949514611E-03f )*Y-\n          1.75257821619926E-02f )*Y+5.28406320615584E-02f;\n      WW1 = (X+X)*F1+expf(-X);\n      RT1 = ((((((((-4.11560117487296E-12f*Y+7.10910223886747E-11f)*Y-\n                      1.73508862390291E-09f )*Y+5.93066856324744E-08f )*Y-\n                  9.76085576741771E-07f )*Y+1.08484384385679E-05f )*Y-\n              1.12608004981982E-04f )*Y+1.16210907653515E-03f )*Y-\n          9.89572595720351E-03f )*Y+6.12589701086408E-02f;\n      RT2 = (((((((((-1.80555625241001E-10f*Y+5.44072475994123E-10f)*Y+\n                        1.603498045240E-08f)*Y-1.497986283037E-07f)*Y-\n                    7.017002532106E-07f)*Y+1.85882653064034E-05f )*Y-\n                2.04685420150802E-05f )*Y-2.49327728643089E-03f )*Y+\n            3.56550690684281E-02f )*Y-2.60417417692375E-01f )*Y+\n        1.12155283108289E+00f;\n      WW2 = ((F1-WW1)*RT1+F1)*(1.0E+00f+RT2)/(RT2-RT1);\n      WW1 = WW1-WW2;\n    } else if (n == 3) {\n      RT1 = ((((((( 1.44265709189601E-11f*Y-4.66622033006074E-10f)*Y+\n                    7.649155832025E-09f)*Y-1.229940017368E-07f)*Y+\n                2.026002142457E-06f)*Y-2.87048671521677E-05f )*Y+\n            3.70326938096287E-04f )*Y-4.21006346373634E-03f )*Y+\n        3.50898470729044E-02f;\n      RT2 = ((((((((-2.65526039155651E-11f*Y+1.97549041402552E-10f)*Y+\n                      2.15971131403034E-09f )*Y-7.95045680685193E-08f )*Y+\n                  5.15021914287057E-07f )*Y+1.11788717230514E-05f )*Y-\n              3.33739312603632E-04f )*Y+5.30601428208358E-03f )*Y-\n          5.93483267268959E-02f )*Y+4.31180523260239E-01f;\n      RT3 = ((((((((-3.92833750584041E-10f*Y-4.16423229782280E-09f)*Y+\n                      4.42413039572867E-08f )*Y+6.40574545989551E-07f )*Y-\n                  3.05512456576552E-06f )*Y-1.05296443527943E-04f )*Y-\n              6.14120969315617E-04f )*Y+4.89665802767005E-02f )*Y-\n          6.24498381002855E-01f )*Y+3.36412312243724E+00f;\n      F2 = ((((((((((-2.36788772599074E-11f*Y+2.89147476459092E-10f )*Y-\n                          3.18111322308846E-09f )*Y+3.25336816562485E-08f )*Y-\n                      3.00873821471489E-07f )*Y+2.48749160874431E-06f )*Y-\n                  1.81353179793672E-05f )*Y+1.14504948737066E-04f )*Y-\n              6.10614987696677E-04f )*Y+2.64584212770942E-03f )*Y-\n          8.66415899015349E-03f )*Y+1.75257821619922E-02f;\n      E = expf(-X);\n      F1 = ((X+X)*F2+E)/3.0E+00f;\n      WW1 = (X+X)*F1+E;\n      T1 = RT1/(RT1+1.0E+00f);\n      T2 = RT2/(RT2+1.0E+00f);\n      T3 = RT3/(RT3+1.0E+00f);\n      A2 = F2-T1*F1;\n      A1 = F1-T1*WW1;\n      WW3 = (A2-T2*A1)/((T3-T2)*(T3-T1));\n      WW2 = (T3*A1-A2)/((T3-T2)*(T2-T1));\n      WW1 = WW1-WW2-WW3;\n    }\n  } else if (X < 10.f) {\n    E = expf(-X);\n    WW1 = (((((( 4.6897511375022E-01f/X-6.9955602298985E-01f)/X +\n                5.3689283271887E-01f)/X-3.2883030418398E-01f)/X +\n            2.4645596956002E-01f)/X-4.9984072848436E-01f)/X -\n        3.1501078774085E-06f)*E + sqrtf(PIE4/X);\n    F1 = (WW1-E)/(X+X);\n    if (n == 1)\n      RT1 = F1/(WW1-F1);\n    else if (n == 2){\n      Y = X-7.5E+00f;\n      RT1 = (((((((((((((-1.43632730148572E-16f*Y+2.38198922570405E-16f)*\n                                Y+1.358319618800E-14f)*Y-7.064522786879E-14f)*Y-\n                            7.719300212748E-13f)*Y+7.802544789997E-12f)*Y+\n                        6.628721099436E-11f)*Y-1.775564159743E-09f)*Y+\n                    1.713828823990E-08f)*Y-1.497500187053E-07f)*Y+\n                2.283485114279E-06f)*Y-3.76953869614706E-05f )*Y+\n            4.74791204651451E-04f )*Y-4.60448960876139E-03f )*Y+\n        3.72458587837249E-02f;\n      RT2 = (((((((((((( 2.48791622798900E-14f*Y-1.36113510175724E-13f)*Y-\n                              2.224334349799E-12f)*Y+4.190559455515E-11f)*Y-\n                          2.222722579924E-10f)*Y-2.624183464275E-09f)*Y+\n                      6.128153450169E-08f)*Y-4.383376014528E-07f)*Y-\n                  2.49952200232910E-06f )*Y+1.03236647888320E-04f )*Y-\n              1.44614664924989E-03f )*Y+1.35094294917224E-02f )*Y-\n          9.53478510453887E-02f )*Y+5.44765245686790E-01f;\n      WW2 = ((F1-WW1)*RT1+F1)*(1.0E+00f+RT2)/(RT2-RT1);\n      WW1 = WW1-WW2;\n    } else if (n == 3) {\n      F2 = (F1+F1+F1-E)/(X+X);\n      Y = X-7.5E+00f;\n      RT1 = ((((((((((( 5.74429401360115E-16f*Y+7.11884203790984E-16f)*Y-\n                            6.736701449826E-14f)*Y-6.264613873998E-13f)*Y+\n                        1.315418927040E-11f)*Y-4.23879635610964E-11f )*Y+\n                    1.39032379769474E-09f )*Y-4.65449552856856E-08f )*Y+\n                7.34609900170759E-07f )*Y-1.08656008854077E-05f )*Y+\n            1.77930381549953E-04f )*Y-2.39864911618015E-03f )*Y+\n        2.39112249488821E-02f;\n      RT2 = ((((((((((( 1.13464096209120E-14f*Y+6.99375313934242E-15f)*Y-\n                            8.595618132088E-13f)*Y-5.293620408757E-12f)*Y-\n                        2.492175211635E-11f)*Y+2.73681574882729E-09f )*Y-\n                    1.06656985608482E-08f )*Y-4.40252529648056E-07f )*Y+\n                9.68100917793911E-06f )*Y-1.68211091755327E-04f )*Y+\n            2.69443611274173E-03f )*Y-3.23845035189063E-02f )*Y+\n        2.75969447451882E-01f;\n      RT3 = (((((((((((( 6.66339416996191E-15f*Y+1.84955640200794E-13f)*Y-\n                              1.985141104444E-12f)*Y-2.309293727603E-11f)*Y+\n                          3.917984522103E-10f)*Y+1.663165279876E-09f)*Y-\n                      6.205591993923E-08f)*Y+8.769581622041E-09f)*Y+\n                  8.97224398620038E-06f )*Y-3.14232666170796E-05f )*Y-\n              1.83917335649633E-03f )*Y+3.51246831672571E-02f )*Y-\n          3.22335051270860E-01f )*Y+1.73582831755430E+00f;\n      T1 = RT1/(RT1+1.0E+00f);\n      T2 = RT2/(RT2+1.0E+00f);\n      T3 = RT3/(RT3+1.0E+00f);\n      A2 = F2-T1*F1;\n      A1 = F1-T1*WW1;\n      WW3 = (A2-T2*A1)/((T3-T2)*(T3-T1));\n      WW2 = (T3*A1-A2)/((T3-T2)*(T2-T1));\n      WW1 = WW1-WW2-WW3;\n    }\n  } else if (X < 15.f) {\n    E = expf(-X);\n    WW1 = (((-1.8784686463512E-01f/X+2.2991849164985E-01f)/X -\n          4.9893752514047E-01f)/X-2.1916512131607E-05f)*E \n      + sqrtf(PIE4/X);\n    F1 = (WW1-E)/(X+X);\n    if (n == 1)\n      RT1 = F1/(WW1-F1);\n    else if (n == 2) {\n      RT1 = ((((-1.01041157064226E-05f*X+1.19483054115173E-03f)*X -\n              6.73760231824074E-02f)*X+1.25705571069895E+00f)*X +\n          (((-8.57609422987199E+03f/X+5.91005939591842E+03f)/X -\n            1.70807677109425E+03f)/X+2.64536689959503E+02f)/X -\n          2.38570496490846E+01f)*E + R12/(X-R12);\n      RT2 = ((( 3.39024225137123E-04f*X-9.34976436343509E-02f)*X -\n            4.22216483306320E+00f)*X +\n          (((-2.08457050986847E+03f/X -\n             1.04999071905664E+03f)/X+3.39891508992661E+02f)/X -\n           1.56184800325063E+02f)/X+8.00839033297501E+00f)*E + R22/(X-R22);\n      WW2 = ((F1-WW1)*RT1+F1)*(1.0E+00f+RT2)/(RT2-RT1);\n      WW1 = WW1-WW2;\n    } else if (n == 3) {\n      F2 = (F1+F1+F1-E)/(X+X);\n      Y = X-12.5E+00f;\n      RT1 = ((((((((((( 4.42133001283090E-16f*Y-2.77189767070441E-15f)*Y-\n                            4.084026087887E-14f)*Y+5.379885121517E-13f)*Y+\n                        1.882093066702E-12f)*Y-8.67286219861085E-11f )*Y+\n                    7.11372337079797E-10f )*Y-3.55578027040563E-09f )*Y+\n                1.29454702851936E-07f )*Y-4.14222202791434E-06f )*Y+\n            8.04427643593792E-05f )*Y-1.18587782909876E-03f )*Y+\n        1.53435577063174E-02f;\n      RT2 = ((((((((((( 6.85146742119357E-15f*Y-1.08257654410279E-14f)*Y-\n                            8.579165965128E-13f)*Y+6.642452485783E-12f)*Y+\n                        4.798806828724E-11f)*Y-1.13413908163831E-09f )*Y+\n                    7.08558457182751E-09f )*Y-5.59678576054633E-08f )*Y+\n                2.51020389884249E-06f )*Y-6.63678914608681E-05f )*Y+\n            1.11888323089714E-03f )*Y-1.45361636398178E-02f )*Y+\n        1.65077877454402E-01f;\n      RT3 = (((((((((((( 3.20622388697743E-15f*Y-2.73458804864628E-14f)*Y-\n                              3.157134329361E-13f)*Y+8.654129268056E-12f)*Y-\n                          5.625235879301E-11f)*Y-7.718080513708E-10f)*Y+\n                      2.064664199164E-08f)*Y-1.567725007761E-07f)*Y-\n                  1.57938204115055E-06f )*Y+6.27436306915967E-05f )*Y-\n              1.01308723606946E-03f )*Y+1.13901881430697E-02f )*Y-\n          1.01449652899450E-01f )*Y+7.77203937334739E-01f;\n      T1 = RT1/(RT1+1.0E+00f);\n      T2 = RT2/(RT2+1.0E+00f);\n      T3 = RT3/(RT3+1.0E+00f);\n      A2 = F2-T1*F1;\n      A1 = F1-T1*WW1;\n      WW3 = (A2-T2*A1)/((T3-T2)*(T3-T1));\n      WW2 = (T3*A1-A2)/((T3-T2)*(T2-T1));\n      WW1 = WW1-WW2-WW3;\n    }\n  } else if (X < 33.f) {\n    E = expf(-X);\n    WW1 = (( 1.9623264149430E-01f/X-4.9695241464490E-01f)/X -\n        6.0156581186481E-05f)*E + sqrtf(PIE4/X);\n    F1 = (WW1-E)/(X+X);\n    if (n == 1)\n      RT1 = F1/(WW1-F1);\n    else if (n == 2){\n      RT1 = ((((-1.14906395546354E-06f*X+1.76003409708332E-04f)*X -\n              1.71984023644904E-02f)*X-1.37292644149838E-01f)*X +\n          (-4.75742064274859E+01f/X+9.21005186542857E+00f)/X -\n          2.31080873898939E-02f)*E + R12/(X-R12);\n      RT2 = ((( 3.64921633404158E-04f*X-9.71850973831558E-02f)*X -\n            4.02886174850252E+00f)*X +\n          (-1.35831002139173E+02f/X -\n           8.66891724287962E+01f)/X+2.98011277766958E+00f)*E + R22/(X-R22);\n      WW2 = ((F1-WW1)*RT1+F1)*(1.0E+00f+RT2)/(RT2-RT1);\n      WW1 = WW1-WW2;\n    } else if (n == 3) {\n      F2 = (F1+F1+F1-E)/(X+X);\n      if (X < 20.f) {\n        RT1 = ((((((-2.43270989903742E-06f*X+3.57901398988359E-04f)*X -\n                    2.34112415981143E-02f)*X+7.81425144913975E-01f)*X -\n                1.73209218219175E+01f)*X+2.43517435690398E+02f)*X +\n            (-1.97611541576986E+04f/X+9.82441363463929E+03f)/X -\n            2.07970687843258E+03f)*E + R13/(X-R13);\n        RT2 = (((((-2.62627010965435E-04f*X+3.49187925428138E-02f)*X -\n                  3.09337618731880E+00f)*X+1.07037141010778E+02f)*X -\n              2.36659637247087E+03f)*X +\n            ((-2.91669113681020E+06f/X +\n              1.41129505262758E+06f)/X-2.91532335433779E+05f)/X +\n            3.35202872835409E+04f)*E + R23/(X-R23);\n        RT3 = ((((( 9.31856404738601E-05f*X-2.87029400759565E-02f)*X -\n                  7.83503697918455E-01f)*X-1.84338896480695E+01f)*X +\n              4.04996712650414E+02f)*X +\n            (-1.89829509315154E+05f/X +\n             5.11498390849158E+04f)/X-6.88145821789955E+03f)*E \n          + R33/(X-R33);\n      } else {\n        RT1 = ((((-4.97561537069643E-04f*X-5.00929599665316E-02f)*X +\n                1.31099142238996E+00f)*X-1.88336409225481E+01f)*X -\n            6.60344754467191E+02f /X+1.64931462413877E+02f)*E \n          + R13/(X-R13);\n        RT2 = ((((-4.48218898474906E-03f*X-5.17373211334924E-01f)*X +\n                1.13691058739678E+01f)*X-1.65426392885291E+02f)*X -\n            6.30909125686731E+03f /X+1.52231757709236E+03f)*E \n          + R23/(X-R23);\n        RT3 = ((((-1.38368602394293E-02f*X-1.77293428863008E+00f)*X +\n                1.73639054044562E+01f)*X-3.57615122086961E+02f)*X -\n            1.45734701095912E+04f /X+2.69831813951849E+03f)*E \n          + R33/(X-R33);\n      }\n      T1 = RT1/(RT1+1.0E+00f);\n      T2 = RT2/(RT2+1.0E+00f);\n      T3 = RT3/(RT3+1.0E+00f);\n      A2 = F2-T1*F1;\n      A1 = F1-T1*WW1;\n      WW3 = (A2-T2*A1)/((T3-T2)*(T3-T1));\n      WW2 = (T3*A1-A2)/((T3-T2)*(T2-T1));\n      WW1 = WW1-WW2-WW3; \n    }\n  } else {\n    WW1 = sqrtf(PIE4/X);\n    if (n == 1)\n      RT1 = 0.5E+00f/(X-0.5E+00f);\n    else if (n == 2) {\n      if (X < 40.f) {\n        E = expf(-X);\n        RT1 = (-8.78947307498880E-01f*X+1.09243702330261E+01f)*E \n          + R12/(X-R12);\n        RT2 = (-9.28903924275977E+00f*X+8.10642367843811E+01f)*E \n          + R22/(X-R22);\n        WW2 = ( 4.46857389308400E+00f*X-7.79250653461045E+01f)*E + W22*WW1;\n        WW1 = WW1-WW2;\n      } else {\n        RT1 = R12/(X-R12);\n        RT2 = R22/(X-R22);\n        WW2 = W22*WW1;\n        WW1 = WW1-WW2;\n      }\n    } else if (n == 3) {\n      if (X < 47.f) {\n        E = expf(-X);\n        RT1 = ((-7.39058467995275E+00f*X+3.21318352526305E+02f)*X -\n            3.99433696473658E+03f)*E + R13/(X-R13);\n        RT2 = ((-7.38726243906513E+01f*X+3.13569966333873E+03f)*X -\n            3.86862867311321E+04f)*E + R23/(X-R23);\n        RT3 = ((-2.63750565461336E+02f*X+1.04412168692352E+04f)*X -\n            1.28094577915394E+05f)*E + R33/(X-R33);\n        WW3 = ((( 1.52258947224714E-01f*X-8.30661900042651E+00f)*X +\n              1.92977367967984E+02f)*X-1.67787926005344E+03f)*E \n          + W33*WW1;\n        WW2 = (( 6.15072615497811E+01f*X-2.91980647450269E+03f)*X +\n            3.80794303087338E+04f)*E + W23*WW1;\n        WW1 = WW1-WW2-WW3;\n      } else {\n        RT1 = R13/(X-R13);\n        RT2 = R23/(X-R23);\n        RT3 = R33/(X-R33);\n        WW2 = W23*WW1;\n        WW3 = W33*WW1;\n        WW1 = WW1-WW2-WW3;\n      }\n    }\n  }\n  roots[0] = RT1;\n  weights[0] = WW1;\n  if (n > 1){\n    roots[1] = RT2;\n    weights[1] = WW2;\n  }\n  if (n > 2) {\n    roots[2] = RT3;\n    weights[2] = WW3;\n  }\n  return;\n}\n\n__device__ void cuda_Root4(float X, float roots[], float weights[]){\n  float R14,PIE4,R24,W24,R34,W34,R44,W44;\n  float RT1=0,RT2=0,RT3=0,RT4=0,WW1=0,WW2=0,WW3=0,WW4=0;\n  float Y,E;\n\n  R14 = 1.45303521503316E-01f;\n  PIE4 = 7.85398163397448E-01f;\n  R24 = 1.33909728812636E+00f;\n  W24 = 2.34479815323517E-01f;\n  R34 = 3.92696350135829E+00f;\n  W34 = 1.92704402415764E-02f;\n  R44 = 8.58863568901199E+00f;\n  W44 = 2.25229076750736E-04f;\n\n  if (X <= 3.0E-7f) {\n    RT1 = 3.48198973061471E-02f -4.09645850660395E-03f *X;\n    RT2 = 3.81567185080042E-01f -4.48902570656719E-02f *X;\n    RT3 = 1.73730726945891E+00f -2.04389090547327E-01f *X;\n    RT4 = 1.18463056481549E+01f -1.39368301742312E+00f *X;\n    WW1 = 3.62683783378362E-01f -3.13844305713928E-02f *X;\n    WW2 = 3.13706645877886E-01f -8.98046242557724E-02f *X;\n    WW3 = 2.22381034453372E-01f -1.29314370958973E-01f *X;\n    WW4 = 1.01228536290376E-01f -8.28299075414321E-02f *X;\n  } else if (X <= 1.f) {\n    RT1 = ((((((-1.95309614628539E-10f*X+5.19765728707592E-09f)*X-\n                1.01756452250573E-07f )*X+1.72365935872131E-06f )*X-\n            2.61203523522184E-05f )*X+3.52921308769880E-04f )*X-\n        4.09645850658433E-03f )*X+3.48198973061469E-02f;\n    RT2 = (((((-1.89554881382342E-08f*X+3.07583114342365E-07f)*X+\n              1.270981734393E-06f)*X-1.417298563884E-04f)*X+\n          3.226979163176E-03f)*X-4.48902570678178E-02f )*X+\n      3.81567185080039E-01f;\n    RT3 = (((((( 1.77280535300416E-09f*X+3.36524958870615E-08f)*X-\n                2.58341529013893E-07f )*X-1.13644895662320E-05f )*X-\n            7.91549618884063E-05f )*X+1.03825827346828E-02f )*X-\n        2.04389090525137E-01f )*X+1.73730726945889E+00f;\n    RT4 = (((((-5.61188882415248E-08f*X-2.49480733072460E-07f)*X+\n              3.428685057114E-06f)*X+1.679007454539E-04f)*X+\n          4.722855585715E-02f)*X-1.39368301737828E+00f )*X+\n      1.18463056481543E+01f;\n    WW1 = ((((((-1.14649303201279E-08f*X+1.88015570196787E-07f)*X-\n                2.33305875372323E-06f )*X+2.68880044371597E-05f )*X-\n            2.94268428977387E-04f )*X+3.06548909776613E-03f )*X-\n        3.13844305680096E-02f )*X+3.62683783378335E-01f;\n    WW2 = ((((((((-4.11720483772634E-09f*X+6.54963481852134E-08f)*X-\n                    7.20045285129626E-07f )*X+6.93779646721723E-06f )*X-\n                6.05367572016373E-05f )*X+4.74241566251899E-04f )*X-\n            3.26956188125316E-03f )*X+1.91883866626681E-02f )*X-\n        8.98046242565811E-02f )*X+3.13706645877886E-01f;\n    WW3 = ((((((((-3.41688436990215E-08f*X+5.07238960340773E-07f)*X-\n                    5.01675628408220E-06f )*X+4.20363420922845E-05f )*X-\n                3.08040221166823E-04f )*X+1.94431864731239E-03f )*X-\n            1.02477820460278E-02f )*X+4.28670143840073E-02f )*X-\n        1.29314370962569E-01f )*X+2.22381034453369E-01f;\n    WW4 = ((((((((( 4.99660550769508E-09f*X-7.94585963310120E-08f)*X+\n                      8.359072409485E-07f)*X-7.422369210610E-06f)*X+\n                  5.763374308160E-05f)*X-3.86645606718233E-04f )*X+\n              2.18417516259781E-03f )*X-9.99791027771119E-03f )*X+\n          3.48791097377370E-02f )*X-8.28299075413889E-02f )*X+\n      1.01228536290376E-01f;\n  } else if (X <= 5.f) {\n    Y = X-3.0E+00f;\n    RT1 = (((((((((-1.48570633747284E-15f*Y-1.33273068108777E-13f)*Y+\n                      4.068543696670E-12f)*Y-9.163164161821E-11f)*Y+\n                  2.046819017845E-09f)*Y-4.03076426299031E-08f )*Y+\n              7.29407420660149E-07f )*Y-1.23118059980833E-05f )*Y+\n          1.88796581246938E-04f )*Y-2.53262912046853E-03f )*Y+\n      2.51198234505021E-02f;\n    RT2 = ((((((((( 1.35830583483312E-13f*Y-2.29772605964836E-12f)*Y-\n                      3.821500128045E-12f)*Y+6.844424214735E-10f)*Y-\n                  1.048063352259E-08f)*Y+1.50083186233363E-08f )*Y+\n              3.48848942324454E-06f )*Y-1.08694174399193E-04f )*Y+\n          2.08048885251999E-03f )*Y-2.91205805373793E-02f )*Y+\n      2.72276489515713E-01f;\n    RT3 = ((((((((( 5.02799392850289E-13f*Y+1.07461812944084E-11f)*Y-\n                      1.482277886411E-10f)*Y-2.153585661215E-09f)*Y+\n                  3.654087802817E-08f)*Y+5.15929575830120E-07f )*Y-\n              9.52388379435709E-06f )*Y-2.16552440036426E-04f )*Y+\n          9.03551469568320E-03f )*Y-1.45505469175613E-01f )*Y+\n      1.21449092319186E+00f;\n    RT4 = (((((((((-1.08510370291979E-12f*Y+6.41492397277798E-11f)*Y+\n                      7.542387436125E-10f)*Y-2.213111836647E-09f)*Y-\n                  1.448228963549E-07f)*Y-1.95670833237101E-06f )*Y-\n              1.07481314670844E-05f )*Y+1.49335941252765E-04f )*Y+\n          4.87791531990593E-02f )*Y-1.10559909038653E+00f )*Y+\n      8.09502028611780E+00f;\n    WW1 = ((((((((((-4.65801912689961E-14f*Y+7.58669507106800E-13f)*Y-\n                        1.186387548048E-11f)*Y+1.862334710665E-10f)*Y-\n                    2.799399389539E-09f)*Y+4.148972684255E-08f)*Y-\n                5.933568079600E-07f)*Y+8.168349266115E-06f)*Y-\n            1.08989176177409E-04f )*Y+1.41357961729531E-03f )*Y-\n        1.87588361833659E-02f )*Y+2.89898651436026E-01f;\n    WW2 = ((((((((((((-1.46345073267549E-14f*Y+2.25644205432182E-13f)*Y-\n                            3.116258693847E-12f)*Y+4.321908756610E-11f)*Y-\n                        5.673270062669E-10f)*Y+7.006295962960E-09f)*Y-\n                    8.120186517000E-08f)*Y+8.775294645770E-07f)*Y-\n                8.77829235749024E-06f )*Y+8.04372147732379E-05f )*Y-\n            6.64149238804153E-04f )*Y+4.81181506827225E-03f )*Y-\n        2.88982669486183E-02f )*Y+1.56247249979288E-01f;\n    WW3 = ((((((((((((( 9.06812118895365E-15f*Y-1.40541322766087E-13f)*\n                              Y+1.919270015269E-12f)*Y-2.605135739010E-11f)*Y+\n                          3.299685839012E-10f)*Y-3.86354139348735E-09f )*Y+\n                      4.16265847927498E-08f )*Y-4.09462835471470E-07f )*Y+\n                  3.64018881086111E-06f )*Y-2.88665153269386E-05f )*Y+\n              2.00515819789028E-04f )*Y-1.18791896897934E-03f )*Y+\n          5.75223633388589E-03f )*Y-2.09400418772687E-02f )*Y+\n      4.85368861938873E-02f;\n    WW4 = ((((((((((((((-9.74835552342257E-16f*Y+1.57857099317175E-14f)*\n                                Y-2.249993780112E-13f)*Y+3.173422008953E-12f)*Y-\n                            4.161159459680E-11f)*Y+5.021343560166E-10f)*Y-\n                        5.545047534808E-09f)*Y+5.554146993491E-08f)*Y-\n                    4.99048696190133E-07f )*Y+3.96650392371311E-06f )*Y-\n                2.73816413291214E-05f )*Y+1.60106988333186E-04f )*Y-\n            7.64560567879592E-04f )*Y+2.81330044426892E-03f )*Y-\n        7.16227030134947E-03f )*Y+9.66077262223353E-03f;\n  } else if (X <= 10.f) {\n    Y = X-7.5E+00f;\n    RT1 = ((((((((( 4.64217329776215E-15f*Y-6.27892383644164E-15f)*Y+\n                      3.462236347446E-13f)*Y-2.927229355350E-11f)*Y+\n                  5.090355371676E-10f)*Y-9.97272656345253E-09f )*Y+\n              2.37835295639281E-07f )*Y-4.60301761310921E-06f )*Y+\n          8.42824204233222E-05f )*Y-1.37983082233081E-03f )*Y+\n      1.66630865869375E-02f;\n    RT2 = ((((((((( 2.93981127919047E-14f*Y+8.47635639065744E-13f)*Y-\n                      1.446314544774E-11f)*Y-6.149155555753E-12f)*Y+\n                  8.484275604612E-10f)*Y-6.10898827887652E-08f )*Y+\n              2.39156093611106E-06f )*Y-5.35837089462592E-05f )*Y+\n          1.00967602595557E-03f )*Y-1.57769317127372E-02f )*Y+\n      1.74853819464285E-01f;\n    RT3 = (((((((((( 2.93523563363000E-14f*Y-6.40041776667020E-14f)*Y-\n                        2.695740446312E-12f)*Y+1.027082960169E-10f)*Y-\n                    5.822038656780E-10f)*Y-3.159991002539E-08f)*Y+\n                4.327249251331E-07f)*Y+4.856768455119E-06f)*Y-\n            2.54617989427762E-04f )*Y+5.54843378106589E-03f )*Y-\n        7.95013029486684E-02f )*Y+7.20206142703162E-01f;\n    RT4 = (((((((((((-1.62212382394553E-14f*Y+7.68943641360593E-13f)*Y+\n                          5.764015756615E-12f)*Y-1.380635298784E-10f)*Y-\n                      1.476849808675E-09f)*Y+1.84347052385605E-08f )*Y+\n                  3.34382940759405E-07f )*Y-1.39428366421645E-06f )*Y-\n              7.50249313713996E-05f )*Y-6.26495899187507E-04f )*Y+\n          4.69716410901162E-02f )*Y-6.66871297428209E-01f )*Y+\n      4.11207530217806E+00f;\n    WW1 = ((((((((((-1.65995045235997E-15f*Y+6.91838935879598E-14f)*Y-\n                        9.131223418888E-13f)*Y+1.403341829454E-11f)*Y-\n                    3.672235069444E-10f)*Y+6.366962546990E-09f)*Y-\n                1.039220021671E-07f)*Y+1.959098751715E-06f)*Y-\n            3.33474893152939E-05f )*Y+5.72164211151013E-04f )*Y-\n        1.05583210553392E-02f )*Y+2.26696066029591E-01f;\n    WW2 = ((((((((((((-3.57248951192047E-16f*Y+6.25708409149331E-15f)*Y-\n                            9.657033089714E-14f)*Y+1.507864898748E-12f)*Y-\n                        2.332522256110E-11f)*Y+3.428545616603E-10f)*Y-\n                    4.698730937661E-09f)*Y+6.219977635130E-08f)*Y-\n                7.83008889613661E-07f )*Y+9.08621687041567E-06f )*Y-\n            9.86368311253873E-05f )*Y+9.69632496710088E-04f )*Y-\n        8.14594214284187E-03f )*Y+8.50218447733457E-02f;\n    WW3 = ((((((((((((( 1.64742458534277E-16f*Y-2.68512265928410E-15f)*\n                              Y+3.788890667676E-14f)*Y-5.508918529823E-13f)*Y+\n                          7.555896810069E-12f)*Y-9.69039768312637E-11f )*Y+\n                      1.16034263529672E-09f )*Y-1.28771698573873E-08f )*Y+\n                  1.31949431805798E-07f )*Y-1.23673915616005E-06f )*Y+\n              1.04189803544936E-05f )*Y-7.79566003744742E-05f )*Y+\n          5.03162624754434E-04f )*Y-2.55138844587555E-03f )*Y+\n      1.13250730954014E-02f;\n    WW4 = ((((((((((((((-1.55714130075679E-17f*Y+2.57193722698891E-16f)*\n                                Y-3.626606654097E-15f)*Y+5.234734676175E-14f)*Y-\n                            7.067105402134E-13f)*Y+8.793512664890E-12f)*Y-\n                        1.006088923498E-10f)*Y+1.050565098393E-09f)*Y-\n                    9.91517881772662E-09f )*Y+8.35835975882941E-08f )*Y-\n                6.19785782240693E-07f )*Y+3.95841149373135E-06f )*Y-\n            2.11366761402403E-05f )*Y+9.00474771229507E-05f )*Y-\n        2.78777909813289E-04f )*Y+5.26543779837487E-04f;\n  } else if (X <= 15.f) {\n    Y = X-12.5E+00f;\n    RT1 = ((((((((((( 4.94869622744119E-17f*Y+8.03568805739160E-16f)*Y-\n                          5.599125915431E-15f)*Y-1.378685560217E-13f)*Y+\n                      7.006511663249E-13f)*Y+1.30391406991118E-11f )*Y+\n                  8.06987313467541E-11f )*Y-5.20644072732933E-09f )*Y+\n              7.72794187755457E-08f )*Y-1.61512612564194E-06f )*Y+\n          4.15083811185831E-05f )*Y-7.87855975560199E-04f )*Y+\n      1.14189319050009E-02f;\n    RT2 = ((((((((((( 4.89224285522336E-16f*Y+1.06390248099712E-14f)*Y-\n                          5.446260182933E-14f)*Y-1.613630106295E-12f)*Y+\n                      3.910179118937E-12f)*Y+1.90712434258806E-10f )*Y+\n                  8.78470199094761E-10f )*Y-5.97332993206797E-08f )*Y+\n              9.25750831481589E-07f )*Y-2.02362185197088E-05f )*Y+\n          4.92341968336776E-04f )*Y-8.68438439874703E-03f )*Y+\n      1.15825965127958E-01f;\n    RT3 = (((((((((( 6.12419396208408E-14f*Y+1.12328861406073E-13f)*Y-\n                        9.051094103059E-12f)*Y-4.781797525341E-11f)*Y+\n                    1.660828868694E-09f)*Y+4.499058798868E-10f)*Y-\n                2.519549641933E-07f)*Y+4.977444040180E-06f)*Y-\n            1.25858350034589E-04f )*Y+2.70279176970044E-03f )*Y-\n        3.99327850801083E-02f )*Y+4.33467200855434E-01f;\n    RT4 = ((((((((((( 4.63414725924048E-14f*Y-4.72757262693062E-14f)*Y-\n                          1.001926833832E-11f)*Y+6.074107718414E-11f)*Y+\n                      1.576976911942E-09f)*Y-2.01186401974027E-08f )*Y-\n                  1.84530195217118E-07f )*Y+5.02333087806827E-06f )*Y+\n              9.66961790843006E-06f )*Y-1.58522208889528E-03f )*Y+\n          2.80539673938339E-02f )*Y-2.78953904330072E-01f )*Y+\n      1.82835655238235E+00f;\n    WW4 = ((((((((((((( 2.90401781000996E-18f*Y-4.63389683098251E-17f)*\n                              Y+6.274018198326E-16f)*Y-8.936002188168E-15f)*Y+\n                          1.194719074934E-13f)*Y-1.45501321259466E-12f )*Y+\n                      1.64090830181013E-11f )*Y-1.71987745310181E-10f )*Y+\n                  1.63738403295718E-09f )*Y-1.39237504892842E-08f )*Y+\n              1.06527318142151E-07f )*Y-7.27634957230524E-07f )*Y+\n          4.12159381310339E-06f )*Y-1.74648169719173E-05f )*Y+\n      8.50290130067818E-05f;\n    WW3 = ((((((((((((-4.19569145459480E-17f*Y+5.94344180261644E-16f)*Y-\n                            1.148797566469E-14f)*Y+1.881303962576E-13f)*Y-\n                        2.413554618391E-12f)*Y+3.372127423047E-11f)*Y-\n                    4.933988617784E-10f)*Y+6.116545396281E-09f)*Y-\n                6.69965691739299E-08f )*Y+7.52380085447161E-07f )*Y-\n            8.08708393262321E-06f )*Y+6.88603417296672E-05f )*Y-\n        4.67067112993427E-04f )*Y+5.42313365864597E-03f;\n    WW2 = ((((((((((-6.22272689880615E-15f*Y+1.04126809657554E-13f)*Y-\n                        6.842418230913E-13f)*Y+1.576841731919E-11f)*Y-\n                    4.203948834175E-10f)*Y+6.287255934781E-09f)*Y-\n                8.307159819228E-08f)*Y+1.356478091922E-06f)*Y-\n            2.08065576105639E-05f )*Y+2.52396730332340E-04f )*Y-\n        2.94484050194539E-03f )*Y+6.01396183129168E-02f;\n    WW1 = (((-1.8784686463512E-01f/X+2.2991849164985E-01f)/X -\n          4.9893752514047E-01f)/X-2.1916512131607E-05f)*expf(-X) +\n      sqrtf(PIE4/X)-WW4-WW3-WW2;\n  } else if (X <= 20.f) {\n    WW1 = sqrtf(PIE4/X);\n    Y = X-17.5E+00f;\n    RT1 = ((((((((((( 4.36701759531398E-17f*Y-1.12860600219889E-16f)*Y-\n                          6.149849164164E-15f)*Y+5.820231579541E-14f)*Y+\n                      4.396602872143E-13f)*Y-1.24330365320172E-11f )*Y+\n                  6.71083474044549E-11f )*Y+2.43865205376067E-10f )*Y+\n              1.67559587099969E-08f )*Y-9.32738632357572E-07f )*Y+\n          2.39030487004977E-05f )*Y-4.68648206591515E-04f )*Y+\n      8.34977776583956E-03f;\n    RT2 = ((((((((((( 4.98913142288158E-16f*Y-2.60732537093612E-16f)*Y-\n                          7.775156445127E-14f)*Y+5.766105220086E-13f)*Y+\n                      6.432696729600E-12f)*Y-1.39571683725792E-10f )*Y+\n                  5.95451479522191E-10f )*Y+2.42471442836205E-09f )*Y+\n              2.47485710143120E-07f )*Y-1.14710398652091E-05f )*Y+\n          2.71252453754519E-04f )*Y-4.96812745851408E-03f )*Y+\n      8.26020602026780E-02f;\n    RT3 = ((((((((((( 1.91498302509009E-15f*Y+1.48840394311115E-14f)*Y-\n                          4.316925145767E-13f)*Y+1.186495793471E-12f)*Y+\n                      4.615806713055E-11f)*Y-5.54336148667141E-10f )*Y+\n                  3.48789978951367E-10f )*Y-2.79188977451042E-09f )*Y+\n              2.09563208958551E-06f )*Y-6.76512715080324E-05f )*Y+\n          1.32129867629062E-03f )*Y-2.05062147771513E-02f )*Y+\n      2.88068671894324E-01f;\n    RT4 = (((((((((((-5.43697691672942E-15f*Y-1.12483395714468E-13f)*Y+\n                          2.826607936174E-12f)*Y-1.266734493280E-11f)*Y-\n                      4.258722866437E-10f)*Y+9.45486578503261E-09f )*Y-\n                  5.86635622821309E-08f )*Y-1.28835028104639E-06f )*Y+\n              4.41413815691885E-05f )*Y-7.61738385590776E-04f )*Y+\n          9.66090902985550E-03f )*Y-1.01410568057649E-01f )*Y+\n      9.54714798156712E-01f;\n    WW4 = ((((((((((((-7.56882223582704E-19f*Y+7.53541779268175E-18f)*Y-\n                            1.157318032236E-16f)*Y+2.411195002314E-15f)*Y-\n                        3.601794386996E-14f)*Y+4.082150659615E-13f)*Y-\n                    4.289542980767E-12f)*Y+5.086829642731E-11f)*Y-\n                6.35435561050807E-10f )*Y+6.82309323251123E-09f )*Y-\n            5.63374555753167E-08f )*Y+3.57005361100431E-07f )*Y-\n        2.40050045173721E-06f )*Y+4.94171300536397E-05f;\n    WW3 = (((((((((((-5.54451040921657E-17f*Y+2.68748367250999E-16f)*Y+\n                          1.349020069254E-14f)*Y-2.507452792892E-13f)*Y+\n                      1.944339743818E-12f)*Y-1.29816917658823E-11f )*Y+\n                  3.49977768819641E-10f )*Y-8.67270669346398E-09f )*Y+\n              1.31381116840118E-07f )*Y-1.36790720600822E-06f )*Y+\n          1.19210697673160E-05f )*Y-1.42181943986587E-04f )*Y+\n      4.12615396191829E-03f;\n    WW2 = (((((((((((-1.86506057729700E-16f*Y+1.16661114435809E-15f)*Y+\n                          2.563712856363E-14f)*Y-4.498350984631E-13f)*Y+\n                      1.765194089338E-12f)*Y+9.04483676345625E-12f )*Y+\n                  4.98930345609785E-10f )*Y-2.11964170928181E-08f )*Y+\n              3.98295476005614E-07f )*Y-5.49390160829409E-06f )*Y+\n          7.74065155353262E-05f )*Y-1.48201933009105E-03f )*Y+\n      4.97836392625268E-02f;\n    WW1 = (( 1.9623264149430E-01f/X-4.9695241464490E-01f)/X -\n        6.0156581186481E-05f)*expf(-X)+WW1-WW2-WW3-WW4;\n  } else if (X <= 35.f) {\n    WW1 = sqrtf(PIE4/X);\n    E = expf(-X);\n    RT1 = ((((((-4.45711399441838E-05f*X+1.27267770241379E-03f)*X -\n                2.36954961381262E-01f)*X+1.54330657903756E+01f)*X -\n            5.22799159267808E+02f)*X+1.05951216669313E+04f)*X +\n        (-2.51177235556236E+06f/X+8.72975373557709E+05f)/X -\n        1.29194382386499E+05f)*E + R14/(X-R14);\n    RT2 = (((((-7.85617372254488E-02f*X+6.35653573484868E+00f)*X -\n              3.38296938763990E+02f)*X+1.25120495802096E+04f)*X -\n          3.16847570511637E+05f)*X +\n        ((-1.02427466127427E+09f/X +\n          3.70104713293016E+08f)/X-5.87119005093822E+07f)/X +\n        5.38614211391604E+06f)*E + R24/(X-R24);\n    RT3 = (((((-2.37900485051067E-01f*X+1.84122184400896E+01f)*X -\n              1.00200731304146E+03f)*X+3.75151841595736E+04f)*X -\n          9.50626663390130E+05f)*X +\n        ((-2.88139014651985E+09f/X +\n          1.06625915044526E+09f)/X-1.72465289687396E+08f)/X +\n        1.60419390230055E+07f)*E + R34/(X-R34);\n    RT4 = ((((((-6.00691586407385E-04f*X-3.64479545338439E-01f)*X +\n                1.57496131755179E+01f)*X-6.54944248734901E+02f)*X +\n            1.70830039597097E+04f)*X-2.90517939780207E+05f)*X +\n        (3.49059698304732E+07f/X-1.64944522586065E+07f)/X +\n        2.96817940164703E+06f)*E + R44/(X-R44);\n    if (X <= 25.f) \n      WW4 = ((((((( 2.33766206773151E-07f*X-\n                      3.81542906607063E-05f)*X +3.51416601267000E-03f)*X-\n                  1.66538571864728E-01f)*X +4.80006136831847E+00f)*X-\n              8.73165934223603E+01f)*X +9.77683627474638E+02f)*X +\n          1.66000945117640E+04f/X -6.14479071209961E+03f)*E + W44*WW1;\n    else\n      WW4 = (((((( 5.74245945342286E-06f*X-\n                    7.58735928102351E-05f)*X +2.35072857922892E-04f)*X-\n                3.78812134013125E-03f)*X +3.09871652785805E-01f)*X-\n            7.11108633061306E+00f)*X +5.55297573149528E+01f)*E + W44*WW1;\n    WW3 = (((((( 2.36392855180768E-04f*X-9.16785337967013E-03f)*X +\n                4.62186525041313E-01f)*X-1.96943786006540E+01f)*X +\n            4.99169195295559E+02f)*X-6.21419845845090E+03f)*X +\n        ((+5.21445053212414E+07f/X-1.34113464389309E+07f)/X +\n         1.13673298305631E+06f)/X-2.81501182042707E+03f)*E + W34*WW1;\n    WW2 = (((((( 7.29841848989391E-04f*X-3.53899555749875E-02f)*X +\n                2.07797425718513E+00f)*X-1.00464709786287E+02f)*X +\n            3.15206108877819E+03f)*X-6.27054715090012E+04f)*X +\n        (+1.54721246264919E+07f/X-5.26074391316381E+06f)/X +\n        7.67135400969617E+05f)*E + W24*WW1;\n    WW1 = (( 1.9623264149430E-01f/X-4.9695241464490E-01f)/X -\n        6.0156581186481E-05f)*E + WW1-WW2-WW3-WW4;\n  } else if (X <= 53.f) {\n    WW1 = sqrtf(PIE4/X);\n    E = expf(-X)*powf(X,4.f);\n    RT4 = ((-2.19135070169653E-03f*X-1.19108256987623E-01f)*X -\n        7.50238795695573E-01f)*E + R44/(X-R44);\n    RT3 = ((-9.65842534508637E-04f*X-4.49822013469279E-02f)*X +\n        6.08784033347757E-01f)*E + R34/(X-R34);\n    RT2 = ((-3.62569791162153E-04f*X-9.09231717268466E-03f)*X +\n        1.84336760556262E-01f)*E + R24/(X-R24);\n    RT1 = ((-4.07557525914600E-05f*X-6.88846864931685E-04f)*X +\n        1.74725309199384E-02f)*E + R14/(X-R14);\n    WW4 = (( 5.76631982000990E-06f*X-7.89187283804890E-05f)*X +\n        3.28297971853126E-04f)*E + W44*WW1;\n    WW3 = (( 2.08294969857230E-04f*X-3.77489954837361E-03f)*X +\n        2.09857151617436E-02f)*E + W34*WW1;\n    WW2 = (( 6.16374517326469E-04f*X-1.26711744680092E-02f)*X +\n        8.14504890732155E-02f)*E + W24*WW1;\n    WW1 = WW1-WW2-WW3-WW4;\n  } else {\n    WW1 = sqrtf(PIE4/X);\n    RT1 = R14/(X-R14);\n    RT2 = R24/(X-R24);\n    RT3 = R34/(X-R34);\n    RT4 = R44/(X-R44);\n    WW4 = W44*WW1;\n    WW3 = W34*WW1;\n    WW2 = W24*WW1;\n    WW1 = WW1-WW2-WW3-WW4;\n  }\n  roots[0] = RT1;\n  weights[0] = WW1;\n  roots[1] = RT2;\n  weights[1] = WW2;\n  roots[2] = RT3;\n  weights[2] = WW3;\n  roots[3] = RT4;\n  weights[3] = WW4;\n  return;\n}\n\n__device__ void cuda_Root5(float X, float roots[], float weights[]){\n  float R15,PIE4,R25,W25,R35,W35,R45,W45,R55,W55;\n  float RT1=0,RT2=0,RT3=0,RT4=0,RT5=0,\n        WW1=0,WW2=0,WW3=0,WW4=0,WW5=0;\n  float Y,E=0,XXX;\n\n  R15 = 1.17581320211778E-01f;\n  PIE4 = 7.85398163397448E-01f;\n  R25 = 1.07456201243690E+00f;\n  W25 = 2.70967405960535E-01f;\n  R35 = 3.08593744371754E+00f;\n  W35 = 3.82231610015404E-02f;\n  R45 = 6.41472973366203E+00f;\n  W45 = 1.51614186862443E-03f;\n  R55 = 1.18071894899717E+01f;\n  W55 = 8.62130526143657E-06f;\n\n  if (X < 3.e-7f){\n    RT1 = 2.26659266316985E-02f -2.15865967920897E-03f *X;\n    RT2 = 2.31271692140903E-01f -2.20258754389745E-02f *X;\n    RT3 = 8.57346024118836E-01f -8.16520023025515E-02f *X;\n    RT4 = 2.97353038120346E+00f -2.83193369647137E-01f *X;\n    RT5 = 1.84151859759051E+01f -1.75382723579439E+00f *X;\n    WW1 = 2.95524224714752E-01f -1.96867576909777E-02f *X;\n    WW2 = 2.69266719309995E-01f -5.61737590184721E-02f *X;\n    WW3 = 2.19086362515981E-01f -9.71152726793658E-02f *X;\n    WW4 = 1.49451349150580E-01f -1.02979262193565E-01f *X;\n    WW5 = 6.66713443086877E-02f -5.73782817488315E-02f *X;\n  } else if (X < 1.f){\n    RT1 = ((((((-4.46679165328413E-11f*X+1.21879111988031E-09f)*X-\n                2.62975022612104E-08f )*X+5.15106194905897E-07f )*X-\n            9.27933625824749E-06f )*X+1.51794097682482E-04f )*X-\n        2.15865967920301E-03f )*X+2.26659266316985E-02f;\n    RT2 = (((((( 1.93117331714174E-10f*X-4.57267589660699E-09f)*X+\n                2.48339908218932E-08f )*X+1.50716729438474E-06f )*X-\n            6.07268757707381E-05f )*X+1.37506939145643E-03f )*X-\n        2.20258754419939E-02f )*X+2.31271692140905E-01f;\n    RT3 = ((((( 4.84989776180094E-09f*X+1.31538893944284E-07f)*X-\n              2.766753852879E-06f)*X-7.651163510626E-05f)*X+\n          4.033058545972E-03f)*X-8.16520022916145E-02f )*X+\n      8.57346024118779E-01f;\n    RT4 = ((((-2.48581772214623E-07f*X-4.34482635782585E-06f)*X-\n            7.46018257987630E-07f )*X+1.01210776517279E-02f )*X-\n        2.83193369640005E-01f )*X+2.97353038120345E+00f;\n    RT5 = (((((-8.92432153868554E-09f*X+1.77288899268988E-08f)*X+\n              3.040754680666E-06f)*X+1.058229325071E-04f)*X+\n          4.596379534985E-02f)*X-1.75382723579114E+00f )*X+\n      1.84151859759049E+01f;\n    WW1 = ((((((-2.03822632771791E-09f*X+3.89110229133810E-08f)*X-\n                5.84914787904823E-07f )*X+8.30316168666696E-06f )*X-\n            1.13218402310546E-04f )*X+1.49128888586790E-03f )*X-\n        1.96867576904816E-02f )*X+2.95524224714749E-01f;\n    WW2 = ((((((( 8.62848118397570E-09f*X-1.38975551148989E-07f)*X+\n                  1.602894068228E-06f)*X-1.646364300836E-05f)*X+\n              1.538445806778E-04f)*X-1.28848868034502E-03f )*X+\n          9.38866933338584E-03f )*X-5.61737590178812E-02f )*X+\n      2.69266719309991E-01f;\n    WW3 = ((((((((-9.41953204205665E-09f*X+1.47452251067755E-07f)*X-\n                    1.57456991199322E-06f )*X+1.45098401798393E-05f )*X-\n                1.18858834181513E-04f )*X+8.53697675984210E-04f )*X-\n            5.22877807397165E-03f )*X+2.60854524809786E-02f )*X-\n        9.71152726809059E-02f )*X+2.19086362515979E-01f;\n    WW4 = ((((((((-3.84961617022042E-08f*X+5.66595396544470E-07f)*X-\n                    5.52351805403748E-06f )*X+4.53160377546073E-05f )*X-\n                3.22542784865557E-04f )*X+1.95682017370967E-03f )*X-\n            9.77232537679229E-03f )*X+3.79455945268632E-02f )*X-\n        1.02979262192227E-01f )*X+1.49451349150573E-01f;\n    WW5 = ((((((((( 4.09594812521430E-09f*X-6.47097874264417E-08f)*X+\n                      6.743541482689E-07f)*X-5.917993920224E-06f)*X+\n                  4.531969237381E-05f)*X-2.99102856679638E-04f )*X+\n              1.65695765202643E-03f )*X-7.40671222520653E-03f )*X+\n          2.50889946832192E-02f )*X-5.73782817487958E-02f )*X+\n      6.66713443086877E-02f;\n  } else if (X < 5.f) {\n    Y = X-3.0E+00f;\n    RT1 = ((((((((-2.58163897135138E-14f*Y+8.14127461488273E-13f)*Y-\n                    2.11414838976129E-11f )*Y+5.09822003260014E-10f )*Y-\n                1.16002134438663E-08f )*Y+2.46810694414540E-07f )*Y-\n            4.92556826124502E-06f )*Y+9.02580687971053E-05f )*Y-\n        1.45190025120726E-03f )*Y+1.73416786387475E-02f;\n    RT2 = ((((((((( 1.04525287289788E-14f*Y+5.44611782010773E-14f)*Y-\n                      4.831059411392E-12f)*Y+1.136643908832E-10f)*Y-\n                  1.104373076913E-09f)*Y-2.35346740649916E-08f )*Y+\n              1.43772622028764E-06f )*Y-4.23405023015273E-05f )*Y+\n          9.12034574793379E-04f )*Y-1.52479441718739E-02f )*Y+\n      1.76055265928744E-01f;\n    RT3 = (((((((((-6.89693150857911E-14f*Y+5.92064260918861E-13f)*Y+\n                      1.847170956043E-11f)*Y-3.390752744265E-10f)*Y-\n                  2.995532064116E-09f)*Y+1.57456141058535E-07f )*Y-\n              3.95859409711346E-07f )*Y-9.58924580919747E-05f )*Y+\n          3.23551502557785E-03f )*Y-5.97587007636479E-02f )*Y+\n      6.46432853383057E-01f;\n    RT4 = ((((((((-3.61293809667763E-12f*Y-2.70803518291085E-11f)*Y+\n                    8.83758848468769E-10f )*Y+1.59166632851267E-08f )*Y-\n                1.32581997983422E-07f )*Y-7.60223407443995E-06f )*Y-\n            7.41019244900952E-05f )*Y+9.81432631743423E-03f )*Y-\n        2.23055570487771E-01f )*Y+2.21460798080643E+00f;\n    RT5 = ((((((((( 7.12332088345321E-13f*Y+3.16578501501894E-12f)*Y-\n                      8.776668218053E-11f)*Y-2.342817613343E-09f)*Y-\n                  3.496962018025E-08f)*Y-3.03172870136802E-07f )*Y+\n              1.50511293969805E-06f )*Y+1.37704919387696E-04f )*Y+\n          4.70723869619745E-02f )*Y-1.47486623003693E+00f )*Y+\n      1.35704792175847E+01f;\n    WW1 = ((((((((( 1.04348658616398E-13f*Y-1.94147461891055E-12f)*Y+\n                      3.485512360993E-11f)*Y-6.277497362235E-10f)*Y+\n                  1.100758247388E-08f)*Y-1.88329804969573E-07f )*Y+\n              3.12338120839468E-06f )*Y-5.04404167403568E-05f )*Y+\n          8.00338056610995E-04f )*Y-1.30892406559521E-02f )*Y+\n      2.47383140241103E-01f;\n    WW2 = ((((((((((( 3.23496149760478E-14f*Y-5.24314473469311E-13f)*Y+\n                          7.743219385056E-12f)*Y-1.146022750992E-10f)*Y+\n                      1.615238462197E-09f)*Y-2.15479017572233E-08f )*Y+\n                  2.70933462557631E-07f )*Y-3.18750295288531E-06f )*Y+\n              3.47425221210099E-05f )*Y-3.45558237388223E-04f )*Y+\n          3.05779768191621E-03f )*Y-2.29118251223003E-02f )*Y+\n      1.59834227924213E-01f;\n    WW3 = ((((((((((((-3.42790561802876E-14f*Y+5.26475736681542E-13f)*Y-\n                            7.184330797139E-12f)*Y+9.763932908544E-11f)*Y-\n                        1.244014559219E-09f)*Y+1.472744068942E-08f)*Y-\n                    1.611749975234E-07f)*Y+1.616487851917E-06f)*Y-\n                1.46852359124154E-05f )*Y+1.18900349101069E-04f )*Y-\n            8.37562373221756E-04f )*Y+4.93752683045845E-03f )*Y-\n        2.25514728915673E-02f )*Y+6.95211812453929E-02f;\n    WW4 = ((((((((((((( 1.04072340345039E-14f*Y-1.60808044529211E-13f)*\n                              Y+2.183534866798E-12f)*Y-2.939403008391E-11f)*Y+\n                          3.679254029085E-10f)*Y-4.23775673047899E-09f )*Y+\n                      4.46559231067006E-08f )*Y-4.26488836563267E-07f )*Y+\n                  3.64721335274973E-06f )*Y-2.74868382777722E-05f )*Y+\n              1.78586118867488E-04f )*Y-9.68428981886534E-04f )*Y+\n          4.16002324339929E-03f )*Y-1.28290192663141E-02f )*Y+\n      2.22353727685016E-02f;\n    WW5 = ((((((((((((((-8.16770412525963E-16f*Y+1.31376515047977E-14f)*\n                                Y-1.856950818865E-13f)*Y+2.596836515749E-12f)*Y-\n                            3.372639523006E-11f)*Y+4.025371849467E-10f)*Y-\n                        4.389453269417E-09f)*Y+4.332753856271E-08f)*Y-\n                    3.82673275931962E-07f )*Y+2.98006900751543E-06f )*Y-\n                2.00718990300052E-05f )*Y+1.13876001386361E-04f )*Y-\n            5.23627942443563E-04f )*Y+1.83524565118203E-03f )*Y-\n        4.37785737450783E-03f )*Y+5.36963805223095E-03f;\n  } else if (X < 10.f) {\n    Y = X-7.5E+00f;\n    RT1 = ((((((((-1.13825201010775E-14f*Y+1.89737681670375E-13f)*Y-\n                    4.81561201185876E-12f )*Y+1.56666512163407E-10f )*Y-\n                3.73782213255083E-09f )*Y+9.15858355075147E-08f )*Y-\n            2.13775073585629E-06f )*Y+4.56547356365536E-05f )*Y-\n        8.68003909323740E-04f )*Y+1.22703754069176E-02f;\n    RT2 = (((((((((-3.67160504428358E-15f*Y+1.27876280158297E-14f)*Y-\n                      1.296476623788E-12f)*Y+1.477175434354E-11f)*Y+\n                  5.464102147892E-10f)*Y-2.42538340602723E-08f )*Y+\n              8.20460740637617E-07f )*Y-2.20379304598661E-05f )*Y+\n          4.90295372978785E-04f )*Y-9.14294111576119E-03f )*Y+\n      1.22590403403690E-01f;\n    RT3 = ((((((((( 1.39017367502123E-14f*Y-6.96391385426890E-13f)*Y+\n                      1.176946020731E-12f)*Y+1.725627235645E-10f)*Y-\n                  3.686383856300E-09f)*Y+2.87495324207095E-08f )*Y+\n              1.71307311000282E-06f )*Y-7.94273603184629E-05f )*Y+\n          2.00938064965897E-03f )*Y-3.63329491677178E-02f )*Y+\n      4.34393683888443E-01f;\n    RT4 = ((((((((((-1.27815158195209E-14f*Y+1.99910415869821E-14f)*Y+\n                        3.753542914426E-12f)*Y-2.708018219579E-11f)*Y-\n                    1.190574776587E-09f)*Y+1.106696436509E-08f)*Y+\n                3.954955671326E-07f)*Y-4.398596059588E-06f)*Y-\n            2.01087998907735E-04f )*Y+7.89092425542937E-03f )*Y-\n        1.42056749162695E-01f )*Y+1.39964149420683E+00f;\n    RT5 = ((((((((((-1.19442341030461E-13f*Y-2.34074833275956E-12f)*Y+\n                        6.861649627426E-12f)*Y+6.082671496226E-10f)*Y+\n                    5.381160105420E-09f)*Y-6.253297138700E-08f)*Y-\n                2.135966835050E-06f)*Y-2.373394341886E-05f)*Y+\n            2.88711171412814E-06f )*Y+4.85221195290753E-02f )*Y-\n        1.04346091985269E+00f )*Y+7.89901551676692E+00f;\n    WW1 = ((((((((( 7.95526040108997E-15f*Y-2.48593096128045E-13f)*Y+\n                      4.761246208720E-12f)*Y-9.535763686605E-11f)*Y+\n                  2.225273630974E-09f)*Y-4.49796778054865E-08f )*Y+\n              9.17812870287386E-07f )*Y-1.86764236490502E-05f )*Y+\n          3.76807779068053E-04f )*Y-8.10456360143408E-03f )*Y+\n      2.01097936411496E-01f;\n    WW2 = ((((((((((( 1.25678686624734E-15f*Y-2.34266248891173E-14f)*Y+\n                          3.973252415832E-13f)*Y-6.830539401049E-12f)*Y+\n                      1.140771033372E-10f)*Y-1.82546185762009E-09f )*Y+\n                  2.77209637550134E-08f )*Y-4.01726946190383E-07f )*Y+\n              5.48227244014763E-06f )*Y-6.95676245982121E-05f )*Y+\n          8.05193921815776E-04f )*Y-8.15528438784469E-03f )*Y+\n      9.71769901268114E-02f;\n    WW3 = ((((((((((((-8.20929494859896E-16f*Y+1.37356038393016E-14f)*Y-\n                            2.022863065220E-13f)*Y+3.058055403795E-12f)*Y-\n                        4.387890955243E-11f)*Y+5.923946274445E-10f)*Y-\n                    7.503659964159E-09f)*Y+8.851599803902E-08f)*Y-\n                9.65561998415038E-07f )*Y+9.60884622778092E-06f )*Y-\n            8.56551787594404E-05f )*Y+6.66057194311179E-04f )*Y-\n        4.17753183902198E-03f )*Y+2.25443826852447E-02f;\n    WW4 = ((((((((((((((-1.08764612488790E-17f*Y+1.85299909689937E-16f)*\n                                Y-2.730195628655E-15f)*Y+4.127368817265E-14f)*Y-\n                            5.881379088074E-13f)*Y+7.805245193391E-12f)*Y-\n                        9.632707991704E-11f)*Y+1.099047050624E-09f)*Y-\n                    1.15042731790748E-08f )*Y+1.09415155268932E-07f )*Y-\n                9.33687124875935E-07f )*Y+7.02338477986218E-06f )*Y-\n            4.53759748787756E-05f )*Y+2.41722511389146E-04f )*Y-\n        9.75935943447037E-04f )*Y+2.57520532789644E-03f;\n    WW5 = ((((((((((((((( 7.28996979748849E-19f*Y-1.26518146195173E-17f)\n                                  *Y+1.886145834486E-16f)*Y-2.876728287383E-15f)*Y+\n                              4.114588668138E-14f)*Y-5.44436631413933E-13f )*Y+\n                          6.64976446790959E-12f )*Y-7.44560069974940E-11f )*Y+\n                      7.57553198166848E-10f )*Y-6.92956101109829E-09f )*Y+\n                  5.62222859033624E-08f )*Y-3.97500114084351E-07f )*Y+\n              2.39039126138140E-06f )*Y-1.18023950002105E-05f )*Y+\n          4.52254031046244E-05f )*Y-1.21113782150370E-04f )*Y+\n      1.75013126731224E-04f;\n  } else if (X < 15.f) {\n    Y = X-12.5E+00f;\n    RT1 = ((((((((((-4.16387977337393E-17f*Y+7.20872997373860E-16f)*Y+\n                        1.395993802064E-14f)*Y+3.660484641252E-14f)*Y-\n                    4.154857548139E-12f)*Y+2.301379846544E-11f)*Y-\n                1.033307012866E-09f)*Y+3.997777641049E-08f)*Y-\n            9.35118186333939E-07f )*Y+2.38589932752937E-05f )*Y-\n        5.35185183652937E-04f )*Y+8.85218988709735E-03f;\n    RT2 = ((((((((((-4.56279214732217E-16f*Y+6.24941647247927E-15f)*Y+\n                        1.737896339191E-13f)*Y+8.964205979517E-14f)*Y-\n                    3.538906780633E-11f)*Y+9.561341254948E-11f)*Y-\n                9.772831891310E-09f)*Y+4.240340194620E-07f)*Y-\n            1.02384302866534E-05f )*Y+2.57987709704822E-04f )*Y-\n        5.54735977651677E-03f )*Y+8.68245143991948E-02f;\n    RT3 = ((((((((((-2.52879337929239E-15f*Y+2.13925810087833E-14f)*Y+\n                        7.884307667104E-13f)*Y-9.023398159510E-13f)*Y-\n                    5.814101544957E-11f)*Y-1.333480437968E-09f)*Y-\n                2.217064940373E-08f)*Y+1.643290788086E-06f)*Y-\n            4.39602147345028E-05f )*Y+1.08648982748911E-03f )*Y-\n        2.13014521653498E-02f )*Y+2.94150684465425E-01f;\n    RT4 = ((((((((((-6.42391438038888E-15f*Y+5.37848223438815E-15f)*Y+\n                        8.960828117859E-13f)*Y+5.214153461337E-11f)*Y-\n                    1.106601744067E-10f)*Y-2.007890743962E-08f)*Y+\n                1.543764346501E-07f)*Y+4.520749076914E-06f)*Y-\n            1.88893338587047E-04f )*Y+4.73264487389288E-03f )*Y-\n        7.91197893350253E-02f )*Y+8.60057928514554E-01f;\n    RT5 = (((((((((((-2.24366166957225E-14f*Y+4.87224967526081E-14f)*Y+\n                          5.587369053655E-12f)*Y-3.045253104617E-12f)*Y-\n                      1.223983883080E-09f)*Y-2.05603889396319E-09f )*Y+\n                  2.58604071603561E-07f )*Y+1.34240904266268E-06f )*Y-\n              5.72877569731162E-05f )*Y-9.56275105032191E-04f )*Y+\n          4.23367010370921E-02f )*Y-5.76800927133412E-01f )*Y+\n      3.87328263873381E+00f;\n    WW1 = ((((((((( 8.98007931950169E-15f*Y+7.25673623859497E-14f)*Y+\n                      5.851494250405E-14f)*Y-4.234204823846E-11f)*Y+\n                  3.911507312679E-10f)*Y-9.65094802088511E-09f )*Y+\n              3.42197444235714E-07f )*Y-7.51821178144509E-06f )*Y+\n          1.94218051498662E-04f )*Y-5.38533819142287E-03f )*Y+\n      1.68122596736809E-01f;\n    WW2 = ((((((((((-1.05490525395105E-15f*Y+1.96855386549388E-14f)*Y-\n                        5.500330153548E-13f)*Y+1.003849567976E-11f)*Y-\n                    1.720997242621E-10f)*Y+3.533277061402E-09f)*Y-\n                6.389171736029E-08f)*Y+1.046236652393E-06f)*Y-\n            1.73148206795827E-05f )*Y+2.57820531617185E-04f )*Y-\n        3.46188265338350E-03f )*Y+7.03302497508176E-02f;\n    WW3 = ((((((((((( 3.60020423754545E-16f*Y-6.24245825017148E-15f)*Y+\n                          9.945311467434E-14f)*Y-1.749051512721E-12f)*Y+\n                      2.768503957853E-11f)*Y-4.08688551136506E-10f )*Y+\n                  6.04189063303610E-09f )*Y-8.23540111024147E-08f )*Y+\n              1.01503783870262E-06f )*Y-1.20490761741576E-05f )*Y+\n          1.26928442448148E-04f )*Y-1.05539461930597E-03f )*Y+\n      1.15543698537013E-02f;\n    WW4 = ((((((((((((( 2.51163533058925E-18f*Y-4.31723745510697E-17f)*\n                              Y+6.557620865832E-16f)*Y-1.016528519495E-14f)*Y+\n                          1.491302084832E-13f)*Y-2.06638666222265E-12f )*Y+\n                      2.67958697789258E-11f )*Y-3.23322654638336E-10f )*Y+\n                  3.63722952167779E-09f )*Y-3.75484943783021E-08f )*Y+\n              3.49164261987184E-07f )*Y-2.92658670674908E-06f )*Y+\n          2.12937256719543E-05f )*Y-1.19434130620929E-04f )*Y+\n      6.45524336158384E-04f;\n    WW5 = ((((((((((((((-1.29043630202811E-19f*Y+2.16234952241296E-18f)*\n                                Y-3.107631557965E-17f)*Y+4.570804313173E-16f)*Y-\n                            6.301348858104E-15f)*Y+8.031304476153E-14f)*Y-\n                        9.446196472547E-13f)*Y+1.018245804339E-11f)*Y-\n                    9.96995451348129E-11f )*Y+8.77489010276305E-10f )*Y-\n                6.84655877575364E-09f )*Y+4.64460857084983E-08f )*Y-\n            2.66924538268397E-07f )*Y+1.24621276265907E-06f )*Y-\n        4.30868944351523E-06f )*Y+9.94307982432868E-06f;\n  } else if (X < 20.f){\n    Y = X-17.5E+00f;\n    RT1 = (((((((((( 1.91875764545740E-16f*Y+7.8357401095707E-16f)*Y-\n                        3.260875931644E-14f)*Y-1.186752035569E-13f)*Y+\n                    4.275180095653E-12f)*Y+3.357056136731E-11f)*Y-\n                1.123776903884E-09f)*Y+1.231203269887E-08f)*Y-\n            3.99851421361031E-07f )*Y+1.45418822817771E-05f )*Y-\n        3.49912254976317E-04f )*Y+6.67768703938812E-03f;\n    RT2 = (((((((((( 2.02778478673555E-15f*Y+1.01640716785099E-14f)*Y-\n                        3.385363492036E-13f)*Y-1.615655871159E-12f)*Y+\n                    4.527419140333E-11f)*Y+3.853670706486E-10f)*Y-\n                1.184607130107E-08f)*Y+1.347873288827E-07f)*Y-\n            4.47788241748377E-06f )*Y+1.54942754358273E-04f )*Y-\n        3.55524254280266E-03f )*Y+6.44912219301603E-02f;\n    RT3 = (((((((((( 7.79850771456444E-15f*Y+6.00464406395001E-14f)*Y-\n                        1.249779730869E-12f)*Y-1.020720636353E-11f)*Y+\n                    1.814709816693E-10f)*Y+1.766397336977E-09f)*Y-\n                4.603559449010E-08f)*Y+5.863956443581E-07f)*Y-\n            2.03797212506691E-05f )*Y+6.31405161185185E-04f )*Y-\n        1.30102750145071E-02f )*Y+2.10244289044705E-01f;\n    RT4 = (((((((((((-2.92397030777912E-15f*Y+1.94152129078465E-14f)*Y+\n                          4.859447665850E-13f)*Y-3.217227223463E-12f)*Y-\n                      7.484522135512E-11f)*Y+7.19101516047753E-10f )*Y+\n                  6.88409355245582E-09f )*Y-1.44374545515769E-07f )*Y+\n              2.74941013315834E-06f )*Y-1.02790452049013E-04f )*Y+\n          2.59924221372643E-03f )*Y-4.35712368303551E-02f )*Y+\n      5.62170709585029E-01f;\n    RT5 = ((((((((((( 1.17976126840060E-14f*Y+1.24156229350669E-13f)*Y-\n                          3.892741622280E-12f)*Y-7.755793199043E-12f)*Y+\n                      9.492190032313E-10f)*Y-4.98680128123353E-09f )*Y-\n                  1.81502268782664E-07f )*Y+2.69463269394888E-06f )*Y+\n              2.50032154421640E-05f )*Y-1.33684303917681E-03f )*Y+\n          2.29121951862538E-02f )*Y-2.45653725061323E-01f )*Y+\n      1.89999883453047E+00f;\n    WW1 = (((((((((( 1.74841995087592E-15f*Y-6.95671892641256E-16f)*Y-\n                        3.000659497257E-13f)*Y+2.021279817961E-13f)*Y+\n                    3.853596935400E-11f)*Y+1.461418533652E-10f)*Y-\n                1.014517563435E-08f)*Y+1.132736008979E-07f)*Y-\n            2.86605475073259E-06f )*Y+1.21958354908768E-04f )*Y-\n        3.86293751153466E-03f )*Y+1.45298342081522E-01f;\n    WW2 = ((((((((((-1.11199320525573E-15f*Y+1.85007587796671E-15f)*Y+\n                        1.220613939709E-13f)*Y+1.275068098526E-12f)*Y-\n                    5.341838883262E-11f)*Y+6.161037256669E-10f)*Y-\n                1.009147879750E-08f)*Y+2.907862965346E-07f)*Y-\n            6.12300038720919E-06f )*Y+1.00104454489518E-04f )*Y-\n        1.80677298502757E-03f )*Y+5.78009914536630E-02f;\n    WW3 = ((((((((((-9.49816486853687E-16f*Y+6.67922080354234E-15f)*Y+\n                        2.606163540537E-15f)*Y+1.983799950150E-12f)*Y-\n                    5.400548574357E-11f)*Y+6.638043374114E-10f)*Y-\n                8.799518866802E-09f)*Y+1.791418482685E-07f)*Y-\n            2.96075397351101E-06f )*Y+3.38028206156144E-05f )*Y-\n        3.58426847857878E-04f )*Y+8.39213709428516E-03f;\n    WW4 = ((((((((((( 1.33829971060180E-17f*Y-3.44841877844140E-16f)*Y+\n                          4.745009557656E-15f)*Y-6.033814209875E-14f)*Y+\n                      1.049256040808E-12f)*Y-1.70859789556117E-11f )*Y+\n                  2.15219425727959E-10f )*Y-2.52746574206884E-09f )*Y+\n              3.27761714422960E-08f )*Y-3.90387662925193E-07f )*Y+\n          3.46340204593870E-06f )*Y-2.43236345136782E-05f )*Y+\n      3.54846978585226E-04f;\n    WW5 = ((((((((((((( 2.69412277020887E-20f*Y-4.24837886165685E-19f)*\n                              Y+6.030500065438E-18f)*Y-9.069722758289E-17f)*Y+\n                          1.246599177672E-15f)*Y-1.56872999797549E-14f )*Y+\n                      1.87305099552692E-13f )*Y-2.09498886675861E-12f )*Y+\n                  2.11630022068394E-11f )*Y-1.92566242323525E-10f )*Y+\n              1.62012436344069E-09f )*Y-1.23621614171556E-08f )*Y+\n          7.72165684563049E-08f )*Y-3.59858901591047E-07f )*Y+\n      2.43682618601000E-06f;\n  } else if (X < 25.f) {\n    Y = X-22.5E+00f;\n    RT1 = (((((((((-1.13927848238726E-15f*Y+7.39404133595713E-15f)*Y+\n                      1.445982921243E-13f)*Y-2.676703245252E-12f)*Y+\n                  5.823521627177E-12f)*Y+2.17264723874381E-10f )*Y+\n              3.56242145897468E-09f )*Y-3.03763737404491E-07f )*Y+\n          9.46859114120901E-06f )*Y-2.30896753853196E-04f )*Y+\n      5.24663913001114E-03f;\n    RT2 = (((((((((( 2.89872355524581E-16f*Y-1.22296292045864E-14f)*Y+\n                        6.184065097200E-14f)*Y+1.649846591230E-12f)*Y-\n                    2.729713905266E-11f)*Y+3.709913790650E-11f)*Y+\n                2.216486288382E-09f)*Y+4.616160236414E-08f)*Y-\n            3.32380270861364E-06f )*Y+9.84635072633776E-05f )*Y-\n        2.30092118015697E-03f )*Y+5.00845183695073E-02f;\n    RT3 = (((((((((( 1.97068646590923E-15f*Y-4.89419270626800E-14f)*Y+\n                        1.136466605916E-13f)*Y+7.546203883874E-12f)*Y-\n                    9.635646767455E-11f)*Y-8.295965491209E-11f)*Y+\n                7.534109114453E-09f)*Y+2.699970652707E-07f)*Y-\n            1.42982334217081E-05f )*Y+3.78290946669264E-04f )*Y-\n        8.03133015084373E-03f )*Y+1.58689469640791E-01f;\n    RT4 = (((((((((( 1.33642069941389E-14f*Y-1.55850612605745E-13f)*Y-\n                        7.522712577474E-13f)*Y+3.209520801187E-11f)*Y-\n                    2.075594313618E-10f)*Y-2.070575894402E-09f)*Y+\n                7.323046997451E-09f)*Y+1.851491550417E-06f)*Y-\n            6.37524802411383E-05f )*Y+1.36795464918785E-03f )*Y-\n        2.42051126993146E-02f )*Y+3.97847167557815E-01f;\n    RT5 = ((((((((((-6.07053986130526E-14f*Y+1.04447493138843E-12f)*Y-\n                        4.286617818951E-13f)*Y-2.632066100073E-10f)*Y+\n                    4.804518986559E-09f)*Y-1.835675889421E-08f)*Y-\n                1.068175391334E-06f)*Y+3.292234974141E-05f)*Y-\n            5.94805357558251E-04f )*Y+8.29382168612791E-03f )*Y-\n        9.93122509049447E-02f )*Y+1.09857804755042E+00f;\n    WW1 = (((((((((-9.10338640266542E-15f*Y+1.00438927627833E-13f)*Y+\n                      7.817349237071E-13f)*Y-2.547619474232E-11f)*Y+\n                  1.479321506529E-10f)*Y+1.52314028857627E-09f )*Y+\n              9.20072040917242E-09f )*Y-2.19427111221848E-06f )*Y+\n          8.65797782880311E-05f )*Y-2.82718629312875E-03f )*Y+\n      1.28718310443295E-01f;\n    WW2 = ((((((((( 5.52380927618760E-15f*Y-6.43424400204124E-14f)*Y-\n                      2.358734508092E-13f)*Y+8.261326648131E-12f)*Y+\n                  9.229645304956E-11f)*Y-5.68108973828949E-09f )*Y+\n              1.22477891136278E-07f )*Y-2.11919643127927E-06f )*Y+\n          4.23605032368922E-05f )*Y-1.14423444576221E-03f )*Y+\n      5.06607252890186E-02f;\n    WW3 = ((((((((( 3.99457454087556E-15f*Y-5.11826702824182E-14f)*Y-\n                      4.157593182747E-14f)*Y+4.214670817758E-12f)*Y+\n                  6.705582751532E-11f)*Y-3.36086411698418E-09f )*Y+\n              6.07453633298986E-08f )*Y-7.40736211041247E-07f )*Y+\n          8.84176371665149E-06f )*Y-1.72559275066834E-04f )*Y+\n      7.16639814253567E-03f;\n    WW4 = (((((((((((-2.14649508112234E-18f*Y-2.45525846412281E-18f)*Y+\n                          6.126212599772E-16f)*Y-8.526651626939E-15f)*Y+\n                      4.826636065733E-14f)*Y-3.39554163649740E-13f )*Y+\n                  1.67070784862985E-11f )*Y-4.42671979311163E-10f )*Y+\n              6.77368055908400E-09f )*Y-7.03520999708859E-08f )*Y+\n          6.04993294708874E-07f )*Y-7.80555094280483E-06f )*Y+\n      2.85954806605017E-04f;\n    WW5 = ((((((((((((-5.63938733073804E-21f*Y+6.92182516324628E-20f)*Y-\n                            1.586937691507E-18f)*Y+3.357639744582E-17f)*Y-\n                        4.810285046442E-16f)*Y+5.386312669975E-15f)*Y-\n                    6.117895297439E-14f)*Y+8.441808227634E-13f)*Y-\n                1.18527596836592E-11f )*Y+1.36296870441445E-10f )*Y-\n            1.17842611094141E-09f )*Y+7.80430641995926E-09f )*Y-\n        5.97767417400540E-08f )*Y+1.65186146094969E-06f;\n  } else if (X < 40.f) {\n    WW1 = sqrtf(PIE4/X);\n    E = expf(-X);\n    RT1 = ((((((((-1.73363958895356E-06f*X+1.19921331441483E-04f)*X -\n                    1.59437614121125E-02f)*X+1.13467897349442E+00f)*X -\n                4.47216460864586E+01f)*X+1.06251216612604E+03f)*X -\n            1.52073917378512E+04f)*X+1.20662887111273E+05f)*X -\n        4.07186366852475E+05f)*E + R15/(X-R15);\n    RT2 = ((((((((-1.60102542621710E-05f*X+1.10331262112395E-03f)*X -\n                    1.50043662589017E-01f)*X+1.05563640866077E+01f)*X -\n                4.10468817024806E+02f)*X+9.62604416506819E+03f)*X -\n            1.35888069838270E+05f)*X+1.06107577038340E+06f)*X -\n        3.51190792816119E+06f)*E + R25/(X-R25);\n    RT3 = ((((((((-4.48880032128422E-05f*X+2.69025112122177E-03f)*X -\n                    4.01048115525954E-01f)*X+2.78360021977405E+01f)*X -\n                1.04891729356965E+03f)*X+2.36985942687423E+04f)*X -\n            3.19504627257548E+05f)*X+2.34879693563358E+06f)*X -\n        7.16341568174085E+06f)*E + R35/(X-R35);\n    RT4 = ((((((((-6.38526371092582E-05f*X-2.29263585792626E-03f)*X -\n                    7.65735935499627E-02f)*X+9.12692349152792E+00f)*X -\n                2.32077034386717E+02f)*X+2.81839578728845E+02f)*X +\n            9.59529683876419E+04f)*X-1.77638956809518E+06f)*X +\n        1.02489759645410E+07f)*E + R45/(X-R45);\n    RT5 = ((((((((-3.59049364231569E-05f*X-2.25963977930044E-02f)*X +\n                    1.12594870794668E+00f)*X-4.56752462103909E+01f)*X +\n                1.05804526830637E+03f)*X-1.16003199605875E+04f)*X -\n            4.07297627297272E+04f)*X+2.22215528319857E+06f)*X -\n        1.61196455032613E+07f)*E + R55/(X-R55);\n    WW5 = (((((((((-4.61100906133970E-10f*X+1.43069932644286E-07f)*X -\n                      1.63960915431080E-05f)*X+1.15791154612838E-03f)*X -\n                  5.30573476742071E-02f)*X+1.61156533367153E+00f)*X -\n              3.23248143316007E+01f)*X+4.12007318109157E+02f)*X -\n          3.02260070158372E+03f)*X+9.71575094154768E+03f)*E + W55*WW1;\n    WW4 = (((((((((-2.40799435809950E-08f*X+8.12621667601546E-06f)*X -\n                      9.04491430884113E-04f)*X+6.37686375770059E-02f)*X -\n                  2.96135703135647E+00f)*X+9.15142356996330E+01f)*X -\n              1.86971865249111E+03f)*X+2.42945528916947E+04f)*X -\n          1.81852473229081E+05f)*X+5.96854758661427E+05f)*E + W45*WW1;\n    WW3 = (((((((( 1.83574464457207E-05f*X-1.54837969489927E-03f)*X +\n                    1.18520453711586E-01f)*X-6.69649981309161E+00f)*X +\n                2.44789386487321E+02f)*X-5.68832664556359E+03f)*X +\n            8.14507604229357E+04f)*X-6.55181056671474E+05f)*X +\n        2.26410896607237E+06f)*E + W35*WW1;\n    WW2 = (((((((( 2.77778345870650E-05f*X-2.22835017655890E-03f)*X +\n                    1.61077633475573E-01f)*X-8.96743743396132E+00f)*X +\n                3.28062687293374E+02f)*X-7.65722701219557E+03f)*X +\n            1.10255055017664E+05f)*X-8.92528122219324E+05f)*X +\n        3.10638627744347E+06f)*E + W25*WW1;\n    WW1 = WW1-0.01962E+00f*E-WW2-WW3-WW4-WW5;\n  } else if (X < 59.f) {\n    WW1 = sqrtf(PIE4/X);\n    XXX = powf(X,3.f);\n    E = XXX*expf(-X);\n    RT1 = (((-2.43758528330205E-02f*X+2.07301567989771E+00f)*X -\n          6.45964225381113E+01f)*X+7.14160088655470E+02f)*E + R15/(X-R15);\n    RT2 = (((-2.28861955413636E-01f*X+1.93190784733691E+01f)*X -\n          5.99774730340912E+02f)*X+6.61844165304871E+03f)*E + R25/(X-R25);\n    RT3 = (((-6.95053039285586E-01f*X+5.76874090316016E+01f)*X -\n          1.77704143225520E+03f)*X+1.95366082947811E+04f)*E + R35/(X-R35);\n    RT4 = (((-1.58072809087018E+00f*X+1.27050801091948E+02f)*X -\n          3.86687350914280E+03f)*X+4.23024828121420E+04f)*E + R45/(X-R45);\n    RT5 = (((-3.33963830405396E+00f*X+2.51830424600204E+02f)*X -\n          7.57728527654961E+03f)*X+8.21966816595690E+04f)*E + R55/(X-R55);\n    E = XXX*E;\n    WW5 = (( 1.35482430510942E-08f*X-3.27722199212781E-07f)*X +\n        2.41522703684296E-06f)*E + W55*WW1;\n    WW4 = (( 1.23464092261605E-06f*X-3.55224564275590E-05f)*X +\n        3.03274662192286E-04f)*E + W45*WW1;\n    WW3 = (( 1.34547929260279E-05f*X-4.19389884772726E-04f)*X +\n        3.87706687610809E-03f)*E + W35*WW1;\n    WW2 = (( 2.09539509123135E-05f*X-6.87646614786982E-04f)*X +\n        6.68743788585688E-03f)*E + W25*WW1;\n    WW1 = WW1-WW2-WW3-WW4-WW5;\n  } else {\n    WW1 = sqrtf(PIE4/X);\n    RT1 = R15/(X-R15);\n    RT2 = R25/(X-R25);\n    RT3 = R35/(X-R35);\n    RT4 = R45/(X-R45);\n    RT5 = R55/(X-R55);\n    WW2 = W25*WW1;\n    WW3 = W35*WW1;\n    WW4 = W45*WW1;\n    WW5 = W55*WW1;\n    WW1 = WW1-WW2-WW3-WW4-WW5;\n  }\n  roots[0] = RT1;\n  weights[0] = WW1;\n  roots[1] = RT2;\n  weights[1] = WW2;\n  roots[2] = RT3;\n  weights[2] = WW3;\n  roots[3] = RT4;\n  weights[3] = WW4;\n  roots[4] = RT5;\n  weights[4] = WW5;\n  return;\n}\n\n__device__ void cuda_Root6(int n,float X, float roots[], float weights[]){\n  // Root6 not implemented yet\n  return;\n}\n\n__device__ float cuda_Int1d(int i, int j, int k, int l,\n    float xi, float xj, float xk, float xl,\n    float alpha_ij_A, float alpha_kl_B, float sqrt_AB,\n    float A, float B, float Px, float Qx,\n    float inv_t1, float B00, float B1, float B1p, \n    float G[][MAXROOTS])\n{\n  // Form G(n,m)=I(n,0,m,0) intermediate values for a Rys polynomial \n  int n = i+j;\n  int m = k+l;\n\n  float xij = xi-xj;\n  float xkl = xk-xl;\n\n  // RecurFactorsGamess\n  float C  = (Px-xi) * inv_t1 + (B*(Qx-xi)+A*(Px-xi))*B00*2.0;\n  float Cp = (Qx-xk) * inv_t1 + (B*(Qx-xk)+A*(Px-xk))*B00*2.0;\n\n  // ABD eq 11. \n  G[0][0] = (float)M_PI * expf(-alpha_ij_A*xij*xij -alpha_kl_B*xkl*xkl) / sqrt_AB;\n\n  if (n > 0) { G[1][0] = C *G[0][0]; } // ABD eq 15 \n  if (m > 0) { G[0][1] = Cp*G[0][0]; } // ABD eq 16 \n\n  for (int a = 2; a < n+1; ++ a) { G[a][0] = B1 *(a-1)*G[a-2][0] + C *G[a-1][0]; } \n  for (int b = 2; b < m+1; ++ b) { G[0][b] = B1p*(b-1)*G[0][b-2] + Cp*G[0][b-1]; } \n\n  if ((m>0) && (n>0)){\n    for (int a=1; a<n+1; ++a){\n      G[a][1] = a*B00*G[a-1][0] + Cp*G[a][0];\n      for (int b=2; b<m+1; ++b)\n        G[a][b] = B1p*(b-1)*G[a][b-2] + a*B00*G[a-1][b-1] + Cp*G[a][b-1];\n    }\n  }\n\n  // Compute and output I(i,j,k,l) from I(i+j,0,k+l,0) (G) \n  float ijkl = 0.0;\n  for (int m=0; m<l+1; ++m){\n    float ijm0 = 0.0;\n    for (int n=0; n<j+1; ++n) // I(i,j,m,0)<-I(n,0,m,0)  \n      ijm0 += cuda_binomial(j,n)*powf(xij,(float)(j-n))*G[n+i][m+k];\n    ijkl += cuda_binomial(l,m)*powf(xkl,(float)(l-m))*ijm0; // I(i,j,k,l)<-I(i,j,m,0) \n  }\n\n  return ijkl;\n}\n\n__device__ void cuda_Roots(int n, float X, float roots[], float weights[]){\n  if (n <= 3)\n    cuda_Root123(n,X, roots,weights);\n  else if (n==4) \n    cuda_Root4(X, roots,weights);\n  else if (n==5)\n    cuda_Root5(X, roots,weights);\n  else\n    cuda_Root6(n,X, roots,weights);\n  return;\n}\n\ninline __host__ __device__ float4 fabs(float4 v)\n{\n    return make_float4(fabs(v.x), fabs(v.y), fabs(v.z), fabs(v.w));\n}\n\n__device__ int cuda_ij2intindex(int i, int j)\n{\n  if (i < j) {\n    int t = i; i = j; j = t;\n  }\n  return i * (i + 1) / 2 + j;\n}\n\n__device__ float cuda_rys_pbf(const double *ptr_i, const double *ptr_j, \n                              const double *ptr_k, const double *ptr_l)\n{\n  // download xyz, lmn, expon, and coef*norm\n  float xa = (float)ptr_i[0];\n  float ya = (float)ptr_i[1];\n  float za = (float)ptr_i[2];\n  int   la = (int)ptr_i[3];\n  int   ma = (int)ptr_i[4];\n  int   na = (int)ptr_i[5];\n  float alphaa = (float)ptr_i[6];\n  float norma  = (float)ptr_i[7];\n\n  float xb = (float)ptr_j[0];\n  float yb = (float)ptr_j[1];\n  float zb = (float)ptr_j[2];\n  int   lb = (int)ptr_j[3];\n  int   mb = (int)ptr_j[4];\n  int   nb = (int)ptr_j[5];\n  float alphab = (float)ptr_j[6];\n  float normb  = (float)ptr_j[7];\n\n  float xc = (float)ptr_k[0];\n  float yc = (float)ptr_k[1];\n  float zc = (float)ptr_k[2];\n  int   lc = (int)ptr_k[3];\n  int   mc = (int)ptr_k[4];\n  int   nc = (int)ptr_k[5];\n  float alphac = (float)ptr_k[6];\n  float normc  = (float)ptr_k[7];\n\n  float xd = (float)ptr_l[0];\n  float yd = (float)ptr_l[1];\n  float zd = (float)ptr_l[2];\n  int   ld = (int)ptr_l[3];\n  int   md = (int)ptr_l[4];\n  int   nd = (int)ptr_l[5];\n  float alphad = (float)ptr_l[6];\n  float normd  = (float)ptr_l[7];\n\n  // calculate primitive integral [ij|kl]\n  int norder,i;\n  float A,B,xp,yp,zp,xq,yq,zq,X,rho,sum,t,Ix,Iy,Iz;\n\n  norder = (la+ma+na+lb+nb+mb+lc+mc+nc+ld+md+nd)/2 + 1;\n  A = alphaa+alphab; \n  B = alphac+alphad;\n\n  xp = (alphaa*xa+alphab*xb)/A;\n  yp = (alphaa*ya+alphab*yb)/A;\n  zp = (alphaa*za+alphab*zb)/A;\n\n  xq = (alphac*xc+alphad*xd)/B;\n  yq = (alphac*yc+alphad*yd)/B;\n  zq = (alphac*zc+alphad*zd)/B;\n\n  rho = A*B/(A+B);\n  X = rho * ((xp-xq)*(xp-xq)+(yp-yq)*(yp-yq)+(zp-zq)*(zp-zq));\n\n  float alpha_ab_A = alphaa * alphab / A;\n  float alpha_cd_B = alphac * alphad / B;\n  float sqrt_AB = sqrtf(A * B);\n\n  float roots[MAXROOTS],weights[MAXROOTS];\n  float G[MAXROOTS][MAXROOTS];\n\n  cuda_Roots(norder,X,roots,weights); // get currect roots/weights\n\n  sum = 0.;\n  for (i=0; i<norder; ++i){\n    t = roots[i];\n\n    float inv_t1, B00, B1, B1p;\n    inv_t1 = 1.f / (1.f + t);\n    B00 = 0.5f * t/(A+B) * inv_t1;\n    B1  = 0.5f / A * inv_t1 + B00;\n    B1p = 0.5f / B * inv_t1 + B00;\n\n    Ix = cuda_Int1d(la,lb,lc,ld, xa,xb,xc,xd,\n        alpha_ab_A,alpha_cd_B,sqrt_AB, A,B,xp,xq, inv_t1,B00,B1,B1p, G);\n    Iy = cuda_Int1d(ma,mb,mc,md, ya,yb,yc,yd,\n        alpha_ab_A,alpha_cd_B,sqrt_AB, A,B,yp,yq, inv_t1,B00,B1,B1p, G);\n    Iz = cuda_Int1d(na,nb,nc,nd, za,zb,zc,zd,\n        alpha_ab_A,alpha_cd_B,sqrt_AB, A,B,zp,zq, inv_t1,B00,B1,B1p, G);\n    sum = sum + Ix*Iy*Iz*weights[i]; /* ABD eq 5 & 9 */\n  }\n\n  // inv_sqrt_pi_2: 2.0*sqrtf(1.0/M_PI) = 1.12837916709551255856\n  return 1.12837916709551255856f * sqrtf(rho)*norma*normb*normc*normd*sum; /* ABD eq 5 & 9 */\n}\n\n__global__ void cuda_mat_K_PI(\n  const double *__restrict pbf_xlec, \n  const int *__restrict pbf_to_cbf, \n  int n_pbf,\n  const double *__restrict mat_D,\n  double *__restrict mat_K_PI,\n  const double *__restrict mat_Q)\n{\n  __shared__ double elem_K_PI[BLOCKSIZE * BLOCKSIZE];\n\n  // each block scans over [i?|k?] and sum up to a primitive K matrix element\n  int i = blockIdx.x;\n  int k = blockIdx.y;\n\n  // avoid accessing out of bounds elements and make use of ij<=>kl symmetry\n  if (i >= n_pbf || k > i) { return; }\n\n  int ik = cuda_ij2intindex(i,k);\n\n  const double *ptr_i = &pbf_xlec[i * 8];\n  const double *ptr_k = &pbf_xlec[k * 8];\n\n  int a = pbf_to_cbf[i];\n  int c = pbf_to_cbf[k];\n\n  // initialize shared array\n  elem_K_PI[threadIdx.x * BLOCKSIZE + threadIdx.y] = 0.0;\n\n  for (int j = threadIdx.x; j < n_pbf; j += BLOCKSIZE)\n  {\n    int b = pbf_to_cbf[j];\n    int ab = cuda_ij2intindex(a,b);\n    const double *ptr_j = &pbf_xlec[j * 8];\n\n    for (int l = threadIdx.y; l < n_pbf; l += BLOCKSIZE)\n    {\n      int d = pbf_to_cbf[l];\n      int cd = cuda_ij2intindex(c,d);\n      int bd = cuda_ij2intindex(b,d);\n\n      // Schwartz screening\n      if (fabs(mat_Q[ab] * mat_Q[cd] * mat_D[bd]) < SCREEN_THR) { continue; }\n\n      const double *ptr_l = &pbf_xlec[l * 8];\n\n      // calculate ERI\n      double this_eri = cuda_rys_pbf(ptr_i, ptr_j, ptr_k, ptr_l);\n\n      // NOTE: no doubling for off-diagonal elements of D\n      elem_K_PI[threadIdx.x * BLOCKSIZE + threadIdx.y] += this_eri * mat_D[bd];\n    }\n  }\n\n  __syncthreads();\n\n  // only update mat_K_PI on one thread of the block\n  if (0 == threadIdx.x && 0 == threadIdx.y)\n  {\n    mat_K_PI[ik] = 0.0; \n    for (int t1 = 0; t1 < BLOCKSIZE; ++ t1) {\n      for (int t2 = 0; t2 < BLOCKSIZE; ++ t2) {\n        mat_K_PI[ik] += elem_K_PI[t1 * BLOCKSIZE + t2];\n      }\n    }\n  }\n}"
        ]
    },
    "voxelization-cuda": {
        "/Users/gbolet/hecbench-roofline/src/voxelization-cuda/kernels.h": [
            "inline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\ninline __host__ __device__ float4 floorf(float4 v)\n{\n    return make_float4(floorf(v.x), floorf(v.y), floorf(v.z), floorf(v.w));\n}\n\n__device__ __forceinline__ uint32 hash(int x) {\n    return hash((uint32)x);\n}\n\n__device__ inline void insertHashTable(const uint32_t key, uint32_t *value,\n                                       const uint32_t hash_size, uint32_t *hash_table) {\n  uint64_t hash_value = hash(key);\n  uint32_t slot = hash_value % (hash_size / 2) /*key, value*/;\n  uint32_t empty_key = UINT32_MAX;\n  while (true) {\n    uint32_t pre_key = atomicCAS(hash_table + slot, empty_key, key);\n    if (pre_key == empty_key) {\n      hash_table[slot + hash_size / 2 /*offset*/] = atomicAdd(value, 1);\n      break;\n    } else if (pre_key == key) {\n      break;\n    }\n    slot = (slot + 1) % (hash_size / 2);\n  }\n}\n\n__global__ void buildHashKernel(const float *points, size_t points_size, float min_x_range,\n                                float max_x_range, float min_y_range, float max_y_range,\n                                float min_z_range, float max_z_range, float voxel_x_size,\n                                float voxel_y_size, float voxel_z_size, int grid_z_size,\n                                int grid_y_size, int grid_x_size, int feature_num,\n                                unsigned int *hash_table, unsigned int *real_voxel_num) {\n  int point_idx = blockIdx.x * blockDim.x + threadIdx.x;\n  if (point_idx >= points_size) {\n    return;\n  }\n\n  float px = points[feature_num * point_idx];\n  float py = points[feature_num * point_idx + 1];\n  float pz = points[feature_num * point_idx + 2];\n\n  int voxel_idx = floorf((px - min_x_range) / voxel_x_size);\n  if (voxel_idx < 0 || voxel_idx >= grid_x_size) return;\n\n  int voxel_idy = floorf((py - min_y_range) / voxel_y_size);\n  if (voxel_idy < 0 || voxel_idy >= grid_y_size) return;\n\n  int voxel_idz = floorf((pz - min_z_range) / voxel_z_size);\n  if (voxel_idz < 0 || voxel_idz >= grid_z_size) return;\n  unsigned int voxel_offset =\n      voxel_idz * grid_y_size * grid_x_size + voxel_idy * grid_x_size + voxel_idx;\n  insertHashTable(voxel_offset, real_voxel_num, points_size * 2 * 2, hash_table);\n}",
            "inline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\ninline __host__ __device__ float4 floorf(float4 v)\n{\n    return make_float4(floorf(v.x), floorf(v.y), floorf(v.z), floorf(v.w));\n}\n\n__device__ __forceinline__ uint32 hash(int x) {\n    return hash((uint32)x);\n}\n\n__device__ inline uint32_t lookupHashTable(const uint32_t key, const uint32_t hash_size,\n                                           const uint32_t *hash_table) {\n  uint64_t hash_value = hash(key);\n  uint32_t slot = hash_value % (hash_size / 2) /*key, value*/;\n  uint32_t empty_key = UINT32_MAX;\n  int cnt = 0;\n  while (true /* need to be adjusted according to data*/) {\n    cnt++;\n    if (hash_table[slot] == key) {\n      return hash_table[slot + hash_size / 2];\n    } else if (hash_table[slot] == empty_key) {\n      return empty_key;\n    } else {\n      slot = (slot + 1) % (hash_size / 2);\n    }\n  }\n  return empty_key;\n}\n\n__global__ void voxelizationKernel(const float *points, size_t points_size, float min_x_range,\n                                   float max_x_range, float min_y_range, float max_y_range,\n                                   float min_z_range, float max_z_range, float voxel_x_size,\n                                   float voxel_y_size, float voxel_z_size, int grid_z_size,\n                                   int grid_y_size, int grid_x_size, int feature_num,\n                                   int max_voxels, int max_points_per_voxel,\n                                   unsigned int *hash_table, unsigned int *num_points_per_voxel,\n                                   float *voxels_temp, unsigned int *voxel_indices,\n                                   unsigned int *real_voxel_num) {\n  int point_idx = blockIdx.x * blockDim.x + threadIdx.x;\n  if (point_idx >= points_size) {\n    return;\n  }\n\n  float px = points[feature_num * point_idx];\n  float py = points[feature_num * point_idx + 1];\n  float pz = points[feature_num * point_idx + 2];\n\n  if (px < min_x_range || px >= max_x_range ||\n      py < min_y_range || py >= max_y_range ||\n      pz < min_z_range || pz >= max_z_range) {\n    return;\n  }\n\n  int voxel_idx = floorf((px - min_x_range) / voxel_x_size);\n  int voxel_idy = floorf((py - min_y_range) / voxel_y_size);\n  int voxel_idz = floorf((pz - min_z_range) / voxel_z_size);\n  if ((voxel_idx < 0 || voxel_idx >= grid_x_size)) {\n    return;\n  }\n  if ((voxel_idy < 0 || voxel_idy >= grid_y_size)) {\n    return;\n  }\n  if ((voxel_idz < 0 || voxel_idz >= grid_z_size)) {\n    return;\n  }\n\n  unsigned int voxel_offset =\n      voxel_idz * grid_y_size * grid_x_size + voxel_idy * grid_x_size + voxel_idx;\n\n  // scatter to voxels\n  unsigned int voxel_id = lookupHashTable(voxel_offset, points_size * 2 * 2, hash_table);\n  if (voxel_id >= max_voxels) {\n    return;\n  }\n\n  unsigned int current_num = atomicAdd(num_points_per_voxel + voxel_id, 1);\n  if (current_num < max_points_per_voxel) {\n    unsigned int dst_offset =\n        voxel_id * (feature_num * max_points_per_voxel) + current_num * feature_num;\n    unsigned int src_offset = point_idx * feature_num;\n    for (int feature_idx = 0; feature_idx < feature_num; ++feature_idx) {\n      voxels_temp[dst_offset + feature_idx] = points[src_offset + feature_idx];\n    }\n\n    // now only deal with batch_size = 1\n    // since not sure what the input format will be if batch size > 1\n    // uint4 idx = {0, voxel_idz, voxel_idy, voxel_idx};\n    uint4 idx = {0, (unsigned int)voxel_idx, (unsigned int)voxel_idy, (unsigned int)voxel_idz};\n    ((uint4 *)voxel_indices)[voxel_id] = idx;\n  }\n}",
            "__global__ void featureExtractionKernel(float *voxels_temp, unsigned int *num_points_per_voxel,\n                                        int max_points_per_voxel, int feature_num,\n                                        half *voxel_features) {\n  int voxel_idx = blockIdx.x * blockDim.x + threadIdx.x;\n  \n  int valid_points_num = num_points_per_voxel[voxel_idx];\n\n  if (valid_points_num > max_points_per_voxel) {\n    num_points_per_voxel[voxel_idx] = max_points_per_voxel;\n    valid_points_num = max_points_per_voxel;\n  }\n\n  int offset = voxel_idx * max_points_per_voxel * feature_num;\n  for (int feature_idx = 0; feature_idx < feature_num; ++feature_idx) {\n    float s = voxels_temp[offset + feature_idx];\n    for (int point_idx = 0; point_idx < valid_points_num - 1; ++point_idx) {\n      s += voxels_temp[offset + (point_idx + 1) * feature_num + feature_idx];\n    }\n    voxels_temp[offset + feature_idx] = s / valid_points_num;\n  }\n\n  // move to be continuous\n  for (int feature_idx = 0; feature_idx < feature_num; ++feature_idx) {\n    int dst_offset = voxel_idx * feature_num;\n    int src_offset = voxel_idx * feature_num * max_points_per_voxel;\n    voxel_features[dst_offset + feature_idx] = __float2half(voxels_temp[src_offset + feature_idx]);\n  }\n}"
        ]
    },
    "permute-cuda": {
        "/Users/gbolet/hecbench-roofline/src/permute-cuda/main.cu": [
            "__global__ void permute_kernel(\n    float* __restrict__ q,\n    float* __restrict__ k,\n    float* __restrict__ v,\n    const float* inp,\n    int B, int T, int NH, int d) {\n  // okay so now, this kernel wants Q,K,V to all be of shape (B, NH, T, d)\n  // but instead, we have a single tensor QKV (inp) of shape (B, T, 3, NH, d)\n  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n\n  // Q[b][nh_][n][d_] = inp[b][n][0][nh_][d_]\n  int C = NH * d;\n\n  if (idx < B * C * T) {\n    int b = idx / (C * T);\n    int rest = idx % (C * T);\n    int nh_ = rest / (T * d);\n    rest = rest % (T * d);\n    int n = rest / d;\n    int d_ = rest % d;\n\n    int inp_idx = \\\n            (b * T * 3 * C)\n            +   (n * 3 * C)\n            +       (0 * C)\n            +          (nh_ * d)\n            +                d_;\n\n    q[idx] = inp[inp_idx];\n    k[idx] = inp[inp_idx + C];\n    v[idx] = inp[inp_idx + 2 * C];\n  }\n}"
        ]
    },
    "bitcracker-cuda": {
        "/Users/gbolet/hecbench-roofline/src/bitcracker-cuda/w_blocks.cu": [
            "__global__\nvoid kernel_w_block(\n    const unsigned char salt[SALT_SIZE],\n    const unsigned char padding[40],\n    uint32_t * d_w_words_uint32)\n{\n  for (uint64_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n                tid < NUM_HASH_BLOCKS;\n                tid += gridDim.x * blockDim.x) {\n\n    uint64_t texBlockId;\n    unsigned char block[HASH_BLOCK_NUM_UINT32];\n\n    int i;\n    // index 0-15\n    for(i = 0; i < SALT_SIZE; i++){\n      block[i] = salt[i];\n    }\n    // index 24-63\n    for(i = 0; i < PADDING_SIZE; i++){\n      block[i + 24] = padding[i];\n    }\n\n    // index 16-23\n    block[16] = (unsigned char) (tid >> (0 * 8));\n    block[17] = (unsigned char) (tid >> (1 * 8));\n    block[18] = (unsigned char) (tid >> (2 * 8));\n    block[19] = (unsigned char) (tid >> (3 * 8));\n    block[20] = (unsigned char) (tid >> (4 * 8));\n    block[21] = (unsigned char) (tid >> (5 * 8));\n    block[22] = (unsigned char) (tid >> (6 * 8));\n    block[23] = (unsigned char) (tid >> (7 * 8));\n\n    texBlockId = HASH_BLOCK_NUM_UINT32 * tid;\n    LOADSCHEDULE_WPRE(texBlockId +  0,  0)\n      LOADSCHEDULE_WPRE(texBlockId +  1,  1)\n      LOADSCHEDULE_WPRE(texBlockId +  2,  2)\n      LOADSCHEDULE_WPRE(texBlockId +  3,  3)\n      LOADSCHEDULE_WPRE(texBlockId +  4,  4)\n      LOADSCHEDULE_WPRE(texBlockId +  5,  5)\n      LOADSCHEDULE_WPRE(texBlockId +  6,  6)\n      LOADSCHEDULE_WPRE(texBlockId +  7,  7)\n      LOADSCHEDULE_WPRE(texBlockId +  8,  8)\n      LOADSCHEDULE_WPRE(texBlockId +  9,  9)\n      LOADSCHEDULE_WPRE(texBlockId + 10, 10)\n      LOADSCHEDULE_WPRE(texBlockId + 11, 11)\n      LOADSCHEDULE_WPRE(texBlockId + 12, 12)\n      LOADSCHEDULE_WPRE(texBlockId + 13, 13)\n      LOADSCHEDULE_WPRE(texBlockId + 14, 14)\n      LOADSCHEDULE_WPRE(texBlockId + 15, 15)\n      CALCSCHEDULE_WPRE(texBlockId + 16)\n      CALCSCHEDULE_WPRE(texBlockId + 17)\n      CALCSCHEDULE_WPRE(texBlockId + 18)\n      CALCSCHEDULE_WPRE(texBlockId + 19)\n      CALCSCHEDULE_WPRE(texBlockId + 20)\n      CALCSCHEDULE_WPRE(texBlockId + 21)\n      CALCSCHEDULE_WPRE(texBlockId + 22)\n      CALCSCHEDULE_WPRE(texBlockId + 23)\n      CALCSCHEDULE_WPRE(texBlockId + 24)\n      CALCSCHEDULE_WPRE(texBlockId + 25)\n      CALCSCHEDULE_WPRE(texBlockId + 26)\n      CALCSCHEDULE_WPRE(texBlockId + 27)\n      CALCSCHEDULE_WPRE(texBlockId + 28)\n      CALCSCHEDULE_WPRE(texBlockId + 29)\n      CALCSCHEDULE_WPRE(texBlockId + 30)\n      CALCSCHEDULE_WPRE(texBlockId + 31)\n      CALCSCHEDULE_WPRE(texBlockId + 32)\n      CALCSCHEDULE_WPRE(texBlockId + 33)\n      CALCSCHEDULE_WPRE(texBlockId + 34)\n      CALCSCHEDULE_WPRE(texBlockId + 35)\n      CALCSCHEDULE_WPRE(texBlockId + 36)\n      CALCSCHEDULE_WPRE(texBlockId + 37)\n      CALCSCHEDULE_WPRE(texBlockId + 38)\n      CALCSCHEDULE_WPRE(texBlockId + 39)\n      CALCSCHEDULE_WPRE(texBlockId + 40)\n      CALCSCHEDULE_WPRE(texBlockId + 41)\n      CALCSCHEDULE_WPRE(texBlockId + 42)\n      CALCSCHEDULE_WPRE(texBlockId + 43)\n      CALCSCHEDULE_WPRE(texBlockId + 44)\n      CALCSCHEDULE_WPRE(texBlockId + 45)\n      CALCSCHEDULE_WPRE(texBlockId + 46)\n      CALCSCHEDULE_WPRE(texBlockId + 47)\n      CALCSCHEDULE_WPRE(texBlockId + 48)\n      CALCSCHEDULE_WPRE(texBlockId + 49)\n      CALCSCHEDULE_WPRE(texBlockId + 50)\n      CALCSCHEDULE_WPRE(texBlockId + 51)\n      CALCSCHEDULE_WPRE(texBlockId + 52)\n      CALCSCHEDULE_WPRE(texBlockId + 53)\n      CALCSCHEDULE_WPRE(texBlockId + 54)\n      CALCSCHEDULE_WPRE(texBlockId + 55)\n      CALCSCHEDULE_WPRE(texBlockId + 56)\n      CALCSCHEDULE_WPRE(texBlockId + 57)\n      CALCSCHEDULE_WPRE(texBlockId + 58)\n      CALCSCHEDULE_WPRE(texBlockId + 59)\n      CALCSCHEDULE_WPRE(texBlockId + 60)\n      CALCSCHEDULE_WPRE(texBlockId + 61)\n      CALCSCHEDULE_WPRE(texBlockId + 62)\n      CALCSCHEDULE_WPRE(texBlockId + 63)\n  }\n}"
        ],
        "/Users/gbolet/hecbench-roofline/src/bitcracker-cuda/attack.cu": [
            "static __device__ __forceinline__ uint32_t LOP3LUT_XOR(uint32_t a, uint32_t b, uint32_t c) {\n            return a^b^c;\n        }\n\n__device__\nvoid encrypt(\n    uint32_t k0,\n    uint32_t k1,\n    uint32_t k2,\n    uint32_t k3,\n    uint32_t k4,\n    uint32_t k5,\n    uint32_t k6,\n    uint32_t k7,\n    uint32_t m0,\n    uint32_t m1,\n    uint32_t m2,\n    uint32_t m3,\n    uint32_t * output0,\n    uint32_t * output1,\n    uint32_t * output2,\n    uint32_t * output3)\n{\n  uint32_t enc_schedule0, enc_schedule1, enc_schedule2, enc_schedule3, enc_schedule4, enc_schedule5, enc_schedule6, enc_schedule7;\n  uint32_t local_key0, local_key1, local_key2, local_key3, local_key4, local_key5, local_key6, local_key7;\n\n  local_key0 = k0;\n  local_key1 = k1;\n  local_key2 = k2;\n  local_key3 = k3;\n  local_key4 = k4;\n  local_key5 = k5;\n  local_key6 = k6;\n  local_key7 = k7;\n\n  enc_schedule0 = __byte_perm(m0, 0, 0x0123) ^ local_key0;\n  enc_schedule1 = __byte_perm(m1, 0, 0x0123) ^ local_key1;\n  enc_schedule2 = __byte_perm(m2, 0, 0x0123) ^ local_key2;\n  enc_schedule3 = __byte_perm(m3, 0, 0x0123) ^ local_key3;\n\n  enc_schedule4 = LOP3LUT_XOR(LOP3LUT_XOR(TS0[enc_schedule0 >> 24], TS1[(enc_schedule1 >> 16) & 0xFF], TS2[(enc_schedule2 >> 8) & 0xFF]) , TS3[enc_schedule3 & 0xFF] , local_key4);\n  enc_schedule5 = LOP3LUT_XOR(LOP3LUT_XOR(TS0[enc_schedule1 >> 24], TS1[(enc_schedule2 >> 16) & 0xFF], TS2[(enc_schedule3 >> 8) & 0xFF]) , TS3[enc_schedule0 & 0xFF] , local_key5);\n  enc_schedule6 = LOP3LUT_XOR(LOP3LUT_XOR(TS0[enc_schedule2 >> 24], TS1[(enc_schedule3 >> 16) & 0xFF], TS2[(enc_schedule0 >> 8) & 0xFF]) , TS3[enc_schedule1 & 0xFF] , local_key6);\n  enc_schedule7 = LOP3LUT_XOR(LOP3LUT_XOR(TS0[enc_schedule3 >> 24], TS1[(enc_schedule0 >> 16) & 0xFF], TS2[(enc_schedule1 >> 8) & 0xFF]) , TS3[enc_schedule2 & 0xFF] , local_key7);\n\n  local_key0 ^= (TS2[(local_key7 >> 24)       ] & 0x000000FF) ^\n    (TS3[(local_key7 >> 16) & 0xFF] & 0xFF000000) ^\n    (TS0[(local_key7 >>  8) & 0xFF] & 0x00FF0000) ^\n    (TS1[(local_key7      ) & 0xFF] & 0x0000FF00) ^ 0x01000000; //RCON[0];\n  local_key1 ^= local_key0;\n  local_key2 ^= local_key1;\n  local_key3 ^= local_key2;\n\n  enc_schedule0 = LOP3LUT_XOR(LOP3LUT_XOR(TS0[enc_schedule4 >> 24], TS1[(enc_schedule5 >> 16) & 0xFF], TS2[(enc_schedule6 >> 8) & 0xFF]) , TS3[enc_schedule7 & 0xFF] , local_key0);\n  enc_schedule1 = LOP3LUT_XOR(LOP3LUT_XOR(TS0[enc_schedule5 >> 24], TS1[(enc_schedule6 >> 16) & 0xFF], TS2[(enc_schedule7 >> 8) & 0xFF]) , TS3[enc_schedule4 & 0xFF] , local_key1);\n  enc_schedule2 = LOP3LUT_XOR(LOP3LUT_XOR(TS0[enc_schedule6 >> 24], TS1[(enc_schedule7 >> 16) & 0xFF], TS2[(enc_schedule4 >> 8) & 0xFF]) , TS3[enc_schedule5 & 0xFF] , local_key2);\n  enc_schedule3 = LOP3LUT_XOR(LOP3LUT_XOR(TS0[enc_schedule7 >> 24], TS1[(enc_schedule4 >> 16) & 0xFF], TS2[(enc_schedule5 >> 8) & 0xFF]) , TS3[enc_schedule6 & 0xFF] , local_key3);\n\n  local_key4 ^= (TS3[(local_key3 >> 24)       ] & 0xFF000000) ^\n    (TS0[(local_key3 >> 16) & 0xFF] & 0x00FF0000) ^\n    (TS1[(local_key3 >>  8) & 0xFF] & 0x0000FF00) ^\n    (TS2[(local_key3      ) & 0xFF] & 0x000000FF);\n  local_key5 ^= local_key4;\n  local_key6 ^= local_key5;\n  local_key7 ^= local_key6;\n\n  enc_schedule4 = LOP3LUT_XOR(LOP3LUT_XOR(TS0[enc_schedule0 >> 24], TS1[(enc_schedule1 >> 16) & 0xFF], TS2[(enc_schedule2 >> 8) & 0xFF]) , TS3[enc_schedule3 & 0xFF] , local_key4);\n  enc_schedule5 = LOP3LUT_XOR(LOP3LUT_XOR(TS0[enc_schedule1 >> 24], TS1[(enc_schedule2 >> 16) & 0xFF], TS2[(enc_schedule3 >> 8) & 0xFF]) , TS3[enc_schedule0 & 0xFF] , local_key5);\n  enc_schedule6 = LOP3LUT_XOR(LOP3LUT_XOR(TS0[enc_schedule2 >> 24], TS1[(enc_schedule3 >> 16) & 0xFF], TS2[(enc_schedule0 >> 8) & 0xFF]) , TS3[enc_schedule1 & 0xFF] , local_key6);\n  enc_schedule7 = LOP3LUT_XOR(LOP3LUT_XOR(TS0[enc_schedule3 >> 24], TS1[(enc_schedule0 >> 16) & 0xFF], TS2[(enc_schedule1 >> 8) & 0xFF]) , TS3[enc_schedule2 & 0xFF] , local_key7);\n\n  local_key0 ^= (TS2[(local_key7 >> 24)       ] & 0x000000FF) ^\n    (TS3[(local_key7 >> 16) & 0xFF] & 0xFF000000) ^\n    (TS0[(local_key7 >>  8) & 0xFF] & 0x00FF0000) ^\n    (TS1[(local_key7      ) & 0xFF] & 0x0000FF00) ^ 0x02000000; //RCON[1];\n  local_key1 ^= local_key0;\n  local_key2 ^= local_key1;\n  local_key3 ^= local_key2;\n\n  enc_schedule0 = LOP3LUT_XOR(LOP3LUT_XOR(TS0[enc_schedule4 >> 24], TS1[(enc_schedule5 >> 16) & 0xFF], TS2[(enc_schedule6 >> 8) & 0xFF]) , TS3[enc_schedule7 & 0xFF] , local_key0);\n  enc_schedule1 = LOP3LUT_XOR(LOP3LUT_XOR(TS0[enc_schedule5 >> 24], TS1[(enc_schedule6 >> 16) & 0xFF], TS2[(enc_schedule7 >> 8) & 0xFF]) , TS3[enc_schedule4 & 0xFF] , local_key1);\n  enc_schedule2 = LOP3LUT_XOR(LOP3LUT_XOR(TS0[enc_schedule6 >> 24], TS1[(enc_schedule7 >> 16) & 0xFF], TS2[(enc_schedule4 >> 8) & 0xFF]) , TS3[enc_schedule5 & 0xFF] , local_key2);\n  enc_schedule3 = LOP3LUT_XOR(LOP3LUT_XOR(TS0[enc_schedule7 >> 24], TS1[(enc_schedule4 >> 16) & 0xFF], TS2[(enc_schedule5 >> 8) & 0xFF]) , TS3[enc_schedule6 & 0xFF] , local_key3);\n\n  local_key4 ^= (TS3[(local_key3 >> 24)       ] & 0xFF000000) ^\n    (TS0[(local_key3 >> 16) & 0xFF] & 0x00FF0000) ^\n    (TS1[(local_key3 >>  8) & 0xFF] & 0x0000FF00) ^\n    (TS2[(local_key3      ) & 0xFF] & 0x000000FF);\n  local_key5 ^= local_key4;\n  local_key6 ^= local_key5;\n  local_key7 ^= local_key6;\n\n  enc_schedule4 = LOP3LUT_XOR(LOP3LUT_XOR(TS0[enc_schedule0 >> 24], TS1[(enc_schedule1 >> 16) & 0xFF], TS2[(enc_schedule2 >> 8) & 0xFF]) , TS3[enc_schedule3 & 0xFF] , local_key4);\n  enc_schedule5 = LOP3LUT_XOR(LOP3LUT_XOR(TS0[enc_schedule1 >> 24], TS1[(enc_schedule2 >> 16) & 0xFF], TS2[(enc_schedule3 >> 8) & 0xFF]) , TS3[enc_schedule0 & 0xFF] , local_key5);\n  enc_schedule6 = LOP3LUT_XOR(LOP3LUT_XOR(TS0[enc_schedule2 >> 24], TS1[(enc_schedule3 >> 16) & 0xFF], TS2[(enc_schedule0 >> 8) & 0xFF]) , TS3[enc_schedule1 & 0xFF] , local_key6);\n  enc_schedule7 = LOP3LUT_XOR(LOP3LUT_XOR(TS0[enc_schedule3 >> 24], TS1[(enc_schedule0 >> 16) & 0xFF], TS2[(enc_schedule1 >> 8) & 0xFF]) , TS3[enc_schedule2 & 0xFF] , local_key7);\n\n  local_key0 ^= (TS2[(local_key7 >> 24)       ] & 0x000000FF) ^\n    (TS3[(local_key7 >> 16) & 0xFF] & 0xFF000000) ^\n    (TS0[(local_key7 >>  8) & 0xFF] & 0x00FF0000) ^\n    (TS1[(local_key7      ) & 0xFF] & 0x0000FF00) ^ 0x04000000; //RCON[2];\n  local_key1 ^= local_key0;\n  local_key2 ^= local_key1;\n  local_key3 ^= local_key2;\n\n  enc_schedule0 = LOP3LUT_XOR(LOP3LUT_XOR(TS0[enc_schedule4 >> 24], TS1[(enc_schedule5 >> 16) & 0xFF], TS2[(enc_schedule6 >> 8) & 0xFF]) , TS3[enc_schedule7 & 0xFF] , local_key0);\n  enc_schedule1 = LOP3LUT_XOR(LOP3LUT_XOR(TS0[enc_schedule5 >> 24], TS1[(enc_schedule6 >> 16) & 0xFF], TS2[(enc_schedule7 >> 8) & 0xFF]) , TS3[enc_schedule4 & 0xFF] , local_key1);\n  enc_schedule2 = LOP3LUT_XOR(LOP3LUT_XOR(TS0[enc_schedule6 >> 24], TS1[(enc_schedule7 >> 16) & 0xFF], TS2[(enc_schedule4 >> 8) & 0xFF]) , TS3[enc_schedule5 & 0xFF] , local_key2);\n  enc_schedule3 = LOP3LUT_XOR(LOP3LUT_XOR(TS0[enc_schedule7 >> 24], TS1[(enc_schedule4 >> 16) & 0xFF], TS2[(enc_schedule5 >> 8) & 0xFF]) , TS3[enc_schedule6 & 0xFF] , local_key3);\n\n  local_key4 ^= (TS3[(local_key3 >> 24)       ] & 0xFF000000) ^\n    (TS0[(local_key3 >> 16) & 0xFF] & 0x00FF0000) ^\n    (TS1[(local_key3 >>  8) & 0xFF] & 0x0000FF00) ^\n    (TS2[(local_key3      ) & 0xFF] & 0x000000FF);\n  local_key5 ^= local_key4;\n  local_key6 ^= local_key5;\n  local_key7 ^= local_key6;\n\n  enc_schedule4 = LOP3LUT_XOR(LOP3LUT_XOR(TS0[enc_schedule0 >> 24], TS1[(enc_schedule1 >> 16) & 0xFF], TS2[(enc_schedule2 >> 8) & 0xFF]) , TS3[enc_schedule3 & 0xFF] , local_key4);\n  enc_schedule5 = LOP3LUT_XOR(LOP3LUT_XOR(TS0[enc_schedule1 >> 24], TS1[(enc_schedule2 >> 16) & 0xFF], TS2[(enc_schedule3 >> 8) & 0xFF]) , TS3[enc_schedule0 & 0xFF] , local_key5);\n  enc_schedule6 = LOP3LUT_XOR(LOP3LUT_XOR(TS0[enc_schedule2 >> 24], TS1[(enc_schedule3 >> 16) & 0xFF], TS2[(enc_schedule0 >> 8) & 0xFF]) , TS3[enc_schedule1 & 0xFF] , local_key6);\n  enc_schedule7 = LOP3LUT_XOR(LOP3LUT_XOR(TS0[enc_schedule3 >> 24], TS1[(enc_schedule0 >> 16) & 0xFF], TS2[(enc_schedule1 >> 8) & 0xFF]) , TS3[enc_schedule2 & 0xFF] , local_key7);\n\n  local_key0 ^= (TS2[(local_key7 >> 24)       ] & 0x000000FF) ^\n    (TS3[(local_key7 >> 16) & 0xFF] & 0xFF000000) ^\n    (TS0[(local_key7 >>  8) & 0xFF] & 0x00FF0000) ^\n    (TS1[(local_key7      ) & 0xFF] & 0x0000FF00) ^ 0x08000000; //RCON[3];\n  local_key1 ^= local_key0;\n  local_key2 ^= local_key1;\n  local_key3 ^= local_key2;\n\n  enc_schedule0 = LOP3LUT_XOR(LOP3LUT_XOR(TS0[enc_schedule4 >> 24], TS1[(enc_schedule5 >> 16) & 0xFF], TS2[(enc_schedule6 >> 8) & 0xFF]) , TS3[enc_schedule7 & 0xFF] , local_key0);\n  enc_schedule1 = LOP3LUT_XOR(LOP3LUT_XOR(TS0[enc_schedule5 >> 24], TS1[(enc_schedule6 >> 16) & 0xFF], TS2[(enc_schedule7 >> 8) & 0xFF]) , TS3[enc_schedule4 & 0xFF] , local_key1);\n  enc_schedule2 = LOP3LUT_XOR(LOP3LUT_XOR(TS0[enc_schedule6 >> 24], TS1[(enc_schedule7 >> 16) & 0xFF], TS2[(enc_schedule4 >> 8) & 0xFF]) , TS3[enc_schedule5 & 0xFF] , local_key2);\n  enc_schedule3 = LOP3LUT_XOR(LOP3LUT_XOR(TS0[enc_schedule7 >> 24], TS1[(enc_schedule4 >> 16) & 0xFF], TS2[(enc_schedule5 >> 8) & 0xFF]) , TS3[enc_schedule6 & 0xFF] , local_key3);\n\n  local_key4 ^= (TS3[(local_key3 >> 24)       ] & 0xFF000000) ^\n    (TS0[(local_key3 >> 16) & 0xFF] & 0x00FF0000) ^\n    (TS1[(local_key3 >>  8) & 0xFF] & 0x0000FF00) ^\n    (TS2[(local_key3      ) & 0xFF] & 0x000000FF);\n  local_key5 ^= local_key4;\n  local_key6 ^= local_key5;\n  local_key7 ^= local_key6;\n\n  enc_schedule4 = LOP3LUT_XOR(LOP3LUT_XOR(TS0[enc_schedule0 >> 24], TS1[(enc_schedule1 >> 16) & 0xFF], TS2[(enc_schedule2 >> 8) & 0xFF]) , TS3[enc_schedule3 & 0xFF] , local_key4);\n  enc_schedule5 = LOP3LUT_XOR(LOP3LUT_XOR(TS0[enc_schedule1 >> 24], TS1[(enc_schedule2 >> 16) & 0xFF], TS2[(enc_schedule3 >> 8) & 0xFF]) , TS3[enc_schedule0 & 0xFF] , local_key5);\n  enc_schedule6 = LOP3LUT_XOR(LOP3LUT_XOR(TS0[enc_schedule2 >> 24], TS1[(enc_schedule3 >> 16) & 0xFF], TS2[(enc_schedule0 >> 8) & 0xFF]) , TS3[enc_schedule1 & 0xFF] , local_key6);\n  enc_schedule7 = LOP3LUT_XOR(LOP3LUT_XOR(TS0[enc_schedule3 >> 24], TS1[(enc_schedule0 >> 16) & 0xFF], TS2[(enc_schedule1 >> 8) & 0xFF]) , TS3[enc_schedule2 & 0xFF] , local_key7);\n\n  local_key0 ^= (TS2[(local_key7 >> 24)       ] & 0x000000FF) ^\n    (TS3[(local_key7 >> 16) & 0xFF] & 0xFF000000) ^\n    (TS0[(local_key7 >>  8) & 0xFF] & 0x00FF0000) ^\n    (TS1[(local_key7      ) & 0xFF] & 0x0000FF00) ^ 0x10000000; //RCON[4];\n  local_key1 ^= local_key0;\n  local_key2 ^= local_key1;\n  local_key3 ^= local_key2;\n\n  enc_schedule0 = LOP3LUT_XOR(LOP3LUT_XOR(TS0[enc_schedule4 >> 24], TS1[(enc_schedule5 >> 16) & 0xFF], TS2[(enc_schedule6 >> 8) & 0xFF]) , TS3[enc_schedule7 & 0xFF] , local_key0);\n  enc_schedule1 = LOP3LUT_XOR(LOP3LUT_XOR(TS0[enc_schedule5 >> 24], TS1[(enc_schedule6 >> 16) & 0xFF], TS2[(enc_schedule7 >> 8) & 0xFF]) , TS3[enc_schedule4 & 0xFF] , local_key1);\n  enc_schedule2 = LOP3LUT_XOR(LOP3LUT_XOR(TS0[enc_schedule6 >> 24], TS1[(enc_schedule7 >> 16) & 0xFF], TS2[(enc_schedule4 >> 8) & 0xFF]) , TS3[enc_schedule5 & 0xFF] , local_key2);\n  enc_schedule3 = LOP3LUT_XOR(LOP3LUT_XOR(TS0[enc_schedule7 >> 24], TS1[(enc_schedule4 >> 16) & 0xFF], TS2[(enc_schedule5 >> 8) & 0xFF]) , TS3[enc_schedule6 & 0xFF] , local_key3);\n\n  local_key4 ^= (TS3[(local_key3 >> 24)       ] & 0xFF000000) ^\n    (TS0[(local_key3 >> 16) & 0xFF] & 0x00FF0000) ^\n    (TS1[(local_key3 >>  8) & 0xFF] & 0x0000FF00) ^\n    (TS2[(local_key3      ) & 0xFF] & 0x000000FF);\n  local_key5 ^= local_key4;\n  local_key6 ^= local_key5;\n  local_key7 ^= local_key6;\n\n  enc_schedule4 = LOP3LUT_XOR(LOP3LUT_XOR(TS0[enc_schedule0 >> 24], TS1[(enc_schedule1 >> 16) & 0xFF], TS2[(enc_schedule2 >> 8) & 0xFF]) , TS3[enc_schedule3 & 0xFF] , local_key4);\n  enc_schedule5 = LOP3LUT_XOR(LOP3LUT_XOR(TS0[enc_schedule1 >> 24], TS1[(enc_schedule2 >> 16) & 0xFF], TS2[(enc_schedule3 >> 8) & 0xFF]) , TS3[enc_schedule0 & 0xFF] , local_key5);\n  enc_schedule6 = LOP3LUT_XOR(LOP3LUT_XOR(TS0[enc_schedule2 >> 24], TS1[(enc_schedule3 >> 16) & 0xFF], TS2[(enc_schedule0 >> 8) & 0xFF]) , TS3[enc_schedule1 & 0xFF] , local_key6);\n  enc_schedule7 = LOP3LUT_XOR(LOP3LUT_XOR(TS0[enc_schedule3 >> 24], TS1[(enc_schedule0 >> 16) & 0xFF], TS2[(enc_schedule1 >> 8) & 0xFF]) , TS3[enc_schedule2 & 0xFF] , local_key7);\n\n  local_key0 ^= (TS2[(local_key7 >> 24)       ] & 0x000000FF) ^\n    (TS3[(local_key7 >> 16) & 0xFF] & 0xFF000000) ^\n    (TS0[(local_key7 >>  8) & 0xFF] & 0x00FF0000) ^\n    (TS1[(local_key7      ) & 0xFF] & 0x0000FF00) ^ 0x20000000; //RCON[5];\n  local_key1 ^= local_key0;\n  local_key2 ^= local_key1;\n  local_key3 ^= local_key2;\n\n  enc_schedule0 = LOP3LUT_XOR(LOP3LUT_XOR(TS0[enc_schedule4 >> 24], TS1[(enc_schedule5 >> 16) & 0xFF], TS2[(enc_schedule6 >> 8) & 0xFF]) , TS3[enc_schedule7 & 0xFF] , local_key0);\n  enc_schedule1 = LOP3LUT_XOR(LOP3LUT_XOR(TS0[enc_schedule5 >> 24], TS1[(enc_schedule6 >> 16) & 0xFF], TS2[(enc_schedule7 >> 8) & 0xFF]) , TS3[enc_schedule4 & 0xFF] , local_key1);\n  enc_schedule2 = LOP3LUT_XOR(LOP3LUT_XOR(TS0[enc_schedule6 >> 24], TS1[(enc_schedule7 >> 16) & 0xFF], TS2[(enc_schedule4 >> 8) & 0xFF]) , TS3[enc_schedule5 & 0xFF] , local_key2);\n  enc_schedule3 = LOP3LUT_XOR(LOP3LUT_XOR(TS0[enc_schedule7 >> 24], TS1[(enc_schedule4 >> 16) & 0xFF], TS2[(enc_schedule5 >> 8) & 0xFF]) , TS3[enc_schedule6 & 0xFF] , local_key3);\n\n  local_key4 ^= (TS3[(local_key3 >> 24)       ] & 0xFF000000) ^\n    (TS0[(local_key3 >> 16) & 0xFF] & 0x00FF0000) ^\n    (TS1[(local_key3 >>  8) & 0xFF] & 0x0000FF00) ^\n    (TS2[(local_key3      ) & 0xFF] & 0x000000FF);\n  local_key5 ^= local_key4;\n  local_key6 ^= local_key5;\n  local_key7 ^= local_key6;\n\n  enc_schedule4 = LOP3LUT_XOR(LOP3LUT_XOR(TS0[enc_schedule0 >> 24], TS1[(enc_schedule1 >> 16) & 0xFF], TS2[(enc_schedule2 >> 8) & 0xFF]) , TS3[enc_schedule3 & 0xFF] , local_key4);\n  enc_schedule5 = LOP3LUT_XOR(LOP3LUT_XOR(TS0[enc_schedule1 >> 24], TS1[(enc_schedule2 >> 16) & 0xFF], TS2[(enc_schedule3 >> 8) & 0xFF]) , TS3[enc_schedule0 & 0xFF] , local_key5);\n  enc_schedule6 = LOP3LUT_XOR(LOP3LUT_XOR(TS0[enc_schedule2 >> 24], TS1[(enc_schedule3 >> 16) & 0xFF], TS2[(enc_schedule0 >> 8) & 0xFF]) , TS3[enc_schedule1 & 0xFF] , local_key6);\n  enc_schedule7 = LOP3LUT_XOR(LOP3LUT_XOR(TS0[enc_schedule3 >> 24], TS1[(enc_schedule0 >> 16) & 0xFF], TS2[(enc_schedule1 >> 8) & 0xFF]) , TS3[enc_schedule2 & 0xFF] , local_key7);\n\n  local_key0 ^= (TS2[(local_key7 >> 24)       ] & 0x000000FF) ^\n    (TS3[(local_key7 >> 16) & 0xFF] & 0xFF000000) ^\n    (TS0[(local_key7 >>  8) & 0xFF] & 0x00FF0000) ^\n    (TS1[(local_key7      ) & 0xFF] & 0x0000FF00) ^ 0x40000000; //RCON[6];\n  local_key1 ^= local_key0;\n  local_key2 ^= local_key1;\n  local_key3 ^= local_key2;\n\n  enc_schedule0 = (TS2[(enc_schedule4 >> 24)       ] & 0xFF000000) ^\n    (TS3[(enc_schedule5 >> 16) & 0xFF] & 0x00FF0000) ^\n    (TS0[(enc_schedule6 >>  8) & 0xFF] & 0x0000FF00) ^\n    (TS1[(enc_schedule7      ) & 0xFF] & 0x000000FF) ^ local_key0;\n\n  enc_schedule1 = (TS2[(enc_schedule5 >> 24)       ] & 0xFF000000) ^\n    (TS3[(enc_schedule6 >> 16) & 0xFF] & 0x00FF0000) ^\n    (TS0[(enc_schedule7 >>  8) & 0xFF] & 0x0000FF00) ^\n    (TS1[(enc_schedule4      ) & 0xFF] & 0x000000FF) ^ local_key1;\n\n  enc_schedule2 = (TS2[(enc_schedule6 >> 24)       ] & 0xFF000000) ^\n    (TS3[(enc_schedule7 >> 16) & 0xFF] & 0x00FF0000) ^\n    (TS0[(enc_schedule4 >>  8) & 0xFF] & 0x0000FF00) ^\n    (TS1[(enc_schedule5      ) & 0xFF] & 0x000000FF) ^ local_key2;\n\n  enc_schedule3 = (TS2[(enc_schedule7 >> 24)       ] & 0xFF000000) ^\n    (TS3[(enc_schedule4 >> 16) & 0xFF] & 0x00FF0000) ^\n    (TS0[(enc_schedule5 >>  8) & 0xFF] & 0x0000FF00) ^\n    (TS1[(enc_schedule6      ) & 0xFF] & 0x000000FF) ^ local_key3;\n\n  output0[0] = __byte_perm(enc_schedule0, 0, 0x0123);\n  output1[0] = __byte_perm(enc_schedule1, 0, 0x0123);\n  output2[0] = __byte_perm(enc_schedule2, 0, 0x0123);\n  output3[0] = __byte_perm(enc_schedule3, 0, 0x0123);\n}\n\n__global__\nvoid decrypt_vmk_with_mac(\n    uint32_t num_pswd_per_kernel_launch,\n    int *found,\n    const unsigned char *__restrict__ vmkKey,\n    const unsigned char *__restrict__ vmkIV,\n    const unsigned char *__restrict__ mac,\n    const unsigned char *__restrict__ macIV,\n    const unsigned char *__restrict__ computedMacIV,\n    int v0,\n    int v1,\n    int v2,\n    int v3,\n    uint32_t s0,\n    uint32_t s1,\n    uint32_t s2,\n    uint32_t s3,\n    const uint32_t *__restrict__ pswd_uint32,\n    const uint32_t *__restrict__ w_words_uint32)\n{\n  uint32_t schedule00, schedule01, schedule02, schedule03, schedule04, schedule05, schedule06, schedule07, schedule08, schedule09;\n  uint32_t schedule10, schedule11, schedule12, schedule13, schedule14, schedule15, schedule16, schedule17, schedule18, schedule19;\n  uint32_t schedule20, schedule21, schedule22, schedule23, schedule24, schedule25, schedule26, schedule27, schedule28, schedule29;\n  uint32_t schedule30, schedule31;\n  uint32_t first_hash0, first_hash1, first_hash2, first_hash3, first_hash4, first_hash5, first_hash6, first_hash7;\n  uint32_t hash0, hash1, hash2, hash3, hash4, hash5, hash6, hash7;\n  uint32_t a, b, c, d, e, f, g, h;\n\n  unsigned int tid = threadIdx.x + blockIdx.x * blockDim.x;\n  if (tid >= num_pswd_per_kernel_launch) return;\n  int index_generic;  // goes from 0-0x100000\n  int indexW;         // first index into pswd_uint32 and then index into w_words_uint32\n  int8_t redo = 0;\n\n  first_hash0 = UINT32_C(0x6A09E667);\n  first_hash1 = UINT32_C(0xBB67AE85);\n  first_hash2 = UINT32_C(0x3C6EF372);\n  first_hash3 = UINT32_C(0xA54FF53A);\n  first_hash4 = UINT32_C(0x510E527F);\n  first_hash5 = UINT32_C(0x9B05688C);\n  first_hash6 = UINT32_C(0x1F83D9AB);\n  first_hash7 = UINT32_C(0x5BE0CD19);\n\n  a = UINT32_C(0x6A09E667);\n  b = UINT32_C(0xBB67AE85);\n  c = UINT32_C(0x3C6EF372);\n  d = UINT32_C(0xA54FF53A);\n  e = UINT32_C(0x510E527F);\n  f = UINT32_C(0x9B05688C);\n  g = UINT32_C(0x1F83D9AB);\n  h = UINT32_C(0x5BE0CD19);\n\n  /**********************************************************************\n   ***************************** FIRST HASH ******************************\n   **********************************************************************/\n  indexW = tid * PSWD_NUM_UINT32; // indexing into pswd_uint32 with block size of 32\n  redo = 0;\n\n  schedule00 = (uint32_t)(pswd_uint32[indexW +  0]);\n  schedule01 = (uint32_t)(pswd_uint32[indexW +  1]);\n  schedule02 = (uint32_t)(pswd_uint32[indexW +  2]);\n  schedule03 = (uint32_t)(pswd_uint32[indexW +  3]);\n  schedule04 = (uint32_t)(pswd_uint32[indexW +  4]);\n  schedule05 = (uint32_t)(pswd_uint32[indexW +  5]);\n  schedule06 = (uint32_t)(pswd_uint32[indexW +  6]);\n  schedule07 = (uint32_t)(pswd_uint32[indexW +  7]);\n  schedule08 = (uint32_t)(pswd_uint32[indexW +  8]);\n  schedule09 = (uint32_t)(pswd_uint32[indexW +  9]);\n  schedule10 = (uint32_t)(pswd_uint32[indexW + 10]);\n  schedule11 = (uint32_t)(pswd_uint32[indexW + 11]);\n  schedule12 = (uint32_t)(pswd_uint32[indexW + 12]);\n  schedule13 = (uint32_t)(pswd_uint32[indexW + 13]);\n  schedule14 = (uint32_t)(pswd_uint32[indexW + 14]);\n  //Input password is shorter than FIRST_LENGHT\n  if(schedule14 == 0xFFFFFFFF) {\n    schedule14 = 0;\n  } else {\n    redo = 1;\n  }\n  schedule15 = (uint32_t)(pswd_uint32[indexW + 15]);\n\n  ALL_SCHEDULE_LAST16()\n    ALL_ROUND_B1_1()\n    ALL_SCHEDULE32()\n    ALL_ROUND_B1_2()\n\n    first_hash0 += a;\n  first_hash1 += b;\n  first_hash2 += c;\n  first_hash3 += d;\n  first_hash4 += e;\n  first_hash5 += f;\n  first_hash6 += g;\n  first_hash7 += h;\n\n  if(redo == 1)\n  {\n    schedule00 = (uint32_t)(pswd_uint32[indexW + 16]);\n    schedule01 = (uint32_t)(pswd_uint32[indexW + 17]);\n    schedule02 = (uint32_t)(pswd_uint32[indexW + 18]);\n    schedule03 = (uint32_t)(pswd_uint32[indexW + 19]);\n    schedule04 = (uint32_t)(pswd_uint32[indexW + 20]);\n    schedule05 = (uint32_t)(pswd_uint32[indexW + 21]);\n    schedule06 = (uint32_t)(pswd_uint32[indexW + 22]);\n    schedule07 = (uint32_t)(pswd_uint32[indexW + 23]);\n    schedule08 = (uint32_t)(pswd_uint32[indexW + 24]);\n    schedule09 = (uint32_t)(pswd_uint32[indexW + 25]);\n    schedule10 = (uint32_t)(pswd_uint32[indexW + 26]);\n    schedule11 = (uint32_t)(pswd_uint32[indexW + 27]);\n    schedule12 = (uint32_t)(pswd_uint32[indexW + 28]);\n    schedule13 = (uint32_t)(pswd_uint32[indexW + 29]);\n    schedule14 = (uint32_t)(pswd_uint32[indexW + 30]);\n    schedule15 = (uint32_t)(pswd_uint32[indexW + 31]);\n\n    a = first_hash0;\n    b = first_hash1;\n    c = first_hash2;\n    d = first_hash3;\n    e = first_hash4;\n    f = first_hash5;\n    g = first_hash6;\n    h = first_hash7;\n\n    ALL_SCHEDULE_LAST16()\n      ALL_ROUND_B1_1()\n      ALL_SCHEDULE32()\n      ALL_ROUND_B1_2()\n\n      first_hash0 += a;\n    first_hash1 += b;\n    first_hash2 += c;\n    first_hash3 += d;\n    first_hash4 += e;\n    first_hash5 += f;\n    first_hash6 += g;\n    first_hash7 += h;\n  }\n\n  /**********************************************************************\n   ***************************** SECOND HASH *****************************\n   **********************************************************************/\n  schedule00 = first_hash0;\n  schedule01 = first_hash1;\n  schedule02 = first_hash2;\n  schedule03 = first_hash3;\n  schedule04 = first_hash4;\n  schedule05 = first_hash5;\n  schedule06 = first_hash6;\n  schedule07 = first_hash7;\n  schedule08 = 0x80000000;\n  schedule09 = 0;\n  schedule10 = 0;\n  schedule11 = 0;\n  schedule12 = 0;\n  schedule13 = 0;\n  schedule14 = 0;\n  schedule15 = 0x100;\n\n  first_hash0 = UINT32_C(0x6A09E667);\n  first_hash1 = UINT32_C(0xBB67AE85);\n  first_hash2 = UINT32_C(0x3C6EF372);\n  first_hash3 = UINT32_C(0xA54FF53A);\n  first_hash4 = UINT32_C(0x510E527F);\n  first_hash5 = UINT32_C(0x9B05688C);\n  first_hash6 = UINT32_C(0x1F83D9AB);\n  first_hash7 = UINT32_C(0x5BE0CD19);\n\n  a = first_hash0;\n  b = first_hash1;\n  c = first_hash2;\n  d = first_hash3;\n  e = first_hash4;\n  f = first_hash5;\n  g = first_hash6;\n  h = first_hash7;\n\n  ALL_SCHEDULE_LAST16()\n\n    // execute first 32 rounds\n    ALL_ROUND_B1_1()\n\n    // compute second 32 W rounds\n    ALL_SCHEDULE32()\n\n    // execute second 32 rounds\n    ALL_ROUND_B1_2()\n\n    first_hash0 += a;\n  first_hash1 += b;\n  first_hash2 += c;\n  first_hash3 += d;\n  first_hash4 += e;\n  first_hash5 += f;\n  first_hash6 += g;\n  first_hash7 += h;\n\n  /**********************************************************************\n   ***************************** LOOP HASH *******************************\n   **********************************************************************/\n\n  hash0 = 0;\n  hash1 = 0;\n  hash2 = 0;\n  hash3 = 0;\n  hash4 = 0;\n  hash5 = 0;\n  hash6 = 0;\n  hash7 = 0;\n\n  indexW = 0; // reusing variable to index into w_words_uint32\n\n  for(index_generic = 0; index_generic < NUM_HASH_BLOCKS; index_generic++)\n  {\n    // set start value\n    a = UINT32_C(0x6A09E667);\n    b = UINT32_C(0xBB67AE85);\n    c = UINT32_C(0x3C6EF372);\n    d = UINT32_C(0xA54FF53A);\n    e = UINT32_C(0x510E527F);\n    f = UINT32_C(0x9B05688C);\n    g = UINT32_C(0x1F83D9AB);\n    h = UINT32_C(0x5BE0CD19);\n\n    // compute first 32 W words\n    schedule00 = hash0;\n    schedule01 = hash1;\n    schedule02 = hash2;\n    schedule03 = hash3;\n    schedule04 = hash4;\n    schedule05 = hash5;\n    schedule06 = hash6;\n    schedule07 = hash7;\n    schedule08 = first_hash0;\n    schedule09 = first_hash1;\n    schedule10 = first_hash2;\n    schedule11 = first_hash3;\n    schedule12 = first_hash4;\n    schedule13 = first_hash5;\n    schedule14 = first_hash6;\n    schedule15 = first_hash7;\n\n    ALL_SCHEDULE_LAST16()\n\n      // execute first 32 rounds\n      ALL_ROUND_B1_1()\n\n      // compute second 32 W words\n      ALL_SCHEDULE32()\n\n      // executer second 32 rounds\n      ALL_ROUND_B1_2()\n\n      // update hash value\n      hash0 = UINT32_C(0x6A09E667) + a;\n    hash1 = UINT32_C(0xBB67AE85) + b;\n    hash2 = UINT32_C(0x3C6EF372) + c;\n    hash3 = UINT32_C(0xA54FF53A) + d;\n    hash4 = UINT32_C(0x510E527F) + e;\n    hash5 = UINT32_C(0x9B05688C) + f;\n    hash6 = UINT32_C(0x1F83D9AB) + g;\n    hash7 = UINT32_C(0x5BE0CD19) + h;\n\n    a = hash0;\n    b = hash1;\n    c = hash2;\n    d = hash3;\n    e = hash4;\n    f = hash5;\n    g = hash6;\n    h = hash7;\n\n    // execute 64 rounds, reading W blocks from w_words_uint32\n    ROUND_SECOND_BLOCK_CONST(a, b, c, d, e, f, g, h,  0, 0x428A2F98, v0)\n      ROUND_SECOND_BLOCK_CONST(h, a, b, c, d, e, f, g,  1, 0x71374491, v1)\n      ROUND_SECOND_BLOCK_CONST(g, h, a, b, c, d, e, f,  2, 0xB5C0FBCF, v2)\n      ROUND_SECOND_BLOCK_CONST(f, g, h, a, b, c, d, e,  3, 0xE9B5DBA5, v3)\n      ROUND_SECOND_BLOCK(e, f, g, h, a, b, c, d,  4, 0x3956C25B, indexW)\n      ROUND_SECOND_BLOCK(d, e, f, g, h, a, b, c,  5, 0x59F111F1, indexW)\n      ROUND_SECOND_BLOCK(c, d, e, f, g, h, a, b,  6, 0x923F82A4, indexW)\n      ROUND_SECOND_BLOCK(b, c, d, e, f, g, h, a,  7, 0xAB1C5ED5, indexW)\n      ROUND_SECOND_BLOCK(a, b, c, d, e, f, g, h,  8, 0xD807AA98, indexW)\n      ROUND_SECOND_BLOCK(h, a, b, c, d, e, f, g,  9, 0x12835B01, indexW)\n      ROUND_SECOND_BLOCK(g, h, a, b, c, d, e, f, 10, 0x243185BE, indexW)\n      ROUND_SECOND_BLOCK(f, g, h, a, b, c, d, e, 11, 0x550C7DC3, indexW)\n      ROUND_SECOND_BLOCK(e, f, g, h, a, b, c, d, 12, 0x72BE5D74, indexW)\n      ROUND_SECOND_BLOCK(d, e, f, g, h, a, b, c, 13, 0x80DEB1FE, indexW)\n      ROUND_SECOND_BLOCK(c, d, e, f, g, h, a, b, 14, 0x9BDC06A7, indexW)\n      ROUND_SECOND_BLOCK(b, c, d, e, f, g, h, a, 15, 0xC19BF174, indexW)\n      ROUND_SECOND_BLOCK(a, b, c, d, e, f, g, h, 16, 0xE49B69C1, indexW)\n      ROUND_SECOND_BLOCK(h, a, b, c, d, e, f, g, 17, 0xEFBE4786, indexW)\n      ROUND_SECOND_BLOCK(g, h, a, b, c, d, e, f, 18, 0x0FC19DC6, indexW)\n      ROUND_SECOND_BLOCK(f, g, h, a, b, c, d, e, 19, 0x240CA1CC, indexW)\n      ROUND_SECOND_BLOCK(e, f, g, h, a, b, c, d, 20, 0x2DE92C6F, indexW)\n      ROUND_SECOND_BLOCK(d, e, f, g, h, a, b, c, 21, 0x4A7484AA, indexW)\n      ROUND_SECOND_BLOCK(c, d, e, f, g, h, a, b, 22, 0x5CB0A9DC, indexW)\n      ROUND_SECOND_BLOCK(b, c, d, e, f, g, h, a, 23, 0x76F988DA, indexW)\n      ROUND_SECOND_BLOCK(a, b, c, d, e, f, g, h, 24, 0x983E5152, indexW)\n      ROUND_SECOND_BLOCK(h, a, b, c, d, e, f, g, 25, 0xA831C66D, indexW)\n      ROUND_SECOND_BLOCK(g, h, a, b, c, d, e, f, 26, 0xB00327C8, indexW)\n      ROUND_SECOND_BLOCK(f, g, h, a, b, c, d, e, 27, 0xBF597FC7, indexW)\n      ROUND_SECOND_BLOCK(e, f, g, h, a, b, c, d, 28, 0xC6E00BF3, indexW)\n      ROUND_SECOND_BLOCK(d, e, f, g, h, a, b, c, 29, 0xD5A79147, indexW)\n      ROUND_SECOND_BLOCK(c, d, e, f, g, h, a, b, 30, 0x06CA6351, indexW)\n      ROUND_SECOND_BLOCK(b, c, d, e, f, g, h, a, 31, 0x14292967, indexW)\n      ROUND_SECOND_BLOCK(a, b, c, d, e, f, g, h, 32, 0x27B70A85, indexW)\n      ROUND_SECOND_BLOCK(h, a, b, c, d, e, f, g, 33, 0x2E1B2138, indexW)\n      ROUND_SECOND_BLOCK(g, h, a, b, c, d, e, f, 34, 0x4D2C6DFC, indexW)\n      ROUND_SECOND_BLOCK(f, g, h, a, b, c, d, e, 35, 0x53380D13, indexW)\n      ROUND_SECOND_BLOCK(e, f, g, h, a, b, c, d, 36, 0x650A7354, indexW)\n      ROUND_SECOND_BLOCK(d, e, f, g, h, a, b, c, 37, 0x766A0ABB, indexW)\n      ROUND_SECOND_BLOCK(c, d, e, f, g, h, a, b, 38, 0x81C2C92E, indexW)\n      ROUND_SECOND_BLOCK(b, c, d, e, f, g, h, a, 39, 0x92722C85, indexW)\n      ROUND_SECOND_BLOCK(a, b, c, d, e, f, g, h, 40, 0xA2BFE8A1, indexW)\n      ROUND_SECOND_BLOCK(h, a, b, c, d, e, f, g, 41, 0xA81A664B, indexW)\n      ROUND_SECOND_BLOCK(g, h, a, b, c, d, e, f, 42, 0xC24B8B70, indexW)\n      ROUND_SECOND_BLOCK(f, g, h, a, b, c, d, e, 43, 0xC76C51A3, indexW)\n      ROUND_SECOND_BLOCK(e, f, g, h, a, b, c, d, 44, 0xD192E819, indexW)\n      ROUND_SECOND_BLOCK(d, e, f, g, h, a, b, c, 45, 0xD6990624, indexW)\n      ROUND_SECOND_BLOCK(c, d, e, f, g, h, a, b, 46, 0xF40E3585, indexW)\n      ROUND_SECOND_BLOCK(b, c, d, e, f, g, h, a, 47, 0x106AA070, indexW)\n      ROUND_SECOND_BLOCK(a, b, c, d, e, f, g, h, 48, 0x19A4C116, indexW)\n      ROUND_SECOND_BLOCK(h, a, b, c, d, e, f, g, 49, 0x1E376C08, indexW)\n      ROUND_SECOND_BLOCK(g, h, a, b, c, d, e, f, 50, 0x2748774C, indexW)\n      ROUND_SECOND_BLOCK(f, g, h, a, b, c, d, e, 51, 0x34B0BCB5, indexW)\n      ROUND_SECOND_BLOCK(e, f, g, h, a, b, c, d, 52, 0x391C0CB3, indexW)\n      ROUND_SECOND_BLOCK(d, e, f, g, h, a, b, c, 53, 0x4ED8AA4A, indexW)\n      ROUND_SECOND_BLOCK(c, d, e, f, g, h, a, b, 54, 0x5B9CCA4F, indexW)\n      ROUND_SECOND_BLOCK(b, c, d, e, f, g, h, a, 55, 0x682E6FF3, indexW)\n      ROUND_SECOND_BLOCK(a, b, c, d, e, f, g, h, 56, 0x748F82EE, indexW)\n      ROUND_SECOND_BLOCK(h, a, b, c, d, e, f, g, 57, 0x78A5636F, indexW)\n      ROUND_SECOND_BLOCK(g, h, a, b, c, d, e, f, 58, 0x84C87814, indexW)\n      ROUND_SECOND_BLOCK(f, g, h, a, b, c, d, e, 59, 0x8CC70208, indexW)\n      ROUND_SECOND_BLOCK(e, f, g, h, a, b, c, d, 60, 0x90BEFFFA, indexW)\n      ROUND_SECOND_BLOCK(d, e, f, g, h, a, b, c, 61, 0xA4506CEB, indexW)\n      ROUND_SECOND_BLOCK(c, d, e, f, g, h, a, b, 62, 0xBEF9A3F7, indexW)\n      ROUND_SECOND_BLOCK(b, c, d, e, f, g, h, a, 63, 0xC67178F2, indexW)\n\n      // update hash value\n      hash0 += a;\n    hash1 += b;\n    hash2 += c;\n    hash3 += d;\n    hash4 += e;\n    hash5 += f;\n    hash6 += g;\n    hash7 += h;\n\n    indexW += HASH_BLOCK_NUM_UINT32;\n  }\n\n  /**********************************************************************\n   *************************** MAC COMPARISON ****************************\n   **********************************************************************/\n\n  a = ((uint32_t *)(vmkIV     ))[0];\n  b = ((uint32_t *)(vmkIV +  4))[0];\n  c = ((uint32_t *)(vmkIV +  8))[0];\n  d = ((uint32_t *)(vmkIV + 12))[0];\n\n  encrypt(\n      hash0, hash1, hash2, hash3, hash4, hash5, hash6, hash7,\n      a, b, c, d,\n      &(schedule00), &(schedule01), &(schedule02), &(schedule03)\n         );\n\n  schedule00 =\n    (((uint32_t)(vmkKey[3] ^ ((uint8_t) (schedule00 >> 24) ))) << 24) | \n    (((uint32_t)(vmkKey[2] ^ ((uint8_t) (schedule00 >> 16) ))) << 16) | \n    (((uint32_t)(vmkKey[1] ^ ((uint8_t) (schedule00 >>  8) ))) <<  8) | \n    (((uint32_t)(vmkKey[0] ^ ((uint8_t) (schedule00)))) << 0);\n\n  schedule01 =\n    (((uint32_t)(vmkKey[7] ^ ((uint8_t) (schedule01 >> 24) ))) << 24) | \n    (((uint32_t)(vmkKey[6] ^ ((uint8_t) (schedule01 >> 16) ))) << 16) | \n    (((uint32_t)(vmkKey[5] ^ ((uint8_t) (schedule01 >>  8) ))) <<  8) | \n    (((uint32_t)(vmkKey[4] ^ ((uint8_t) (schedule01)))) << 0);\n\n  schedule02 =\n    (((uint32_t)(vmkKey[11] ^ ((uint8_t) (schedule02 >> 24) ))) << 24) | \n    (((uint32_t)(vmkKey[10] ^ ((uint8_t) (schedule02 >> 16) ))) << 16) | \n    (((uint32_t)(vmkKey[9]  ^ ((uint8_t) (schedule02 >>  8) ))) <<  8) | \n    (((uint32_t)(vmkKey[8]  ^ ((uint8_t) (schedule02)))) << 0);\n\n  schedule03 =\n    (((uint32_t)(vmkKey[15] ^ ((uint8_t) (schedule03 >> 24) ))) << 24) | \n    (((uint32_t)(vmkKey[14] ^ ((uint8_t) (schedule03 >> 16) ))) << 16) | \n    (((uint32_t)(vmkKey[13] ^ ((uint8_t) (schedule03 >>  8) ))) <<  8) | \n    (((uint32_t)(vmkKey[12] ^ ((uint8_t) (schedule03)))) << 0);\n\n  d += 0x01000000;\n\n  encrypt(\n      hash0, hash1, hash2, hash3, hash4, hash5, hash6, hash7,\n      a, b, c, d,\n      &(schedule04), &(schedule05), &(schedule06), &(schedule07)\n         );\n\n  schedule04 =\n    (((uint32_t)(vmkKey[19] ^ ((uint8_t) (schedule04 >> 24) ))) << 24) | \n    (((uint32_t)(vmkKey[18] ^ ((uint8_t) (schedule04 >> 16) ))) << 16) | \n    (((uint32_t)(vmkKey[17] ^ ((uint8_t) (schedule04 >>  8) ))) <<  8) | \n    (((uint32_t)(vmkKey[16] ^ ((uint8_t) (schedule04)))) << 0);\n\n  schedule05 =\n    (((uint32_t)(vmkKey[23] ^ ((uint8_t) (schedule05 >> 24) ))) << 24) | \n    (((uint32_t)(vmkKey[22] ^ ((uint8_t) (schedule05 >> 16) ))) << 16) | \n    (((uint32_t)(vmkKey[21] ^ ((uint8_t) (schedule05 >>  8) ))) <<  8) | \n    (((uint32_t)(vmkKey[20] ^ ((uint8_t) (schedule05)))) << 0);\n\n  schedule06 =\n    (((uint32_t)(vmkKey[27] ^ ((uint8_t) (schedule06 >> 24) ))) << 24) | \n    (((uint32_t)(vmkKey[26] ^ ((uint8_t) (schedule06 >> 16) ))) << 16) | \n    (((uint32_t)(vmkKey[25] ^ ((uint8_t) (schedule06 >>  8) ))) <<  8) | \n    (((uint32_t)(vmkKey[24] ^ ((uint8_t) (schedule06)))) << 0);\n\n  schedule07 =\n    (((uint32_t)(vmkKey[31] ^ ((uint8_t) (schedule07 >> 24) ))) << 24) | \n    (((uint32_t)(vmkKey[30] ^ ((uint8_t) (schedule07 >> 16) ))) << 16) | \n    (((uint32_t)(vmkKey[29] ^ ((uint8_t) (schedule07 >>  8) ))) <<  8) | \n    (((uint32_t)(vmkKey[28] ^ ((uint8_t) (schedule07)))) << 0);\n\n  d += 0x01000000;\n\n  encrypt(\n      hash0, hash1, hash2, hash3, hash4, hash5, hash6, hash7,\n      a, b, c, d,\n      &(schedule08), &(schedule09), &(schedule10), &(schedule11)\n         );\n\n  schedule08 =\n    (((uint32_t)(vmkKey[35] ^ ((uint8_t) (schedule08 >> 24) ))) << 24) | \n    (((uint32_t)(vmkKey[34] ^ ((uint8_t) (schedule08 >> 16) ))) << 16) | \n    (((uint32_t)(vmkKey[33] ^ ((uint8_t) (schedule08 >>  8) ))) <<  8) | \n    (((uint32_t)(vmkKey[32] ^ ((uint8_t) (schedule08)))) << 0);\n\n  schedule09 =\n    (((uint32_t)(vmkKey[39] ^ ((uint8_t) (schedule09 >> 24) ))) << 24) | \n    (((uint32_t)(vmkKey[38] ^ ((uint8_t) (schedule09 >> 16) ))) << 16) | \n    (((uint32_t)(vmkKey[37] ^ ((uint8_t) (schedule09 >>  8) ))) <<  8) | \n    (((uint32_t)(vmkKey[36] ^ ((uint8_t) (schedule09)))) << 0);\n\n  schedule10 =\n    (((uint32_t)(vmkKey[43] ^ ((uint8_t) (schedule10 >> 24) ))) << 24) | \n    (((uint32_t)(vmkKey[42] ^ ((uint8_t) (schedule10 >> 16) ))) << 16) | \n    (((uint32_t)(vmkKey[41] ^ ((uint8_t) (schedule10 >>  8) ))) <<  8) | \n    (((uint32_t)(vmkKey[40] ^ ((uint8_t) (schedule10)))) << 0);\n\n  encrypt(\n      hash0, hash1, hash2, hash3, hash4, hash5, hash6, hash7,\n      ((uint32_t *)(macIV     ))[0],\n      ((uint32_t *)(macIV +  4))[0],\n      ((uint32_t *)(macIV +  8))[0],\n      ((uint32_t *)(macIV + 12))[0],\n      &(schedule16), &(schedule17), &(schedule18), &(schedule19)\n         );\n\n  encrypt(\n      hash0, hash1, hash2, hash3, hash4, hash5, hash6, hash7,\n      ((uint32_t *)(computedMacIV     ))[0],\n      ((uint32_t *)(computedMacIV +  4))[0],\n      ((uint32_t *)(computedMacIV +  8))[0],\n      ((uint32_t *)(computedMacIV + 12))[0],\n      &(schedule12), &(schedule13), &(schedule14), &(schedule15)\n         );\n\n  schedule28 = schedule00 ^ schedule12;\n  schedule29 = schedule01 ^ schedule13;\n  schedule30 = schedule02 ^ schedule14;\n  schedule31 = schedule03 ^ schedule15;\n\n  encrypt(\n      hash0, hash1, hash2, hash3, hash4, hash5, hash6, hash7,\n      schedule28, schedule29, schedule30, schedule31,\n      &(schedule12), &(schedule13), &(schedule14), &(schedule15)\n         );\n\n  schedule28 = schedule04 ^ schedule12;\n  schedule29 = schedule05 ^ schedule13;\n  schedule30 = schedule06 ^ schedule14;\n  schedule31 = schedule07 ^ schedule15;\n\n  encrypt(\n      hash0, hash1, hash2, hash3, hash4, hash5, hash6, hash7,\n      schedule28, schedule29, schedule30, schedule31,\n      &(schedule12), &(schedule13), &(schedule14), &(schedule15)\n         );\n\n  schedule28 = schedule08 ^ schedule12;\n  schedule29 = schedule09 ^ schedule13;\n  schedule30 = schedule10 ^ schedule14;\n  schedule31 = schedule15;\n\n  encrypt(\n      hash0, hash1, hash2, hash3, hash4, hash5, hash6, hash7,\n      schedule28, schedule29, schedule30, schedule31,\n      &(schedule12), &(schedule13), &(schedule14), &(schedule15)\n         );\n\n  auto condition = (\n      schedule12 == ( (uint32_t)\n        (((uint32_t)(mac[3] ^ ((uint8_t) (schedule16 >> 24) ))) << 24) | \n        (((uint32_t)(mac[2] ^ ((uint8_t) (schedule16 >> 16) ))) << 16) | \n        (((uint32_t)(mac[1] ^ ((uint8_t) (schedule16 >>  8) ))) <<  8) | \n        (((uint32_t)(mac[0] ^ ((uint8_t) (schedule16)))) << 0) )\n      )\n    &&\n    (\n     schedule13 == ( (uint32_t)\n       (((uint32_t)(mac[7] ^ ((uint8_t) (schedule17 >> 24) ))) << 24) | \n       (((uint32_t)(mac[6] ^ ((uint8_t) (schedule17 >> 16) ))) << 16) | \n       (((uint32_t)(mac[5] ^ ((uint8_t) (schedule17 >>  8) ))) <<  8) | \n       (((uint32_t)(mac[4] ^ ((uint8_t) (schedule17)))) << 0) )\n    )\n    &&\n    (\n     schedule14 == ( (uint32_t)\n       (((uint32_t)(mac[11] ^ ((uint8_t) (schedule18 >> 24) ))) << 24) | \n       (((uint32_t)(mac[10] ^ ((uint8_t) (schedule18 >> 16) ))) << 16) | \n       (((uint32_t)(mac[9]  ^ ((uint8_t) (schedule18 >>  8) ))) <<  8) | \n       (((uint32_t)(mac[8]  ^ ((uint8_t) (schedule18)))) << 0) )\n    )\n    &&\n    (\n     schedule15 == ( (uint32_t)\n       (((uint32_t)(mac[15] ^ ((uint8_t) (schedule19 >> 24) ))) << 24) | \n       (((uint32_t)(mac[14] ^ ((uint8_t) (schedule19 >> 16) ))) << 16) | \n       (((uint32_t)(mac[13] ^ ((uint8_t) (schedule19 >>  8) ))) <<  8) | \n       (((uint32_t)(mac[12] ^ ((uint8_t) (schedule19)))) << 0) )\n    );\n\n  if (condition) {  // having this if statement makes the code 50k times slower!!\n    *found += 1;\n  }\n}"
        ]
    },
    "ising-cuda": {
        "/Users/gbolet/hecbench-roofline/src/ising-cuda/main.cu": [
            "__global__ void init_spins(signed char* lattice,\n                           const float* __restrict__ randvals,\n                           const long long nx,\n                           const long long ny) {\n  const long long  tid = static_cast<long long>(blockDim.x) * blockIdx.x + threadIdx.x;\n  if (tid >= nx * ny) return;\n\n  float randval = randvals[tid];\n  signed char val = (randval < 0.5f) ? -1 : 1;\n  lattice[tid] = val;\n}",
            "inline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\ninline __host__ __device__ float4 expf(float4 v)\n{\n    return make_float4(expf(v.x), expf(v.y), expf(v.z), expf(v.w));\n}\n\n__global__ void update_lattice(signed char* lattice,\n                               const signed char* __restrict__ op_lattice,\n                               const float* __restrict__ randvals,\n                               const float inv_temp,\n                               const long long nx,\n                               const long long ny) {\n  const long long tid = static_cast<long long>(blockDim.x) * blockIdx.x + threadIdx.x;\n  const int i = tid / ny;\n  const int j = tid % ny;\n\n  if (i >= nx || j >= ny) return;\n\n  // Set stencil indices with periodicity\n  int ipp = (i + 1 < nx) ? i + 1 : 0;\n  int inn = (i - 1 >= 0) ? i - 1: nx - 1;\n  int jpp = (j + 1 < ny) ? j + 1 : 0;\n  int jnn = (j - 1 >= 0) ? j - 1: ny - 1;\n\n  // Select off-column index based on color and row index parity\n  int joff;\n  if (is_black) {\n    joff = (i % 2) ? jpp : jnn;\n  } else {\n    joff = (i % 2) ? jnn : jpp;\n  }\n\n  // Compute sum of nearest neighbor spins\n  signed char nn_sum = op_lattice[inn * ny + j] + op_lattice[i * ny + j] + op_lattice[ipp * ny + j] + op_lattice[i * ny + joff];\n\n  // Determine whether to flip spin\n  signed char lij = lattice[i * ny + j];\n  float acceptance_ratio = expf(-2.0f * inv_temp * nn_sum * lij);\n  if (randvals[i*ny + j] < acceptance_ratio) {\n    lattice[i * ny + j] = -lij;\n  }\n}"
        ]
    },
    "snake-cuda": {
        "/Users/gbolet/hecbench-roofline/src/snake-cuda/kernel.h": [
            "__host__ __device__\ninline uint lsl(uint x, int sa) {\n  if (sa > 0 && sa < 32) return (x << sa);\n  return x;\n}\n\n__host__ __device__\ninline uint lsr(uint x, int sa) {\n  if(sa > 0 && sa < 32) return (x >> sa);\n  return x;\n}\n\n__host__ __device__\ninline uint set_bit(uint &data, int y) {\n  data |= lsl(1, y);\n  return data;\n}\n\n__global__ void sneaky_snake(\n  const uint*__restrict__ F_ReadSeq,\n  const uint*__restrict__ F_RefSeq, \n  int*__restrict__ Ftest_Results, \n  const int NumReads,\n  const int F_ErrorThreshold)\n{\n  int tid = threadIdx.x + blockIdx.x * blockDim.x;\n  if(tid >= NumReads) return;\n\n  uint ReadsPerThread[NBytes];\n  uint RefsPerThread[NBytes];\n\n#pragma unroll\n  for (int i = 0; i < NBytes; i++)\n  {\n    ReadsPerThread[i] = F_ReadSeq[tid*8 + i];\n    RefsPerThread[i] = F_RefSeq[tid*8 + i];\n  }\n\n  /////////////////////////////////////////////////////////////////////////////\n  Ftest_Results[tid] = 1;\n\n  uint ReadCompTmp = 0;\n  uint RefCompTmp = 0;\n  uint DiagonalResult = 0;\n\n  uint ReadTmp1 = 0;\n  uint ReadTmp2 = 0;\n\n  uint RefTmp1 = 0;\n  uint RefTmp2 = 0;\n\n  uint CornerCase = 0;\n\n  int localCounter= 0;\n  int localCounterMax=0;\n  int globalCounter = 0;\n  int Max_leading_zeros = 0;\n  int AccumulatedErrs = 0;\n\n  int Diagonal = 0;\n  int ShiftValue = 0;\n\n  int j = 0; //specifying the j-th int that we are reading in each read-ref comparison (can be from 0 to 7)\n\n  while ( (j < 7) && (globalCounter < 200))\n  {\n    Diagonal = 0;\n    RefTmp1 = lsl(RefsPerThread[j], ShiftValue);\n    RefTmp2 = lsr(RefsPerThread[j + 1], 32 - ShiftValue);\n    ReadTmp1 = lsl(ReadsPerThread[j], ShiftValue);\n    ReadTmp2 = lsr(ReadsPerThread[j + 1], 32 - ShiftValue);\n\n    ReadCompTmp = ReadTmp1 | ReadTmp2;\n    RefCompTmp = RefTmp1 | RefTmp2;\n    DiagonalResult = ReadCompTmp ^ RefCompTmp;\n    localCounterMax = __clz(DiagonalResult);\n\n    //////////////////// Upper diagonals /////////////////////\n\n    for(int e = 1; e <= F_ErrorThreshold; e++)\n    {\n      Diagonal += 1;\n      CornerCase = 0;\n      if ( (j == 0) && ( (ShiftValue - (2*e)) < 0 ) )\n      {\n        ReadTmp1 = lsr(ReadsPerThread[j], 2*e - ShiftValue);\n        ReadTmp2 = 0;\n\n        ReadCompTmp = ReadTmp1 | ReadTmp2;\n        RefCompTmp = RefTmp1 | RefTmp2;\n\n        DiagonalResult = ReadCompTmp ^ RefCompTmp;\n\n        CornerCase = 0;\n        for(int Ci = 0; Ci < (2*e) - ShiftValue; Ci++)\n        {\n          set_bit(CornerCase, 31 - Ci);\n        }\n\n        DiagonalResult  = DiagonalResult | CornerCase;\n        localCounter = __clz(DiagonalResult);\n\n      }\n      else if ( (ShiftValue - (2*e) ) < 0 )\n      {\n        ReadTmp1 = lsl(ReadsPerThread[j-1], 32 - (2*e - ShiftValue));\n        ReadTmp2 = lsr(ReadsPerThread[j], 2*e - ShiftValue);\n\n        ReadCompTmp = ReadTmp1 | ReadTmp2;\n        RefCompTmp = RefTmp1 | RefTmp2;\n\n        DiagonalResult = ReadCompTmp ^ RefCompTmp;\n\n        localCounter = __clz(DiagonalResult);\n      }\n      else\n      {\n        ReadTmp1 = lsl(ReadsPerThread[j], ShiftValue - 2*e);\n        ReadTmp2 = lsr(ReadsPerThread[j+1], 32 - (ShiftValue - 2*e)) ;\n\n        ReadCompTmp = ReadTmp1 | ReadTmp2;\n        RefCompTmp = RefTmp1 | RefTmp2;\n\n        DiagonalResult = ReadCompTmp ^ RefCompTmp;\n\n        localCounter = __clz(DiagonalResult);\n      }\n      if (localCounter>localCounterMax)\n        localCounterMax=localCounter;\n    }\n\n    /*\n       sh = shift\n       up = upper diagonal\n       RC = ReadCompTmp\n       FC = RefCompTmp\n       D = DiagonalResult\n       DN = diagonal\n       LC = localCounter\n       */\n\n    //////////////////// Lower diagonals /////////////////////\n\n    for(int e = 1; e <= F_ErrorThreshold; e++)\n    {\n      Diagonal += 1;\n      CornerCase = 0;\n      if (j<5)\n      {\n        if ((ShiftValue + 2*e) < 32)\n        {\n          ReadTmp1 = lsl(ReadsPerThread[j], ShiftValue + 2*e);\n          ReadTmp2 = lsr(ReadsPerThread[j+1], 32 - (ShiftValue + 2*e));\n\n          ReadCompTmp = ReadTmp1 | ReadTmp2;\n          RefCompTmp = RefTmp1 | RefTmp2;\n\n          DiagonalResult = ReadCompTmp ^ RefCompTmp;\n          localCounter = __clz(DiagonalResult);\n        }\n        else\n        {\n          ReadTmp1 = lsl(ReadsPerThread[j+1], (ShiftValue + 2*e) % 32);\n          ReadTmp2 = lsr(ReadsPerThread[j+2], 32 - (ShiftValue + 2*e) % 32);\n\n          ReadCompTmp = ReadTmp1 | ReadTmp2;\n          RefCompTmp = RefTmp1 | RefTmp2;\n\n          DiagonalResult = 0xffffffff;//ReadCompTmp ^ RefCompTmp;\n\n          DiagonalResult = ReadCompTmp ^ RefCompTmp;\n\n          localCounter = __clz(DiagonalResult);\n        }\n      }\n      else\n      {\n        ReadTmp1 = lsl(ReadsPerThread[j], ShiftValue + 2*e);\n        ReadTmp2 = lsr(ReadsPerThread[j+1], 32 - (ShiftValue + 2*e));\n\n        ReadCompTmp = ReadTmp1 | ReadTmp2;\n        RefCompTmp = RefTmp1 | RefTmp2;\n        DiagonalResult = ReadCompTmp ^ RefCompTmp;\n\n        CornerCase = 0;\n        if ((globalCounter+32)>200) {\n\n          for(int Ci = globalCounter+32-200; Ci < globalCounter+32-200+2*e; Ci++)\n          {\n            set_bit(CornerCase, Ci);\n          }\n        }\n\n        else if ((globalCounter+32)>=(200- (2*e))){\n\n          for(int Ci = 0; Ci < (2*e); Ci++)\n          {\n            set_bit(CornerCase, Ci);\n          }\n        }\n        DiagonalResult = DiagonalResult | CornerCase;\n\n        localCounter = __clz(DiagonalResult);\n      }\n\n      if (localCounter>localCounterMax)\n        localCounterMax=localCounter;\n    }\n\n    /*\n       CC = CornerCase\n       sh = shift\n       up = upper diagonal\n       RC = ReadCompTmp\n       FC = RefCompTmp\n       D = DiagonalResult\n       DN = diagonal\n       LC = localCounter\n       */\n\n    Max_leading_zeros = 0;\n\n    if ( (j == 6) && ( ((localCounterMax/2)*2) >= 8) )\n    {\n      Max_leading_zeros = 8;\n      break;\n    }\n    else if((localCounterMax/2*2) > Max_leading_zeros)\n    {\n      Max_leading_zeros = ((localCounterMax/2)*2);\n    }\n\n    if (((Max_leading_zeros/2) < 16) && (j < 5))\n    {\n      AccumulatedErrs += 1;\n    }\n    else if ((j == 6) && ((Max_leading_zeros/2) < 4))\n    {\n      AccumulatedErrs += 1;\n    }\n\n    if(AccumulatedErrs > F_ErrorThreshold)\n    {\n      Ftest_Results[tid] = 0;\n      break;\n    }\n\n    if(ShiftValue + Max_leading_zeros + 2 >= 32)\n    {\n      j += 1;\n    }\n\n    // ShiftValue_2Ref = (ShiftValue_2Ref + Max_leading_zeros + 2) %32;\n    if (Max_leading_zeros == 32)\n    {\n      globalCounter += Max_leading_zeros;\n    }\n    else\n    {\n      ShiftValue = ((ShiftValue + Max_leading_zeros + 2) % 32);\n      globalCounter += (Max_leading_zeros + 2);\n    }\n  }\n}"
        ]
    },
    "eigenvalue-cuda": {
        "/Users/gbolet/hecbench-roofline/src/eigenvalue-cuda/kernels.cu": [
            "__device__\nfloat calNumEigenValuesLessThan(\n   const float x, \n   const uint width, \n   const float *__restrict__ diagonal, \n   const float *__restrict__ offDiagonal)\n{\n  uint count = 0;\n\n  float prev_diff = (diagonal[0] - x);\n  count += (prev_diff < 0)? 1 : 0;\n  for(uint i = 1; i < width ; i += 1)\n  {\n    float diff = (diagonal[i] - x) - ((offDiagonal[i-1] * offDiagonal[i-1]) / prev_diff);\n\n    count += (diff < 0) ? 1 : 0;\n    prev_diff = diff;\n  }\n  return count;\n}\n\n__global__\nvoid calNumEigenValueInterval(\n    uint  *__restrict__ numEigenIntervals,\n    const float *__restrict__ eigenIntervals,\n    const float *__restrict__ diagonal, \n    const float *__restrict__ offDiagonal,\n    const uint     width)\n{\n  uint gid = blockIdx.x * blockDim.x + threadIdx.x;\n  uint lowerId = 2 * gid; \n  uint upperId = lowerId + 1;\n  float lowerLimit = eigenIntervals[lowerId];\n  float upperLimit = eigenIntervals[upperId];\n  uint lower = calNumEigenValuesLessThan(lowerLimit, width, diagonal, offDiagonal);\n  uint upper = calNumEigenValuesLessThan(upperLimit, width, diagonal, offDiagonal);\n  numEigenIntervals[gid] = upper - lower;\n}",
            "__device__\nfloat calNumEigenValuesLessThan(\n   const float x, \n   const uint width, \n   const float *__restrict__ diagonal, \n   const float *__restrict__ offDiagonal)\n{\n  uint count = 0;\n\n  float prev_diff = (diagonal[0] - x);\n  count += (prev_diff < 0)? 1 : 0;\n  for(uint i = 1; i < width ; i += 1)\n  {\n    float diff = (diagonal[i] - x) - ((offDiagonal[i-1] * offDiagonal[i-1]) / prev_diff);\n\n    count += (diff < 0) ? 1 : 0;\n    prev_diff = diff;\n  }\n  return count;\n}\n\n__global__\nvoid recalculateEigenIntervals(\n          float *__restrict__ newEigenIntervals,\n    const float *__restrict__ eigenIntervals,\n    const uint  *__restrict__ numEigenIntervals,\n    const float *__restrict__ diagonal,\n    const float *__restrict__ offDiagonal,\n    const    uint    width,  \n    const    float   tolerance)\n{\n  uint gid = blockIdx.x * blockDim.x + threadIdx.x;\n  uint lowerId = 2 * gid; \n  uint upperId = lowerId + 1;\n  uint currentIndex = gid;\n\n  uint index = 0;\n  while(currentIndex >= numEigenIntervals[index])\n  {\n    currentIndex -= numEigenIntervals[index];\n    ++index;\n  }\n\n  uint lId = 2 * index;\n  uint uId = lId + 1;\n\n  /* if the number of eigenvalues in the interval is just 1 */\n  if(numEigenIntervals[index] == 1)\n  {\n    float midValue = (eigenIntervals[uId] + eigenIntervals[lId])/2;\n    float n        = calNumEigenValuesLessThan(midValue, width, diagonal, offDiagonal);\n    n -= calNumEigenValuesLessThan(eigenIntervals[lId], width, diagonal, offDiagonal);\n\n    /* check if the interval size is less than tolerance levels */\n    if(eigenIntervals[uId] - eigenIntervals[lId] < tolerance)\n    {\n      newEigenIntervals[lowerId] = eigenIntervals[lId];\n      newEigenIntervals[upperId] = eigenIntervals[uId];\n    }\n    else if(n == 0) /* if the eigenvalue lies in the right half of the interval */\n    {\n      newEigenIntervals[lowerId] = midValue;\n      newEigenIntervals[upperId] = eigenIntervals[uId];\n    }\n    else           /* if the eigenvalue lies in the left half of the interval */\n    {\n      newEigenIntervals[lowerId] = eigenIntervals[lId];\n      newEigenIntervals[upperId] = midValue;\n    }\n  }\n  /* split the intervals into equal intervals of size divisionWidth */\n  else /* (numEigenIntervals[index] > 1) */\n  {\n    float divisionWidth = (eigenIntervals[uId] - eigenIntervals[lId]) / numEigenIntervals[index];\n    newEigenIntervals[lowerId] = eigenIntervals[lId] + divisionWidth * currentIndex;\n    newEigenIntervals[upperId] = newEigenIntervals[lowerId] + divisionWidth;\n  }  \n}"
        ]
    },
    "mask-cuda": {
        "/Users/gbolet/hecbench-roofline/src/mask-cuda/main.cu": [
            "#define N\t\t\t\t\t  6\n\n\n#define T ((int)32)\n\n\n__global__ void sequenceMaskKernel(\n    int N,\n    int M,\n    int B,\n    const T* in,\n    const int* seq_lengths,\n    T fill_val,\n    T* out)\n{\n  if (B >= 0) {\n    KERNEL_LOOP(index, B * N * M) {\n      int k = index % M;\n      int j = (index - k) / M % N;\n      int i = (index - M * j - k) / (N * M);\n      int ind = N * M * i + M * j + k;\n      out[ind] = (k >= seq_lengths[j] ? fill_val : in[ind]);\n    }\n  } else {\n    KERNEL_LOOP(index, N * M) {\n      int i = index / M;\n      int j = index % M;\n      out[index] = (j >= seq_lengths[i] ? fill_val : in[index]);\n    }\n  }\n}",
            "#define N\t\t\t\t\t  6\n\n\n#define T ((int)32)\n\n\n__global__ void windowMaskKernel(\n    int N,\n    int M,\n    int B,\n    const T* in,\n    const int* window_centers,\n    const int radius,\n    T fill_val,\n    T* out) {\n  if (B >= 0) {\n    KERNEL_LOOP(index, B * N * M) {\n      int k = index % M;\n      int j = (index - k) / M % N;\n      int i = (index - M * j - k) / (N * M);\n\n      int ind = N * M * i + M * j + k;\n      out[ind] =\n          (k < window_centers[j] - radius || k > window_centers[j] + radius\n               ? fill_val\n               : in[ind]);\n    }\n  } else {\n    KERNEL_LOOP(index, N * M) {\n      int i = index / M;\n      int j = index % M;\n\n      out[index] =\n          (j < window_centers[i] - radius || j > window_centers[i] + radius\n               ? fill_val\n               : in[index]);\n    }\n  }\n}",
            "#define N\t\t\t\t\t  6\n\n\n#define T ((int)32)\n\n\n__global__ void\nupperMaskKernel(int N, int M, int B, const T* in, T fill_val, T* out) {\n  if (B >= 0) {\n    KERNEL_LOOP(index, B * N * M) {\n      int k = index % M;\n      int j = (index - k) / M % N;\n      int i = (index - M * j - k) / (N * M);\n\n      int ind = N * M * i + M * j + k;\n      out[ind] = (k > j ? fill_val : in[ind]);\n    }\n  } else {\n    KERNEL_LOOP(index, N * M) {\n      int i = index / M;\n      int j = index % M;\n\n      out[index] = (j > i ? fill_val : in[index]);\n    }\n  }\n}",
            "#define N\t\t\t\t\t  6\n\n\n#define T ((int)32)\n\n\n__global__ void\nlowerMaskKernel(int N, int M, int B, const T* in, T fill_val, T* out) {\n  if (B >= 0) {\n    KERNEL_LOOP(index, B * N * M) {\n      int k = index % M;\n      int j = (index - k) / M % N;\n      int i = (index - M * j - k) / (N * M);\n\n      int ind = N * M * i + M * j + k;\n      out[ind] = (k < j ? fill_val : in[ind]);\n    }\n  } else {\n    KERNEL_LOOP(index, N * M) {\n      int i = index / M;\n      int j = index % M;\n\n      out[index] = (j < i ? fill_val : in[index]);\n    }\n  }\n}",
            "#define N\t\t\t\t\t  6\n\n\n#define T ((int)32)\n\n\n__global__ void\nupperDiagMaskKernel(int N, int M, int B, const T* in, T fill_val, T* out) {\n  if (B >= 0) {\n    KERNEL_LOOP(index, B * N * M) {\n      int k = index % M;\n      int j = (index - k) / M % N;\n      int i = (index - M * j - k) / (N * M);\n\n      int ind = N * M * i + M * j + k;\n      out[ind] = (k >= j ? fill_val : in[ind]);\n    }\n  } else {\n    KERNEL_LOOP(index, N * M) {\n      int i = index / M;\n      int j = index % M;\n\n      out[index] = (j >= i ? fill_val : in[index]);\n    }\n  }\n}",
            "#define N\t\t\t\t\t  6\n\n\n#define T ((int)32)\n\n\n__global__ void\nlowerDiagMaskKernel(int N, int M, int B, const T* in, T fill_val, T* out) {\n  if (B >= 0) {\n    KERNEL_LOOP(index, B * N * M) {\n      int k = index % M;\n      int j = (index - k) / M % N;\n      int i = (index - M * j - k) / (N * M);\n\n      int ind = N * M * i + M * j + k;\n      out[ind] = (k <= j ? fill_val : in[ind]);\n    }\n  } else {\n    KERNEL_LOOP(index, N * M) {\n      int i = index / M;\n      int j = index % M;\n\n      out[index] = (j <= i ? fill_val : in[index]);\n    }\n  }\n}"
        ]
    },
    "morphology-cuda": {
        "/Users/gbolet/hecbench-roofline/src/morphology-cuda/morphology.cu": [
            "inline __host__ __device__ uint4 make_uint4(int4 a)\n{\n    return make_uint4(uint(a.x), uint(a.y), uint(a.z), uint(a.w));\n}\n\ninline __host__ __device__ uint4 max(uint4 a, uint4 b)\n{\n    return make_uint4(max(a.x,b.x), max(a.y,b.y), max(a.z,b.z), max(a.w,b.w));\n}\n\ninline __device__ unsigned char elementOp<MorphOpType::DILATE>(unsigned char lhs, unsigned char rhs)\n{\n  return max(lhs, rhs);\n}\n\n__device__ void twoWayScan(\n    const unsigned char* __restrict__ buffer,\n          unsigned char* __restrict__ opArray,\n    const int selSize,\n    const int tid)\n{\n  opArray[tid] = buffer[tid];\n  opArray[tid + selSize] = buffer[tid + selSize];\n  __syncthreads();\n\n  for (int offset = 1; offset < selSize; offset *= 2) {\n    if (tid >= offset) {\n      opArray[tid + selSize - 1] = \n        elementOp<opType>(opArray[tid + selSize - 1], opArray[tid + selSize - 1 - offset]);\n    }\n    if (tid <= selSize - 1 - offset) {\n      opArray[tid] = elementOp<opType>(opArray[tid], opArray[tid + offset]);\n    }\n    __syncthreads();\n  }\n}\n\n__global__ void vhgw_horiz(\n          unsigned char* __restrict__ dst,\n    const unsigned char* __restrict__ src,\n    const int width,\n    const int height,\n    const int selSize\n    )\n{\n  extern __shared__ unsigned char sMem[];\n  unsigned char* buffer = sMem;\n  unsigned char* opArray = buffer + 2 * selSize;\n\n  const int tidx = threadIdx.x + blockIdx.x * blockDim.x;\n  const int tidy = threadIdx.y + blockIdx.y * blockDim.y;\n\n  if (tidx >= width || tidy >= height) return;\n\n  buffer[threadIdx.x] = src[tidy * width + tidx];\n  if (tidx + selSize < width) {\n    buffer[threadIdx.x + selSize] = src[tidy * width + tidx + selSize];\n  }\n  __syncthreads();\n\n  twoWayScan<opType>(buffer, opArray, selSize, threadIdx.x);\n\n  if (tidx + selSize/2 < width - selSize/2) {\n    dst[tidy * width + tidx + selSize/2] = \n      elementOp<opType>(opArray[threadIdx.x], opArray[threadIdx.x + selSize - 1]);\n  }\n}",
            "inline __host__ __device__ uint4 make_uint4(int4 a)\n{\n    return make_uint4(uint(a.x), uint(a.y), uint(a.z), uint(a.w));\n}\n\ninline __host__ __device__ uint4 max(uint4 a, uint4 b)\n{\n    return make_uint4(max(a.x,b.x), max(a.y,b.y), max(a.z,b.z), max(a.w,b.w));\n}\n\ninline __device__ unsigned char elementOp<MorphOpType::DILATE>(unsigned char lhs, unsigned char rhs)\n{\n  return max(lhs, rhs);\n}\n\ninline __device__ unsigned char borderValue<MorphOpType::DILATE>()\n{\n  return WHITE;\n}\n\n__device__ void twoWayScan(\n    const unsigned char* __restrict__ buffer,\n          unsigned char* __restrict__ opArray,\n    const int selSize,\n    const int tid)\n{\n  opArray[tid] = buffer[tid];\n  opArray[tid + selSize] = buffer[tid + selSize];\n  __syncthreads();\n\n  for (int offset = 1; offset < selSize; offset *= 2) {\n    if (tid >= offset) {\n      opArray[tid + selSize - 1] = \n        elementOp<opType>(opArray[tid + selSize - 1], opArray[tid + selSize - 1 - offset]);\n    }\n    if (tid <= selSize - 1 - offset) {\n      opArray[tid] = elementOp<opType>(opArray[tid], opArray[tid + offset]);\n    }\n    __syncthreads();\n  }\n}\n\n__global__ void vhgw_vert(\n          unsigned char* __restrict__ dst,\n    const unsigned char* __restrict__ src,\n    const int width,\n    const int height,\n    const int selSize)\n{\n  extern __shared__ unsigned char sMem[];\n  unsigned char* buffer = sMem;\n  unsigned char* opArray = buffer + 2 * selSize;\n\n  const int tidx = threadIdx.x + blockIdx.x * blockDim.x;\n  const int tidy = threadIdx.y + blockIdx.y * blockDim.y;\n  if (tidy >= height || tidx >= width) {\n    return;\n  }\n\n  buffer[threadIdx.y] = src[tidy * width + tidx];\n  if (tidy + selSize < height) {\n    buffer[threadIdx.y + selSize] = src[(tidy + selSize) * width + tidx];\n  }\n  __syncthreads();\n\n  twoWayScan<opType>(buffer, opArray, selSize, threadIdx.y);\n\n  if (tidy + selSize/2 < height - selSize/2) {\n    dst[(tidy + selSize/2) * width + tidx] = \n      elementOp<opType>(opArray[threadIdx.y], opArray[threadIdx.y + selSize - 1]);\n  }\n\n  if (tidy < selSize/2 || tidy >= height - selSize/2) {\n    dst[tidy * width + tidx] = borderValue<opType>();\n  }\n}"
        ]
    },
    "mdh-cuda": {
        "/Users/gbolet/hecbench-roofline/src/mdh-cuda/main.cu": [
            "inline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\ninline __host__ __device__ float4 expf(float4 v)\n{\n    return make_float4(expf(v.x), expf(v.y), expf(v.z), expf(v.w));\n}\n\ninline __host__ __device__ float4 sqrtf(float4 v)\n{\n    return make_float4(sqrtf(v.x), sqrtf(v.y), sqrtf(v.z), sqrtf(v.w));\n}\n\n__global__ void mdh (\n    const float *__restrict__ ax, \n    const float *__restrict__ ay,\n    const float *__restrict__ az,\n    const float *__restrict__ gx, \n    const float *__restrict__ gy, \n    const float *__restrict__ gz,\n    const float *__restrict__ charge, \n    const float *__restrict__ size, \n          float *__restrict__ val,\n    const float pre1,\n    const float xkappa, \n    const int natom)\n{\n  extern __shared__ float shared[];\n\n  int lid = threadIdx.x;\n  int lsize = blockDim.x;\n  int igrid = blockIdx.x * lsize + lid;\n  float4 v = make_float4(0.0f);\n  float4 lgx = reinterpret_cast<const float4*>(gx)[igrid];\n  float4 lgy = reinterpret_cast<const float4*>(gy)[igrid];\n  float4 lgz = reinterpret_cast<const float4*>(gz)[igrid];\n\n  for(int jatom = 0; jatom < natom; jatom+=lsize )\n  {\n    if((jatom+lsize) > natom) lsize = natom - jatom;\n\n    if((jatom + lid) < natom) {\n      shared[lid * 5    ] = ax[jatom + lid];\n      shared[lid * 5 + 1] = ay[jatom + lid];\n      shared[lid * 5 + 2] = az[jatom + lid];\n      shared[lid * 5 + 3] = charge[jatom + lid];\n      shared[lid * 5 + 4] = size[jatom + lid];\n    }\n    __syncthreads();\n\n    for(int i=0; i<lsize; i++) {\n      float4 dx = lgx - shared[i * 5    ];\n      float4 dy = lgy - shared[i * 5 + 1];\n      float4 dz = lgz - shared[i * 5 + 2];\n      float4 dist = sqrtf( dx * dx + dy * dy + dz * dz );\n      v += pre1 * (shared[i * 5 + 3] / dist)  *\n           expf( -xkappa * (dist - shared[i * 5 + 4])) /\n           (1.0f + xkappa * shared[i * 5 + 4]);\n    }\n    __syncthreads();\n  }\n  reinterpret_cast<float4*>(val)[ igrid ] = v;\n}",
            "inline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\ninline __host__ __device__ float4 expf(float4 v)\n{\n    return make_float4(expf(v.x), expf(v.y), expf(v.z), expf(v.w));\n}\n\ninline __host__ __device__ float4 sqrtf(float4 v)\n{\n    return make_float4(sqrtf(v.x), sqrtf(v.y), sqrtf(v.z), sqrtf(v.w));\n}\n\n__global__ void mdh2 (\n    const float *__restrict__ ax, \n    const float *__restrict__ ay,\n    const float *__restrict__ az,\n    const float *__restrict__ gx, \n    const float *__restrict__ gy, \n    const float *__restrict__ gz,\n    const float *__restrict__ charge, \n    const float *__restrict__ size, \n          float *__restrict__ val,\n    const float pre1, \n    const float xkappa, \n    const int natom)\n{\n  extern __shared__ float shared[];\n\n  int lid = threadIdx.x;\n  int lsize = blockDim.x;\n  int igrid = blockIdx.x * lsize + lid;\n  float4 v = make_float4(0.0f);\n  float4 lgx = reinterpret_cast<const float4*>(gx)[igrid];\n  float4 lgy = reinterpret_cast<const float4*>(gy)[igrid];\n  float4 lgz = reinterpret_cast<const float4*>(gz)[igrid];\n\n  for(int jatom = 0; jatom < natom; jatom+=lsize )\n  {\n    if((jatom+lsize) > natom) lsize = natom - jatom;\n\n    if((jatom + lid) < natom) {\n      shared[lid          ] = ax[jatom + lid];\n      shared[lid +   lsize] = ay[jatom + lid];\n      shared[lid + 2*lsize] = az[jatom + lid];\n      shared[lid + 3*lsize] = charge[jatom + lid];\n      shared[lid + 4*lsize] = size[jatom + lid];\n    }\n    __syncthreads();\n\n    for(int i=0; i<lsize; i++) {\n      float4 dx = lgx - shared[i          ];\n      float4 dy = lgy - shared[i +   lsize];\n      float4 dz = lgz - shared[i + 2*lsize];\n      float4 dist = sqrtf( dx * dx + dy * dy + dz * dz );\n      v += pre1 * ( shared[i + 3*lsize] / dist )  *\n           expf( -xkappa * (dist - shared[i + 4*lsize])) /\n           (1.0f + xkappa * shared[i + 4*lsize]);\n    }\n    __syncthreads();\n  }\n  reinterpret_cast<float4*>(val)[ igrid ] = v;\n}",
            "inline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\ninline __device__ float4 fast_expf(float4 v)\n{\n    return make_float4(__expf(v.x), __expf(v.y), __expf(v.z), __expf(v.w));\n}\n\ninline __device__ float4 fast_sqrtf(float4 v)\n{\n    return make_float4(__fsqrt_rn(v.x), __fsqrt_rn(v.y), __fsqrt_rn(v.z), __fsqrt_rn(v.w));\n}\n\n__global__ void mdh3 (\n    const float *__restrict__ ax, \n    const float *__restrict__ ay,\n    const float *__restrict__ az,\n    const float *__restrict__ gx, \n    const float *__restrict__ gy, \n    const float *__restrict__ gz,\n    const float *__restrict__ charge, \n    const float *__restrict__ size, \n          float *__restrict__ val,\n    const float pre1, \n    const float xkappa, \n    const int natom)\n{\n  extern __shared__ float shared[];\n\n  int lid = threadIdx.x;\n  int lsize = blockDim.x;\n  int igrid = blockIdx.x * lsize + lid;\n  float4 v = make_float4(0.0f);\n  float4 lgx = reinterpret_cast<const float4*>(gx)[igrid];\n  float4 lgy = reinterpret_cast<const float4*>(gy)[igrid];\n  float4 lgz = reinterpret_cast<const float4*>(gz)[igrid];\n\n  for(int jatom = 0; jatom < natom; jatom+=lsize )\n  {\n    if((jatom+lsize) > natom) lsize = natom - jatom;\n\n    if((jatom + lid) < natom) {\n      shared[lid          ] = ax[jatom + lid];\n      shared[lid +   lsize] = ay[jatom + lid];\n      shared[lid + 2*lsize] = az[jatom + lid];\n      shared[lid + 3*lsize] = charge[jatom + lid];\n      shared[lid + 4*lsize] = size[jatom + lid];\n    }\n    __syncthreads();\n\n    for(int i=0; i<lsize; i++) {\n      float4 dx = lgx - shared[i          ];\n      float4 dy = lgy - shared[i +   lsize];\n      float4 dz = lgz - shared[i + 2*lsize];\n      float4 dist = fast_sqrtf( dx * dx + dy * dy + dz * dz );\n      v += pre1 * ( shared[i + 3*lsize] / dist )  *\n           fast_expf( -xkappa * (dist - shared[i + 4*lsize])) /\n           (1.0f + xkappa * shared[i + 4*lsize]);\n    }\n    __syncthreads();\n  }\n  reinterpret_cast<float4*>(val)[ igrid ] = v;\n}"
        ]
    },
    "ldpc-cuda": {
        "/Users/gbolet/hecbench-roofline/src/ldpc-cuda/kernel.cu": [
            "__global__ void ldpc_cnp_kernel_1st_iter(\n    const float * dev_llr,\n    float * dev_dt,\n    float * dev_R,\n    const char * dev_h_element_count1,\n    const h_element * dev_h_compact1)\n{\n#if MODE == WIFI\n  if(threadIdx.x >= Z)\n    return;\n#endif\n\n  int iCW = threadIdx.y; // index of CW in a MCW\n  int iMCW = blockIdx.y; // index of MCW\n  int iCurrentCW = iMCW * CW + iCW;\n\n\n  //for step 1: update dt\n  int iBlkRow; // block row in h_base\n  int iBlkCol; // block col in h_base\n  int iSubRow; // row index in sub_block of h_base\n  int iCol; // overall col index in h_base\n  int offsetR;\n\n  iSubRow = threadIdx.x;\n  iBlkRow = blockIdx.x;\n\n  int size_llr_CW = COL; // size of one llr CW block\n  int size_R_CW = ROW * BLK_COL;  // size of one R/dt CW block\n  int shift_t;\n\n  // For 2-min algorithm.\n  char Q_sign = 0;\n  char sq;\n  float Q, Q_abs;\n  float R_temp;\n\n  float sign = 1.0f;\n  float rmin1 = 1000.0f;\n  float rmin2 = 1000.0f;\n  char idx_min = 0;\n\n  h_element h_element_t;\n  int s = dev_h_element_count1[iBlkRow];\n  offsetR = size_R_CW * iCurrentCW + iBlkRow * Z + iSubRow;\n\n  // The 1st recursion\n  for(int i = 0; i < s; i++) // loop through all the ZxZ sub-blocks in a row\n  {\n    h_element_t = dev_h_compact1[i * H_COMPACT1_ROW + iBlkRow];\n\n    iBlkCol = h_element_t.y;\n    shift_t = h_element_t.value;\n\n    shift_t = (iSubRow + shift_t);\n    if(shift_t >= Z) shift_t = shift_t - Z;\n\n    iCol = iBlkCol * Z + shift_t;\n\n    Q = dev_llr[size_llr_CW * iCurrentCW + iCol];// - R_temp;\n    Q_abs = fabsf(Q);\n    sq = Q < 0;\n\n    // quick version\n    sign = sign * (1 - sq * 2);\n    Q_sign |= sq << i;\n\n    if (Q_abs < rmin1)\n    {\n      rmin2 = rmin1;\n      rmin1 = Q_abs;\n      idx_min = i;\n    } else if (Q_abs < rmin2)\n    {\n      rmin2 = Q_abs;\n    }\n  }\n\n  // The 2nd recursion\n  for(int i = 0; i < s; i ++)\n  {\n    // v0: Best performance so far. 0.75f is the value of alpha.\n    sq = 1 - 2 * ((Q_sign >> i) & 0x01);\n    R_temp = 0.75f * sign * sq * (i != idx_min ? rmin1 : rmin2);\n\n    // write results to global memory\n    h_element_t = dev_h_compact1[i * H_COMPACT1_ROW + iBlkRow];\n    int addr_temp = offsetR + h_element_t.y * ROW;\n    dev_dt[addr_temp] = R_temp;// - R1[i]; // compute the dt value for current llr.\n    dev_R[addr_temp] = R_temp; // update R, R=R'.\n  }\n}",
            "__global__ void ldpc_cnp_kernel(\n    const float * dev_llr,\n    float * dev_dt,\n    float * dev_R,\n    const char * dev_h_element_count1,\n    const h_element * dev_h_compact1)\n{\n#if MODE == WIFI\n  if(threadIdx.x >= Z)\n    return;\n#endif\n\n  // Define cache for R: Rcache[NON_EMPTY_ELMENT][nThreadPerBlock]\n  // extern means that the memory is allocated dynamically at run-time\n  extern __shared__ float RCache[];\n  int iRCacheLine = threadIdx.y * blockDim.x + threadIdx.x;\n\n  int iCW = threadIdx.y; // index of CW in a MCW\n  int iMCW = blockIdx.y; // index of MCW\n  int iCurrentCW = iMCW * CW + iCW;\n\n\n  //for step 1: update dt\n  int iBlkRow; // block row in h_base\n  int iBlkCol; // block col in h_base\n  int iSubRow; // row index in sub_block of h_base\n  int iCol; // overall col index in h_base\n  int offsetR;\n\n  iSubRow = threadIdx.x;\n  iBlkRow = blockIdx.x;\n\n  int size_llr_CW = COL; // size of one llr CW block\n  int size_R_CW = ROW * BLK_COL;  // size of one R/dt CW block\n\n  //float R1[NON_EMPTY_ELMENT];\n  int shift_t;\n\n  // For 2-min algorithm.\n  char Q_sign = 0;\n  char sq;\n  float Q, Q_abs;\n  float R_temp;\n\n  float sign = 1.0f;\n  float rmin1 = 1000.0f;\n  float rmin2 = 1000.0f;\n  char idx_min = 0;\n\n  h_element h_element_t;\n  int s = dev_h_element_count1[iBlkRow];\n  offsetR = size_R_CW * iCurrentCW + iBlkRow * Z + iSubRow;\n\n  // The 1st recursion\n  // TODO: Is s always the same? If so we can unroll the loop with #pragma unroll\n  for(int i = 0; i < s; i++) // loop through all the ZxZ sub-blocks in a row\n  {\n    h_element_t = dev_h_compact1[i * H_COMPACT1_ROW + iBlkRow];\n\n    iBlkCol = h_element_t.y;\n    shift_t = h_element_t.value;\n\n    shift_t = (iSubRow + shift_t);\n    if(shift_t >= Z) shift_t = shift_t - Z;\n\n    iCol = iBlkCol * Z + shift_t;\n\n    R_temp = dev_R[offsetR + iBlkCol * ROW];\n\n    RCache[i * THREADS_PER_BLOCK + iRCacheLine] =  R_temp;\n\n    Q = dev_llr[size_llr_CW * iCurrentCW + iCol] - R_temp;\n    Q_abs = fabsf(Q);\n\n    sq = Q < 0;\n    sign = sign * (1 - sq * 2);\n    Q_sign |= sq << i;\n\n    if (Q_abs < rmin1)\n    {\n      rmin2 = rmin1;\n      rmin1 = Q_abs;\n      idx_min = i;\n    } else if (Q_abs < rmin2)\n    {\n      rmin2 = Q_abs;\n    }\n  }\n\n  __syncthreads();\n\n  // The 2nd recursion\n  //#pragma unroll\n  for(int i = 0; i < s; i ++)\n  {\n    sq = 1 - 2 * ((Q_sign >> i) & 0x01);\n    R_temp = 0.75f * sign * sq * (i != idx_min ? rmin1 : rmin2);\n\n    // write results to global memory\n    h_element_t = dev_h_compact1[i * H_COMPACT1_ROW + iBlkRow];\n    int addr_temp = h_element_t.y * ROW + offsetR;\n    dev_dt[addr_temp] = R_temp - RCache[i * THREADS_PER_BLOCK + iRCacheLine];\n    dev_R[addr_temp] = R_temp; // update R, R=R'.\n  }\n}",
            "__global__ void\nldpc_vnp_kernel_normal(\n    float * dev_llr, \n    float * dev_dt, \n    const char *dev_h_element_count2,\n    const h_element *dev_h_compact2)\n{\n\n#if MODE == WIFI\n  if(threadIdx.x >= Z)\n    return;\n#endif\n\n  int  iCW = threadIdx.y; // index of CW in a MCW\n  int iMCW = blockIdx.y; // index of MCW\n  int iCurrentCW = iMCW * CW + iCW;\n\n\n  int iBlkCol;\n  int iBlkRow;\n  int iSubCol;\n  int iRow;\n  int iCol;\n\n  int shift_t, sf;\n  int llr_index;\n  float APP;\n\n  h_element h_element_t;\n\n  iBlkCol = blockIdx.x;\n  iSubCol = threadIdx.x;\n\n  int size_llr_CW = COL; // size of one llr CW block\n  int size_R_CW = ROW * BLK_COL;  // size of one R/dt CW block\n\n  // update all the llr values\n  iCol = iBlkCol * Z + iSubCol;\n  llr_index = size_llr_CW * iCurrentCW + iCol;\n\n  APP = dev_llr[llr_index];\n  int offsetDt = size_R_CW * iCurrentCW + iBlkCol * ROW;\n\n  for(int i = 0; i < dev_h_element_count2[iBlkCol]; i++)\n  {\n    h_element_t = dev_h_compact2[i * H_COMPACT2_COL + iBlkCol];\n\n    shift_t = h_element_t.value;\n    iBlkRow = h_element_t.x;\n\n    sf = iSubCol - shift_t;\n    if(sf < 0) sf = sf + Z;\n\n    iRow = iBlkRow * Z + sf;\n    APP = APP + dev_dt[offsetDt + iRow];\n  }\n  // write back to device global memory\n  dev_llr[llr_index] = APP;\n\n  // No hard decision for non-last iteration.\n}",
            "__global__ void ldpc_vnp_kernel_last_iter(\n    const float * dev_llr,\n    const float * dev_dt,\n    int * dev_hd,\n    const char *dev_h_element_count2,\n    const h_element *dev_h_compact2)\n{\n#if MODE == WIFI\n  if(threadIdx.x >= Z)\n    return;\n#endif\n\n  int  iCW = threadIdx.y; // index of CW in a MCW\n  int iMCW = blockIdx.y; // index of MCW\n  int iCurrentCW = iMCW * CW + iCW;\n\n\n  int iBlkCol;\n  int iBlkRow;\n  int iSubCol;\n  int iRow;\n  int iCol;\n\n  int shift_t, sf;\n  int llr_index;\n  float APP;\n\n  h_element h_element_t;\n\n  iBlkCol = blockIdx.x;\n  iSubCol = threadIdx.x;\n\n  int size_llr_CW = COL; // size of one llr CW block\n  int size_R_CW = ROW * BLK_COL;  // size of one R/dt CW block\n\n  // update all the llr values\n  iCol = iBlkCol * Z + iSubCol;\n  llr_index = size_llr_CW * iCurrentCW + iCol;\n\n  APP = dev_llr[llr_index];\n\n  int offsetDt = size_R_CW * iCurrentCW + iBlkCol * ROW;\n\n  for(int i = 0; i < dev_h_element_count2[iBlkCol]; i ++)\n  {\n    h_element_t = dev_h_compact2[i * H_COMPACT2_COL + iBlkCol];\n\n    shift_t = h_element_t.value;\n    iBlkRow = h_element_t.x;\n\n    sf = iSubCol - shift_t;\n    if(sf < 0) sf = sf + Z;\n\n    iRow = iBlkRow * Z + sf;\n    APP = APP + dev_dt[offsetDt + iRow];\n  }\n\n  // For the last iteration, we don't need to write intermediate results to\n  // global memory. Instead, we directly make a hard decision.\n  if(APP > 0)\n    dev_hd[llr_index] = 0;\n  else\n    dev_hd[llr_index] = 1;\n}"
        ]
    },
    "tissue-cuda": {
        "/Users/gbolet/hecbench-roofline/src/tissue-cuda/main.cu": [
            "inline __host__ __device__ int4 make_int4(float4 a)\n{\n    return make_int4(int(a.x), int(a.y), int(a.z), int(a.w));\n}\n\ninline __host__ __device__ int4 abs(int4 v)\n{\n    return make_int4(abs(v.x), abs(v.y), abs(v.z), abs(v.w));\n}\n\n__global__ void tissue(\n    const   int *__restrict__ d_tisspoints,\n    const float *__restrict__ d_gtt,\n    const float *__restrict__ d_gbartt,\n          float *__restrict__ d_ct,\n    const float *__restrict__ d_ctprev,\n    const float *__restrict__ d_qt,\n    int nnt, int nntDev, int step, int isp)\n{\n  int jtp,ixyz,ix,iy,iz,jx,jy,jz,istep;\n  int nnt2 = 2*nnt;\n  float p = 0.f;\n\n  const int i = blockDim.x * blockIdx.x + threadIdx.x;\n  const int itp = i/step;\n  const int itp1 = i%step;\n  if(itp < nnt) {\n    ix = d_tisspoints[itp];\n    iy = d_tisspoints[itp+nnt];\n    iz = d_tisspoints[itp+nnt2];\n    for(jtp = itp1; jtp < nnt; jtp += step) {\n      jx = d_tisspoints[jtp];\n      jy = d_tisspoints[jtp+nnt];\n      jz = d_tisspoints[jtp+nnt2];\n      ixyz = abs(jx-ix) + abs(jy-iy) + abs(jz-iz) + (isp-1)*nntDev;\n      p += d_gtt[ixyz]*d_ctprev[jtp] + d_gbartt[ixyz]*d_qt[jtp];\n    }\n    if(itp1 == 0) d_ct[itp] = p;\n  }\n  // d_ct is incremented in sequence from the needed threads\n  for(istep=1; istep<step; istep++)\n    if(itp1 == istep && itp < nnt) d_ct[itp] += p;\n}"
        ]
    },
    "reaction-cuda": {
        "/Users/gbolet/hecbench-roofline/src/reaction-cuda/kernels.cu": [
            "__global__ void reaction_gray_scott(\n    const float *__restrict__ fx,\n    const float *__restrict__ fy,\n    float *__restrict__ drx,\n    float *__restrict__ dry,\n    const unsigned int ncells,\n    const float d_c1,\n    const float d_c2)\n{\n  int index = blockIdx.x * blockDim.x + threadIdx.x;\n  int stride = blockDim.x * gridDim.x;\n\n  for(int i = index; i < ncells; i += stride) {\n    float r = fx[i] * fy[i] * fy[i];\n    drx[i] = -r + d_c1 * (1.f - fx[i]);\n    dry[i] = r - (d_c1 + d_c2) * fy[i];\n  }\n}",
            "__global__ void derivative_x2_pbc(\n    const float *__restrict__ f,\n    float *__restrict__ df,\n    const unsigned int mx,\n    const unsigned int my,\n    const unsigned int pencils)\n{\n  const int offset = 1;\n  extern __shared__ float s_f[]; // 2-wide halo\n\n  int i   = threadIdx.x;\n  int j   = blockIdx.x * blockDim.y + threadIdx.y;\n  int k   = blockIdx.y;\n  int si  = i + offset;  // local i for shared memory access + halo offset\n  int sj  = threadIdx.y; // local j for shared memory access\n\n  int globalIdx = k * mx * my + j * mx + i;\n\n  s_f[sj * (mx + 2 * offset) + si] = f[globalIdx];\n\n  __syncthreads();\n\n  // fill in periodic images in shared memory array\n  if (i < offset) {\n    s_f[sj * (mx + 2 * offset) + si - offset]  = s_f[sj * (mx + 2 * offset) + si + mx - offset];\n    s_f[sj * (mx + 2 * offset) + si + mx] = s_f[sj * (mx + 2 * offset) + si];\n  }\n\n  __syncthreads();\n\n  df[globalIdx] = s_f[sj * (mx + 2 * offset) + si + 1] - 2.f * s_f[sj * (mx + 2 * offset) + si] + s_f[sj * (mx + 2 * offset) + si - 1];\n}",
            "__global__ void derivative_x2_zeroflux(\n    const float *__restrict__ f,\n    float *__restrict__ df,\n    const unsigned int mx,\n    const unsigned int my)\n{\n  extern __shared__ float s_f[];\n\n  int i   = threadIdx.x;\n  int j   = blockIdx.x * blockDim.y + threadIdx.y;\n  int k   = blockIdx.y;\n  int sj  = threadIdx.y; // local j for shared memory access\n\n  int globalIdx = k * mx * my + j * mx + i;\n\n  s_f[sj * mx + i] = f[globalIdx];\n\n  __syncthreads();\n\n  if(i == 0) {\n    df[globalIdx] = s_f[sj * mx + i + 1] - s_f[sj * mx + i];\n  } else if(i == (mx - 1)) {\n    df[globalIdx] = s_f[sj * mx + i - 1] - s_f[sj * mx + i];\n  } else {\n    df[globalIdx] = s_f[sj * mx + i + 1] - 2.f * s_f[sj * mx + i] + s_f[sj * mx + i - 1];\n  }\n}",
            "__global__ void derivative_y2_pbc(\n    const float *__restrict__ f,\n    float *__restrict__ df,\n    const unsigned int mx,\n    const unsigned int my,\n    const unsigned int pencils)\n{\n  const int offset = 1;\n  extern __shared__ float s_f[]; // 2-wide halo\n\n  int i  = blockIdx.x * blockDim.x + threadIdx.x;\n  int j  = threadIdx.y;\n  int k  = blockIdx.y;\n  int si = threadIdx.x;\n  int sj = j + offset;\n\n  int globalIdx = k * mx * my + j * mx + i;\n\n  s_f[sj * pencils + si] = f[globalIdx];\n\n  __syncthreads();\n\n  // fill in periodic images in shared memory array\n  if (j < offset) {\n    s_f[(sj - offset) * pencils + si]  = s_f[(sj + my - offset) * pencils + si];\n    s_f[(sj + my) * pencils + si] = s_f[sj * pencils + si];\n  }\n\n  __syncthreads();\n\n  df[globalIdx] = s_f[(sj+1) * pencils + si] - 2.f * s_f[sj * pencils + si] + s_f[(sj-1) * pencils + si];\n}",
            "__global__ void derivative_y2_zeroflux(\n    const float *__restrict__ f,\n    float *__restrict__ df,\n    const unsigned int mx,\n    const unsigned int my,\n    const unsigned int pencils)\n{\n  extern __shared__ float s_f[];\n\n  int i  = blockIdx.x * blockDim.x + threadIdx.x;\n  int j  = threadIdx.y;\n  int k  = blockIdx.y;\n  int si = threadIdx.x;\n\n  int globalIdx = k * mx * my + j * mx + i;\n\n  s_f[j * pencils + si] = f[globalIdx];\n\n  __syncthreads();\n\n  if(j == 0) {\n    df[globalIdx] = s_f[(j+1) * pencils + si] - s_f[j * pencils + si];\n  } else if(j == (my - 1)) {\n    df[globalIdx] = s_f[(j-1) * pencils + si] - s_f[j * pencils + si];\n  } else {\n    df[globalIdx] = s_f[(j+1) * pencils + si] - 2.f * s_f[j * pencils + si] + s_f[(j-1) * pencils + si];\n  }\n}",
            "__global__ void derivative_z2_pbc(\n    const float *__restrict__ f,\n    float *__restrict__ df,\n    const unsigned int mx,\n    const unsigned int my,\n    const unsigned int mz,\n    const unsigned int pencils)\n{\n  const int offset = 1;\n  extern __shared__ float s_f[]; // 2-wide halo\n\n  int i  = blockIdx.x * blockDim.x + threadIdx.x;\n  int j  = blockIdx.y;\n  int k  = threadIdx.y;\n  int si = threadIdx.x;\n  int sk = k + offset; // halo offset\n\n  int globalIdx = k * mx * my + j * mx + i;\n\n  s_f[sk * pencils + si] = f[globalIdx];\n\n  __syncthreads();\n\n  // fill in periodic images in shared memory array\n  if (k < offset) {\n    s_f[(sk - offset) * pencils + si]  = s_f[(sk + mz - offset) * pencils + si];\n    s_f[(sk + mz) * pencils + si] = s_f[sk * pencils + si];\n  }\n\n  __syncthreads();\n\n  df[globalIdx] = s_f[(sk+1) * pencils + si] - 2.f * s_f[sk * pencils + si] + s_f[(sk-1) * pencils + si];\n}",
            "__global__ void derivative_z2_zeroflux(\n    const float *__restrict__ f,\n    float *__restrict__ df,\n    const unsigned int mx,\n    const unsigned int my,\n    const unsigned int mz,\n    const unsigned int pencils)\n{\n  extern __shared__ float s_f[]; // 2-wide halo\n\n  int i  = blockIdx.x * blockDim.x + threadIdx.x;\n  int j  = blockIdx.y;\n  int k  = threadIdx.y;\n  int si = threadIdx.x;\n\n  int globalIdx = k * mx * my + j * mx + i;\n\n  s_f[k * pencils + si] = f[globalIdx];\n\n  __syncthreads();\n\n  if(k == 0) {\n    df[globalIdx] = s_f[(k+1) * pencils + si] - s_f[k * pencils + si];\n  } else if(k == (mz - 1)) {\n    df[globalIdx] = s_f[(k-1) * pencils + si] - s_f[k * pencils + si];\n  } else {\n    df[globalIdx] = s_f[(k+1) * pencils + si] - 2.f * s_f[k * pencils + si] + s_f[(k-1) * pencils + si];\n  }\n}",
            "__global__ void construct_laplacian(\n    float *__restrict__ df,\n    const float *__restrict__ dfx,\n    const float *__restrict__ dfy,\n    const float *__restrict__ dfz,\n    const unsigned int ncells,\n    const float d_diffcon)\n{\n  int index = blockIdx.x * blockDim.x + threadIdx.x;\n  int stride = blockDim.x * gridDim.x;\n\n  for(int i = index; i < ncells; i += stride) {\n    df[i] = d_diffcon * (dfx[i] + dfy[i] + dfz[i]);\n  }\n}"
        ]
    },
    "d2q9-bgk-cuda": {
        "/Users/gbolet/hecbench-roofline/src/d2q9-bgk-cuda/main.cu": [
            "__device__ bool \nisGreater(const float x, const float y) \n{\n  return x > y ? 1 : 0;\n}\n\n__global__ void d2q9_bgk(\n  const float* __restrict__ Speed0A,\n  const float* __restrict__ Speed1A,\n  const float* __restrict__ Speed2A,\n  const float* __restrict__ Speed3A,\n  const float* __restrict__ Speed4A,\n  const float* __restrict__ Speed5A,\n  const float* __restrict__ Speed6A,\n  const float* __restrict__ Speed7A,\n  const float* __restrict__ Speed8A,\n  float* __restrict__ Tmp0A,\n  float* __restrict__ Tmp1A,\n  float* __restrict__ Tmp2A,\n  float* __restrict__ Tmp3A,\n  float* __restrict__ Tmp4A,\n  float* __restrict__ Tmp5A,\n  float* __restrict__ Tmp6A,\n  float* __restrict__ Tmp7A,\n  float* __restrict__ Tmp8A,\n  const int* __restrict__ ObstaclesA,\n  float* __restrict__ Partial_Sum,\n  int* __restrict__ Partial_Sum2,\n  const float densityaccel,\n  const float omega,\n  const int nx,\n  const int ny,\n  const int tt)\n{\n  //setup local memory\n  __shared__ int local_sum2[LOCALSIZEX*LOCALSIZEY];\n  __shared__ float local_sum[LOCALSIZEX*LOCALSIZEY];\n\n  /* get column and row indices */\n  const int ii = blockIdx.x * blockDim.x + threadIdx.x;\n  const int jj = blockIdx.y * blockDim.y + threadIdx.y;\n\n  const float c_sq_inv = 3.f;\n  const float c_sq = 1.f/c_sq_inv; /* square of speed of sound */\n  const float temp1 = 4.5f;\n  const float w1 = 1.f/9.f;\n  const float w0 = 4.f * w1;  /* weighting factor */\n  const float w2 = 1.f/36.f; /* weighting factor */\n  const float w11 = densityaccel * w1;\n  const float w21 = densityaccel * w2;\n\n  /* determine indices of axis-direction neighbours\n   ** respecting periodic boundary conditions (wrap around) */\n  const int y_n = (jj + 1) % ny;\n  const int x_e = (ii + 1) % nx;\n  const int y_s = (jj == 0) ? (jj + ny - 1) : (jj - 1);\n  const int x_w = (ii == 0) ? (ii + nx - 1) : (ii - 1);\n\n  /* propagate densities from neighbouring cells, following\n   ** appropriate directions of travel and writing into\n   ** scratch space grid */\n\n  float tmp_s0 = Speed0A[ii + jj*nx];\n  float tmp_s1 = (jj == ny-2 && (!ObstaclesA[x_w + jj*nx] && isGreater((Speed3A[x_w + jj*nx] - w11) , 0.f) && isGreater((Speed6A[x_w + jj*nx] - w21) , 0.f) && isGreater((Speed7A[x_w + jj*nx] - w21) , 0.f))) ? Speed1A[x_w + jj*nx]+w11 : Speed1A[x_w + jj*nx];\n  float tmp_s2 = Speed2A[ii + y_s*nx];\n  float tmp_s3 = (jj == ny-2 && (!ObstaclesA[x_e + jj*nx] && isGreater((Speed3A[x_e + jj*nx] - w11) , 0.f) && isGreater((Speed6A[x_e + jj*nx] - w21) , 0.f) && isGreater((Speed7A[x_e + jj*nx] - w21) , 0.f))) ? Speed3A[x_e + jj*nx]-w11 : Speed3A[x_e + jj*nx];\n  float tmp_s4 = Speed4A[ii + y_n*nx];\n  float tmp_s5 = (y_s == ny-2 && (!ObstaclesA[x_w + y_s*nx] && isGreater((Speed3A[x_w + y_s*nx] - w11) , 0.f) && isGreater((Speed6A[x_w + y_s*nx] - w21) , 0.f) && isGreater((Speed7A[x_w + y_s*nx] - w21) , 0.f))) ? Speed5A[x_w + y_s*nx]+w21 : Speed5A[x_w + y_s*nx];\n  float tmp_s6 = (y_s == ny-2 && (!ObstaclesA[x_e + y_s*nx] && isGreater((Speed3A[x_e + y_s*nx] - w11) , 0.f) && isGreater((Speed6A[x_e + y_s*nx] - w21) , 0.f) && isGreater((Speed7A[x_e + y_s*nx] - w21) , 0.f))) ? Speed6A[x_e + y_s*nx]-w21 : Speed6A[x_e + y_s*nx];\n  float tmp_s7 = (y_n == ny-2 && (!ObstaclesA[x_e + y_n*nx] && isGreater((Speed3A[x_e + y_n*nx] - w11) , 0.f) && isGreater((Speed6A[x_e + y_n*nx] - w21) , 0.f) && isGreater((Speed7A[x_e + y_n*nx] - w21) , 0.f))) ? Speed7A[x_e + y_n*nx]-w21 : Speed7A[x_e + y_n*nx];\n  float tmp_s8 = (y_n == ny-2 && (!ObstaclesA[x_w + y_n*nx] && isGreater((Speed3A[x_w + y_n*nx] - w11) , 0.f) && isGreater((Speed6A[x_w + y_n*nx] - w21) , 0.f) && isGreater((Speed7A[x_w + y_n*nx] - w21) , 0.f))) ? Speed8A[x_w + y_n*nx]+w21 : Speed8A[x_w + y_n*nx];\n\n  /* compute local density total */\n  float local_density = tmp_s0 + tmp_s1 + tmp_s2 + tmp_s3 + tmp_s4  + tmp_s5  + tmp_s6  + tmp_s7  + tmp_s8;\n  const float local_density_recip = 1.f/(local_density);\n  /* compute x velocity component */\n  float u_x = (tmp_s1\n      + tmp_s5\n      + tmp_s8\n      - tmp_s3\n      - tmp_s6\n      - tmp_s7)\n    * local_density_recip;\n  /* compute y velocity component */\n  float u_y = (tmp_s2\n      + tmp_s5\n      + tmp_s6\n      - tmp_s4\n      - tmp_s8\n      - tmp_s7)\n    * local_density_recip;\n\n  /* velocity squared */\n  const float temp2 = - (u_x * u_x + u_y * u_y)/(2.f * c_sq);\n\n  /* equilibrium densities */\n  float d_equ[NSPEEDS];\n  /* zero velocity density: weight w0 */\n  d_equ[0] = w0 * local_density\n    * (1.f + temp2);\n  /* axis speeds: weight w1 */\n  d_equ[1] = w1 * local_density * (1.f + u_x * c_sq_inv\n      + (u_x * u_x) * temp1\n      + temp2);\n  d_equ[2] = w1 * local_density * (1.f + u_y * c_sq_inv\n      + (u_y * u_y) * temp1\n      + temp2);\n  d_equ[3] = w1 * local_density * (1.f - u_x * c_sq_inv\n      + (u_x * u_x) * temp1\n      + temp2);\n  d_equ[4] = w1 * local_density * (1.f - u_y * c_sq_inv\n      + (u_y * u_y) * temp1\n      + temp2);\n  /* diagonal speeds: weight w2 */\n  d_equ[5] = w2 * local_density * (1.f + (u_x + u_y) * c_sq_inv\n      + ((u_x + u_y) * (u_x + u_y)) * temp1\n      + temp2);\n  d_equ[6] = w2 * local_density * (1.f + (-u_x + u_y) * c_sq_inv\n      + ((-u_x + u_y) * (-u_x + u_y)) * temp1\n      + temp2);\n  d_equ[7] = w2 * local_density * (1.f + (-u_x - u_y) * c_sq_inv\n      + ((-u_x - u_y) * (-u_x - u_y)) * temp1\n      + temp2);\n  d_equ[8] = w2 * local_density * (1.f + (u_x - u_y) * c_sq_inv\n      + ((u_x - u_y) * (u_x - u_y)) * temp1\n      + temp2);\n\n  float tmp;\n  int expression = ObstaclesA[ii + jj*nx];\n  tmp_s0 = expression ? tmp_s0 : (tmp_s0 + omega * (d_equ[0] - tmp_s0));\n  tmp = tmp_s1;\n  tmp_s1 = expression ? tmp_s3 : (tmp_s1 + omega * (d_equ[1] - tmp_s1));\n  tmp_s3 = expression ? tmp : (tmp_s3 + omega * (d_equ[3] - tmp_s3));\n  tmp = tmp_s2;\n  tmp_s2 = expression ? tmp_s4 : (tmp_s2 + omega * (d_equ[2] - tmp_s2));\n  tmp_s4 = expression ? tmp : (tmp_s4 + omega * (d_equ[4] - tmp_s4));\n  tmp = tmp_s5;\n  tmp_s5 = expression ? tmp_s7 : (tmp_s5 + omega * (d_equ[5] - tmp_s5));\n  tmp_s7 = expression ? tmp : (tmp_s7 + omega * (d_equ[7] - tmp_s7));\n  tmp = tmp_s6;\n  tmp_s6 = expression ? tmp_s8 : (tmp_s6 + omega * (d_equ[6] - tmp_s6));\n  tmp_s8 = expression ? tmp : (tmp_s8 + omega * (d_equ[8] - tmp_s8));\n\n  /* local density total */\n  local_density = 1.f/(tmp_s0 + tmp_s1 + tmp_s2 + tmp_s3 + tmp_s4 + tmp_s5 + tmp_s6 + tmp_s7 + tmp_s8);\n\n  /* x-component of velocity */\n  u_x = (tmp_s1\n      + tmp_s5\n      + tmp_s8\n      - tmp_s3\n      - tmp_s6\n      - tmp_s7)\n    * local_density;\n  /* compute y velocity component */\n  u_y = (tmp_s2\n      + tmp_s5\n      + tmp_s6\n      - tmp_s4\n      - tmp_s7\n      - tmp_s8)\n    * local_density;\n\n  Tmp0A[ii + jj*nx] = tmp_s0;\n  Tmp1A[ii + jj*nx] = tmp_s1;\n  Tmp2A[ii + jj*nx] = tmp_s2;\n  Tmp3A[ii + jj*nx] = tmp_s3;\n  Tmp4A[ii + jj*nx] = tmp_s4;\n  Tmp5A[ii + jj*nx] = tmp_s5;\n  Tmp6A[ii + jj*nx] = tmp_s6;\n  Tmp7A[ii + jj*nx] = tmp_s7;\n  Tmp8A[ii + jj*nx] = tmp_s8;\n\n\n  int local_idi = threadIdx.x;\n  int local_idj = threadIdx.y;\n  int local_sizei = blockDim.x;\n  int local_sizej = blockDim.y;\n\n  /* accumulate the norm of x- and y- velocity components */\n  local_sum[local_idi + local_idj*local_sizei] = (ObstaclesA[ii + jj*nx]) ? 0 : hypotf(u_x,u_y);\n  /* increase counter of inspected cells */\n  local_sum2[local_idi + local_idj*local_sizei] = (ObstaclesA[ii + jj*nx]) ? 0 : 1 ;\n\n  __syncthreads();\n\n  int group_id = blockIdx.x;\n  int group_id2 = blockIdx.y; \n  int group_size = gridDim.x;\n  int group_size2 = gridDim.y;\n  if(local_idi == 0 && local_idj == 0){\n    float sum = 0.0f;\n    int sum2 = 0;\n    for(int i = 0; i<local_sizei*local_sizej; i++){\n      sum += local_sum[i];\n      sum2 += local_sum2[i];\n    }\n    Partial_Sum[group_id+group_id2*group_size+tt*group_size*group_size2] = sum;\n    Partial_Sum2[group_id+group_id2*group_size+tt*group_size*group_size2] = sum2;\n  }\n}"
        ]
    },
    "rfs-cuda": {
        "/Users/gbolet/hecbench-roofline/src/rfs-cuda/main.cu": [
            "__host__ __device__ inline float\ntruncateWithRoundingFactor(float roundingFactor, float x) {\n  return (roundingFactor + x) -  // rounded\n         roundingFactor;         // exactly\n}\n\n__global__ void sumArray (\n  const float factor, \n  const   int length,\n  const float *__restrict__ x,\n        float *__restrict__ r)\n{\n  for (int i = blockDim.x * blockIdx.x + threadIdx.x; i < length; \n           i += blockDim.x * gridDim.x) {\n    float q = truncateWithRoundingFactor(factor, x[i]);\n    atomicAdd(r, q);  // sum in any order\n  }\n}",
            "__host__ __device__ inline float\ncreateRoundingFactor(float max, int n) {\n  float delta = (max * (float)n) / (1.f - 2.f * (float)n * FLT_EPSILON);\n\n  // Calculate ceil(log_2(delta)).\n  // frexpf() calculates exp and returns `x` such that\n  // delta = x * 2^exp, where `x` in (-1.0, -0.5] U [0.5, 1).\n  // Because |x| < 1, exp is exactly ceil(log_2(delta)).\n  int exp;\n  frexpf(delta, &exp);\n\n  // return M = 2 ^ ceil(log_2(delta))\n  return ldexpf(1.f, exp);\n}\n\n__host__ __device__ inline float\ntruncateWithRoundingFactor(float roundingFactor, float x) {\n  return (roundingFactor + x) -  // rounded\n         roundingFactor;         // exactly\n}\n\n__global__ void sumArrays (\n  const int nArrays,\n  const int length,\n  const float *__restrict__ x,\n        float *__restrict__ r,\n  const float *__restrict__ maxVal)\n{\n  for (int i = blockDim.x * blockIdx.x + threadIdx.x; i < nArrays;\n           i += blockDim.x * gridDim.x) {\n    x += i * length;\n    float factor = createRoundingFactor(maxVal[i], length);\n    float s = 0;\n    for (int n = length-1; n >= 0; n--)  // sum in reverse order\n      s += truncateWithRoundingFactor(factor, x[n]);\n    r[i] = s;\n  }\n}"
        ]
    },
    "expdist-cuda": {
        "/Users/gbolet/hecbench-roofline/src/expdist-cuda/kernel.h": [
            "#define T ((int)32)\n\n\n__global__\nvoid reduce_cross_term(\n        T *__restrict__ output,\n  const T *__restrict__ cross_term,\n  int m, int n, int nblocks)\n{\n  int tx = threadIdx.x;\n\n  __shared__ T sum;\n\n  if (tx == 0) sum = 0;\n  __syncthreads();\n\n  T s_cross_term = 0;\n  for (int i=tx; i<nblocks; i+=reduce_block_size)\n    s_cross_term += cross_term[i];\n\n  atomicAdd(&sum, s_cross_term);\n\n  __syncthreads();\n\n  if (tx == 0) output[0] = sum;\n}"
        ]
    },
    "bitpacking-cuda": {
        "/Users/gbolet/hecbench-roofline/src/bitpacking-cuda/kernels.cu": [
            "#define LIMIT -999\n\n\n#define T ((int)32)\n\n\ninline __host__ __device__ uint4 make_uint4(int4 a)\n{\n    return make_uint4(uint(a.x), uint(a.y), uint(a.z), uint(a.w));\n}\n\ninline __host__ __device__ uint4 max(uint4 a, uint4 b)\n{\n    return make_uint4(max(a.x,b.x), max(a.y,b.y), max(a.z,b.z), max(a.w,b.w));\n}\n\ninline __host__ __device__ uint4 min(uint4 a, uint4 b)\n{\n    return make_uint4(min(a.x,b.x), min(a.y,b.y), min(a.z,b.z), min(a.w,b.w));\n}\n\n__device__ void\nreduceMinAndMax(T* const minBuffer, T* const maxBuffer, int const blockEnd)\n{\n  // cooperatively compute min and max\n  for (int d = BLOCK_SIZE / 2; d > 0; d >>= 1) {\n    if (threadIdx.x < BLOCK_SIZE / 2) {\n      int const idx = threadIdx.x;\n      if (idx < d && idx + d < blockEnd) {\n        minBuffer[idx] = min(minBuffer[idx], minBuffer[d + idx]);\n      }\n    } else {\n      int const idx = threadIdx.x - (BLOCK_SIZE / 2);\n      if (idx < d && idx + d < blockEnd) {\n        maxBuffer[idx] = max(maxBuffer[idx], maxBuffer[d + idx]);\n      }\n    }\n    __syncthreads();\n  }\n}\n\nconstexpr __host__ __device__ U roundUpDiv(U const num, T const chunk)\n{\n  return (num / chunk) + (num % chunk > 0);\n}\n\n__global__ void bitPackConfigScanKernel(\n    LIMIT* const minValue,\n    LIMIT* const maxValue,\n    INPUT const* const in,\n    const size_t* const numDevice)\n{\n  static_assert(BLOCK_SIZE % 64 == 0, \"BLOCK_SIZE must a multiple of 64\");\n\n  //assert(BLOCK_SIZE == blockDim.x);\n\n  const size_t num = *numDevice;\n  const int numBlocks = roundUpDiv(num, BLOCK_SIZE);\n\n  //assert(num > 0);\n  //assert(threadIdx.x < BLOCK_SIZE);\n\n  if (blockIdx.x < numBlocks) {\n    // each block processes it's chunks, updates min/max\n    __shared__ LIMIT minBuffer[BLOCK_SIZE];\n    __shared__ LIMIT maxBuffer[BLOCK_SIZE];\n\n    LIMIT localMin = 0;\n    LIMIT localMax = 0;\n\n    int lastThread = 0;\n    for (int block = blockIdx.x; block < numBlocks; block += gridDim.x) {\n\n      int const blockOffset = BLOCK_SIZE * block;\n      int const blockEnd = min(static_cast<int>(num) - blockOffset, BLOCK_SIZE);\n\n      lastThread = max(lastThread, blockEnd);\n\n      if (threadIdx.x < blockEnd) {\n        LIMIT const val = in[blockOffset + threadIdx.x];\n        if (block == blockIdx.x) {\n          // first iteration just set values\n          localMax = val;\n          localMin = val;\n        } else {\n          localMin = min(val, localMin);\n          localMax = max(val, localMax);\n        }\n      }\n    }\n\n    minBuffer[threadIdx.x] = localMin;\n    maxBuffer[threadIdx.x] = localMax;\n\n    __syncthreads();\n\n    // cooperatively compute min and max\n    reduceMinAndMax(minBuffer, maxBuffer, lastThread);\n\n    if (threadIdx.x == 0) {\n      minValue[blockIdx.x] = minBuffer[0];\n      maxValue[blockIdx.x] = maxBuffer[0];\n    }\n  }\n}",
            "#define LIMIT -999\n\n\n#define T ((int)32)\n\n\ninline __host__ __device__ uint4 make_uint4(int4 a)\n{\n    return make_uint4(uint(a.x), uint(a.y), uint(a.z), uint(a.w));\n}\n\ninline __host__ __device__ uint4 min(uint4 a, uint4 b)\n{\n    return make_uint4(min(a.x,b.x), min(a.y,b.y), min(a.z,b.z), min(a.w,b.w));\n}\n\ninline __host__ __device__ uint4 max(uint4 a, uint4 b)\n{\n    return make_uint4(max(a.x,b.x), max(a.y,b.y), max(a.z,b.z), max(a.w,b.w));\n}\n\n__device__ void readMinAndMax(\n    T const* const inMin,\n    T const* const inMax,\n    T* const minBuffer,\n    T* const maxBuffer,\n    int const blockOffset,\n    int const blockEnd)\n{\n  static_assert(\n      BLOCK_SIZE <= BLOCK_WIDTH,\n      \"BLOCK_SIZE must be less than or equal to BLOCK_WIDTH\");\n\n  if (threadIdx.x < blockEnd) {\n    T localMin = inMin[blockOffset + threadIdx.x];\n    T localMax = inMax[blockOffset + threadIdx.x];\n    for (int i = threadIdx.x + BLOCK_SIZE; i < BLOCK_WIDTH && i < blockEnd;\n         i += BLOCK_SIZE) {\n      int const readIdx = blockOffset + i;\n      localMin = min(inMin[readIdx], localMin);\n      localMax = max(inMax[readIdx], localMax);\n    }\n    minBuffer[threadIdx.x] = localMin;\n    maxBuffer[threadIdx.x] = localMax;\n  }\n}\n\n__device__ void\nreduceMinAndMax(T* const minBuffer, T* const maxBuffer, int const blockEnd)\n{\n  // cooperatively compute min and max\n  for (int d = BLOCK_SIZE / 2; d > 0; d >>= 1) {\n    if (threadIdx.x < BLOCK_SIZE / 2) {\n      int const idx = threadIdx.x;\n      if (idx < d && idx + d < blockEnd) {\n        minBuffer[idx] = min(minBuffer[idx], minBuffer[d + idx]);\n      }\n    } else {\n      int const idx = threadIdx.x - (BLOCK_SIZE / 2);\n      if (idx < d && idx + d < blockEnd) {\n        maxBuffer[idx] = max(maxBuffer[idx], maxBuffer[d + idx]);\n      }\n    }\n    __syncthreads();\n  }\n}\n\nconstexpr __host__ __device__ U roundUpDiv(U const num, T const chunk)\n{\n  return (num / chunk) + (num % chunk > 0);\n}\n\n__global__ void bitPackConfigFinalizeKernel(\n    LIMIT const* const inMin,\n    LIMIT const* const inMax,\n    unsigned char* const numBitsPtr,\n    INPUT* const outMinValPtr,\n    const size_t* const numDevice)\n{\n  static_assert(\n      BLOCK_SIZE <= BLOCK_WIDTH,\n      \"BLOCK_SIZE must be less than or equal to BLOCK_WIDTH\");\n  static_assert(\n      BLOCK_WIDTH % BLOCK_SIZE == 0,\n      \"BLOCK_WIDTH must be a multiple of BLOCK_SIZE\");\n  static_assert(BLOCK_SIZE % 64 == 0, \"BLOCK_SIZE must a multiple of 64\");\n\n  //assert(blockIdx.x == 0);\n\n  const size_t num = min(\n      roundUpDiv(*numDevice, BLOCK_SIZE), static_cast<size_t>(BLOCK_WIDTH));\n\n  //assert(num > 0);\n\n  // each block processes it's chunk, updates min/max, and the calculates\n  // the bitwidth based on the last update\n  __shared__ LIMIT minBuffer[BLOCK_SIZE];\n  __shared__ LIMIT maxBuffer[BLOCK_SIZE];\n\n  // load data\n  readMinAndMax(inMin, inMax, minBuffer, maxBuffer, 0, num);\n\n  __syncthreads();\n\n  // cooperatively compute min and max\n  reduceMinAndMax(minBuffer, maxBuffer, min(BLOCK_SIZE, (int)num));\n\n  if (threadIdx.x == 0) {\n    *outMinValPtr = static_cast<INPUT>(minBuffer[0]);\n    // we need to update the number of bits\n    if (sizeof(LIMIT) > sizeof(int)) {\n      const uint64_t range = static_cast<uint64_t>(maxBuffer[0]) - static_cast<uint64_t>(minBuffer[0]);\n      // need 64 bit clz\n      *numBitsPtr = sizeof(uint64_t) * 8 - __clzll(range);\n    } else {\n      const uint32_t range = static_cast<uint32_t>(maxBuffer[0]) - static_cast<uint32_t>(minBuffer[0]);\n      // can use 32 bit clz\n      *numBitsPtr = sizeof(uint32_t) * 8 - __clz(range);\n    }\n  }\n}",
            "#define T ((int)32)\n\n\ninline __host__ __device__ uint4 make_uint4(int4 a)\n{\n    return make_uint4(uint(a.x), uint(a.y), uint(a.z), uint(a.w));\n}\n\ninline __host__ __device__ uint4 max(uint4 a, uint4 b)\n{\n    return make_uint4(max(a.x,b.x), max(a.y,b.y), max(a.z,b.z), max(a.w,b.w));\n}\n\ninline __host__ __device__ uint4 min(uint4 a, uint4 b)\n{\n    return make_uint4(min(a.x,b.x), min(a.y,b.y), min(a.z,b.z), min(a.w,b.w));\n}\n\nconstexpr __host__ __device__ U roundUpDiv(U const num, T const chunk)\n{\n  return (num / chunk) + (num % chunk > 0);\n}\n\n__device__\nunsigned char clamp(float f)\n{\n  int i = (int)(f * 255.5f);\n\n  if (i < 0) i = 0;\n  if (i > 255) i = 255;\n\n  return (unsigned char)i;\n}\n\nconstexpr __host__ __device__ U roundDownTo(U const num, T const chunk)\n{\n  return (num / chunk) * chunk;\n}\n\nconstexpr __host__ __device__ U roundUpTo(U const num, T const chunk)\n{\n  return roundUpDiv(num, chunk) * chunk;\n}\n\n__global__ void bitPackKernel(\n    unsigned char const* const numBitsPtr,\n    INPUT const* const valueOffsetPtr,\n    OUTPUT* const outPtr,\n    INPUT const* const in,\n    const size_t* const numDevice)\n{\n  using UINPUT = typename std::make_unsigned<INPUT>::type;\n\n  const size_t num = *numDevice;\n\n  const int numBlocks = roundUpDiv(num, BLOCK_SIZE);\n\n  OUTPUT* const out = outPtr;\n  int const numBits = *numBitsPtr;\n  INPUT const valueOffset = *valueOffsetPtr;\n\n  __shared__ UINPUT inBuffer[BLOCK_SIZE];\n\n  for (int blockId = blockIdx.x; blockId < numBlocks; blockId += gridDim.x) {\n    // The kernel works by assigning an output index to each thread.\n    // The kernel then iterates over chunks of input, filling the bits\n    // for each thread.\n    // And then writing the stored bits to the output.\n    int const outputIdx = threadIdx.x + blockId * BLOCK_SIZE;\n    //assert(outputIdx >= 0);\n    //assert(*numBitsPtr <= sizeof(INPUT) * 8U);\n\n    size_t const bitStart = outputIdx * sizeof(*out) * 8U;\n    size_t const bitEnd = bitStart + (sizeof(*out) * 8U);\n\n    int const startIdx = clamp(bitStart / static_cast<size_t>(numBits), num);\n    int const endIdx = clamp(roundUpDiv(bitEnd, numBits), num);\n    //assert(startIdx >= 0);\n\n    size_t const blockStartBit = blockId * BLOCK_SIZE * sizeof(*out) * 8U;\n    size_t const blockEndBit = (blockId + 1) * BLOCK_SIZE * sizeof(*out) * 8U;\n    //assert(blockStartBit < blockEndBit);\n\n    int const blockStartIdx = clamp(\n        roundDownTo(blockStartBit / static_cast<size_t>(numBits), BLOCK_SIZE),\n        num);\n    int const blockEndIdx\n        = clamp(roundUpTo(roundUpDiv(blockEndBit, numBits), BLOCK_SIZE), num);\n    //assert(blockStartIdx >= 0);\n    //assert(blockStartIdx <= blockEndIdx);\n\n    OUTPUT val = 0;\n    for (int bufferStart = blockStartIdx; bufferStart < blockEndIdx;\n         bufferStart += BLOCK_SIZE) {\n      __syncthreads();\n\n      // fill input buffer\n      int const inputIdx = bufferStart + threadIdx.x;\n      if (inputIdx < num) {\n        inBuffer[threadIdx.x] = in[inputIdx] - valueOffset;\n      }\n\n      __syncthreads();\n\n      int const currentStartIdx = max(startIdx, bufferStart);\n      int const currentEndIdx = min(endIdx, bufferStart + BLOCK_SIZE);\n\n      for (int idx = currentStartIdx; idx < currentEndIdx; ++idx) {\n        int const localIdx = idx - bufferStart;\n\n        // keep only bits we're interested in\n        OUTPUT bits = static_cast<OUTPUT>(inBuffer[localIdx]);\n        int const offset = static_cast<int>(\n            static_cast<ssize_t>(idx * numBits)\n            - static_cast<ssize_t>(bitStart));\n        //assert(std::abs(offset) < sizeof(bits) * 8U);\n\n        if (offset > 0) {\n          bits <<= offset;\n        } else {\n          bits >>= -offset;\n        }\n\n        // update b\n        val |= bits;\n      }\n    }\n\n    if (startIdx < num) {\n      out[outputIdx] = val;\n    }\n  }\n}"
        ]
    },
    "babelstream-cuda": {
        "/Users/gbolet/hecbench-roofline/src/babelstream-cuda/main.cu": [
            "#define T ((int)32)\n\n\n__global__ void init_kernel(\n  T *__restrict__ a,\n  T *__restrict__ b,\n  T *__restrict__ c,\n  T initA, T initB, T initC)\n{\n  const int i = blockDim.x * blockIdx.x + threadIdx.x;\n  a[i] = initA;\n  b[i] = initB;\n  c[i] = initC;\n}",
            "#define T ((int)32)\n\n\n__global__ void copy_kernel(\n  const T *__restrict__ a,\n        T *__restrict__ c)\n{\n  const int i = blockDim.x * blockIdx.x + threadIdx.x;\n  c[i] = a[i];\n}",
            "#define T ((int)32)\n\n\n__global__ void mul_kernel(\n        T *__restrict__ b,\n  const T *__restrict__ c)\n{\n  const T scalar = SCALAR;\n  const int i = blockDim.x * blockIdx.x + threadIdx.x;\n  b[i] = scalar * c[i];\n}",
            "#define T ((int)32)\n\n\n__global__ void add_kernel(\n  const T *__restrict__ a,\n  const T *__restrict__ b,\n        T *__restrict__ c)\n{\n  const int i = blockDim.x * blockIdx.x + threadIdx.x;\n  c[i] = a[i] + b[i];\n}",
            "#define T ((int)32)\n\n\n__global__ void triad_kernel(\n        T *__restrict__ a,\n  const T *__restrict__ b,\n  const T *__restrict__ c)\n{\n  const T scalar = SCALAR;\n  const int i = blockDim.x * blockIdx.x + threadIdx.x;\n  a[i] = b[i] + scalar * c[i];\n}",
            "#define T ((int)32)\n\n\n__global__ void nstream_kernel(\n        T *__restrict__ a,\n  const T *__restrict__ b,\n  const T *__restrict__ c)\n{\n  const T scalar = SCALAR;\n  const int i = blockDim.x * blockIdx.x + threadIdx.x;\n  a[i] += b[i] + scalar * c[i];\n}",
            "#define T ((int)32)\n\n\n__global__ void dot_kernel(\n  const T *__restrict__ a,\n  const T *__restrict__ b,\n        T *__restrict__ sum,\n  int array_size)\n{\n  __shared__ T tb_sum[TBSIZE];\n\n  const size_t local_i = threadIdx.x;\n\n  tb_sum[local_i] = 0.0;\n  for (int i = blockDim.x * blockIdx.x + threadIdx.x;\n       i < array_size; i += blockDim.x*gridDim.x)\n    tb_sum[local_i] += a[i] * b[i];\n\n  for (int offset = blockDim.x / 2; offset > 0; offset /= 2)\n  {\n    __syncthreads();\n    if (local_i < offset)\n    {\n      tb_sum[local_i] += tb_sum[local_i+offset];\n    }\n  }\n\n  if (local_i == 0)\n    sum[blockIdx.x] = tb_sum[local_i];\n}"
        ]
    },
    "rng-wallace-cuda": {
        "/Users/gbolet/hecbench-roofline/src/rng-wallace-cuda/wallace_kernel.cu": [
            "__device__ void Hadamard4x4a(float &p, float &q, float &r, float &s)\n{\n\tfloat t = (p + q + r + s) / 2;\n\tp = p - t;\n\tq = q - t;\n\tr = t - r;\n\ts = t - s;\n}\n\n__device__ void Hadamard4x4b(float &p, float &q, float &r, float &s)\n{\n\tfloat t = (p + q + r + s) / 2;\n\tp = t - p;\n\tq = t - q;\n\tr = r - t;\n\ts = s - t;\n}\n\n__global__ void rng_wallace(unsigned m_seed, float *globalPool, float *generatedRandomNumberPool, float *chi2Corrections)\n{\n\n  __shared__ float pool[WALLACE_POOL_SIZE + WALLACE_CHI2_SHARED_SIZE];\n\n\tconst unsigned lcg_a = 241;\n\tconst unsigned lcg_c = 59;\n\tconst unsigned lcg_m = 256;\n\tconst unsigned mod_mask = lcg_m - 1;\n\n\tunsigned offset = __mul24(WALLACE_POOL_SIZE, blockIdx.x);\n\n  #pragma unroll\n\tfor (int i = 0; i < 8; i++)\n\t  pool[threadIdx.x + WALLACE_NUM_THREADS * i] = globalPool[offset + threadIdx.x + WALLACE_NUM_THREADS * i];\n\n\t__syncthreads();\n\n\t// Loop generating generatedRandomNumberPools repeatedly\n\tfor (int loop = 0; loop < WALLACE_NUM_OUTPUTS_PER_RUN; loop++)\n\t{\n\n\t\tm_seed = (1664525U * m_seed + 1013904223U) & 0xFFFFFFFF;\n\n\t\tunsigned intermediate_address = __mul24(loop, 8 * WALLACE_TOTAL_NUM_THREADS) + \n\t\t\t__mul24(8 * WALLACE_NUM_THREADS, blockIdx.x) + threadIdx.x;\n\n\t\tif (threadIdx.x == 0)\n\t\t\tpool[WALLACE_CHI2_OFFSET] = chi2Corrections[__mul24(blockIdx.x, WALLACE_NUM_OUTPUTS_PER_RUN) + loop];\n\t\t__syncthreads();\n\t\tfloat chi2CorrAndScale = pool[WALLACE_CHI2_OFFSET];\n\t\tfor (int i = 0; i < 8; i++)\n\t\t{\n\t\t\tgeneratedRandomNumberPool[intermediate_address + i * WALLACE_NUM_THREADS] = \n\t\t\t\tpool[__mul24(i, WALLACE_NUM_THREADS) + threadIdx.x] * chi2CorrAndScale;\n\t\t}\n\n\t\tfloat rin0_0, rin1_0, rin2_0, rin3_0, rin0_1, rin1_1, rin2_1, rin3_1;\n\t\tfor (int i = 0; i < WALLACE_NUM_POOL_PASSES; i++)\n\t\t{\n\t\t\tunsigned seed = (m_seed + threadIdx.x) & mod_mask;\n\t\t\t__syncthreads();\n\t\t\tseed = (__mul24(seed, lcg_a) + lcg_c) & mod_mask;\n\t\t\trin0_0 = pool[((seed << 3))];\n\t\t\tseed = (__mul24(seed, lcg_a) + lcg_c) & mod_mask;\n\t\t\trin1_0 = pool[((seed << 3) + 1)];\n\t\t\tseed = (__mul24(seed, lcg_a) + lcg_c) & mod_mask;\n\t\t\trin2_0 = pool[((seed << 3) + 2)];\n\t\t\tseed = (__mul24(seed, lcg_a) + lcg_c) & mod_mask;\n\t\t\trin3_0 = pool[((seed << 3) + 3)];\n\t\t\tseed = (__mul24(seed, lcg_a) + lcg_c) & mod_mask;\n\t\t\trin0_1 = pool[((seed << 3) + 4)];\n\t\t\tseed = (__mul24(seed, lcg_a) + lcg_c) & mod_mask;\n\t\t\trin1_1 = pool[((seed << 3) + 5)];\n\t\t\tseed = (__mul24(seed, lcg_a) + lcg_c) & mod_mask;\n\t\t\trin2_1 = pool[((seed << 3) + 6)];\n\t\t\tseed = (__mul24(seed, lcg_a) + lcg_c) & mod_mask;\n\t\t\trin3_1 = pool[((seed << 3) + 7)];\n\n\t\t\t__syncthreads();\n\n\t\t\tHadamard4x4a(rin0_0, rin1_0, rin2_0, rin3_0);\n\t\t\tpool[0 * WALLACE_NUM_THREADS + threadIdx.x] = rin0_0;\n\t\t\tpool[1 * WALLACE_NUM_THREADS + threadIdx.x] = rin1_0;\n\t\t\tpool[2 * WALLACE_NUM_THREADS + threadIdx.x] = rin2_0;\n\t\t\tpool[3 * WALLACE_NUM_THREADS + threadIdx.x] = rin3_0;\n\n\t\t\tHadamard4x4b(rin0_1, rin1_1, rin2_1, rin3_1);\n\t\t\tpool[4 * WALLACE_NUM_THREADS + threadIdx.x] = rin0_1;\n\t\t\tpool[5 * WALLACE_NUM_THREADS + threadIdx.x] = rin1_1;\n\t\t\tpool[6 * WALLACE_NUM_THREADS + threadIdx.x] = rin2_1;\n\t\t\tpool[7 * WALLACE_NUM_THREADS + threadIdx.x] = rin3_1;\n\n\t\t\t__syncthreads();\n\t\t}\n\t}\n\n\t__syncthreads();\n\n  #pragma unroll\n\tfor (int i = 0; i < 8; i++)\n\t  globalPool[offset + threadIdx.x + WALLACE_NUM_THREADS * i] = pool[threadIdx.x + WALLACE_NUM_THREADS * i];\n}"
        ]
    },
    "ecdh-cuda": {
        "/Users/gbolet/hecbench-roofline/src/ecdh-cuda/ecdh.h": [
            "__global__ void k_slow (\n  const int sk,\n  const int P_x,\n  const int P_y, \n  int *__restrict__ T_x,\n  int *__restrict__ T_y,\n  const unsigned int m,\n  const int a,\n  const int num_pk)\n{\n  int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < num_pk)\n    make_pk_slow(sk, P_x, P_y, T_x+i, T_y+i, m, a);\n}",
            "__global__ void k_fast (\n  const int sk,\n  const int P_x,\n  const int P_y, \n  int *__restrict__ T_x,\n  int *__restrict__ T_y,\n  const unsigned int m,\n  const int a,\n  const int num_pk)\n{\n  int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < num_pk)\n    make_pk_fast(sk, P_x, P_y, T_x+i, T_y+i, m, a);\n}"
        ]
    },
    "bsearch-cuda": {
        "/Users/gbolet/hecbench-roofline/src/bsearch-cuda/main.cu": [
            "#define T ((int)32)\n\n\n__global__ void\nkernel_BS (const T* __restrict__ acc_a,\n           const T* __restrict__ acc_z,\n            size_t* __restrict__ acc_r,\n           const size_t n)\n{ \n  size_t i = blockIdx.x*blockDim.x+threadIdx.x;\n  T z = acc_z[i];\n  size_t low = 0;\n  size_t high = n;\n  while (high - low > 1) {\n    size_t mid = low + (high - low)/2;\n    if (z < acc_a[mid])\n      high = mid;\n    else\n      low = mid;\n  }\n  acc_r[i] = low;\n}",
            "#define T ((int)32)\n\n\n__global__ void\nkernel_BS2 (const T* __restrict__ acc_a,\n            const T* __restrict__ acc_z,\n             size_t* __restrict__ acc_r,\n            const size_t n)\n{\n  size_t i = blockIdx.x*blockDim.x+threadIdx.x;\n  unsigned  nbits = 0;\n  while (n >> nbits) nbits++;\n  size_t k = 1ULL << (nbits - 1);\n  T z = acc_z[i];\n  size_t idx = (acc_a[k] <= z) ? k : 0;\n  while (k >>= 1) {\n    size_t r = idx | k;\n    if (r < n && z >= acc_a[r]) { \n      idx = r;\n    }\n  }\n  acc_r[i] = idx;\n}",
            "#define T ((int)32)\n\n\n__global__ void\nkernel_BS3 (const T* __restrict__ acc_a,\n            const T* __restrict__ acc_z,\n             size_t* __restrict__ acc_r,\n            const size_t n)\n{\n  size_t i = blockIdx.x*blockDim.x+threadIdx.x;\n  unsigned nbits = 0;\n  while (n >> nbits) nbits++;\n  size_t k = 1ULL << (nbits - 1);\n  T z = acc_z[i];\n  size_t idx = (acc_a[k] <= z) ? k : 0;\n  while (k >>= 1) {\n    size_t r = idx | k;\n    size_t w = r < n ? r : n; \n    if (z >= acc_a[w]) { \n      idx = r;\n    }\n  }\n  acc_r[i] = idx;\n}",
            "#define T ((int)32)\n\n\n__global__ void\nkernel_BS4 (const T* __restrict__ acc_a,\n            const T* __restrict__ acc_z,\n             size_t* __restrict__ acc_r,\n            const size_t n)\n{\n  __shared__  size_t k;\n\n  size_t gid = blockIdx.x*blockDim.x+threadIdx.x;\n  size_t lid = threadIdx.x; \n\n  if (lid == 0) {\n    unsigned nbits = 0;\n    while (n >> nbits) nbits++;\n    k = 1ULL << (nbits - 1);\n  }\n  __syncthreads();\n\n  size_t p = k;\n  T z = acc_z[gid];\n  size_t idx = (acc_a[p] <= z) ? p : 0;\n  while (p >>= 1) {\n    size_t r = idx | p;\n    size_t w = r < n ? r : n;\n    if (z >= acc_a[w]) { \n      idx = r;\n    }\n  }\n  acc_r[gid] = idx;\n}"
        ]
    },
    "bsw-cuda": {
        "/Users/gbolet/hecbench-roofline/src/bsw-cuda/kernel.cu": [
            "inline __host__ __device__ uint4 make_uint4(int4 a)\n{\n    return make_uint4(uint(a.x), uint(a.y), uint(a.z), uint(a.w));\n}\n\ninline __host__ __device__ uint4 max(uint4 a, uint4 b)\n{\n    return make_uint4(max(a.x,b.x), max(a.y,b.y), max(a.z,b.z), max(a.w,b.w));\n}\n\n__inline__ __device__ short\nwarpReduceMax_with_index(short val, short& myIndex, short& myIndex2, unsigned lengthSeqB, bool inverse)\n{\n  int   warpSize = 32;\n  short myMax    = 0;\n  short newInd   = 0;\n  short newInd2  = 0;\n  short ind      = myIndex;\n  short ind2     = myIndex2;\n  myMax          = val;\n  unsigned mask  = __ballot_sync(0xffffffff, threadIdx.x < lengthSeqB);  // blockDim.x\n  for (int offset = warpSize / 2; offset > 0; offset /= 2)\n  {\n\n    short tempVal = __shfl_down_sync(mask, val, offset);\n    val     = max(val,tempVal);\n    newInd  = __shfl_down_sync(mask, ind, offset);\n    newInd2 = __shfl_down_sync(mask, ind2, offset);\n\n    if(val != myMax)\n    {\n      ind   = newInd;\n      ind2  = newInd2;\n      myMax = val;\n    }\n    else if(val == tempVal)  // this is kind of redundant and has been done purely to match the results\n      // with SSW to get the smallest alignment with highest score. Theoreticaly\n      // all the alignmnts with same score are same.\n    {\n      if (inverse) {\n        if(newInd2 > ind2) {\n          ind = newInd;\n          ind2 = newInd2;\n        }\n      } else {\n        if(newInd < ind) {\n          ind = newInd;\n          ind2 = newInd2;\n        }\n      }\n    }\n  }\n  myIndex  = ind;\n  myIndex2 = ind2;\n  val      = myMax;\n  return val;\n}\n\n__device__ short\nblockShuffleReduce_with_index(short myVal, short& myIndex, short& myIndex2, unsigned lengthSeqB, bool inverse)\n{\n  int              laneId = threadIdx.x % 32;\n  int              warpId = threadIdx.x / 32;\n  __shared__ short locTots[32];\n  __shared__ short locInds[32];\n  __shared__ short locInds2[32];\n  short            myInd  = myIndex;\n  short            myInd2 = myIndex2;\n  myVal                   = warpReduceMax_with_index(myVal, myInd, myInd2, lengthSeqB, inverse);\n\n  if(laneId == 0)\n    locTots[warpId] = myVal;\n  if(laneId == 0)\n    locInds[warpId] = myInd;\n  if(laneId == 0)\n    locInds2[warpId] = myInd2;\n\n  __syncthreads();\n\n  // number of warps in a thread block\n  unsigned nblocks = ((32 + blockDim.x - 1) / 32);\n  if(threadIdx.x < nblocks)\n  {\n    myVal  = locTots[threadIdx.x];\n    myInd  = locInds[threadIdx.x];\n    myInd2 = locInds2[threadIdx.x];\n  }\n  else\n  {\n    myVal  = 0;\n    myInd  = -1;\n    myInd2 = -1;\n  }\n  __syncthreads();\n\n  if(warpId == 0)\n  {\n    myVal    = warpReduceMax_with_index(myVal, myInd, myInd2, lengthSeqB, inverse);\n    myIndex  = myInd;\n    myIndex2 = myInd2;\n  }\n  return myVal;\n}\n\n__device__ short\nfindMaxFour(short first, short second, short third, short fourth)\n{\n  short maxScore = 0;\n\n  maxScore = max(first,second);\n  maxScore = max(maxScore, third);\n  maxScore = max(maxScore, fourth);\n\n  return maxScore;\n}\n\n__global__ void\nsequence_aa_kernel(\n          bool  inverse,\n    const char*__restrict__ seqA_array,\n    const char*__restrict__ seqB_array,\n    const unsigned*__restrict__ prefix_lengthA,\n    const unsigned*__restrict__ prefix_lengthB,\n          short*__restrict__ seqA_align_begin,\n          short*__restrict__ seqA_align_end,\n          short*__restrict__ seqB_align_begin,\n          short*__restrict__ seqB_align_end,\n          short*__restrict__ top_scores,\n    const short startGap,\n    const short extendGap,\n    const short*__restrict__ scoring_matrix,\n    const short*__restrict__ encoding_matrix)\n{\n  int block_Id  = blockIdx.x;\n  int thread_Id = threadIdx.x;\n  short laneId = threadIdx.x%32;\n  short warpId = threadIdx.x/32;\n\n  unsigned lengthSeqA;\n  unsigned lengthSeqB;\n  // local pointers\n  const char* seqA;\n  const char* seqB;\n  const char* longer_seq;\n\n  extern __shared__ char is_valid_array[]; \n  char* is_valid = &is_valid_array[0];\n\n  // setting up block local sequences and their lengths.\n  if(block_Id == 0)\n  {\n    lengthSeqA = prefix_lengthA[0];\n    lengthSeqB = prefix_lengthB[0];\n    seqA       = seqA_array;\n    seqB       = seqB_array;\n  }\n  else\n  {\n    lengthSeqA = prefix_lengthA[block_Id] - prefix_lengthA[block_Id - 1];\n    lengthSeqB = prefix_lengthB[block_Id] - prefix_lengthB[block_Id - 1];\n    seqA       = seqA_array + prefix_lengthA[block_Id - 1];\n    seqB       = seqB_array + prefix_lengthB[block_Id - 1];\n  }\n\n  if(inverse) {\n    lengthSeqA = seqA_align_end[block_Id];\n    lengthSeqB = seqB_align_end[block_Id];\n  }\n\n  // what is the max length and what is the min length\n  unsigned maxSize = lengthSeqA > lengthSeqB ? lengthSeqA : lengthSeqB;\n  unsigned minSize = lengthSeqA < lengthSeqB ? lengthSeqA : lengthSeqB;\n\n  // shared memory space for storing longer of the two strings\n  for(int p = thread_Id; p < minSize; p+=32){\n    is_valid[p] = 0;\n  }\n\n  is_valid += minSize;\n  for(int p = thread_Id; p < minSize; p+=32){\n    is_valid[p] = 1;\n  }\n\n  is_valid += minSize;\n  for(int p = thread_Id; p < minSize; p+=32){\n    is_valid[p] = 0;\n  }\n\n  char myColumnChar;\n  // the shorter of the two strings is stored in thread registers\n  if(lengthSeqA < lengthSeqB)\n  {\n    if(thread_Id < lengthSeqA) {\n      myColumnChar = inverse ? seqA[(lengthSeqA - 1) - thread_Id] : seqA[thread_Id];  // read only once\n      longer_seq = seqB;\n    }\n  }\n  else\n  {\n    if(thread_Id < lengthSeqB) {\n      myColumnChar = inverse ? seqB[(lengthSeqB - 1) - thread_Id] : seqB[thread_Id];\n      longer_seq = seqA;\n    }\n  }\n\n  __syncthreads(); // this is required here so that complete sequence has been copied to shared memory\n\n  int   i            = 1;\n  short thread_max   = 0; // to maintain the thread max score\n  short thread_max_i = 0; // to maintain the DP coordinate i for the longer string\n  short thread_max_j = 0;// to maintain the DP cooirdinate j for the shorter string\n\n  //initializing registers for storing diagonal values for three recent most diagonals (separate tables for\n  //H, E and F)\n  short _curr_H = 0, _curr_F = 0, _curr_E = 0;\n  short _prev_H = 0, _prev_F = 0, _prev_E = 0;\n  short _prev_prev_H = 0, _prev_prev_F = 0, _prev_prev_E = 0;\n  short _temp_Val = 0;\n\n  __shared__ short sh_prev_E[32]; // one such element is required per warp\n  __shared__ short sh_prev_H[32];\n  __shared__ short sh_prev_prev_H[32];\n\n  __shared__ short local_spill_prev_E[1024];// each threads local spill,\n  __shared__ short local_spill_prev_H[1024];\n  __shared__ short local_spill_prev_prev_H[1024];\n\n  __shared__ short sh_aa_encoding[ENCOD_MAT_SIZE];// length = 91\n  __shared__ short sh_aa_scoring[SCORE_MAT_SIZE];\n\n  int max_threads = blockDim.x;\n  for (int p = thread_Id; p < SCORE_MAT_SIZE; p += max_threads) {\n    sh_aa_scoring[p] = scoring_matrix[p];\n  }\n  for (int p = thread_Id; p < ENCOD_MAT_SIZE; p += max_threads) {\n    sh_aa_encoding[p] = encoding_matrix[p];\n  }\n\n  __syncthreads(); // to make sure all shmem allocations have been initialized\n\n  for (int diag = 0; diag < lengthSeqA + lengthSeqB - 1; diag++)\n  {  // iterate for the number of anti-diagonals\n\n    is_valid = is_valid - (diag < minSize || diag >= maxSize); //move the pointer to left by 1 if cnd true\n\n    _temp_Val = _prev_H; // value exchange happens here to setup registers for next iteration\n    _prev_H = _curr_H;\n    _curr_H = _prev_prev_H;\n    _prev_prev_H = _temp_Val;\n    _curr_H = 0;\n\n    _temp_Val = _prev_E;\n    _prev_E = _curr_E;\n    _curr_E = _prev_prev_E;\n    _prev_prev_E = _temp_Val;\n    _curr_E = 0;\n\n    _temp_Val = _prev_F;\n    _prev_F = _curr_F;\n    _curr_F = _prev_prev_F;\n    _prev_prev_F = _temp_Val;\n    _curr_F = 0;\n\n\n    if(laneId == 31)\n    { // if you are the last thread in your warp then spill your values to shmem\n      sh_prev_E[warpId] = _prev_E;\n      sh_prev_H[warpId] = _prev_H;\n      sh_prev_prev_H[warpId] = _prev_prev_H;\n    }\n\n    if(diag >= maxSize)\n    { // if you are invalid in this iteration, spill your values to shmem\n      local_spill_prev_E[thread_Id] = _prev_E;\n      local_spill_prev_H[thread_Id] = _prev_H;\n      local_spill_prev_prev_H[thread_Id] = _prev_prev_H;\n    }\n\n    __syncthreads(); // this is needed so that all the shmem writes are completed.\n\n    if(is_valid[thread_Id] && thread_Id < minSize)\n    {\n      unsigned mask  = __ballot_sync(__activemask(), (is_valid[thread_Id] &&( thread_Id < minSize)));\n      short fVal = _prev_F + extendGap;\n      short hfVal = _prev_H + startGap;\n      short valeShfl = __shfl_sync(mask, _prev_E, laneId - 1, 32);\n      short valheShfl = __shfl_sync(mask, _prev_H, laneId - 1, 32);\n\n      short eVal=0, heVal = 0;\n\n      if(diag >= maxSize) // when the previous thread has phased out, get value from shmem\n      {\n        eVal = local_spill_prev_E[thread_Id - 1] + extendGap;\n        heVal = local_spill_prev_H[thread_Id - 1] + startGap;\n      }\n      else\n      {\n        eVal =((warpId !=0 && laneId == 0)?sh_prev_E[warpId-1]: valeShfl) + extendGap;\n        heVal =((warpId !=0 && laneId == 0)?sh_prev_H[warpId-1]:valheShfl) + startGap;\n      }\n\n      if(warpId == 0 && laneId == 0) // make sure that values for lane 0 in warp 0 is not undefined\n      {\n        eVal = 0;\n        heVal = 0;\n      }\n      _curr_F = (fVal > hfVal) ? fVal : hfVal;\n      _curr_E = (eVal > heVal) ? eVal : heVal;\n\n      short testShufll = __shfl_sync(mask, _prev_prev_H, laneId - 1, 32);\n      short final_prev_prev_H = 0;\n      if(diag >= maxSize)\n      {\n        final_prev_prev_H = local_spill_prev_prev_H[thread_Id - 1];\n      }\n      else\n      {\n        final_prev_prev_H =(warpId !=0 && laneId == 0)?sh_prev_prev_H[warpId-1]:testShufll;\n      }\n\n\n      if(warpId == 0 && laneId == 0) final_prev_prev_H = 0;\n\n      char to_comp = inverse ? longer_seq[maxSize -i] : longer_seq[i - 1];\n      short mat_index_q = sh_aa_encoding[(int)to_comp]; //encoding_matrix\n      short mat_index_r = sh_aa_encoding[(int)myColumnChar];\n\n      short add_score = sh_aa_scoring[mat_index_q*24 + mat_index_r]; // doesnt really matter in what order these indices are used, since the scoring table is symmetrical\n\n      short diag_score = final_prev_prev_H + add_score;\n\n      _curr_H = findMaxFour(diag_score, _curr_F, _curr_E, 0);\n\n      int _i = inverse ? maxSize - i : i;\n      int _j = inverse ? minSize - thread_Id - 1: thread_Id + 1;\n\n      thread_max_i = (thread_max >= _curr_H) ? thread_max_i : _i;\n      thread_max_j = (thread_max >= _curr_H) ? thread_max_j : _j;\n      thread_max   = (thread_max >= _curr_H) ? thread_max : _curr_H;\n      i++;\n    }\n\n    __syncthreads(); // why do I need this? commenting it out breaks it\n\n  }\n\n  thread_max = blockShuffleReduce_with_index(thread_max, thread_max_i, thread_max_j,\n      minSize, inverse);  // thread 0 will have the correct values\n\n  if(inverse) {\n    if(thread_Id == 0){\n      if(lengthSeqA < lengthSeqB){\n        seqB_align_begin[block_Id] = (thread_max_i);\n        seqA_align_begin[block_Id] = (thread_max_j);\n      }\n      else{\n        seqA_align_begin[block_Id] = (thread_max_i);\n        seqB_align_begin[block_Id] = (thread_max_j);\n      }\n    }\n  } else {  \n    if(thread_Id == 0) {\n       if(lengthSeqA < lengthSeqB) {\n         seqB_align_end[block_Id] = thread_max_i;\n         seqA_align_end[block_Id] = thread_max_j;\n         top_scores[block_Id] = thread_max;\n       }\n       else {\n         seqA_align_end[block_Id] = thread_max_i;\n         seqB_align_end[block_Id] = thread_max_j;\n         top_scores[block_Id] = thread_max;\n       }\n    }\n  }\n}"
        ]
    },
    "dslash-cuda": {
        "/Users/gbolet/hecbench-roofline/src/dslash-cuda/kernels.cu": [
            "#define su3_matrix    dsu3_matrix\n\n\n__device__\ninline void su3_adjoint( const su3_matrix *a, su3_matrix *b ){\n  int i,j;\n  for(i=0;i<3;i++)for(j=0;j<3;j++){\n      CONJG( a->e[j][i], b->e[i][j] );\n    }\n}\n\n__global__ void make_back(\n    const su3_matrix*__restrict__ d_fat,\n    const su3_matrix*__restrict__ d_lng,\n    const size_t*__restrict__ d_bck, \n    const size_t*__restrict__ d_bck3,\n          su3_matrix*__restrict__ d_fatbck,\n          su3_matrix*__restrict__ d_lngbck,\n    const int total_even_sites)\n{\n  size_t mySite = blockIdx.x * blockDim.x + threadIdx.x;\n  if (mySite < total_even_sites) {\n    for(int dir = 0; dir < 4; dir++) {\n      su3_adjoint( d_fat + 4*d_bck[4*mySite+dir]+dir, \n          d_fatbck + 4*mySite+dir );\n      su3_adjoint( d_lng + 4*d_bck3[4*mySite+dir]+dir, \n          d_lngbck + 4*mySite+dir );\n    }\n  }\n}",
            "#define Complx        dcomplex\n\n\n#define su3_matrix    dsu3_matrix\n\n\n#define su3_vector    dsu3_vector\n\n\n__host__ __device__\ninline void add_su3_vector( const su3_vector *a, const su3_vector *b, su3_vector *c ){\n  int i;\n  for(i=0;i<3;i++){\n    CADD( a->c[i], b->c[i], c->c[i] );\n  }\n}\n\n__host__ __device__\ninline void mult_su3_mat_vec( const su3_matrix *a, const su3_vector *b, \n\t\t\t      su3_vector *c  ){\n  int i,j;\n  for(i=0;i<3;i++){\n    Complx x = {0.0, 0.0};\n    for(j=0;j<3;j++){\n      CMULSUM( a->e[i][j] , b->c[j] , x );\n    }\n    c->c[i] = x;\n  }\n}\n\n__host__ __device__\ninline void mult_su3_mat_vec_sum( const su3_matrix *a, const su3_vector *b, \n\t\t\t\t  su3_vector *c ){\n  int i,j;\n  for(i=0;i<3;i++){\n    Complx x = {0.0, 0.0};\n    for(j=0;j<3;j++){\n      CMULSUM( a->e[i][j] , b->c[j] , x );\n    }\n    c->c[i].real += x.real;\n    c->c[i].imag += x.imag;\n  }\n}\n\n__host__ __device__\ninline void sub_su3_vector( const su3_vector *a, const su3_vector *b, su3_vector *c ){\n  int i;\n  for(i=0;i<3;i++){\n    CSUB( a->c[i], b->c[i], c->c[i] );\n  }\n}\n\n__global__ void dslash (\n    const su3_matrix*__restrict__ d_fat,\n    const su3_matrix*__restrict__ d_lng,\n    const su3_matrix*__restrict__ d_fatbck,\n    const su3_matrix*__restrict__ d_lngbck,\n    const su3_vector*__restrict__ d_src,\n          su3_vector*__restrict__ d_dst,\n    const size_t*__restrict__ d_fwd,\n    const size_t*__restrict__ d_bck,\n    const size_t*__restrict__ d_fwd3,\n    const size_t*__restrict__ d_bck3,\n    const int total_even_sites)\n{\n  size_t mySite = blockIdx.x * blockDim.x + threadIdx.x;\n  if (mySite < total_even_sites) {\n    su3_vector v;\n    for (size_t k=0; k<4; ++k) {\n      auto a = d_fat + mySite*4 + k;\n      auto b = d_src + d_fwd[4*mySite + k];\n      if (k == 0)\n        mult_su3_mat_vec(a, b, &d_dst[mySite]);\n      else \n        mult_su3_mat_vec_sum(a, b, &d_dst[mySite]);\n    }\n    for (size_t k=0; k<4; ++k) {\n      auto a = d_lng + mySite*4 + k;\n      auto b = d_src + d_fwd3[4*mySite + k];\n      if (k == 0) \n        mult_su3_mat_vec(a, b, &v);\n      else\n        mult_su3_mat_vec_sum(a, b, &v);\n    }\n    add_su3_vector(&d_dst[mySite], &v, &d_dst[mySite]);\n    for (size_t k=0; k<4; ++k) {\n      auto a = d_fatbck + mySite*4 + k;\n      auto b = d_src + d_bck[4*mySite + k];\n      if (k == 0) \n        mult_su3_mat_vec(a, b, &v);\n      else\n        mult_su3_mat_vec_sum(a, b, &v);\n    }\n    sub_su3_vector(&d_dst[mySite], &v, &d_dst[mySite]);\n    for (size_t k=0; k<4; ++k) {\n      auto a = d_lngbck + mySite*4 + k;\n      auto b = d_src + d_bck3[4*mySite + k];\n      if (k == 0) \n        mult_su3_mat_vec(a, b, &v);\n      else\n        mult_su3_mat_vec_sum(a, b, &v);\n    }\n    sub_su3_vector(&d_dst[mySite], &v, &d_dst[mySite]);\n  } // end of if mySite\n}"
        ]
    },
    "cm-cuda": {
        "/Users/gbolet/hecbench-roofline/src/cm-cuda/kernels.h": [
            "__global__ void computeDotProductHelper(\n        int *__restrict__ result,\n  const int *__restrict__ v1,\n  const int *__restrict__ v2,\n  const int vLength) \n{\n\n  extern  __shared__ int cache[];\n\n  int cacheIndex = threadIdx.x;\n  int tid = threadIdx.x + blockIdx.x * blockDim.x;\n\n  int temp = 0;\n\n  while (tid < vLength) {\n    temp += v1[tid] * v2[tid];\n    tid += blockDim.x * gridDim.x;\n  }\n\n  cache[cacheIndex] = temp;\n  __syncthreads();\n  int i = blockDim.x/2;\n  while(i != 0) {\n    if (cacheIndex < i)\n      cache[cacheIndex] += cache[cacheIndex + i];\n    __syncthreads();\n    i /= 2;\n  }\n\n  if (cacheIndex == 0)\n    result[blockIdx.x] = cache[0];\n}",
            "__global__ void countAboveThresholdHelper(\n  const float *__restrict__ array, \n  const float threshold,\n  int *__restrict__ counter,\n  const int arrayLength)\n{\n  extern __shared__ int binomial[];\n\n  int cacheIndex=threadIdx.x;\n  int tid = threadIdx.x + blockIdx.x * blockDim.x;\n\n  int ptemp = 0;\n\n  while (tid < arrayLength) {\n    if (array[tid] > threshold) {\n      ptemp++;\n    }\n    tid += blockDim.x * gridDim.x;\n  }\n\n  binomial[cacheIndex] = ptemp;\n  __syncthreads();\n  int i = blockDim.x/2;\n  while(i != 0) {\n    if (cacheIndex < i)\n      binomial[cacheIndex] += binomial[cacheIndex + i];\n    __syncthreads();\n    i /= 2;\n  }\n\n  if (cacheIndex == 0)\n    counter[blockIdx.x] = binomial[0];\n}",
            "__global__ void computeRandomConnectionScores(\n  const float *__restrict__ random,\n  const int *__restrict__ reffile,\n        float *__restrict__ output,\n  const int M, const float UCmax, const int setSize,\n  const int nRandomGenerations)\n{\n  float temp = 0.0;\n  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  if (idx < nRandomGenerations) {\n    for(int col = idx; col < M; col += nRandomGenerations) {\n      // For converting to in-range indices, it helps to have the random numbers from 0 (inclusive) to 1 (non-inclusive) - so just flip them\n      float n = 1.0f - random[col];\n      // We'll ultimately want to normalize our results by the setSize - do it now, when round-off errors will hopefully cost less\n      float regulateFactor = 1.0f / setSize;\n      // If our random number now is >= 0.5, we'll downregulate - and subtract 0.5 so that 0 <= random < 0.5\n      if (n >= 0.5f) {\n        regulateFactor = -regulateFactor;\n        n -= 0.5f;\n      }\n      // Scale up random to become an integer index < arraySizeEnd\n      int rangeInArray = __float2int_rd(n * U133AArrayLength * 2);\n      // Add or subtract the randomly selected value from the array to the cumulative total\n      temp += reffile[rangeInArray] * regulateFactor;\n    }\n    // Update the output, further normalizing by UCmax\n    output[idx] = temp / UCmax;\n  }\n}"
        ]
    },
    "accuracy-cuda": {
        "/Users/gbolet/hecbench-roofline/src/accuracy-cuda/main.cu": [
            "#define GPU_NUM_THREADS 256\n\n\n__global__\nvoid accuracy_kernel(\n    const int N,\n    const int D,\n    const int top_k,\n    const float* Xdata,\n    const int* labelData,\n    int* accuracy)\n{\n  typedef cub::BlockReduce<int, GPU_NUM_THREADS> BlockReduce;\n  __shared__ typename BlockReduce::TempStorage temp_storage;\n  int count = 0;\n\n  for (int row = blockIdx.x; row < N; row += gridDim.x) {\n    const int label = labelData[row];\n    const float label_pred = Xdata[row * D + label];\n    int ngt = 0;\n    for (int col = threadIdx.x; col < D; col += blockDim.x) {\n      const float pred = Xdata[row * D + col];\n      if (pred > label_pred || (pred == label_pred && col <= label)) {\n        ++ngt;\n      }\n    }\n    ngt = BlockReduce(temp_storage).Sum(ngt);\n    if (ngt <= top_k) {\n      ++count;\n    }\n    __syncthreads();\n  }\n  if (threadIdx.x == 0) { \n    atomicAdd(accuracy, count);\n  }\n}"
        ]
    },
    "d3q19-bgk-cuda": {
        "/Users/gbolet/hecbench-roofline/src/d3q19-bgk-cuda/kernels.h": [
            "__global__ void make_flag(\n  flag_type *__restrict__ flags,\n        int *__restrict__ bounce_val,\n  flag_type *__restrict__ dir,\n  BoxCU domain,\n  outer_wall wall_type,\n  int width, int height, int depth,\n  int iter)\n{\n  int nx = domain.nx;\n  int ny = domain.ny;\n  int nz = domain.nz;\n\n  for (int z = blockIdx.z; z < nz; z += gridDim.z) {\n    for (int y = threadIdx.y + blockIdx.y*blockDim.y; y < ny; y += blockDim.y*gridDim.y) {\n      for (int x = threadIdx.x + blockIdx.x*blockDim.x; x < nx; x += blockDim.x*gridDim.x) {\n\n        if (x + domain.x0 == 0 ) {\n          SET_FLAG(wall_type.xmin, dir::x)\n        }else if (x + domain.x0 == width - 1) {\n          SET_FLAG(wall_type.xmax, dir::x_)\n        }\n\n        if (y + domain.y0 == 0 ) {\n          SET_FLAG(wall_type.ymin, dir::y)\n        }else if (y + domain.y0 == height - 1) {\n          SET_FLAG(wall_type.ymax, dir::y_)\n        }\n\n        if (z + domain.z0 == 0 ) {\n          SET_FLAG(wall_type.zmin, dir::z)\n        }else if (z + domain.z0 == depth - 1) {\n          SET_FLAG(wall_type.zmax, dir::z_)\n        }\n      }\n    }\n  }\n}",
            "__global__ void find_wall(\n  flag_type *__restrict__ flags,\n  flag_type *__restrict__ dir,\n  int *__restrict__ where,\n  const BoxCU domain,\n  int iter)\n{\n  int nx = domain.nx;\n  int ny = domain.ny;\n  int nz = domain.nz;\n\n  for (int z = blockIdx.z; z < nz; z += gridDim.z) {\n    for (int y = threadIdx.y + blockIdx.y*blockDim.y; y < ny; y += blockDim.y*gridDim.y) {\n      for (int x = threadIdx.x + blockIdx.x*blockDim.x; x < nx; x += blockDim.x*gridDim.x) {\n        if (flags[IDX(x, y, z, nx, ny, nz)] < bounce) {\n\n          for (int i = 3; i < 3*nb_directions; i += 3){\n\n            int xx = x + C_dirs[i    ];\n            int yy = y + C_dirs[i + 1];\n            int zz = z + C_dirs[i + 2];\n\n            if (flags[IDX(xx, yy, zz, nx, ny, nz)] == moving_wall ) {\n              flags[IDX(x, y, z, nx, ny, nz)] = wall_m;\n              dir[IDX(x, y, z, nx, ny, nz)] = dir[IDX(xx, yy, zz, nx, ny, nz)];\n              where[IDX(x, y, z, nx, ny, nz)]  |= (1 << IBAR((i/3), nb_directions));\n            }\n            if (flags[IDX(xx, yy, zz, nx, ny, nz)] == bounce ) {\n              if (flags[IDX(x, y, z, nx, ny, nz)] != wall_m)flags[IDX(x, y, z, nx, ny, nz)] = wall;\n              where[IDX(x, y, z, nx, ny, nz)]  |= (1 << IBAR((i/3), nb_directions));\n            }\n          }\n        }\n      }\n    }\n  }\n}",
            "__device__\ninline void d_equilibrium(double* fin, double rho, const float u0, const float u1, const float u2)\n{\n  double usqr = 3*(u0*u0 + u1*u1 + u2*u2)/2.;\n  double cu;\n  rho /= 36;\n  fin[0 ] = EQUILIBRIUM(rho, 12, 0 , usqr);\n  fin[1 ] = EQUILIBRIUM(rho, 2,3*( - u0          ), usqr);\n  fin[10] = fin[1] - rho *12*(-u0);\n  fin[2 ] = EQUILIBRIUM(rho, 2,3*(      - u1     ), usqr);\n  fin[11] = fin[2] - rho*12*(-u1);\n  fin[3 ] = EQUILIBRIUM(rho, 2,3*(           - u2), usqr);\n  fin[12] = fin[3] - rho*12*(-u2);\n  cu = 3 * (-u0 - u1);\n  fin[4 ] = EQUILIBRIUM(rho, 1, cu, usqr);\n  fin[13] = fin[4] - rho*2*cu;\n  cu = 3 * (-u0 + u1);\n  fin[5 ] = EQUILIBRIUM(rho, 1,cu, usqr);\n  fin[14] = fin[5] -  rho*2*cu;\n  cu = 3 * (-u0 - u2);\n  fin[6 ] = EQUILIBRIUM(rho, 1,cu, usqr);\n  fin[15] = fin[6] -  rho*2*cu;\n  cu = 3*(-u0 + u2);\n  fin[7 ] = EQUILIBRIUM(rho, 1,cu, usqr);\n  fin[16] = fin[7] - rho*2*cu;\n  cu = 3*(-u1 - u2);\n  fin[8 ] = EQUILIBRIUM(rho, 1,cu, usqr);\n  fin[17] = fin[8] - rho*2*cu;\n  cu = 3*(-u1 + u2);\n  fin[9 ] = EQUILIBRIUM(rho, 1, cu, usqr);\n  fin[18] = fin[9] -  rho*2*cu;\n}\n\n__global__ void init_velocity_g(\n  lbm_vars d_vars,\n  BoxCU domain,\n  BoxCU domain_vel,\n  double depth,\n  float u0 , float u1, float u2,\n  double rho)\n{\n  int nx = domain.nx;\n  int ny = domain.ny;\n  int nz = domain.nz;\n\n  double finl[nb_directions];\n\n  for (int z = blockIdx.z; z < nz; z += gridDim.z) {\n    for (int y = threadIdx.y + blockIdx.y*blockDim.y; y < ny; y += blockDim.y*gridDim.y) {\n      for (int x = threadIdx.x + blockIdx.x*blockDim.x; x < nx; x += blockDim.x*gridDim.x) {\n\n        int ugi = IDX(x - domain_vel.x0 + domain.x0,\n            y - domain_vel.y0 + domain.y0,\n            z - domain_vel.z0 + domain.z0,\n            domain_vel.nx, domain_vel.ny, domain_vel.nz);\n\n        d_vars.r[ugi] = rho;\n\n        d_vars.u_star.u0[ugi] = u0;\n        d_vars.u_star.u1[ugi] = u1;\n        d_vars.u_star.u2[ugi] = u2;\n\n        d_equilibrium(finl, rho, u0, u1, u2);\n\n        for (int i = 0; i < nb_directions; ++i) {\n          d_vars.f0[IDF(x, y, z, i, nx, ny, nz)] =  finl[i];\n          d_vars.f1[IDF(x, y, z, i, nx, ny, nz)] =  finl[i];\n        }\n      }\n    }\n  }\n}",
            "__device__\ninline void d_equilibrium(double* fin, double rho, const float u0, const float u1, const float u2)\n{\n  double usqr = 3*(u0*u0 + u1*u1 + u2*u2)/2.;\n  double cu;\n  rho /= 36;\n  fin[0 ] = EQUILIBRIUM(rho, 12, 0 , usqr);\n  fin[1 ] = EQUILIBRIUM(rho, 2,3*( - u0          ), usqr);\n  fin[10] = fin[1] - rho *12*(-u0);\n  fin[2 ] = EQUILIBRIUM(rho, 2,3*(      - u1     ), usqr);\n  fin[11] = fin[2] - rho*12*(-u1);\n  fin[3 ] = EQUILIBRIUM(rho, 2,3*(           - u2), usqr);\n  fin[12] = fin[3] - rho*12*(-u2);\n  cu = 3 * (-u0 - u1);\n  fin[4 ] = EQUILIBRIUM(rho, 1, cu, usqr);\n  fin[13] = fin[4] - rho*2*cu;\n  cu = 3 * (-u0 + u1);\n  fin[5 ] = EQUILIBRIUM(rho, 1,cu, usqr);\n  fin[14] = fin[5] -  rho*2*cu;\n  cu = 3 * (-u0 - u2);\n  fin[6 ] = EQUILIBRIUM(rho, 1,cu, usqr);\n  fin[15] = fin[6] -  rho*2*cu;\n  cu = 3*(-u0 + u2);\n  fin[7 ] = EQUILIBRIUM(rho, 1,cu, usqr);\n  fin[16] = fin[7] - rho*2*cu;\n  cu = 3*(-u1 - u2);\n  fin[8 ] = EQUILIBRIUM(rho, 1,cu, usqr);\n  fin[17] = fin[8] - rho*2*cu;\n  cu = 3*(-u1 + u2);\n  fin[9 ] = EQUILIBRIUM(rho, 1, cu, usqr);\n  fin[18] = fin[9] -  rho*2*cu;\n}\n\n__device__ __host__\ninline void macroscopic(double *f, double* rho, u_type* u0, u_type* u1, u_type* u2)\n{\n  double X_M1 = f[ 1] + f[ 4] + f[ 5] + f[ 6] + f[ 7];\n  double X_P1 = f[10] + f[13] + f[14] + f[15] + f[16];\n  double X_0  = f[ 0] + f[ 2] + f[ 3] + f[ 8] + f[ 9] + f[11] + f[12] + f[17] + f[18];\n  double Y_M1 = f[ 2] + f[ 4] + f[ 8] + f[ 9] + f[14];\n  double Y_P1 = f[ 5] + f[11] + f[13] + f[17] + f[18];\n  double Z_M1 = f[ 3] + f[ 6] + f[ 8] + f[16] + f[18];\n  double Z_P1 = f[ 7] + f[ 9] + f[12] + f[15] + f[17];\n  *rho = X_M1 + X_P1 + X_0;\n  double one_over_rho = 1./ *rho;\n  *u0 = (X_P1 - X_M1)* one_over_rho;\n  *u1 = (Y_P1 - Y_M1)* one_over_rho;\n  *u2 = (Z_P1 - Z_M1)* one_over_rho;\n}\n\n__device__\ninline void streaming(\n  lbm_vars d_vars,\n  double *fin, \n  const int x, const int y, const int z,\n const int nx, const int ny, const int nz)\n{\n  fin[0 ] = d_vars.f0[IDF(x   , y   , z   , 0 , nx, ny, nz)];\n  fin[1 ] = d_vars.f0[IDF(x +1, y   , z   , 1 , nx, ny, nz)];\n  fin[2 ] = d_vars.f0[IDF(x   , y +1, z   , 2 , nx, ny, nz)];\n  fin[3 ] = d_vars.f0[IDF(x   , y   , z +1, 3 , nx, ny, nz)];\n  fin[4 ] = d_vars.f0[IDF(x +1, y +1, z   , 4 , nx, ny, nz)];\n  fin[5 ] = d_vars.f0[IDF(x +1, y -1, z   , 5 , nx, ny, nz)];\n  fin[6 ] = d_vars.f0[IDF(x +1, y   , z +1, 6 , nx, ny, nz)];\n  fin[7 ] = d_vars.f0[IDF(x +1, y   , z -1, 7 , nx, ny, nz)];\n  fin[8 ] = d_vars.f0[IDF(x   , y +1, z +1, 8 , nx, ny, nz)];\n  fin[9 ] = d_vars.f0[IDF(x   , y +1, z -1, 9 , nx, ny, nz)];\n  fin[10] = d_vars.f0[IDF(x -1, y   , z   , 10, nx, ny, nz)];\n  fin[11] = d_vars.f0[IDF(x   , y -1, z   , 11, nx, ny, nz)];\n  fin[12] = d_vars.f0[IDF(x   , y   , z -1, 12, nx, ny, nz)];\n  fin[13] = d_vars.f0[IDF(x -1, y -1, z   , 13, nx, ny, nz)];\n  fin[14] = d_vars.f0[IDF(x -1, y +1, z   , 14, nx, ny, nz)];\n  fin[15] = d_vars.f0[IDF(x -1, y   , z -1, 15, nx, ny, nz)];\n  fin[16] = d_vars.f0[IDF(x -1, y   , z +1, 16, nx, ny, nz)];\n  fin[17] = d_vars.f0[IDF(x   , y -1, z -1, 17, nx, ny, nz)];\n  fin[18] = d_vars.f0[IDF(x   , y -1, z +1, 18, nx, ny, nz)];\n}\n\n__device__\ninline void streaming_bounce(\n  lbm_vars d_vars,\n  double *fin,\n  const int x, const int y, const int z,\n  const int nx, const int ny, const int nz,\n  int where)\n{\n  fin[0] = d_vars.f0[IDF(x, y, z, 0, nx, ny, nz)];\n  if (where & (1 << 1)) {\n    fin[1] = d_vars.f0[IDF(x, y, z, 10, nx, ny, nz)];\n  }\n\n  if (where & (1 << 2)) {\n    fin[2] = d_vars.f0[IDF(x, y, z, 11, nx, ny, nz)];\n  }\n\n  if (where & (1 << 3)) {\n    fin[3] = d_vars.f0[IDF(x, y, z, 12, nx, ny, nz)];\n  }\n\n  if (where & (1 << 4)) {\n    fin[4] = d_vars.f0[IDF(x, y, z, 13, nx, ny, nz)];\n  }\n\n  if (where & (1 << 5)) {\n    fin[5] = d_vars.f0[IDF(x, y, z, 14, nx, ny, nz)];\n  }\n\n  if (where & (1 << 6)) {\n    fin[6] = d_vars.f0[IDF(x, y, z, 15, nx, ny, nz)];\n  }\n\n  if (where & (1 << 7)) {\n    fin[7] = d_vars.f0[IDF(x, y, z, 16, nx, ny, nz)];\n  }\n\n  if (where & (1 << 8)) {\n    fin[8] = d_vars.f0[IDF(x, y, z, 17, nx, ny, nz)];\n  }\n\n  if (where & (1 << 9)) {\n    fin[9] = d_vars.f0[IDF(x, y, z, 18, nx, ny, nz)];\n  }\n\n  if (where & (1 << 10)) {\n    fin[10] = d_vars.f0[IDF(x, y, z, 1, nx, ny, nz)];\n  }\n\n  if (where & (1 << 11)) {\n    fin[11] = d_vars.f0[IDF(x, y, z, 2, nx, ny, nz)];\n  }\n\n  if (where & (1 << 12)) {\n    fin[12] = d_vars.f0[IDF(x, y, z, 3, nx, ny, nz)];\n  }\n\n  if (where & (1 << 13)) {\n    fin[13] = d_vars.f0[IDF(x, y, z, 4, nx, ny, nz)];\n  }\n\n  if (where & (1 << 14)) {\n    fin[14] = d_vars.f0[IDF(x, y, z, 5, nx, ny, nz)];\n  }\n\n  if (where & (1 << 15)) {\n    fin[15] = d_vars.f0[IDF(x, y, z, 6, nx, ny, nz)];\n  }\n\n  if (where & (1 << 16)) {\n    fin[16] = d_vars.f0[IDF(x, y, z, 7, nx, ny, nz)];\n  }\n\n  if (where & (1 << 17)) {\n    fin[17] = d_vars.f0[IDF(x, y, z, 8, nx, ny, nz)];\n  }\n\n  if (where & (1 << 18)) {\n    fin[18] = d_vars.f0[IDF(x, y, z, 9, nx, ny, nz)];\n  }\n}\n\n__device__ inline void streaming_wall2(\n  lbm_vars d_vars,\n  double *fin,\n  const int x, const int y, const int z,\n  const int nx, const int ny, const int nz,\n  flag_type dir,\n  flag_type* boundary,\n  u_type u)\n{\n  u_type u0 = u;\n  u_type u1 = 0.;\n  u_type u2 = 0.;\n\n  if  ( 0 || dir == 1){\n    fin[1 ] +=  2*1./18*3*( - u0          );\n  }\n  if  ( 0 || dir == 3){\n    fin[2 ] +=  2*1./18*3*(      - u1     );\n  }\n  if  ( 0 || dir == 5){\n    fin[3 ] +=  2*1./18*3*(           - u2);\n  }\n  if  ( 0 || dir == 1|| dir == 3){\n    fin[4 ] +=  2*1./36*3*( - u0 - u1     );\n  }\n  if  ( 0 || dir == 1|| dir == 2){\n    fin[5 ] +=  2*1./36*3*( - u0 + u1     );\n  }\n  if  ( 0 || dir == 1|| dir == 5){\n    fin[6 ] +=  2*1./36*3*( - u0      - u2);\n  }\n  if  ( 0 || dir == 1|| dir == 4){\n    fin[7 ] +=  2*1./36*3*( - u0      + u2);\n  }\n  if  ( 0 || dir == 3|| dir == 5){\n    fin[8 ] +=  2*1./36*3*(      - u1 - u2);\n  }\n  if  ( 0 || dir == 3|| dir == 4){\n    fin[9 ] +=  2*1./36*3*(      - u1 + u2);\n  }\n  if  ( 0 || dir == 0){\n    fin[10] +=  2*1./18*3*(   u0          );\n  }\n  if  ( 0 || dir == 2){\n    fin[11] +=  2*1./18*3*(      + u1     );\n  }\n  if  ( 0 || dir == 4){\n    fin[12] +=  2*1./18*3*(           + u2);\n  }\n  if  ( 0 || dir == 0|| dir == 2){\n    fin[13] +=  2*1./36*3*(   u0 + u1     );\n  }\n  if  ( 0 || dir == 0|| dir == 3){\n    fin[14] +=  2*1./36*3*(   u0 - u1     );\n  }\n  if  ( 0 || dir == 0|| dir == 4){\n    fin[15] +=  2*1./36*3*(   u0      + u2);\n  }\n  if  ( 0 || dir == 0|| dir == 5){\n    fin[16] +=  2*1./36*3*(   u0      - u2);\n  }\n  if  ( 0 || dir == 2|| dir == 4){\n    fin[17] +=  2*1./36*3*(      + u1 + u2);\n  }\n  if  ( 0 || dir == 2|| dir == 5){\n    fin[18] +=  2*1./36*3*(      + u1 - u2);\n  }\n}\n\n__launch_bounds__(64)\n__global__ void collide_and_stream_g(\n  lbm_vars d_vars,\n  const BoxCU domain,\n  const double ulb,\n  const double omega,\n  bool out_u,\n  int iter)\n{\n  int nx = domain.nx;\n  int ny = domain.ny;\n  int nz = domain.nz;\n\n  for (int z = blockIdx.z; z < nz; z += gridDim.z) {\n    for (int y = threadIdx.y + blockIdx.y*blockDim.y; y < ny; y += blockDim.y*gridDim.y) {\n      for (int x = threadIdx.x + blockIdx.x*blockDim.x; x < nx; x += blockDim.x*gridDim.x) {\n\n        double  finl[nb_directions], feq[nb_directions];\n        u_type u0, u1, u2;\n        double rho;\n\n        flag_type boundary = d_vars.boundary_flag[IDX(x, y, z, nx, ny, nz)];\n\n        if(boundary == bounce || boundary == moving_wall) continue;\n        flag_type vel_dir  = d_vars.boundary_dirs[IDX(x, y, z, nx, ny, nz)];\n\n        int ugi = IDX(x,y,z, domain.nx, domain.ny, domain.nz);\n\n        streaming(d_vars, finl, x, y, z, nx, ny, nz);\n\n        if (boundary == wall || boundary == wall_m){\n          int where = d_vars.boundary_values[IDX(x, y, z, nx, ny, nz)];\n          streaming_bounce(d_vars, finl, x, y, z, nx, ny, nz, where);\n        }\n\n        const u_type lid_vel = ulb;\n\n        if (boundary == wall_m) {\n          streaming_wall2(d_vars, finl, x, y, z, nx, ny, nz, vel_dir, d_vars.boundary_flag, lid_vel);\n        }\n        macroscopic(finl, &rho, &u0, &u1, &u2);\n\n        if (boundary < bounce ) {\n\n          d_equilibrium(feq, rho, u0, u1, u2);\n          // BGK collision model.\n          for (int i = 0; i < nb_directions; ++i){\n            finl[i] = (1.-omega)*finl[i] +omega*feq[i];\n          }\n        }\n\n        if(out_u && boundary < bounce){\n\n          d_vars.u_star.u0[ugi] = u0;\n          d_vars.u_star.u1[ugi] = u1;\n          d_vars.u_star.u2[ugi] = u2;\n        }\n\n        for (int i = 0; i < nb_directions; ++i) {\n          d_vars.f1[IDF(x, y, z, i, nx, ny, nz)] = finl[i];\n        }\n      }\n    }\n  }\n}"
        ]
    },
    "glu-cuda": {
        "/Users/gbolet/hecbench-roofline/src/glu-cuda/main.cu": [
            "inline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\ninline __host__ __device__ float4 expf(float4 v)\n{\n    return make_float4(expf(v.x), expf(v.y), expf(v.z), expf(v.w));\n}\n\n__global__\nvoid glu_kernel(\n   const int M,\n   const int split_dim_size,\n   const int N,\n   const float* Xdata,\n         float* Ydata)\n{\n  const int xOffset = 2 * split_dim_size * N;\n  const int yOffset = split_dim_size * N;\n  int index = blockIdx.x * blockDim.x + threadIdx.x; \n  if (index >= M * split_dim_size * N) return;\n\n  const int i = index / split_dim_size / N;\n  const int j = index / N % split_dim_size;\n  const int k = index % N;\n  const float x1 = Xdata[i * xOffset + j * N + k];\n  const float x2 = Xdata[i * xOffset + (j + split_dim_size) * N + k];\n  Ydata[i * yOffset + j * N + k] = x1 * (1.f / (1.f + expf(-x2)));\n}"
        ]
    },
    "bitpermute-cuda": {
        "/Users/gbolet/hecbench-roofline/src/bitpermute-cuda/kernels.h": [
            "#define T ((int)32)\n\n\n__device__ __forceinline__\nT bit_rev(T i, unsigned int nbits)\n{\n    if (sizeof(i) == 4 || nbits <= 32)\n        return __brev(i) >> (8*sizeof(unsigned int) - nbits);\n    else\n        return __brevll(i) >> (8*sizeof(unsigned long long) - nbits);\n}\n\n__global__\nvoid bit_rev_permutation(fr_t* d_out, const fr_t *d_in, uint32_t lg_domain_size)\n{\n    if (gridDim.x == 1 && blockDim.x == (1 << lg_domain_size)) {\n        uint32_t idx = threadIdx.x;\n        uint32_t rev = bit_rev(idx, lg_domain_size);\n\n        fr_t t = d_in[idx];\n        if (d_out == d_in)\n            __syncthreads();\n        d_out[rev] = t;\n    } else {\n        index_t idx = threadIdx.x + blockDim.x * (index_t)blockIdx.x;\n        index_t rev = bit_rev(idx, lg_domain_size);\n        bool copy = d_out != d_in && idx == rev;\n\n        if (idx < rev || copy) {\n            fr_t t0 = d_in[idx];\n            if (!copy) {\n                fr_t t1 = d_in[rev];\n                d_out[idx] = t1;\n            }\n            d_out[rev] = t0;\n        }\n    }\n}",
            "#define T ((int)32)\n\n\n__device__ __forceinline__\nT bit_rev(T i, unsigned int nbits)\n{\n    if (sizeof(i) == 4 || nbits <= 32)\n        return __brev(i) >> (8*sizeof(unsigned int) - nbits);\n    else\n        return __brevll(i) >> (8*sizeof(unsigned long long) - nbits);\n}\n\nstatic __device__ __host__ constexpr uint32_t lg2(T n)\n{   uint32_t ret=0; while (n>>=1) ret++; return ret;   }\n\n__global__\nvoid bit_rev_permutation_z(fr_t* out, const fr_t* in, uint32_t lg_domain_size)\n{\n    const uint32_t Z_COUNT = 256 / sizeof(fr_t);\n    const uint32_t LG_Z_COUNT = lg2(Z_COUNT);\n\n    extern __shared__ fr_t exchange[];\n    fr_t (*xchg)[Z_COUNT][Z_COUNT] = reinterpret_cast<decltype(xchg)>(exchange);\n\n    uint32_t gid = threadIdx.x / Z_COUNT;\n    uint32_t idx = threadIdx.x % Z_COUNT;\n    uint32_t rev = bit_rev(idx, LG_Z_COUNT);\n\n    index_t step = (index_t)1 << (lg_domain_size - LG_Z_COUNT);\n    index_t tid = threadIdx.x + blockDim.x * (index_t)blockIdx.x;\n\n    #pragma unroll 1\n    do {\n        index_t group_idx = tid >> LG_Z_COUNT;\n        index_t group_rev = bit_rev(group_idx, lg_domain_size - 2*LG_Z_COUNT);\n\n        if (group_idx > group_rev)\n            continue;\n\n        index_t base_idx = group_idx * Z_COUNT + idx;\n        index_t base_rev = group_rev * Z_COUNT + idx;\n\n        fr_t regs[Z_COUNT];\n\n        #pragma unroll\n        for (uint32_t i = 0; i < Z_COUNT; i++) {\n            xchg[gid][i][rev] = (regs[i] = in[i * step + base_idx]);\n            if (group_idx != group_rev)\n                regs[i] = in[i * step + base_rev];\n        }\n\n        (Z_COUNT > WARP_SZ) ? __syncthreads() : __syncwarp();\n\n        #pragma unroll\n        for (uint32_t i = 0; i < Z_COUNT; i++)\n            out[i * step + base_rev] = xchg[gid][rev][i];\n\n        if (group_idx == group_rev)\n            continue;\n\n        (Z_COUNT > WARP_SZ) ? __syncthreads() : __syncwarp();\n\n        #pragma unroll\n        for (uint32_t i = 0; i < Z_COUNT; i++)\n            xchg[gid][i][rev] = regs[i];\n\n        (Z_COUNT > WARP_SZ) ? __syncthreads() : __syncwarp();\n\n        #pragma unroll\n        for (uint32_t i = 0; i < Z_COUNT; i++)\n            out[i * step + base_idx] = xchg[gid][rev][i];\n\n    } while (Z_COUNT <= WARP_SZ && (tid += blockDim.x*gridDim.x) < step);\n    // without \"Z_COUNT <= WARP_SZ\" compiler spills 128 bytes to stack :-(\n}"
        ]
    },
    "convolutionDeformable-cuda": {
        "/Users/gbolet/hecbench-roofline/src/convolutionDeformable-cuda/src/cuda/dcn_v2_im2col_cuda.cu": [
            "#define T ((int)32)\n\n\n__host__ __device__ __forceinline__ constexpr T floor(T a, T b) {\n  return (a - b + 1) / b;\n}\n\n__device__ float dmcn_im2col_bilinear_cuda(const float *bottom_data, const int data_width,\n                                      const int height, const int width, float h, float w)\n{\n  int h_low = floor(h);\n  int w_low = floor(w);\n  int h_high = h_low + 1;\n  int w_high = w_low + 1;\n\n  float lh = h - h_low;\n  float lw = w - w_low;\n  float hh = 1 - lh, hw = 1 - lw;\n\n  float v1 = 0;\n  if (h_low >= 0 && w_low >= 0)\n    v1 = bottom_data[h_low * data_width + w_low];\n  float v2 = 0;\n  if (h_low >= 0 && w_high <= width - 1)\n    v2 = bottom_data[h_low * data_width + w_high];\n  float v3 = 0;\n  if (h_high <= height - 1 && w_low >= 0)\n    v3 = bottom_data[h_high * data_width + w_low];\n  float v4 = 0;\n  if (h_high <= height - 1 && w_high <= width - 1)\n    v4 = bottom_data[h_high * data_width + w_high];\n\n  float w1 = hh * hw, w2 = hh * lw, w3 = lh * hw, w4 = lh * lw;\n\n  float val = (w1 * v1 + w2 * v2 + w3 * v3 + w4 * v4);\n  return val;\n}\n\n__global__ void modulated_deformable_im2col_gpu_kernel(const int n,\n                                                       const float *data_im, const float *data_offset, const float *data_mask,\n                                                       const int height, const int width, const int kernel_h, const int kernel_w,\n                                                       const int pad_h, const int pad_w,\n                                                       const int stride_h, const int stride_w,\n                                                       const int dilation_h, const int dilation_w,\n                                                       const int channel_per_deformable_group,\n                                                       const int batch_size, const int num_channels, const int deformable_group,\n                                                       const int height_col, const int width_col,\n                                                       float *data_col)\n{\n  // launch channels * batch_size * height_col * width_col cores\n  CUDA_KERNEL_LOOP(index, n)\n  {\n    // NOTE(CharlesShang): different from Dai Jifeng's MXNet implementation, col_buffer is of shape (c*kw*kh, N, oh, ow)\n    // here columns is of shape (N, c*kw*kh, oh * ow), need to adapt axis\n\n    // index index of output matrix\n    const int w_col = index % width_col;\n    const int h_col = (index / width_col) % height_col;\n    // const int b_col = (index / width_col / height_col) % batch_size;\n    const int b_col = (index / width_col / height_col / num_channels) % batch_size;\n    // const int c_im = (index / width_col / height_col) / batch_size;\n    const int c_im = (index / width_col / height_col) % num_channels;\n    // const int c_col = c_im * kernel_h * kernel_w;\n    const int c_col = c_im * kernel_h * kernel_w;\n\n    // compute deformable group index\n    const int deformable_group_index = c_im / channel_per_deformable_group;\n\n    const int h_in = h_col * stride_h - pad_h;\n    const int w_in = w_col * stride_w - pad_w;\n\n    //  float *data_col_ptr = data_col + ((c_col * batch_size + b_col) * height_col + h_col) * width_col + w_col;\n    float *data_col_ptr = data_col + ((b_col * num_channels * kernel_w * kernel_h + c_col) * height_col + h_col) * width_col + w_col;\n    //const float* data_im_ptr = data_im + ((b_col * num_channels + c_im) * height + h_in) * width + w_in;\n    const float *data_im_ptr = data_im + (b_col * num_channels + c_im) * height * width;\n    const float *data_offset_ptr = data_offset + (b_col * deformable_group + deformable_group_index) * 2 * kernel_h * kernel_w * height_col * width_col;\n\n    const float *data_mask_ptr = data_mask + (b_col * deformable_group + deformable_group_index) * kernel_h * kernel_w * height_col * width_col;\n\n    for (int i = 0; i < kernel_h; ++i)\n    {\n      for (int j = 0; j < kernel_w; ++j)\n      {\n        const int data_offset_h_ptr = ((2 * (i * kernel_w + j)) * height_col + h_col) * width_col + w_col;\n        const int data_offset_w_ptr = ((2 * (i * kernel_w + j) + 1) * height_col + h_col) * width_col + w_col;\n        const int data_mask_hw_ptr = ((i * kernel_w + j) * height_col + h_col) * width_col + w_col;\n        const float offset_h = data_offset_ptr[data_offset_h_ptr];\n        const float offset_w = data_offset_ptr[data_offset_w_ptr];\n        const float mask = data_mask_ptr[data_mask_hw_ptr];\n        float val = static_cast<float>(0);\n        const float h_im = h_in + i * dilation_h + offset_h;\n        const float w_im = w_in + j * dilation_w + offset_w;\n        //if (h_im >= 0 && w_im >= 0 && h_im < height && w_im < width) {\n        if (h_im > -1 && w_im > -1 && h_im < height && w_im < width)\n        {\n          //const float map_h = i * dilation_h + offset_h;\n          //const float map_w = j * dilation_w + offset_w;\n          //const int cur_height = height - h_in;\n          //const int cur_width = width - w_in;\n          //val = dmcn_im2col_bilinear_cuda(data_im_ptr, width, cur_height, cur_width, map_h, map_w);\n          val = dmcn_im2col_bilinear_cuda(data_im_ptr, width, height, width, h_im, w_im);\n        }\n        *data_col_ptr = val * mask;\n        // data_col_ptr += batch_size * height_col * width_col;\n        data_col_ptr += height_col * width_col;\n      }\n    }\n  }\n}",
            "#define T ((int)32)\n\n\ninline __host__ __device__ int4 make_int4(float4 a)\n{\n    return make_int4(int(a.x), int(a.y), int(a.z), int(a.w));\n}\n\ninline __host__ __device__ int4 abs(int4 v)\n{\n    return make_int4(abs(v.x), abs(v.y), abs(v.z), abs(v.w));\n}\n\n__host__ __device__ __forceinline__ constexpr T floor(T a, T b) {\n  return (a - b + 1) / b;\n}\n\n__device__ float dmcn_get_gradient_weight_cuda(float argmax_h, float argmax_w,\n                                          const int h, const int w, const int height, const int width)\n{\n  if (argmax_h <= -1 || argmax_h >= height || argmax_w <= -1 || argmax_w >= width)\n  {\n    //empty\n    return 0;\n  }\n\n  int argmax_h_low = floor(argmax_h);\n  int argmax_w_low = floor(argmax_w);\n  int argmax_h_high = argmax_h_low + 1;\n  int argmax_w_high = argmax_w_low + 1;\n\n  float weight = 0;\n  if (h == argmax_h_low && w == argmax_w_low)\n    weight = (h + 1 - argmax_h) * (w + 1 - argmax_w);\n  if (h == argmax_h_low && w == argmax_w_high)\n    weight = (h + 1 - argmax_h) * (argmax_w + 1 - w);\n  if (h == argmax_h_high && w == argmax_w_low)\n    weight = (argmax_h + 1 - h) * (w + 1 - argmax_w);\n  if (h == argmax_h_high && w == argmax_w_high)\n    weight = (argmax_h + 1 - h) * (argmax_w + 1 - w);\n  return weight;\n}\n\n__global__ void modulated_deformable_col2im_gpu_kernel(const int n,\n                                                       const float *data_col, const float *data_offset, const float *data_mask,\n                                                       const int channels, const int height, const int width,\n                                                       const int kernel_h, const int kernel_w,\n                                                       const int pad_h, const int pad_w,\n                                                       const int stride_h, const int stride_w,\n                                                       const int dilation_h, const int dilation_w,\n                                                       const int channel_per_deformable_group,\n                                                       const int batch_size, const int deformable_group,\n                                                       const int height_col, const int width_col,\n                                                       float *grad_im)\n{\n  CUDA_KERNEL_LOOP(index, n)\n  {\n    const int j = (index / width_col / height_col / batch_size) % kernel_w;\n    const int i = (index / width_col / height_col / batch_size / kernel_w) % kernel_h;\n    const int c = index / width_col / height_col / batch_size / kernel_w / kernel_h;\n    // compute the start and end of the output\n\n    const int deformable_group_index = c / channel_per_deformable_group;\n\n    int w_out = index % width_col;\n    int h_out = (index / width_col) % height_col;\n    int b = (index / width_col / height_col) % batch_size;\n    int w_in = w_out * stride_w - pad_w;\n    int h_in = h_out * stride_h - pad_h;\n\n    const float *data_offset_ptr = data_offset + (b * deformable_group + deformable_group_index) * 2 * kernel_h * kernel_w * height_col * width_col;\n    const float *data_mask_ptr = data_mask + (b * deformable_group + deformable_group_index) * kernel_h * kernel_w * height_col * width_col;\n    const int data_offset_h_ptr = ((2 * (i * kernel_w + j)) * height_col + h_out) * width_col + w_out;\n    const int data_offset_w_ptr = ((2 * (i * kernel_w + j) + 1) * height_col + h_out) * width_col + w_out;\n    const int data_mask_hw_ptr = ((i * kernel_w + j) * height_col + h_out) * width_col + w_out;\n    const float offset_h = data_offset_ptr[data_offset_h_ptr];\n    const float offset_w = data_offset_ptr[data_offset_w_ptr];\n    const float mask = data_mask_ptr[data_mask_hw_ptr];\n    const float cur_inv_h_data = h_in + i * dilation_h + offset_h;\n    const float cur_inv_w_data = w_in + j * dilation_w + offset_w;\n\n    const float cur_top_grad = data_col[index] * mask;\n    const int cur_h = (int)cur_inv_h_data;\n    const int cur_w = (int)cur_inv_w_data;\n    for (int dy = -2; dy <= 2; dy++)\n    {\n      for (int dx = -2; dx <= 2; dx++)\n      {\n        if (cur_h + dy >= 0 && cur_h + dy < height &&\n            cur_w + dx >= 0 && cur_w + dx < width &&\n            abs(cur_inv_h_data - (cur_h + dy)) < 1 &&\n            abs(cur_inv_w_data - (cur_w + dx)) < 1)\n        {\n          int cur_bottom_grad_pos = ((b * channels + c) * height + cur_h + dy) * width + cur_w + dx;\n          float weight = dmcn_get_gradient_weight_cuda(cur_inv_h_data, cur_inv_w_data, cur_h + dy, cur_w + dx, height, width);\n          atomicAdd(grad_im + cur_bottom_grad_pos, weight * cur_top_grad);\n        }\n      }\n    }\n  }\n}",
            "#define T ((int)32)\n\n\n__host__ __device__ __forceinline__ constexpr T floor(T a, T b) {\n  return (a - b + 1) / b;\n}\n\n__device__ float dmcn_get_coordinate_weight_cuda(float argmax_h, float argmax_w,\n                                            const int height, const int width, const float *im_data,\n                                            const int data_width, const int bp_dir)\n{\n  if (argmax_h <= -1 || argmax_h >= height || argmax_w <= -1 || argmax_w >= width)\n  {\n    //empty\n    return 0;\n  }\n\n  int argmax_h_low = floor(argmax_h);\n  int argmax_w_low = floor(argmax_w);\n  int argmax_h_high = argmax_h_low + 1;\n  int argmax_w_high = argmax_w_low + 1;\n\n  float weight = 0;\n\n  if (bp_dir == 0)\n  {\n    if (argmax_h_low >= 0 && argmax_w_low >= 0)\n      weight += -1 * (argmax_w_low + 1 - argmax_w) * im_data[argmax_h_low * data_width + argmax_w_low];\n    if (argmax_h_low >= 0 && argmax_w_high <= width - 1)\n      weight += -1 * (argmax_w - argmax_w_low) * im_data[argmax_h_low * data_width + argmax_w_high];\n    if (argmax_h_high <= height - 1 && argmax_w_low >= 0)\n      weight += (argmax_w_low + 1 - argmax_w) * im_data[argmax_h_high * data_width + argmax_w_low];\n    if (argmax_h_high <= height - 1 && argmax_w_high <= width - 1)\n      weight += (argmax_w - argmax_w_low) * im_data[argmax_h_high * data_width + argmax_w_high];\n  }\n  else if (bp_dir == 1)\n  {\n    if (argmax_h_low >= 0 && argmax_w_low >= 0)\n      weight += -1 * (argmax_h_low + 1 - argmax_h) * im_data[argmax_h_low * data_width + argmax_w_low];\n    if (argmax_h_low >= 0 && argmax_w_high <= width - 1)\n      weight += (argmax_h_low + 1 - argmax_h) * im_data[argmax_h_low * data_width + argmax_w_high];\n    if (argmax_h_high <= height - 1 && argmax_w_low >= 0)\n      weight += -1 * (argmax_h - argmax_h_low) * im_data[argmax_h_high * data_width + argmax_w_low];\n    if (argmax_h_high <= height - 1 && argmax_w_high <= width - 1)\n      weight += (argmax_h - argmax_h_low) * im_data[argmax_h_high * data_width + argmax_w_high];\n  }\n\n  return weight;\n}\n\n__device__ float dmcn_im2col_bilinear_cuda(const float *bottom_data, const int data_width,\n                                      const int height, const int width, float h, float w)\n{\n  int h_low = floor(h);\n  int w_low = floor(w);\n  int h_high = h_low + 1;\n  int w_high = w_low + 1;\n\n  float lh = h - h_low;\n  float lw = w - w_low;\n  float hh = 1 - lh, hw = 1 - lw;\n\n  float v1 = 0;\n  if (h_low >= 0 && w_low >= 0)\n    v1 = bottom_data[h_low * data_width + w_low];\n  float v2 = 0;\n  if (h_low >= 0 && w_high <= width - 1)\n    v2 = bottom_data[h_low * data_width + w_high];\n  float v3 = 0;\n  if (h_high <= height - 1 && w_low >= 0)\n    v3 = bottom_data[h_high * data_width + w_low];\n  float v4 = 0;\n  if (h_high <= height - 1 && w_high <= width - 1)\n    v4 = bottom_data[h_high * data_width + w_high];\n\n  float w1 = hh * hw, w2 = hh * lw, w3 = lh * hw, w4 = lh * lw;\n\n  float val = (w1 * v1 + w2 * v2 + w3 * v3 + w4 * v4);\n  return val;\n}\n\n__global__ void modulated_deformable_col2im_coord_gpu_kernel(const int n,\n                                                             const float *data_col, const float *data_im,\n                                                             const float *data_offset, const float *data_mask,\n                                                             const int channels, const int height, const int width,\n                                                             const int kernel_h, const int kernel_w,\n                                                             const int pad_h, const int pad_w,\n                                                             const int stride_h, const int stride_w,\n                                                             const int dilation_h, const int dilation_w,\n                                                             const int channel_per_deformable_group,\n                                                             const int batch_size, const int offset_channels, const int deformable_group,\n                                                             const int height_col, const int width_col,\n                                                             float *grad_offset, float *grad_mask)\n{\n  CUDA_KERNEL_LOOP(index, n)\n  {\n    float val = 0, mval = 0;\n    int w = index % width_col;\n    int h = (index / width_col) % height_col;\n    int c = (index / width_col / height_col) % offset_channels;\n    int b = (index / width_col / height_col) / offset_channels;\n    // compute the start and end of the output\n\n    const int deformable_group_index = c / (2 * kernel_h * kernel_w);\n    const int col_step = kernel_h * kernel_w;\n    int cnt = 0;\n    const float *data_col_ptr = data_col + deformable_group_index * channel_per_deformable_group * batch_size * width_col * height_col;\n    const float *data_im_ptr = data_im + (b * deformable_group + deformable_group_index) * channel_per_deformable_group / kernel_h / kernel_w * height * width;\n    const float *data_offset_ptr = data_offset + (b * deformable_group + deformable_group_index) * 2 * kernel_h * kernel_w * height_col * width_col;\n    const float *data_mask_ptr = data_mask + (b * deformable_group + deformable_group_index) * kernel_h * kernel_w * height_col * width_col;\n\n    const int offset_c = c - deformable_group_index * 2 * kernel_h * kernel_w;\n\n    for (int col_c = (offset_c / 2); col_c < channel_per_deformable_group; col_c += col_step)\n    {\n      const int col_pos = (((col_c * batch_size + b) * height_col) + h) * width_col + w;\n      const int bp_dir = offset_c % 2;\n\n      int j = (col_pos / width_col / height_col / batch_size) % kernel_w;\n      int i = (col_pos / width_col / height_col / batch_size / kernel_w) % kernel_h;\n      int w_out = col_pos % width_col;\n      int h_out = (col_pos / width_col) % height_col;\n      int w_in = w_out * stride_w - pad_w;\n      int h_in = h_out * stride_h - pad_h;\n      const int data_offset_h_ptr = (((2 * (i * kernel_w + j)) * height_col + h_out) * width_col + w_out);\n      const int data_offset_w_ptr = (((2 * (i * kernel_w + j) + 1) * height_col + h_out) * width_col + w_out);\n      const int data_mask_hw_ptr = (((i * kernel_w + j) * height_col + h_out) * width_col + w_out);\n      const float offset_h = data_offset_ptr[data_offset_h_ptr];\n      const float offset_w = data_offset_ptr[data_offset_w_ptr];\n      const float mask = data_mask_ptr[data_mask_hw_ptr];\n      float inv_h = h_in + i * dilation_h + offset_h;\n      float inv_w = w_in + j * dilation_w + offset_w;\n      if (inv_h <= -1 || inv_w <= -1 || inv_h >= height || inv_w >= width)\n      {\n        inv_h = inv_w = -2;\n      }\n      else\n      {\n        mval += data_col_ptr[col_pos] * dmcn_im2col_bilinear_cuda(data_im_ptr + cnt * height * width, width, height, width, inv_h, inv_w);\n      }\n      const float weight = dmcn_get_coordinate_weight_cuda(\n          inv_h, inv_w,\n          height, width, data_im_ptr + cnt * height * width, width, bp_dir);\n      val += weight * data_col_ptr[col_pos] * mask;\n      cnt += 1;\n    }\n    // KERNEL_ASSIGN(grad_offset[index], offset_req, val);\n    grad_offset[index] = val;\n    if (offset_c % 2 == 0)\n      // KERNEL_ASSIGN(grad_mask[(((b * deformable_group + deformable_group_index) * kernel_h * kernel_w + offset_c / 2) * height_col + h) * width_col + w], mask_req, mval);\n      grad_mask[(((b * deformable_group + deformable_group_index) * kernel_h * kernel_w + offset_c / 2) * height_col + h) * width_col + w] = mval;\n  }\n}"
        ]
    },
    "sheath-cuda": {
        "/Users/gbolet/hecbench-roofline/src/sheath-cuda/main.cu": [
            "__device__ double XtoL(double pos)\n{\n  double li = (pos - 0) / DX;\n  return li;\n}\n\n__device__ void scatter(double lc, float value, float* field)\n{\n  int i    = (int)lc;\n  float di = lc - i;\n  atomicAdd(&(field[i]), value * (1 - di));\n  atomicAdd(&(field[i + 1]), value * (di));\n}\n\n__global__ void scatterParticle(const Particle* __restrict particles, float*__restrict den, long N)\n{\n  /*get particle id*/\n  long p = blockIdx.x * blockDim.x + threadIdx.x;\n  if (p < N && particles[p].alive)\n  {\n    double lc = XtoL(particles[p].x);\n    scatter(lc, 1.f, den);\n  }\n}",
            "__device__ double XtoL(double pos)\n{\n  double li = (pos - 0) / DX;\n  return li;\n}\n\n__device__ double gather(double lc, const double* field)\n{\n  int i     = (int)lc;\n  double di = lc - i;\n\n  /*gather field value onto particle position*/\n  double val = field[i] * (1 - di) + field[i + 1] * (di);\n  return val;\n}\n\n__global__ void pushParticle(Particle*__restrict particles, const double*__restrict ef, double qm, long N)\n{\n  /*get particle id*/\n  long p = blockIdx.x * blockDim.x + threadIdx.x;\n\n  if (p < N && particles[p].alive)\n  {\n    /*grab pointer to this particle*/\n    Particle* part = &particles[p];\n\n    /*compute particle node position*/\n    double lc = XtoL(part->x);\n\n    /*gather electric field onto particle position*/\n    double part_ef = gather(lc, ef);\n\n    /*advance velocity*/\n    part->v += DT * qm * part_ef;\n\n    /*advance position*/\n    part->x += DT * part->v;\n\n    /*remove particles leaving the domain*/\n    if (part->x < X0 || part->x >= XMAX)\n      part->alive = false;\n  }\n}",
            "__device__ double XtoL(double pos)\n{\n  double li = (pos - 0) / DX;\n  return li;\n}\n\n__device__ double gather(double lc, const double* field)\n{\n  int i     = (int)lc;\n  double di = lc - i;\n\n  /*gather field value onto particle position*/\n  double val = field[i] * (1 - di) + field[i + 1] * (di);\n  return val;\n}\n\n__global__ void rewindParticle(Particle*__restrict particles, const double*__restrict ef, double qm, long N)\n{\n  /*get particle id*/\n  long p = blockIdx.x * blockDim.x + threadIdx.x;\n\n  if (p < N && particles[p].alive)\n  {\n    /*grab pointer to this particle*/\n    Particle* part = &particles[p];\n\n    /*compute particle node position*/\n    double lc = XtoL(part->x);\n\n    /*gather electric field onto particle position*/\n    double part_ef = gather(lc, ef);\n\n    /*advance velocity*/\n    part->v -= 0.5 * DT * qm * part_ef;\n  }\n}"
        ]
    },
    "quicksort-cuda": {
        "/Users/gbolet/hecbench-roofline/src/quicksort-cuda/QuicksortKernels.cuh": [
            "#define P double\n\n\n#define T ((int)32)\n\n\n__device__\nT select (T a, T b, P c) {\n  return c ? b : a;\n}\n\n__device__\nuint median(uint x1, uint x2, uint x3) {\n\tif (x1 < x2) {\n\t\tif (x2 < x3) {\n\t\t\treturn x2;\n\t\t} else {\n      return select(x1, x3, x1 < x3);\n\t\t}\n\t} else { // x1 >= x2\n\t\tif (x1 < x3) {\n\t\t\treturn x1;\n\t\t} else { // x1 >= x3\n      return select(x2, x3, x2 < x3);\n\t\t}\n\t}\n}\n\n__device__\nvoid plus_prescan( T *a,  T *b) {\n  T av = *a;\n  T bv = *b;\n  *a = bv;\n  *b = bv + av;\n}\n\n__global__ void\ngqsort_kernel( T* d,  T* dn,  block_record<T>* blocks,  parent_record* parents,  work_record<T>* result) \n{\n  const uint blockid    = blockIdx.x;\n  const uint localid    = threadIdx.x;\n  __shared__ uint lt[GQSORT_LOCAL_WORKGROUP_SIZE+1], gt[GQSORT_LOCAL_WORKGROUP_SIZE+1], ltsum, gtsum, lbeg, gbeg;\n  uint i, lfrom, gfrom, lpivot, gpivot, tmp, ltp = 0, gtp = 0;\n\n  // Get the sequence block assigned to this work group\n  block_record<T> block = blocks[blockid];\n  uint start = block.start, end = block.end, direction = block.direction;\n  T pivot = block.pivot; \n\n  parent_record* pparent = parents + block.parent; \n  uint* psstart, *psend, *poldstart, *poldend, *pblockcount;\n  T *s, *sn;\n\n  // GPU-Quicksort cannot sort in place, as the regular quicksort algorithm can.\n  // It therefore needs two arrays to sort things out. We start sorting in the \n  // direction of d -> dn and then change direction after each run of gqsort_kernel.\n  // Which direction we are sorting: d -> dn or dn -> d?\n  if (direction == 1) {\n    s = d;\n    sn = dn;\n  } else {\n    s = dn;\n    sn = d;\n  }\n\n  // Set thread __shared__ counters to zero\n  lt[localid] = gt[localid] = 0;\n  __syncthreads();\n\n  // Align thread accesses for coalesced reads.\n  // Go through data...\n  for(i = start + localid; i < end; i += GQSORT_LOCAL_WORKGROUP_SIZE) {\n    tmp = s[i];\n    // counting elements that are smaller ...\n    if (tmp < pivot)\n      ltp++;\n    // or larger compared to the pivot.\n    if (tmp > pivot) \n      gtp++;\n  }\n  lt[localid] = ltp;\n  gt[localid] = gtp;\n  __syncthreads();\n\n  // calculate cumulative sums\n  uint n;\n  for(i = 1; i < GQSORT_LOCAL_WORKGROUP_SIZE; i <<= 1) {\n    n = 2*i - 1;\n    if ((localid & n) == n) {\n      lt[localid] += lt[localid-i];\n      gt[localid] += gt[localid-i];\n    }\n    __syncthreads();\n  }\n\n  if ((localid & n) == n) {\n    lt[GQSORT_LOCAL_WORKGROUP_SIZE] = ltsum = lt[localid];\n    gt[GQSORT_LOCAL_WORKGROUP_SIZE] = gtsum = gt[localid];\n    lt[localid] = 0;\n    gt[localid] = 0;\n  }\n\n  for(i = GQSORT_LOCAL_WORKGROUP_SIZE/2; i >= 1; i >>= 1) {\n    n = 2*i - 1;\n    if ((localid & n) == n) {\n      plus_prescan(&lt[localid - i], &lt[localid]);\n      plus_prescan(&gt[localid - i], &gt[localid]);\n    }\n    __syncthreads();\n  }\n\n  // Allocate memory in the sequence this block is a part of\n  if (localid == 0) {\n    // get shared variables\n    psstart = &pparent->sstart;\n    psend = &pparent->send;\n    poldstart = &pparent->oldstart;\n    poldend = &pparent->oldend;\n    pblockcount = &pparent->blockcount;\n    // Atomic increment allocates memory to write to.\n    lbeg = atomicAdd(psstart, ltsum);\n    // Atomic is necessary since multiple blocks access this\n    gbeg = atomicSub(psend, gtsum) - gtsum;\n  }\n  __syncthreads();\n\n  // Allocate locations for work items\n  lfrom = lbeg + lt[localid];\n  gfrom = gbeg + gt[localid];\n\n  // go thru data again writing elements to their correct position\n  for(i = start + localid; i < end; i += GQSORT_LOCAL_WORKGROUP_SIZE) {\n    tmp = s[i];\n    // increment counts\n    if (tmp < pivot) \n      sn[lfrom++] = tmp;\n\n    if (tmp > pivot) \n      sn[gfrom++] = tmp;\n  }\n  __syncthreads();\n\n  if (localid == 0) {\n    //if (atomic_dec(pblockcount) == 0) {\n    if (atomicSub(pblockcount, 1) == 0) {\n      uint sstart = *psstart;\n      uint send = *psend;\n      uint oldstart = *poldstart;\n      uint oldend = *poldend;\n\n      // Store the pivot value between the new sequences\n      for(i = sstart; i < send; i ++) {\n        d[i] = pivot;\n      }\n\n      lpivot = sn[oldstart];\n      gpivot = sn[oldend-1];\n      if (oldstart < sstart) {\n        lpivot = median(lpivot,sn[(oldstart+sstart) >> 1], sn[sstart-1]);\n      } \n      if (send < oldend) {\n        gpivot = median(sn[send],sn[(oldend+send) >> 1], gpivot);\n      }\n\n      work_record<T>* result1 = result + 2*blockid;\n      work_record<T>* result2 = result1 + 1;\n\n      // change the direction of the sort.\n      direction ^= 1;\n\n      work_record<T> r1 = {oldstart, sstart, lpivot, direction};\n      *result1 = r1;\n\n      work_record<T> r2 = {send, oldend, gpivot, direction};\n      *result2 = r2;\n    }\n  }\n  }",
            "#define P double\n\n\n#define T ((int)32)\n\n\n__device__\nT select (T a, T b, P c) {\n  return c ? b : a;\n}\n\n__device__ void bitonic_sort(int *values, const int N)\n{\n  unsigned int idx = threadIdx.x;\n\n  for (int k = 2; k <= N; k <<= 1)\n  {\n    for (int j = k >> 1; j > 0; j = j >> 1)\n    {\n      while(idx < N) \n      {\n        int ixj = idx^j;\n        if (ixj > idx) \n        {\n          if ((idx&k) == 0 && values[idx] > values[ixj]) \n          {\n            //exchange(idx, ixj);\n            int tmp = values[idx];\n            values[idx] = values[ixj];\n            values[ixj] = tmp;\n          }\n          if ((idx&k) != 0 && values[idx] < values[ixj]) \n          {\n            //exchange(idx, ixj);\n            int tmp = values[idx];\n            values[idx] = values[ixj];\n            values[ixj] = tmp;\n          }\n        }\n        idx += blockDim.x;\n      }\n      __syncthreads();\n      idx = threadIdx.x;\n    }\n  }\n}\n\n__device__\nuint median(uint x1, uint x2, uint x3) {\n\tif (x1 < x2) {\n\t\tif (x2 < x3) {\n\t\t\treturn x2;\n\t\t} else {\n      return select(x1, x3, x1 < x3);\n\t\t}\n\t} else { // x1 >= x2\n\t\tif (x1 < x3) {\n\t\t\treturn x1;\n\t\t} else { // x1 >= x3\n      return select(x2, x3, x2 < x3);\n\t\t}\n\t}\n}\n\n__device__\nvoid plus_prescan( T *a,  T *b) {\n  T av = *a;\n  T bv = *b;\n  *a = bv;\n  *b = bv + av;\n}\n\n__device__\nvoid sort_threshold( T* data_in, \n    T* data_out,\n    uint start, \n    uint end,  \n    T* temp, \n    uint localid) \n{\n  uint tsum = end - start;\n  if (tsum == SORT_THRESHOLD) {\n    bitonic_sort(data_in+start, localid);\n    for (uint i = localid; i < SORT_THRESHOLD; i += LQSORT_LOCAL_WORKGROUP_SIZE) {\n      data_out[start + i] = data_in[start + i];\n    }\n  } else if (tsum > 1) {\n    for (uint i = localid; i < SORT_THRESHOLD; i += LQSORT_LOCAL_WORKGROUP_SIZE) {\n      if (i < tsum) {\n        temp[i] = data_in[start + i];\n      } else {\n        temp[i] = UINT_MAX;\n      }\n    }\n    __syncthreads();\n    bitonic_sort(temp, localid);\n\n    for (uint i = localid; i < tsum; i += LQSORT_LOCAL_WORKGROUP_SIZE) {\n      data_out[start + i] = temp[i];\n    }\n  } else if (tsum == 1 && localid == 0) {\n    data_out[start] = data_in[start];\n  } \n}\n\n__global__ void \nlqsort_kernel(T* d, T* dn, work_record<T>* seqs) \n{\n  const uint blockid    = blockIdx.x;\n  const uint localid    = threadIdx.x;\n\n  // workstack: stores the start and end of the sequences, direction of sort\n  // If the sequence is less that SORT_THRESHOLD, it gets sorted. \n  // It will only be pushed on the stack if it greater than the SORT_THRESHOLD. \n  // Note, that the sum of ltsum + gtsum is less than QUICKSORT_BLOCK_SIZE. \n  // The total sum of the length of records on the stack cannot exceed QUICKSORT_BLOCK_SIZE, \n  // but each individual record should be greater than SORT_THRESHOLD, so the maximum length \n  // of the stack is QUICKSORT_BLOCK_SIZE/SORT_THRESHOLD - in the case of BDW GT2 the length \n  // of the stack is 2 :)\n  __shared__ workstack_record workstack[QUICKSORT_BLOCK_SIZE/SORT_THRESHOLD]; \n  __shared__ int workstack_pointer;\n\n  __shared__ T mys[QUICKSORT_BLOCK_SIZE], mysn[QUICKSORT_BLOCK_SIZE], temp[SORT_THRESHOLD];\n  __shared__ T *s, *sn;\n  __shared__ uint ltsum, gtsum;\n  __shared__ uint lt[LQSORT_LOCAL_WORKGROUP_SIZE+1], gt[LQSORT_LOCAL_WORKGROUP_SIZE+1];\n  uint i, tmp, ltp, gtp;\n\n  work_record<T> block = seqs[blockid];\n  const uint d_offset = block.start;\n  uint start = 0; \n  uint end   = block.end - d_offset;\n\n  uint direction = 1; // which direction to sort\n  // initialize workstack and workstack_pointer: push the initial sequence on the stack\n  if (localid == 0) {\n    workstack_pointer = 0; // beginning of the stack\n    workstack_record wr = { start, end, direction };\n    workstack[0] = wr;\n  }\n  // copy block of data to be sorted by one workgroup into __shared__ memory\n  // note that indeces of __shared__ data go from 0 to end-start-1\n  if (block.direction == 1) {\n    for (i = localid; i < end; i += LQSORT_LOCAL_WORKGROUP_SIZE) {\n      mys[i] = d[i+d_offset];\n    }\n  } else {\n    for (i = localid; i < end; i += LQSORT_LOCAL_WORKGROUP_SIZE) {\n      mys[i] = dn[i+d_offset];\n    }\n  }\n  __syncthreads();\n\n  while (workstack_pointer >= 0) { \n    // pop up the stack\n    workstack_record wr = workstack[workstack_pointer];\n    start = wr.start;\n    end = wr.end;\n    direction = wr.direction;\n    __syncthreads();\n    if (localid == 0) {\n      --workstack_pointer;\n\n      ltsum = gtsum = 0;  \n    }\n    if (direction == 1) {\n      s = mys;\n      sn = mysn;\n    } else {\n      s = mysn;\n      sn = mys;\n    }\n    // Set thread __shared__ counters to zero\n    lt[localid] = gt[localid] = 0;\n    ltp = gtp = 0;\n    __syncthreads();\n\n    // Pick a pivot\n    uint pivot = s[start];\n    if (start < end) {\n      pivot = median(pivot, s[(start+end) >> 1], s[end-1]);\n    }\n    // Align work item accesses for coalesced reads.\n    // Go through data...\n    for(i = start + localid; i < end; i += LQSORT_LOCAL_WORKGROUP_SIZE) {\n      tmp = s[i];\n      // counting elements that are smaller ...\n      if (tmp < pivot)\n        ltp++;\n      // or larger compared to the pivot.\n      if (tmp > pivot) \n        gtp++;\n    }\n    lt[localid] = ltp;\n    gt[localid] = gtp;\n    __syncthreads();\n\n    // calculate cumulative sums\n    uint n;\n    for(i = 1; i < LQSORT_LOCAL_WORKGROUP_SIZE; i <<= 1) {\n      n = 2*i - 1;\n      if ((localid & n) == n) {\n        lt[localid] += lt[localid-i];\n        gt[localid] += gt[localid-i];\n      }\n      __syncthreads();\n    }\n\n    if ((localid & n) == n) {\n      lt[LQSORT_LOCAL_WORKGROUP_SIZE] = ltsum = lt[localid];\n      gt[LQSORT_LOCAL_WORKGROUP_SIZE] = gtsum = gt[localid];\n      lt[localid] = 0;\n      gt[localid] = 0;\n    }\n\n    for(i = LQSORT_LOCAL_WORKGROUP_SIZE/2; i >= 1; i >>= 1) {\n      n = 2*i - 1;\n      if ((localid & n) == n) {\n        plus_prescan(&lt[localid - i], &lt[localid]);\n        plus_prescan(&gt[localid - i], &gt[localid]);\n      }\n      __syncthreads();\n    }\n\n    // Allocate locations for work items\n    uint lfrom = start + lt[localid];\n    uint gfrom = end - gt[localid+1];\n\n    // go thru data again writing elements to their correct position\n    for (i = start + localid; i < end; i += LQSORT_LOCAL_WORKGROUP_SIZE) {\n      tmp = s[i];\n      // increment counts\n      if (tmp < pivot) \n        sn[lfrom++] = tmp;\n\n      if (tmp > pivot) \n        sn[gfrom++] = tmp;\n    }\n    __syncthreads();\n\n    // Store the pivot value between the new sequences\n    for (i = start + ltsum + localid;i < end - gtsum; i += LQSORT_LOCAL_WORKGROUP_SIZE) {\n      d[i+d_offset] = pivot;\n    }\n    __syncthreads();\n\n    // if the sequence is shorter than SORT_THRESHOLD\n    // sort it using an alternative sort and place result in d\n    if (ltsum <= SORT_THRESHOLD) {\n      sort_threshold(sn, d+d_offset, start, start + ltsum, temp, localid);\n    } else {\n      PUSH(start, start + ltsum);\n    }\n\n    if (gtsum <= SORT_THRESHOLD) {\n      sort_threshold(sn, d+d_offset, end - gtsum, end, temp, localid);\n    } else {\n      PUSH(end - gtsum, end);\n    }\n  }\n}"
        ]
    },
    "dwconv-cuda": {
        "/Users/gbolet/hecbench-roofline/src/dwconv-cuda/main.cu": [
            "__global__ void conv_depthwise2d_forward_kernel(\n    const PackedTensorAccessor32<scalar_t, 4, RestrictPtrTraits> input,\n          PackedTensorAccessor32<scalar_t, 4, RestrictPtrTraits> output,\n    const PackedTensorAccessor32<scalar_t, 4, RestrictPtrTraits> weight,\n    const PackedTensorAccessor32<scalar_t, 1, RestrictPtrTraits> bias,\n    bool biasEnabled,\n    index_t totalElements,\n    const int outputChannels,\n    const int depthwiseMultiplier,\n    const int inputWidth, const int inputHeight,\n    const int outputWidth, const int outputHeight,\n    const int kernelWidth, const int kernelHeight,\n    const int strideWidth, const int strideHeight,\n    const int padWidth, const int padHeight,\n    const int dilationWidth, const int dilationHeight)\n{\n  const int KW_LIMIT = (kSize != 0) ? kSize : kernelWidth;\n  const int KH_LIMIT = (kSize != 0) ? kSize : kernelHeight;\n\n  index_t linearIndex = blockIdx.x * blockDim.x + threadIdx.x;\n  if (linearIndex < totalElements) {\n    //calculate n,c,h,w indices, replacing modulos by divide and multiply add,\n    //result is same as would be in the code below\n    //const int n = linearIndex / batchStride; //batchStride = outputChannels * outputHeight * outputWidth\n    //const int c = (linearIndex / channelStride) % outputChannels; //channelStride = outputHeight * outputWidth\n    //const int h = (linearIndex / outputWidth) % outputHeight;\n    //const int w = linearIndex % outputWidth;\n\n    int indtmp1 = linearIndex/outputWidth;\n    const int w = linearIndex - indtmp1 * outputWidth;\n    int indtmp2 = indtmp1/outputHeight;\n    const int h = indtmp1 - indtmp2 * outputHeight;\n    indtmp1 = indtmp2;\n    indtmp2 = indtmp1/outputChannels;\n    const int c = indtmp1 - indtmp2 * outputChannels;\n    const int n = indtmp2;\n\n    int inputChannel = c;\n    int inputChannels = outputChannels;\n    if (depthwiseMultiplier !=1) {\n      inputChannel /= depthwiseMultiplier;\n      inputChannels /= depthwiseMultiplier;\n    }\n\n    int weightOffset = c * kernelHeight * kernelWidth;\n\n    acc_t value = biasEnabled ? static_cast<acc_t>(bias.data()[c]) : acc_t(0);\n    const index_t offset0 = (n * inputChannels + inputChannel) * inputHeight * inputWidth;\n    #pragma unroll\n    for (int kH = 0; kH < KH_LIMIT; ++kH) {\n      #pragma unroll\n      for (int kW = 0; kW < KW_LIMIT; ++kW) {\n        const int h_in = -padHeight + h * strideHeight + kH * dilationHeight;\n        const int w_in = -padWidth + w * strideWidth + kW * dilationWidth;\n\n        if ((h_in >= 0) && (h_in < inputHeight) && (w_in >= 0) && (w_in < inputWidth)) {\n          const index_t offset = offset0 + h_in * inputWidth + w_in;\n          value += (static_cast<acc_t>(weight.data()[weightOffset]) *\n                    static_cast<acc_t>(input.data()[offset]));\n        }\n        ++weightOffset;\n      }\n    }\n    output.data()[linearIndex] = static_cast<scalar_t>(value);\n  }\n}"
        ]
    },
    "minisweep-cuda": {
        "/Users/gbolet/hecbench-roofline/src/minisweep-cuda/kernels.cu": [
            "#define P double\n\n\n__host__ __device__\ninline int Dir_z( int octant ) { return octant & (1<<2) ? DIR_DN * 1 : DIR_UP * 1; }\n\n__device__\nP Quantities_init_face_acceldir(int ia, int ie, int iu, int scalefactor_space, int octant)\n{\n  /*--- Quantities_affinefunction_ inline ---*/\n  return ( (P) (1 + ia) ) \n\n    /*--- Quantities_scalefactor_angle_ inline ---*/\n    * ( (P) (1 << (ia & ( (1<<3) - 1))) ) \n\n    /*--- Quantities_scalefactor_space_ inline ---*/\n    * ( (P) scalefactor_space)\n\n    /*--- Quantities_scalefactor_energy_ inline ---*/\n    * ( (P) (1 << ((( (ie) * 1366 + 150889) % 714025) & ( (1<<2) - 1))) )\n\n    /*--- Quantities_scalefactor_unknown_ inline ---*/\n    * ( (P) (1 << ((( (iu) * 741 + 60037) % 312500) & ( (1<<2) - 1))) )\n\n    /*--- Quantities_scalefactor_octant_ ---*/\n    * ( (P) 1 + octant);\n}\n\n__device__\nint Quantities_scalefactor_space_acceldir(int ix_g, int iy_g, int iz_g)\n{\n  int result = 0;\n\n#ifndef RELAXED_TESTING\n  const int im = 134456;\n  const int ia = 8121;\n  const int ic = 28411;\n\n  result = ( (result+(ix_g+2))*ia + ic ) % im;\n  result = ( (result+(iy_g+2))*ia + ic ) % im;\n  result = ( (result+(iz_g+2))*ia + ic ) % im;\n  result = ( (result+(ix_g+3*iy_g+7*iz_g+2))*ia + ic ) % im;\n  result = ix_g+3*iy_g+7*iz_g+2;\n  result = result & ( (1<<2) - 1 );\n#endif\n  result = 1 << result;\n\n  return result;\n}\n\n__global__ void init_facexy(\n    const int ix_base, \n    const int iy_base,\n    const int dims_b_ne,\n    const int dims_b_na,\n    const int dims_b_ncell_x,\n    const int dims_b_ncell_y,\n    const int dims_b_ncell_z,\n    const int dims_ncell_z,\n    P* facexy) \n{\n  int ix = blockDim.x * blockIdx.x + threadIdx.x; \n  int iy = blockDim.y * blockIdx.y + threadIdx.y; \n  int octant = blockDim.z * blockIdx.z + threadIdx.z; \n  if (ix >= dims_b_ncell_x || iy >= dims_b_ncell_y || octant >= NOCTANT) return;\n\n  for(int ie=0; ie<dims_b_ne; ++ie )\n    for(int iu=0; iu<NU; ++iu )\n      for(int ia=0; ia<dims_b_na; ++ia )\n      {\n        const int dir_z = Dir_z( octant );\n        const int iz = dir_z == DIR_UP ? -1 : dims_b_ncell_z;\n\n        const int ix_g = ix + ix_base; // dims_b_ncell_x * proc_x;\n        const int iy_g = iy + iy_base; // dims_b_ncell_y * proc_y;\n        const int iz_g = iz + (dir_z == DIR_UP ? 0 : dims_ncell_z - dims_b_ncell_z);\n        //const int iz_g = iz + stepinfoall.stepinfo[octant].block_z * dims_b_ncell_z;\n\n        /*--- Quantities_scalefactor_space_ inline ---*/\n        const int scalefactor_space\n          = Quantities_scalefactor_space_acceldir(ix_g, iy_g, iz_g);\n\n        /*--- ref_facexy inline ---*/\n        facexy[FACEXY_ADDR(dims_b_ncell_x, dims_b_ncell_y)]\n          /*--- Quantities_init_face routine ---*/\n          = Quantities_init_face_acceldir(ia, ie, iu, scalefactor_space, octant);\n        //printf(\"kernel facexy: %d %d %d %d %d %f\\n\", \n          //ia, ie, iu, scalefactor_space, octant,\n          //Quantities_init_face_acceldir(ia, ie, iu, scalefactor_space, octant));\n      } /*---for---*/\n}",
            "#define P double\n\n\n__host__ __device__\ninline int Dir_y( int octant ) { return octant & (1<<1) ? DIR_DN * 1: DIR_UP * 1; }\n\n__device__\nP Quantities_init_face_acceldir(int ia, int ie, int iu, int scalefactor_space, int octant)\n{\n  /*--- Quantities_affinefunction_ inline ---*/\n  return ( (P) (1 + ia) ) \n\n    /*--- Quantities_scalefactor_angle_ inline ---*/\n    * ( (P) (1 << (ia & ( (1<<3) - 1))) ) \n\n    /*--- Quantities_scalefactor_space_ inline ---*/\n    * ( (P) scalefactor_space)\n\n    /*--- Quantities_scalefactor_energy_ inline ---*/\n    * ( (P) (1 << ((( (ie) * 1366 + 150889) % 714025) & ( (1<<2) - 1))) )\n\n    /*--- Quantities_scalefactor_unknown_ inline ---*/\n    * ( (P) (1 << ((( (iu) * 741 + 60037) % 312500) & ( (1<<2) - 1))) )\n\n    /*--- Quantities_scalefactor_octant_ ---*/\n    * ( (P) 1 + octant);\n}\n\n__device__\nint Quantities_scalefactor_space_acceldir(int ix_g, int iy_g, int iz_g)\n{\n  int result = 0;\n\n#ifndef RELAXED_TESTING\n  const int im = 134456;\n  const int ia = 8121;\n  const int ic = 28411;\n\n  result = ( (result+(ix_g+2))*ia + ic ) % im;\n  result = ( (result+(iy_g+2))*ia + ic ) % im;\n  result = ( (result+(iz_g+2))*ia + ic ) % im;\n  result = ( (result+(ix_g+3*iy_g+7*iz_g+2))*ia + ic ) % im;\n  result = ix_g+3*iy_g+7*iz_g+2;\n  result = result & ( (1<<2) - 1 );\n#endif\n  result = 1 << result;\n\n  return result;\n}\n\n__global__ void init_facexz(\n    const int ix_base, \n    const int iy_base,\n    const int dims_b_ne,\n    const int dims_b_na,\n    const int dims_b_ncell_x,\n    const int dims_b_ncell_y,\n    const int dims_b_ncell_z,\n    const int proc_y_min,\n    const int proc_y_max,\n    StepInfoAll stepinfoall,\n    P* facexz) \n{\n  int ix = blockDim.x * blockIdx.x + threadIdx.x; \n  int iz = blockDim.y * blockIdx.y + threadIdx.y; \n  int octant = blockDim.z * blockIdx.z + threadIdx.z; \n  if (ix >= dims_b_ncell_x || iz >= dims_b_ncell_z || octant >= NOCTANT) return;\n\n  for(int ie=0; ie<dims_b_ne; ++ie )\n    for(int iu=0; iu<NU; ++iu )\n      for(int ia=0; ia<dims_b_na; ++ia )\n      {\n        const int dir_y = Dir_y( octant );\n        const int iy = dir_y == DIR_UP ? -1 : dims_b_ncell_y;\n\n        const int ix_g = ix + ix_base; // dims_b_ncell_x * proc_x;\n        const int iy_g = iy + iy_base; // dims_b_ncell_y * proc_y;\n        const int iz_g = iz + stepinfoall.stepinfo[octant].block_z * dims_b_ncell_z;\n\n        if ((dir_y == DIR_UP && proc_y_min) || (dir_y == DIR_DN && proc_y_max)) {\n\n          /*--- Quantities_scalefactor_space_ inline ---*/\n          const int scalefactor_space\n            = Quantities_scalefactor_space_acceldir(ix_g, iy_g, iz_g);\n\n          /*--- ref_facexz inline ---*/\n        facexz[FACEXZ_ADDR(dims_b_ncell_x, dims_b_ncell_z)]\n            /*--- Quantities_init_face routine ---*/\n            = Quantities_init_face_acceldir(ia, ie, iu, scalefactor_space, octant);\n        } /*---if---*/\n      } /*---for---*/\n}",
            "#define P double\n\n\n__host__ __device__\ninline int Dir_x( int octant ) { return octant & (1<<0) ? DIR_DN * 1: DIR_UP * 1; }\n\n__device__\nP Quantities_init_face_acceldir(int ia, int ie, int iu, int scalefactor_space, int octant)\n{\n  /*--- Quantities_affinefunction_ inline ---*/\n  return ( (P) (1 + ia) ) \n\n    /*--- Quantities_scalefactor_angle_ inline ---*/\n    * ( (P) (1 << (ia & ( (1<<3) - 1))) ) \n\n    /*--- Quantities_scalefactor_space_ inline ---*/\n    * ( (P) scalefactor_space)\n\n    /*--- Quantities_scalefactor_energy_ inline ---*/\n    * ( (P) (1 << ((( (ie) * 1366 + 150889) % 714025) & ( (1<<2) - 1))) )\n\n    /*--- Quantities_scalefactor_unknown_ inline ---*/\n    * ( (P) (1 << ((( (iu) * 741 + 60037) % 312500) & ( (1<<2) - 1))) )\n\n    /*--- Quantities_scalefactor_octant_ ---*/\n    * ( (P) 1 + octant);\n}\n\n__device__\nint Quantities_scalefactor_space_acceldir(int ix_g, int iy_g, int iz_g)\n{\n  int result = 0;\n\n#ifndef RELAXED_TESTING\n  const int im = 134456;\n  const int ia = 8121;\n  const int ic = 28411;\n\n  result = ( (result+(ix_g+2))*ia + ic ) % im;\n  result = ( (result+(iy_g+2))*ia + ic ) % im;\n  result = ( (result+(iz_g+2))*ia + ic ) % im;\n  result = ( (result+(ix_g+3*iy_g+7*iz_g+2))*ia + ic ) % im;\n  result = ix_g+3*iy_g+7*iz_g+2;\n  result = result & ( (1<<2) - 1 );\n#endif\n  result = 1 << result;\n\n  return result;\n}\n\n__global__ void init_faceyz(\n    const int ix_base, \n    const int iy_base,\n    const int dims_b_ne,\n    const int dims_b_na,\n    const int dims_b_ncell_x,\n    const int dims_b_ncell_y,\n    const int dims_b_ncell_z,\n    const int proc_x_min,\n    const int proc_x_max,\n    StepInfoAll stepinfoall,\n    P* faceyz)\n{\n  int iy = blockDim.x * blockIdx.x + threadIdx.x; \n  int iz = blockDim.y * blockIdx.y + threadIdx.y; \n  int octant = blockDim.z * blockIdx.z + threadIdx.z; \n  if (iy >= dims_b_ncell_y || iz >= dims_b_ncell_z || octant >= NOCTANT) return;\n\n  for(int ie=0; ie<dims_b_ne; ++ie )\n    for(int iu=0; iu<NU; ++iu )\n      for(int ia=0; ia<dims_b_na; ++ia )\n      {\n\n        const int dir_x = Dir_x( octant );\n        const int ix = dir_x == DIR_UP ? -1 : dims_b_ncell_x;\n\n        const int ix_g = ix + ix_base; // dims_b_ncell_x * proc_x;\n        const int iy_g = iy + iy_base; // dims_b_ncell_y * proc_y;\n        const int iz_g = iz + stepinfoall.stepinfo[octant].block_z * dims_b_ncell_z;\n\n        if ((dir_x == DIR_UP && proc_x_min) || (dir_x == DIR_DN && proc_x_max)) {\n\n          /*--- Quantities_scalefactor_space_ inline ---*/\n          const int scalefactor_space\n            = Quantities_scalefactor_space_acceldir(ix_g, iy_g, iz_g);\n\n          /*--- ref_faceyz inline ---*/\n          faceyz[FACEYZ_ADDR(dims_b_ncell_y, dims_b_ncell_z)]\n            /*--- Quantities_init_face routine ---*/\n            = Quantities_init_face_acceldir(ia, ie, iu, scalefactor_space, octant);\n        } /*---if---*/\n      } /*---for---*/\n}",
            "#define P double\n\n\n__host__ __device__\ninline int Dir_x( int octant ) { return octant & (1<<0) ? DIR_DN * 1: DIR_UP * 1; }\n\n__host__ __device__\ninline int Dir_y( int octant ) { return octant & (1<<1) ? DIR_DN * 1: DIR_UP * 1; }\n\n__host__ __device__\ninline int Dir_z( int octant ) { return octant & (1<<2) ? DIR_DN * 1 : DIR_UP * 1; }\n\n__device__\nint Quantities_scalefactor_space_acceldir(int ix_g, int iy_g, int iz_g)\n{\n  int result = 0;\n\n#ifndef RELAXED_TESTING\n  const int im = 134456;\n  const int ia = 8121;\n  const int ic = 28411;\n\n  result = ( (result+(ix_g+2))*ia + ic ) % im;\n  result = ( (result+(iy_g+2))*ia + ic ) % im;\n  result = ( (result+(iz_g+2))*ia + ic ) % im;\n  result = ( (result+(ix_g+3*iy_g+7*iz_g+2))*ia + ic ) % im;\n  result = ix_g+3*iy_g+7*iz_g+2;\n  result = result & ( (1<<2) - 1 );\n#endif\n  result = 1 << result;\n\n  return result;\n}\n\n__device__\nvoid Quantities_solve_acceldir(P* vs_local, Dimensions dims, P* facexy, P* facexz, P* faceyz,\n                             int ix, int iy, int iz,\n                             int ix_g, int iy_g, int iz_g,\n                             int ie, int ia,\n                             int octant, int octant_in_block, int noctant_per_block)\n{\n  const int dir_x = Dir_x( octant );\n  const int dir_y = Dir_y( octant );\n  const int dir_z = Dir_z( octant );\n\n  int iu = 0;\n\n  /*---Average the face values and accumulate---*/\n\n  /*---The state value and incoming face values are first adjusted to\n    normalized values by removing the spatial scaling.\n    They are then combined using a weighted average chosen in a special\n    way to give just the expected result.\n    Finally, spatial scaling is applied to the result which is then\n    stored.\n    ---*/\n\n  /*--- Quantities_scalefactor_octant_ inline ---*/\n  const P scalefactor_octant = 1 + octant;\n  const P scalefactor_octant_r = ((P)1) / scalefactor_octant;\n\n  /*---Quantities_scalefactor_space_ inline ---*/\n  const P scalefactor_space = (P)Quantities_scalefactor_space_acceldir(ix_g, iy_g, iz_g);\n  const P scalefactor_space_r = ((P)1) / scalefactor_space;\n  const P scalefactor_space_x_r = ((P)1) /\n    Quantities_scalefactor_space_acceldir( ix_g - dir_x, iy_g, iz_g );\n  const P scalefactor_space_y_r = ((P)1) /\n    Quantities_scalefactor_space_acceldir( ix_g, iy_g - dir_y, iz_g );\n  const P scalefactor_space_z_r = ((P)1) /\n    Quantities_scalefactor_space_acceldir( ix_g, iy_g, iz_g - dir_z );\n\n#ifdef USE_OPENMP_TARGET\n// no equivalent\n#elif defined(USE_ACC)\n#pragma acc loop seq\n#endif\n  for( iu=0; iu<NU; ++iu )\n    {\n\n      int vs_local_index = ia + dims.na * (\n                           iu + NU  * (\n                           ie + dims.ne * (\n                           ix + dims.ncell_x * (\n                           iy + dims.ncell_y * (\n                           octant + NOCTANT * (\n                           0))))));\n\n      const P result = ( vs_local[vs_local_index] * scalefactor_space_r + \n               (\n                /*--- ref_facexy inline ---*/\n                facexy[ia + dims.na      * (\n                        iu + NU           * (\n                        ie + dims.ne      * (\n                        ix + dims.ncell_x * (\n                        iy + dims.ncell_y * (\n                        octant + NOCTANT * (\n                        0 )))))) ]\n\n               /*--- Quantities_xfluxweight_ inline ---*/\n               * (P) ( 1 / (P) 2 )\n\n               * scalefactor_space_z_r\n\n               /*--- ref_facexz inline ---*/\n               + facexz[ia + dims.na      * (\n                        iu + NU           * (\n                        ie + dims.ne      * (\n                        ix + dims.ncell_x * (\n                        iz + dims.ncell_z * (\n                        octant + NOCTANT * (\n                        0 )))))) ]\n\n               /*--- Quantities_yfluxweight_ inline ---*/\n               * (P) ( 1 / (P) 4 )\n\n               * scalefactor_space_y_r\n\n               /*--- ref_faceyz inline ---*/\n               + faceyz[ia + dims.na      * (\n                        iu + NU           * (\n                        ie + dims.ne      * (\n                        iy + dims.ncell_y * (\n                        iz + dims.ncell_z * (\n                        octant + NOCTANT * (\n                        0 )))))) ]\n\n                        /*--- Quantities_zfluxweight_ inline ---*/\n                        * (P) ( 1 / (P) 4 - 1 / (P) (1 << ( ia & ( (1<<3) - 1 ) )) )\n\n               * scalefactor_space_x_r\n               ) \n               * scalefactor_octant_r ) * scalefactor_space;\n\n      vs_local[vs_local_index] = result;\n\n      const P result_scaled = result * scalefactor_octant;\n      /*--- ref_facexy inline ---*/\n      facexy[ia + dims.na      * (\n             iu + NU           * (\n             ie + dims.ne      * (\n             ix + dims.ncell_x * (\n             iy + dims.ncell_y * (\n             octant + NOCTANT * (\n             0 )))))) ] = result_scaled;\n\n      /*--- ref_facexz inline ---*/\n      facexz[ia + dims.na      * (\n             iu + NU           * (\n             ie + dims.ne      * (\n             ix + dims.ncell_x * (\n             iz + dims.ncell_z * (\n             octant + NOCTANT * (\n             0 )))))) ] = result_scaled;\n\n      /*--- ref_faceyz inline ---*/\n      faceyz[ia + dims.na      * (\n             iu + NU           * (\n             ie + dims.ne      * (\n             iy + dims.ncell_y * (\n             iz + dims.ncell_z * (\n             octant + NOCTANT * (\n             0 )))))) ] = result_scaled;\n\n    } /*---for---*/\n}\n\n__device__\nvoid Sweeper_sweep_cell_acceldir( const Dimensions &dims,\n                                  int wavefront,\n                                  int octant,\n                                  int ix, int iy,\n                                  int ix_g, int iy_g, int iz_g,\n                                  int dir_x, int dir_y, int dir_z,\n                                  P* __restrict__ facexy,\n                                  P* __restrict__ facexz,\n                                  P* __restrict__ faceyz,\n                                  const P* __restrict__ a_from_m,\n                                  const P* __restrict__ m_from_a,\n                                  const P* __restrict__ vi,\n                                  P* __restrict__ vo,\n                                  P* __restrict__ vs_local,\n                                  int octant_in_block,\n                                  int noctant_per_block,\n                                  int ie)\n{\n  /*---Declarations---*/\n//  int iz = 0;\n//  int ie = 0;\n  int im = 0;\n  int ia = 0;\n  int iu = 0;\n  /* int octant = 0; */\n\n  /*--- Dimensions ---*/\n  int dims_ncell_x = dims.ncell_x;\n  int dims_ncell_y = dims.ncell_y;\n  int dims_ncell_z = dims.ncell_z;\n  int dims_ne = dims.ne;\n  int dims_na = dims.na;\n  int dims_nm = dims.nm;\n\n  /*--- Solve for Z dimension, and check bounds.\n    The sum of the dimensions should equal the wavefront number.\n    If z < 0 or z > wavefront number, we are out of bounds.\n    Z also shouldn't exceed the spacial bound for the z dimension.\n\n    The calculation is adjusted for the direction of each axis\n    in a given octant.\n  ---*/\n\n  const int ixwav = dir_x==DIR_UP ? ix : (dims_ncell_x-1) - ix;\n  const int iywav = dir_y==DIR_UP ? iy : (dims_ncell_y-1) - iy;\n  const int izwav = wavefront - ixwav - iywav;\n  const int iz = dir_z==DIR_UP ? izwav : (dims_ncell_z-1) - izwav;\n\n//  int ixwav, iywav, izwav;\n//  if (dir_x==DIR_UP) { ixwav = ix; } else { ixwav = (dims_ncell_x-1) - ix; }\n//  if (dir_y==DIR_UP) { iywav = iy; } else { iywav = (dims_ncell_y-1) - iy; }\n  \n//  if (dir_z==DIR_UP) {\n//    iz = wavefront - (ixwav + iywav); } \n//  else { \n//    iz = (dims_ncell_z-1) - (wavefront - (ixwav + iywav));\n//  }\n\n  /*--- Bounds check ---*/\n  if ((iz >= 0 && iz < dims_ncell_z) )// &&\n    /* ((dir_z==DIR_UP && iz <= wavefront) || */\n    /*  (dir_z==DIR_DN && (dims_ncell_z-1-iz) <= wavefront))) */\n    {\n\n   /*---Loop over energy groups---*/\n//      for( ie=0; ie<dims_ne; ++ie )\n      {\n\n      /*--------------------*/\n      /*---Transform state vector from moments to angles---*/\n      /*--------------------*/\n\n      /*---This loads values from the input state vector,\n           does the small dense matrix-vector product,\n           and stores the result in a relatively small local\n           array that is hopefully small enough to fit into\n           processor cache.\n      ---*/\n\n      for( iu=0; iu<NU; ++iu )\n      for( ia=0; ia<dims_na; ++ia )\n      { \n        // reset reduction\n        P result = (P)0;\n        for( im=0; im < dims_nm; ++im )\n        {\n          /*--- const_ref_a_from_m inline ---*/\n          result += a_from_m[ ia     + dims_na * (\n                              im     +      NM * (\n                              octant + NOCTANT * (\n                              0 ))) ] * \n\n            /*--- const_ref_state inline ---*/\n            vi[im + dims.nm      * (\n                          iu + NU           * (\n                          ix + dims_ncell_x * (\n                          iy + dims_ncell_y * (\n                          ie + dims_ne      * (\n                          iz + dims_ncell_z * ( /*---NOTE: This axis MUST be slowest-varying---*/\n                          0 ))))))];\n        }\n\n        /*--- ref_vslocal inline ---*/\n        vs_local[ ia + dims.na * (\n                  iu + NU  * (\n                  ie + dims_ne * (\n                  ix + dims_ncell_x * (\n                  iy + dims_ncell_y * (\n                  octant + NOCTANT * (\n                                       0)))))) ] = result;\n      }\n      }\n\n      /*--------------------*/\n      /*---Perform solve---*/\n      /*--------------------*/\n\n//   /*---Loop over energy groups---*/\n      for( ia=0; ia<dims_na; ++ia )\n      {\n        Quantities_solve_acceldir(vs_local, dims, facexy, facexz, faceyz, \n                             ix, iy, iz,\n                             ix_g, iy_g, iz_g,\n                             ie, ia,\n                             octant, octant_in_block, noctant_per_block);\n      }\n\n      /*--------------------*/\n      /*---Transform state vector from angles to moments---*/\n      /*--------------------*/\n\n      /*---Perform small dense matrix-vector products and store\n           the result in the output state vector.\n      ---*/\n\n   /*---Loop over energy groups---*/\n      for( iu=0; iu<NU; ++iu )\n      for( im=0; im<dims_nm; ++im )\n      {\n        P result = (P)0;\n        for( ia=0; ia<dims_na; ++ia )\n        {\n         /*--- const_ref_m_from_a ---*/\n         result += m_from_a[ im     +      NM * (\n                             ia     + dims_na * (\n                             octant + NOCTANT * (\n                             0 ))) ] *\n\n         /*--- const_ref_vslocal ---*/\n         vs_local[ ia + dims_na * (\n                   iu + NU    * (\n                   ie + dims_ne * (\n                   ix + dims_ncell_x * (\n                   iy + dims_ncell_y * (\n                   octant + NOCTANT * (\n                   0 )))))) ];\n        }\n\n        /*--- ref_state inline ---*/\n        atomicAdd(\n        &vo[im + dims.nm     * (\n           iu + NU           * (\n           ix + dims_ncell_x * (\n           iy + dims_ncell_y * (\n           ie + dims_ne      * (\n           iz + dims_ncell_z * ( /*---NOTE: This axis MUST be slowest-varying---*/\n           0 ))))))] , result);\n      }\n\n//      } /*---ie---*/\n\n    } /*--- iz ---*/\n}\n\n__global__ void wavefronts(\n    const int num_wavefronts,  \n    const int ix_base, \n    const int iy_base,\n    const int v_b_size,\n    const int noctant_per_block,\n    const Dimensions dims_b,\n    StepInfoAll stepinfoall,\n    P*__restrict__ facexy, \n    P*__restrict__ facexz, \n    P*__restrict__ faceyz, \n    P*__restrict__ a_from_m,\n    P*__restrict__ m_from_a,\n    P*__restrict__ vi,\n    P*__restrict__ vo,\n    P*__restrict__ vs_local)\n{\n  int octant = blockDim.x * blockIdx.x + threadIdx.x; \n  int ie = blockDim.y * blockIdx.y + threadIdx.y; \n  if (ie >= dims_b.ne || octant >= NOCTANT) return;\n\n  const int dims_b_ncell_x = dims_b.ncell_x;\n  const int dims_b_ncell_y = dims_b.ncell_y;\n  const int dims_b_ncell_z = dims_b.ncell_z;\n\n  /*--- Loop over wavefronts ---*/\n  for (int wavefront = 0; wavefront < num_wavefronts; wavefront++)\n  {\n    for( int iywav=0; iywav<dims_b_ncell_y; ++iywav )\n      for( int ixwav=0; ixwav<dims_b_ncell_x; ++ixwav )\n      {\n\n        if (stepinfoall.stepinfo[octant].is_active) {\n\n          /*---Decode octant directions from octant number---*/\n\n          const int dir_x = Dir_x( octant );\n          const int dir_y = Dir_y( octant );\n          const int dir_z = Dir_z( octant );\n\n          const int octant_in_block = octant;\n\n          const int ix = dir_x==DIR_UP ? ixwav : dims_b_ncell_x - 1 - ixwav;\n          const int iy = dir_y==DIR_UP ? iywav : dims_b_ncell_y - 1 - iywav;\n          const int izwav = wavefront - ixwav - iywav;\n          const int iz = dir_z==DIR_UP ? izwav : (dims_b_ncell_z-1) - izwav;\n\n          const int ix_g = ix + ix_base; // dims_b_ncell_x * proc_x;\n          const int iy_g = iy + iy_base; // dims_b_ncell_y * proc_y;\n          const int iz_g = iz + stepinfoall.stepinfo[octant].block_z * dims_b_ncell_z;\n\n          const int v_offset = stepinfoall.stepinfo[octant].block_z * v_b_size;\n\n          /*--- In-gridcell computations ---*/\n          Sweeper_sweep_cell_acceldir( dims_b, wavefront, octant, ix, iy,\n              ix_g, iy_g, iz_g,\n              dir_x, dir_y, dir_z,\n              facexy, facexz, faceyz,\n              a_from_m, m_from_a,\n              &(vi[v_offset]), &(vo[v_offset]), vs_local,\n              octant_in_block, noctant_per_block, ie );\n\n        } /*---if---*/\n\n      } /*---octant/ix/iy---*/\n\n  } /*--- wavefront ---*/\n}"
        ]
    },
    "recursiveGaussian-cuda": {
        "/Users/gbolet/hecbench-roofline/src/recursiveGaussian-cuda/main.cu": [
            "__global__ \nvoid Transpose(const unsigned int* uiDataIn, \n                     unsigned int* uiDataOut, \n               const int iWidth, const int iHeight)\n{\n    // read the matrix tile into LMEM\n    unsigned int xIndex = blockIdx.x * blockDim.x + threadIdx.x;\n    unsigned int yIndex = blockIdx.y * blockDim.y + threadIdx.y;\n    __shared__ unsigned int uiLocalBuff[16*17];\n\n    if((xIndex < iWidth) && (yIndex < iHeight))\n    {\n        //uiLocalBuff[get_local_id(1) * (get_local_size(0) + 1) + get_local_id(0)] = uiDataIn[(yIndex * iWidth) + xIndex];\n        uiLocalBuff[threadIdx.y * (blockDim.x + 1) + threadIdx.x] = uiDataIn[(yIndex * iWidth) + xIndex];\n    }\n\n    // Synchronize the read into LMEM\n    __syncthreads();\n\n    // write the transposed matrix tile to global memory\n    xIndex = __mul24(blockIdx.y, blockDim.y) + threadIdx.x;\n    yIndex = __mul24(blockIdx.x, blockDim.x) + threadIdx.y;\n    if((xIndex < iHeight) && (yIndex < iWidth))\n    {\n        uiDataOut[(yIndex * iHeight) + xIndex] = \n         //uiLocalBuff[get_local_id(0) * (get_local_size(1) + 1) + get_local_id(1)];\n         uiLocalBuff[threadIdx.x * (blockDim.y + 1) + threadIdx.y];\n    }\n}",
            "inline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\n__device__\nunsigned int rgbaFloat4ToUint(const float4 rgba)\n{\n    unsigned int uiPackedRGBA = 0U;\n    uiPackedRGBA |= 0x000000FF & (unsigned int)rgba.x;\n    uiPackedRGBA |= 0x0000FF00 & (((unsigned int)rgba.y) << 8);\n    uiPackedRGBA |= 0x00FF0000 & (((unsigned int)rgba.z) << 16);\n    uiPackedRGBA |= 0xFF000000 & (((unsigned int)rgba.w) << 24);\n    return uiPackedRGBA;\n}\n\n__device__\nfloat4 rgbaUintToFloat4(const unsigned int uiPackedRGBA)\n{\n    float4 rgba;\n    rgba.x = uiPackedRGBA & 0xff;\n    rgba.y = (uiPackedRGBA >> 8) & 0xff;\n    rgba.z = (uiPackedRGBA >> 16) & 0xff;\n    rgba.w = (uiPackedRGBA >> 24) & 0xff;\n    return rgba;\n}\n\n__global__ void SimpleRecursiveRGBA(\n  const unsigned int* uiDataIn,\n        unsigned int* uiDataOut,\n  const int iWidth, const int iHeight, const float a)\n{\n    // compute X pixel location and check in-bounds\n  unsigned int X = blockIdx.x * blockDim.x + threadIdx.x;\n  if (X >= iWidth) return;\n    \n  // advance global pointers to correct column for this work item and x position\n  uiDataIn += X;    \n  uiDataOut += X;\n\n  // start forward filter pass\n  float4 yp = rgbaUintToFloat4(*uiDataIn);  // previous output\n  for (int Y = 0; Y < iHeight; Y++) \n  {\n    float4 xc = rgbaUintToFloat4(*uiDataIn);\n    float4 yc = xc + (yp - xc) * make_float4(a, a, a, a);   \n    *uiDataOut = rgbaFloat4ToUint(yc);\n    yp = yc;\n    uiDataIn += iWidth;     // move to next row\n    uiDataOut += iWidth;    // move to next row\n  }\n\n  // reset global pointers to point to last element in column for this work item and x position\n  uiDataIn -= iWidth;\n  uiDataOut -= iWidth;\n\n  // start reverse filter pass: ensures response is symmetrical\n  yp = rgbaUintToFloat4(*uiDataIn);\n  for (int Y = iHeight - 1; Y > -1; Y--) \n  {\n    float4 xc = rgbaUintToFloat4(*uiDataIn);\n    float4 yc = xc + (yp - xc) * make_float4(a, a, a, a);\n    *uiDataOut = rgbaFloat4ToUint((rgbaUintToFloat4(*uiDataOut) + yc) * 0.5f);\n    yp = yc;\n    uiDataIn -= iWidth;   // move to previous row\n    uiDataOut -= iWidth;  // move to previous row\n  }\n}",
            "inline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\n__device__\nunsigned int rgbaFloat4ToUint(const float4 rgba)\n{\n    unsigned int uiPackedRGBA = 0U;\n    uiPackedRGBA |= 0x000000FF & (unsigned int)rgba.x;\n    uiPackedRGBA |= 0x0000FF00 & (((unsigned int)rgba.y) << 8);\n    uiPackedRGBA |= 0x00FF0000 & (((unsigned int)rgba.z) << 16);\n    uiPackedRGBA |= 0xFF000000 & (((unsigned int)rgba.w) << 24);\n    return uiPackedRGBA;\n}\n\n__device__\nfloat4 rgbaUintToFloat4(const unsigned int uiPackedRGBA)\n{\n    float4 rgba;\n    rgba.x = uiPackedRGBA & 0xff;\n    rgba.y = (uiPackedRGBA >> 8) & 0xff;\n    rgba.z = (uiPackedRGBA >> 16) & 0xff;\n    rgba.w = (uiPackedRGBA >> 24) & 0xff;\n    return rgba;\n}\n\n__global__ void RecursiveRGBA(\n  const unsigned int* uiDataIn, \n  unsigned int* uiDataOut, \n  const int iWidth, const int iHeight, \n  const float a0, const float a1, \n  const float a2, const float a3, \n  const float b1, const float b2, \n  const float coefp, const float coefn)\n{\n    // compute X pixel location and check in-bounds\n    //unsigned int X = mul24(get_group_id(0), get_local_size(0)) + get_local_id(0);\n    unsigned int X = blockIdx.x * blockDim.x + threadIdx.x;\n    if (X >= iWidth) return;\n\n    // advance global pointers to correct column for this work item and x position\n    uiDataIn += X;    \n    uiDataOut += X;\n\n    // start forward filter pass\n    float4 xp = make_float4(0.0f, 0.0f, 0.0f, 0.0f);  // previous input\n    float4 yp = make_float4(0.0f, 0.0f, 0.0f, 0.0f);  // previous output\n    float4 yb = make_float4(0.0f, 0.0f, 0.0f, 0.0f);  // previous output by 2\n\n#ifdef CLAMP_TO_EDGE\n    xp = rgbaUintToFloat4(*uiDataIn); \n    yb = xp * make_float4(coefp,coefp,coefp,coefp); \n    yp = yb;\n#endif\n\n    for (int Y = 0; Y < iHeight; Y++) \n    {\n        float4 xc = rgbaUintToFloat4(*uiDataIn);\n        float4 yc = (xc * a0) + (xp * a1) - (yp * b1) - (yb * b2);\n\t\t*uiDataOut = rgbaFloat4ToUint(yc);\n        xp = xc; \n        yb = yp; \n        yp = yc; \n        uiDataIn += iWidth;     // move to next row\n        uiDataOut += iWidth;    // move to next row\n    }\n\n    // reset global pointers to point to last element in column for this work item and x position\n    uiDataIn -= iWidth;\n    uiDataOut -= iWidth;\n\n    // start reverse filter pass: ensures response is symmetrical\n    float4 xn = make_float4(0.0f, 0.0f, 0.0f, 0.0f);\n    float4 xa = make_float4(0.0f, 0.0f, 0.0f, 0.0f);\n    float4 yn = make_float4(0.0f, 0.0f, 0.0f, 0.0f);\n    float4 ya = make_float4(0.0f, 0.0f, 0.0f, 0.0f);\n\n#ifdef CLAMP_TO_EDGE\n    xn = rgbaUintToFloat4(*uiDataIn);\n    xa = xn; \n    yn = xn * make_float4(coefn,coefn,coefn,coefn); \n    ya = yn;\n#endif\n\n    for (int Y = iHeight - 1; Y > -1; Y--) \n    {\n        float4 xc = rgbaUintToFloat4(*uiDataIn);\n        float4 yc = (xn * a2) + (xa * a3) - (yn * b1) - (ya * b2);\n        xa = xn; \n        xn = xc; \n        ya = yn; \n        yn = yc;\n        *uiDataOut = rgbaFloat4ToUint(rgbaUintToFloat4(*uiDataOut) + yc);\n        uiDataIn -= iWidth;   // move to previous row\n        uiDataOut -= iWidth;  // move to previous row\n    }\n}"
        ]
    },
    "dct8x8-cuda": {
        "/Users/gbolet/hecbench-roofline/src/dct8x8-cuda/kernels.cu": [
            "__device__\ninline void DCT8(float *D){\n    float X07P = D[0] + D[7];\n    float X16P = D[1] + D[6];\n    float X25P = D[2] + D[5];\n    float X34P = D[3] + D[4];\n\n    float X07M = D[0] - D[7];\n    float X61M = D[6] - D[1];\n    float X25M = D[2] - D[5];\n    float X43M = D[4] - D[3];\n\n    float X07P34PP = X07P + X34P;\n    float X07P34PM = X07P - X34P;\n    float X16P25PP = X16P + X25P;\n    float X16P25PM = X16P - X25P;\n\n    D[0] = C_norm * (X07P34PP + X16P25PP);\n    D[2] = C_norm * (C_b * X07P34PM + C_e * X16P25PM);\n    D[4] = C_norm * (X07P34PP - X16P25PP);\n    D[6] = C_norm * (C_e * X07P34PM - C_b * X16P25PM);\n\n    D[1] = C_norm * (C_a * X07M - C_c * X61M + C_d * X25M - C_f * X43M);\n    D[3] = C_norm * (C_c * X07M + C_f * X61M - C_a * X25M + C_d * X43M);\n    D[5] = C_norm * (C_d * X07M + C_a * X61M + C_f * X25M - C_c * X43M);\n    D[7] = C_norm * (C_f * X07M + C_d * X61M + C_c * X25M + C_a * X43M);\n}\n\n__global__ void DCT8x8_kernel(\n    float*__restrict__ d_Dst,\n    const float*__restrict__ d_Src,\n    const unsigned int stride,\n    const unsigned int imageH,\n    const unsigned int imageW\n){\n    const unsigned int    localX = threadIdx.x;\n    const unsigned int    localY = BLOCK_SIZE * threadIdx.y;\n    const unsigned int modLocalX = localX & (BLOCK_SIZE - 1);\n    const unsigned int   globalX = blockIdx.x * BLOCK_X + localX;\n    const unsigned int   globalY = blockIdx.y * BLOCK_Y + localY;\n\n    __shared__ float l_Transpose[BLOCK_Y * (BLOCK_X+1)];\n\n    //Process only full blocks\n    if( (globalX - modLocalX + BLOCK_SIZE - 1 >= imageW) || (globalY + BLOCK_SIZE - 1 >= imageH) )\n        return;\n\n    float *l_V = &l_Transpose[localY * (BLOCK_X+1) + localX];\n    float *l_H = &l_Transpose[(localY + modLocalX) * (BLOCK_X+1) + localX - modLocalX];\n    d_Src += globalY * stride + globalX;\n    d_Dst += globalY * stride + globalX;\n\n    float D[8];\n    for(unsigned int i = 0; i < BLOCK_SIZE; i++)\n        l_V[i * (BLOCK_X + 1)] = d_Src[i * stride];\n\n    for(unsigned int i = 0; i < BLOCK_SIZE; i++)\n        D[i] = l_H[i];\n    DCT8(D);\n    for(unsigned int i = 0; i < BLOCK_SIZE; i++)\n        l_H[i] = D[i];\n\n    for(unsigned int i = 0; i < BLOCK_SIZE; i++)\n        D[i] = l_V[i * (BLOCK_X + 1)];\n    DCT8(D);\n\n    for(unsigned int i = 0; i < BLOCK_SIZE; i++)\n        d_Dst[i * stride] = D[i];\n}",
            "__device__\ninline void IDCT8(float *D){\n    float Y04P   = D[0] + D[4];\n    float Y2b6eP = C_b * D[2] + C_e * D[6];\n\n    float Y04P2b6ePP = Y04P + Y2b6eP;\n    float Y04P2b6ePM = Y04P - Y2b6eP;\n    float Y7f1aP3c5dPP = C_f * D[7] + C_a * D[1] + C_c * D[3] + C_d * D[5];\n    float Y7a1fM3d5cMP = C_a * D[7] - C_f * D[1] + C_d * D[3] - C_c * D[5];\n\n    float Y04M   = D[0] - D[4];\n    float Y2e6bM = C_e * D[2] - C_b * D[6];\n\n    float Y04M2e6bMP = Y04M + Y2e6bM;\n    float Y04M2e6bMM = Y04M - Y2e6bM;\n    float Y1c7dM3f5aPM = C_c * D[1] - C_d * D[7] - C_f * D[3] - C_a * D[5];\n    float Y1d7cP3a5fMM = C_d * D[1] + C_c * D[7] - C_a * D[3] + C_f * D[5];\n\n    D[0] = C_norm * (Y04P2b6ePP + Y7f1aP3c5dPP);\n    D[7] = C_norm * (Y04P2b6ePP - Y7f1aP3c5dPP);\n    D[4] = C_norm * (Y04P2b6ePM + Y7a1fM3d5cMP);\n    D[3] = C_norm * (Y04P2b6ePM - Y7a1fM3d5cMP);\n\n    D[1] = C_norm * (Y04M2e6bMP + Y1c7dM3f5aPM);\n    D[5] = C_norm * (Y04M2e6bMM - Y1d7cP3a5fMM);\n    D[2] = C_norm * (Y04M2e6bMM + Y1d7cP3a5fMM);\n    D[6] = C_norm * (Y04M2e6bMP - Y1c7dM3f5aPM);\n}\n\n__global__ void IDCT8x8_kernel(\n    float*__restrict__ d_Dst,\n    const float*__restrict__ d_Src,\n    const unsigned int stride,\n    const unsigned int imageH,\n    const unsigned int imageW\n){\n    const unsigned int    localX = threadIdx.x;\n    const unsigned int    localY = BLOCK_SIZE * threadIdx.y;\n    const unsigned int modLocalX = localX & (BLOCK_SIZE - 1);\n    const unsigned int   globalX = blockIdx.x * BLOCK_X + localX;\n    const unsigned int   globalY = blockIdx.y * BLOCK_Y + localY;\n\n    __shared__ float l_Transpose[BLOCK_Y * (BLOCK_X+1)];\n\n    //Process only full blocks\n    if( (globalX - modLocalX + BLOCK_SIZE - 1 >= imageW) || (globalY + BLOCK_SIZE - 1 >= imageH) )\n        return;\n\n    float *l_V = &l_Transpose[localY * (BLOCK_X+1) + localX];\n    float *l_H = &l_Transpose[(localY + modLocalX) * (BLOCK_X+1) + localX - modLocalX];\n    d_Src += globalY * stride + globalX;\n    d_Dst += globalY * stride + globalX;\n\n    float D[8];\n    for(unsigned int i = 0; i < BLOCK_SIZE; i++)\n        l_V[i * (BLOCK_X + 1)] = d_Src[i * stride];\n\n    for(unsigned int i = 0; i < BLOCK_SIZE; i++)\n        D[i] = l_H[i];\n    IDCT8(D);\n    for(unsigned int i = 0; i < BLOCK_SIZE; i++)\n        l_H[i] = D[i];\n\n    for(unsigned int i = 0; i < BLOCK_SIZE; i++)\n        D[i] = l_V[i * (BLOCK_X + 1)];\n    IDCT8(D);\n    for(unsigned int i = 0; i < BLOCK_SIZE; i++)\n        d_Dst[i * stride] = D[i];\n}"
        ]
    },
    "fluidSim-cuda": {
        "/Users/gbolet/hecbench-roofline/src/fluidSim-cuda/kernels.cu": [
            "__device__\ndouble ced(double rho, double weight, const double2 dir, const double2 u)\n{\n  double u2 = (u.x * u.x) + (u.y * u.y);\n  double eu = (dir.x * u.x) + (dir.y * u.y);\n  return rho * weight * (1.0 + 3.0 * eu + 4.5 * eu * eu - 1.5 * u2);\n}\n\nstatic __forceinline__ __device__\nValueType dot(BasicVector<ValueType> a, BasicVector<ValueType> b)\n{\n    return a.dot(b);\n}\n\ninline __device__ int8 fma8 (const uint &a, const int8 &b, const int8 &c)\n{\n  int8 r;\n  r.s0 = a * b.s0 + c.s0;\n  r.s1 = a * b.s1 + c.s1;\n  r.s2 = a * b.s2 + c.s2;\n  r.s3 = a * b.s3 + c.s3;\n  r.s4 = a * b.s4 + c.s4;\n  r.s5 = a * b.s5 + c.s5;\n  r.s6 = a * b.s6 + c.s6;\n  r.s7 = a * b.s7 + c.s7;\n  return r;\n}\n\ninline __device__ int8 newPos (const int p, const double8 &dir)\n{\n  int8 np;\n  np.s0 = p + (int)dir.s0;\n  np.s1 = p + (int)dir.s1;\n  np.s2 = p + (int)dir.s2;\n  np.s3 = p + (int)dir.s3;\n  np.s4 = p + (int)dir.s4;\n  np.s5 = p + (int)dir.s5;\n  np.s6 = p + (int)dir.s6;\n  np.s7 = p + (int)dir.s7;\n  return np;\n}\n\n__global__ void lbm (\n    const double *__restrict__ if0,\n          double *__restrict__ of0, \n    const double4 *__restrict__ if1234,\n          double4 *__restrict__ of1234,\n    const double4 *__restrict__ if5678,\n          double4 *__restrict__ of5678,\n    const bool *__restrict__ type,\n    const double8 dirX,\n    const double8 dirY,\n    const double *__restrict__ weight,\n    double omega)\n{\n  uint idx = blockDim.x * blockIdx.x + threadIdx.x;\n  uint idy = blockDim.y * blockIdx.y + threadIdx.y;\n  uint width = gridDim.x * blockDim.x;\n  uint height = gridDim.y * blockDim.y;\n  uint pos = idx + width * idy;\n\n  // Read input distributions\n  double f0 = if0[pos];\n  double4 f1234 = if1234[pos];\n  double4 f5678 = if5678[pos];\n\n  // intermediate results\n  double e0;\n  double4 e1234;\n  double4 e5678;\n\n  double rho; // Density\n  double2 u;  // Velocity\n\n\n  // Collide\n  if(type[pos]) // Boundary\n  {\n    e0 = f0;\n    // Swap directions \n    // f1234.xyzw = f1234.zwxy;\n    e1234.x = f1234.z;\n    e1234.y = f1234.w;\n    e1234.z = f1234.x;\n    e1234.w = f1234.y;\n\n    // f5678.xyzw = f5678.zwxy;\n    e5678.x = f5678.z;\n    e5678.y = f5678.w;\n    e5678.z = f5678.x;\n    e5678.w = f5678.y;\n\n    rho = 0;\n    u = make_double2(0, 0);\n  }\n  else // Fluid\n  {\n    // Compute rho and u\n    // Rho is computed by doing a reduction on f\n    double4 temp = f1234 + f5678;\n    rho = f0 + temp.x + temp.y + temp.z + temp.w;\n\n    // Compute velocity in x and y directions\n    double4 x1234 = make_double4(dirX.s0, dirX.s1, dirX.s2, dirX.s3);\n    double4 x5678 = make_double4(dirX.s4, dirX.s5, dirX.s6, dirX.s7);\n    double4 y1234 = make_double4(dirY.s0, dirY.s1, dirY.s2, dirY.s3);\n    double4 y5678 = make_double4(dirY.s4, dirY.s5, dirY.s6, dirY.s7);\n    u.x = (dot(f1234, x1234) + dot(f5678, x5678)) / rho;\n    u.y = (dot(f1234, y1234) + dot(f5678, y5678)) / rho;\n\n    // Compute f\n    e0 = ced(rho, weight[0], make_double2(0, 0), u);\n    e1234.x = ced(rho, weight[1], make_double2(dirX.s0, dirY.s0), u);\n    e1234.y = ced(rho, weight[2], make_double2(dirX.s1, dirY.s1), u);\n    e1234.z = ced(rho, weight[3], make_double2(dirX.s2, dirY.s2), u);\n    e1234.w = ced(rho, weight[4], make_double2(dirX.s3, dirY.s3), u);\n    e5678.x = ced(rho, weight[5], make_double2(dirX.s4, dirY.s4), u);\n    e5678.y = ced(rho, weight[6], make_double2(dirX.s5, dirY.s5), u);\n    e5678.z = ced(rho, weight[7], make_double2(dirX.s6, dirY.s6), u);\n    e5678.w = ced(rho, weight[8], make_double2(dirX.s7, dirY.s7), u);\n\n    e0 = (1.0 - omega) * f0 + omega * e0;\n    e1234 = (1.0 - omega) * f1234 + omega * e1234;\n    e5678 = (1.0 - omega) * f5678 + omega * e5678;\n  }\n\n  // Propagate\n  bool t3 = idx > 0;          // Not on Left boundary\n  bool t1 = idx < width - 1;  // Not on Right boundary\n  bool t4 = idy > 0;          // Not on Upper boundary\n  bool t2 = idy < height - 1; // Not on lower boundary\n\n  if (t1 && t2 && t3 && t4) {\n    // New positions to write (Each thread will write 8 values)\n    // Note the propagation sources imply the OLD locations for each thread\n    int8 nX = newPos(idx, dirX);\n    int8 nY = newPos(idy, dirY);\n    int8 nPos = fma8(width, nY, nX);\n\n    // Write center distribution to thread's location\n    of0[pos] = e0;\n\n    // Propagate to right cell\n    of1234[nPos.s0].x = e1234.x;\n\n    // Propagate to Lower cell\n    of1234[nPos.s1].y = e1234.y;\n\n    // Propagate to left cell\n    of1234[nPos.s2].z = e1234.z;\n\n    // Propagate to Upper cell\n    of1234[nPos.s3].w = e1234.w;\n\n    // Propagate to Lower-Right cell\n    of5678[nPos.s4].x = e5678.x;\n\n    // Propogate to Lower-Left cell\n    of5678[nPos.s5].y = e5678.y;\n\n    // Propagate to Upper-Left cell\n    of5678[nPos.s6].z = e5678.z;\n\n    // Propagate to Upper-Right cell\n    of5678[nPos.s7].w = e5678.w;\n  }\n}"
        ]
    },
    "slu-cuda": {
        "/Users/gbolet/hecbench-roofline/src/slu-cuda/src/numeric.cu": [
            "#define REAL DOUBLE\n\n\n__global__ void RL(\n    const unsigned* __restrict__ sym_c_ptr_dev,\n    const unsigned* __restrict__ sym_r_idx_dev,\n    REAL* __restrict__ val_dev,\n    const unsigned* __restrict__ l_col_ptr_dev,\n    const unsigned* __restrict__ csr_r_ptr_dev,\n    const unsigned* __restrict__ csr_c_idx_dev,\n    const unsigned* __restrict__ csr_diag_ptr_dev,\n    const int* __restrict__ level_idx_dev,\n    REAL* __restrict__ tmpMem,\n    const unsigned n,\n    const int levelHead,\n    const int inLevPos)\n{\n  const int tid = threadIdx.x;\n  const int bid = blockIdx.x;\n  const int wid = threadIdx.x / 32;\n\n  const unsigned currentCol = level_idx_dev[levelHead+inLevPos+bid];\n  const unsigned currentLColSize = sym_c_ptr_dev[currentCol + 1] - l_col_ptr_dev[currentCol] - 1;\n  const unsigned currentLPos = l_col_ptr_dev[currentCol] + tid + 1;\n\n  extern __shared__ REAL s[];\n\n  //update current col\n\n  int offset = 0;\n  while (currentLColSize > offset)\n  {\n    if (tid + offset < currentLColSize)\n    {\n      unsigned ridx = sym_r_idx_dev[currentLPos + offset];\n\n      val_dev[currentLPos + offset] /= val_dev[l_col_ptr_dev[currentCol]];\n      tmpMem[bid*n + ridx]= val_dev[currentLPos + offset];\n    }\n    offset += blockDim.x;\n  }\n  __syncthreads();\n\n  //broadcast to submatrix\n  const unsigned subColPos = csr_diag_ptr_dev[currentCol] + wid + 1;\n  const unsigned subMatSize = csr_r_ptr_dev[currentCol + 1] - csr_diag_ptr_dev[currentCol] - 1;\n  unsigned subCol;\n  const int tidInWarp = threadIdx.x % 32;\n  unsigned subColElem = 0;\n\n  int woffset = 0;\n  while (subMatSize > woffset)\n  {\n    if (wid + woffset < subMatSize)\n    {\n      offset = 0;\n      subCol = csr_c_idx_dev[subColPos + woffset];\n      while(offset < sym_c_ptr_dev[subCol + 1] - sym_c_ptr_dev[subCol])\n      {\n        if (tidInWarp + offset < sym_c_ptr_dev[subCol + 1] - sym_c_ptr_dev[subCol])\n        {\n\n          subColElem = sym_c_ptr_dev[subCol] + tidInWarp + offset;\n          unsigned ridx = sym_r_idx_dev[subColElem];\n\n          if (ridx == currentCol)\n          {\n            s[wid] = val_dev[subColElem];\n          }\n          //Threads in a warp are always synchronized\n          //__syncthreads();\n          if (ridx > currentCol)\n          {\n            //elem in currentCol same row with subColElem might be 0, so\n            //clearing tmpMem is necessary\n            atomicAdd(&val_dev[subColElem], -tmpMem[ridx+n*bid]*s[wid]);\n          }\n        }\n        offset += 32;\n      }\n    }\n    woffset += blockDim.x/32;\n  }\n\n  __syncthreads();\n  //Clear tmpMem\n  offset = 0;\n  while (currentLColSize > offset)\n  {\n    if (tid + offset < currentLColSize)\n    {\n      unsigned ridx = sym_r_idx_dev[currentLPos + offset];\n      tmpMem[bid*n + ridx]= 0;\n    }\n    offset += blockDim.x;\n  }\n}",
            "#define REAL DOUBLE\n\n\ninline __host__ __device__ int4 make_int4(float4 a)\n{\n    return make_int4(int(a.x), int(a.y), int(a.z), int(a.w));\n}\n\ninline __host__ __device__ int4 abs(int4 v)\n{\n    return make_int4(abs(v.x), abs(v.y), abs(v.z), abs(v.w));\n}\n\n__global__ void RL_perturb(\n    const unsigned* __restrict__ sym_c_ptr_dev,\n    const unsigned* __restrict__ sym_r_idx_dev,\n    REAL* __restrict__ val_dev,\n    const unsigned* __restrict__ l_col_ptr_dev,\n    const unsigned* __restrict__ csr_r_ptr_dev,\n    const unsigned* __restrict__ csr_c_idx_dev,\n    const unsigned* __restrict__ csr_diag_ptr_dev,\n    const int* __restrict__ level_idx_dev,\n    REAL* __restrict__ tmpMem,\n    const unsigned n,\n    const int levelHead,\n    const int inLevPos,\n    const float pert)\n{\n  const int tid = threadIdx.x;\n  const int bid = blockIdx.x;\n  const int wid = threadIdx.x / 32;\n\n  const unsigned currentCol = level_idx_dev[levelHead+inLevPos+bid];\n  const unsigned currentLColSize = sym_c_ptr_dev[currentCol + 1] - l_col_ptr_dev[currentCol] - 1;\n  const unsigned currentLPos = l_col_ptr_dev[currentCol] + tid + 1;\n\n  extern __shared__ REAL s[];\n\n  //update current col\n\n  int offset = 0;\n  while (currentLColSize > offset)\n  {\n    if (tid + offset < currentLColSize)\n    {\n      unsigned ridx = sym_r_idx_dev[currentLPos + offset];\n\n      if (abs(val_dev[l_col_ptr_dev[currentCol]]) < pert)\n        val_dev[l_col_ptr_dev[currentCol]] = pert;\n\n      val_dev[currentLPos + offset] /= val_dev[l_col_ptr_dev[currentCol]];\n      tmpMem[bid*n + ridx]= val_dev[currentLPos + offset];\n    }\n    offset += blockDim.x;\n  }\n  __syncthreads();\n\n  //broadcast to submatrix\n  const unsigned subColPos = csr_diag_ptr_dev[currentCol] + wid + 1;\n  const unsigned subMatSize = csr_r_ptr_dev[currentCol + 1] - csr_diag_ptr_dev[currentCol] - 1;\n  unsigned subCol;\n  const int tidInWarp = threadIdx.x % 32;\n  unsigned subColElem = 0;\n\n  int woffset = 0;\n  while (subMatSize > woffset)\n  {\n    if (wid + woffset < subMatSize)\n    {\n      offset = 0;\n      subCol = csr_c_idx_dev[subColPos + woffset];\n      while(offset < sym_c_ptr_dev[subCol + 1] - sym_c_ptr_dev[subCol])\n      {\n        if (tidInWarp + offset < sym_c_ptr_dev[subCol + 1] - sym_c_ptr_dev[subCol])\n        {\n\n          subColElem = sym_c_ptr_dev[subCol] + tidInWarp + offset;\n          unsigned ridx = sym_r_idx_dev[subColElem];\n\n          if (ridx == currentCol)\n          {\n            s[wid] = val_dev[subColElem];\n          }\n          //Threads in a warp are always synchronized\n          //__syncthreads();\n          if (ridx > currentCol)\n          {\n            //elem in currentCol same row with subColElem might be 0, so\n            //clearing tmpMem is necessary\n            atomicAdd(&val_dev[subColElem], -tmpMem[ridx+n*bid]*s[wid]);\n          }\n        }\n        offset += 32;\n      }\n    }\n    woffset += blockDim.x/32;\n  }\n\n  __syncthreads();\n  //Clear tmpMem\n  offset = 0;\n  while (currentLColSize > offset)\n  {\n    if (tid + offset < currentLColSize)\n    {\n      unsigned ridx = sym_r_idx_dev[currentLPos + offset];\n      tmpMem[bid*n + ridx]= 0;\n    }\n    offset += blockDim.x;\n  }\n}",
            "#define REAL DOUBLE\n\n\n__global__ void RL_onecol_factorizeCurrentCol(\n    const unsigned* __restrict__ sym_c_ptr_dev,\n    const unsigned* __restrict__ sym_r_idx_dev,\n    REAL* __restrict__ val_dev,\n    const unsigned* __restrict__ l_col_ptr_dev,\n    const unsigned currentCol,\n    REAL* __restrict__ tmpMem,\n    const int stream,\n    const unsigned n)\n{\n  const int tid = threadIdx.x;\n\n  const unsigned currentLColSize = sym_c_ptr_dev[currentCol + 1] - l_col_ptr_dev[currentCol] - 1;\n  const unsigned currentLPos = l_col_ptr_dev[currentCol] + tid + 1;\n\n  //update current col\n\n  int offset = 0;\n  while (currentLColSize > offset)\n  {\n    if (tid + offset < currentLColSize)\n    {\n      unsigned ridx = sym_r_idx_dev[currentLPos + offset];\n\n      val_dev[currentLPos + offset] /= val_dev[l_col_ptr_dev[currentCol]];\n      tmpMem[stream * n + ridx]= val_dev[currentLPos + offset];\n    }\n    offset += blockDim.x;\n  }\n}",
            "#define REAL DOUBLE\n\n\ninline __host__ __device__ int4 make_int4(float4 a)\n{\n    return make_int4(int(a.x), int(a.y), int(a.z), int(a.w));\n}\n\ninline __host__ __device__ int4 abs(int4 v)\n{\n    return make_int4(abs(v.x), abs(v.y), abs(v.z), abs(v.w));\n}\n\n__global__ void RL_onecol_factorizeCurrentCol_perturb(\n    const unsigned* __restrict__ sym_c_ptr_dev,\n    const unsigned* __restrict__ sym_r_idx_dev,\n    REAL* __restrict__ val_dev,\n    const unsigned* __restrict__ l_col_ptr_dev,\n    const unsigned currentCol,\n    REAL* __restrict__ tmpMem,\n    const int stream,\n    const unsigned n,\n    const float pert)\n{\n  const int tid = threadIdx.x;\n\n  const unsigned currentLColSize = sym_c_ptr_dev[currentCol + 1] - l_col_ptr_dev[currentCol] - 1;\n  const unsigned currentLPos = l_col_ptr_dev[currentCol] + tid + 1;\n\n  //update current col\n\n  int offset = 0;\n  while (currentLColSize > offset)\n  {\n    if (tid + offset < currentLColSize)\n    {\n      unsigned ridx = sym_r_idx_dev[currentLPos + offset];\n\n      if (abs(val_dev[l_col_ptr_dev[currentCol]]) < pert)\n        val_dev[l_col_ptr_dev[currentCol]] = pert;\n\n      val_dev[currentLPos + offset] /= val_dev[l_col_ptr_dev[currentCol]];\n      tmpMem[stream * n + ridx]= val_dev[currentLPos + offset];\n    }\n    offset += blockDim.x;\n  }\n}",
            "#define REAL DOUBLE\n\n\n__global__ void RL_onecol_updateSubmat(\n    const unsigned* __restrict__ sym_c_ptr_dev,\n    const unsigned* __restrict__ sym_r_idx_dev,\n    REAL* __restrict__ val_dev,\n    const unsigned* __restrict__ csr_c_idx_dev,\n    const unsigned* __restrict__ csr_diag_ptr_dev,\n    const unsigned currentCol,\n    REAL* __restrict__ tmpMem,\n    const int stream,\n    const unsigned n)\n{\n  const int tid = threadIdx.x;\n  const int bid = blockIdx.x;\n  __shared__ REAL s;\n\n  //broadcast to submatrix\n  const unsigned subColPos = csr_diag_ptr_dev[currentCol] + bid + 1;\n  unsigned subCol;\n  unsigned subColElem = 0;\n\n  int offset = 0;\n  subCol = csr_c_idx_dev[subColPos];\n  while(offset < sym_c_ptr_dev[subCol + 1] - sym_c_ptr_dev[subCol])\n  {\n    if (tid + offset < sym_c_ptr_dev[subCol + 1] - sym_c_ptr_dev[subCol])\n    {\n      subColElem = sym_c_ptr_dev[subCol] + tid + offset;\n      unsigned ridx = sym_r_idx_dev[subColElem];\n\n      if (ridx == currentCol)\n      {\n        s = val_dev[subColElem];\n      }\n      __syncthreads();\n      if (ridx > currentCol)\n      {\n        atomicAdd(&val_dev[subColElem], -tmpMem[stream * n + ridx] * s);\n      }\n    }\n    offset += blockDim.x;\n  }\n}",
            "#define REAL DOUBLE\n\n\n__global__ void RL_onecol_cleartmpMem(\n    const unsigned* __restrict__ sym_c_ptr_dev,\n    const unsigned* __restrict__ sym_r_idx_dev,\n    const unsigned* __restrict__ l_col_ptr_dev,\n    const unsigned currentCol,\n    REAL* __restrict__ tmpMem,\n    const int stream,\n    const unsigned n)\n{\n  const int tid = threadIdx.x;\n\n  const unsigned currentLColSize = sym_c_ptr_dev[currentCol + 1] - l_col_ptr_dev[currentCol] - 1;\n  const unsigned currentLPos = l_col_ptr_dev[currentCol] + tid + 1;\n\n  unsigned offset = 0;\n  while (currentLColSize > offset)\n  {\n    if (tid + offset < currentLColSize)\n    {\n      unsigned ridx = sym_r_idx_dev[currentLPos + offset];\n      tmpMem[stream * n + ridx]= 0;\n    }\n    offset += blockDim.x;\n  }\n}"
        ]
    },
    "p4-cuda": {
        "/Users/gbolet/hecbench-roofline/src/p4-cuda/main.cu": [
            "inline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\ninline __host__ __device__ float4 expf(float4 v)\n{\n    return make_float4(expf(v.x), expf(v.y), expf(v.z), expf(v.w));\n}\n\ninline __host__ __device__ float4 floorf(float4 v)\n{\n    return make_float4(floorf(v.x), floorf(v.y), floorf(v.z), floorf(v.w));\n}\n\ninline __host__ __device__ float4 sqrtf(float4 v)\n{\n    return make_float4(sqrtf(v.x), sqrtf(v.y), sqrtf(v.z), sqrtf(v.w));\n}\n\n__device__\nfloat sigmoid(const float x) { return 1.0f / (1.0f + expf(-x)); }\n\n__global__\nvoid postprocess (\n  const float *__restrict__ cls_input,\n        float *__restrict__ box_input,\n  const float *__restrict__ dir_cls_input,\n  const float *__restrict__ anchors,\n  const float *__restrict__ anchor_bottom_heights,\n        float *__restrict__ bndbox_output,\n        int *__restrict__ object_counter,\n  const float min_x_range,\n  const float max_x_range,\n  const float min_y_range,\n  const float max_y_range,\n  const int feature_x_size,\n  const int feature_y_size,\n  const int num_anchors,\n  const int num_classes,\n  const int num_box_values,\n  const float score_thresh,\n  const float dir_offset)\n{\n  int loc_index = blockIdx.x;\n  int ith_anchor = threadIdx.x;\n  if (ith_anchor >= num_anchors) return;\n\n  int col = loc_index % feature_x_size;\n  int row = loc_index / feature_x_size;\n  float x_offset = min_x_range + col * (max_x_range - min_x_range) / (feature_x_size - 1);\n  float y_offset = min_y_range + row * (max_y_range - min_y_range) / (feature_y_size - 1);\n  int cls_offset = loc_index * num_anchors * num_classes + ith_anchor * num_classes;\n  float dev_cls[2] = {-1.f, 0.f};\n\n  const float *scores = cls_input + cls_offset;\n  float max_score = sigmoid(scores[0]);\n  int cls_id = 0;\n  for (int i = 1; i < num_classes; i++) {\n    float cls_score = sigmoid(scores[i]);\n    if (cls_score > max_score) {\n      max_score = cls_score;\n      cls_id = i;\n    }\n  }\n  dev_cls[0] = static_cast<float>(cls_id);\n  dev_cls[1] = max_score;\n\n  if (dev_cls[1] >= score_thresh)\n  {\n    const int box_offset = loc_index * num_anchors * num_box_values + ith_anchor * num_box_values;\n    const int dir_cls_offset = loc_index * num_anchors * 2 + ith_anchor * 2;\n    const float *anchor_ptr = anchors + ith_anchor * 4;\n    const float z_offset = anchor_ptr[2] / 2 + anchor_bottom_heights[ith_anchor / 2];\n    const float anchor[7] = {x_offset, y_offset, z_offset, anchor_ptr[0], anchor_ptr[1], anchor_ptr[2], anchor_ptr[3]};\n    float *box_encodings = box_input + box_offset;\n\n    const float xa = anchor[0];\n    const float ya = anchor[1];\n    const float za = anchor[2];\n    const float dxa = anchor[3];\n    const float dya = anchor[4];\n    const float dza = anchor[5];\n    const float ra = anchor[6];\n    const float diagonal = sqrtf(dxa * dxa + dya * dya);\n    box_encodings[0] = box_encodings[0] * diagonal + xa;\n    box_encodings[1] = box_encodings[1] * diagonal + ya;\n    box_encodings[2] = box_encodings[2] * dza + za;\n    box_encodings[3] = expf(box_encodings[3]) * dxa;\n    box_encodings[4] = expf(box_encodings[4]) * dya;\n    box_encodings[5] = expf(box_encodings[5]) * dza;\n    box_encodings[6] = box_encodings[6] + ra;\n\n    const int dir_label = dir_cls_input[dir_cls_offset] > dir_cls_input[dir_cls_offset + 1] ? 0 : 1;\n    const float period = (float)M_PI;\n    const float val = box_input[box_offset + 6] - dir_offset;\n    const float dir_rot = val - floorf(val / (period + 1e-8f)) * period;\n    const float yaw = dir_rot + dir_offset + period * dir_label;\n\n    int resCount = (int)atomicAdd(object_counter, 1);\n    bndbox_output[0] = resCount+1;\n    float *data = bndbox_output + 1 + resCount * 9;\n    data[0] = box_input[box_offset];\n    data[1] = box_input[box_offset + 1];\n    data[2] = box_input[box_offset + 2];\n    data[3] = box_input[box_offset + 3];\n    data[4] = box_input[box_offset + 4];\n    data[5] = box_input[box_offset + 5];\n    data[6] = yaw;\n    data[7] = dev_cls[0];\n    data[8] = dev_cls[1];\n  }\n}"
        ]
    },
    "gamma-correction-cuda": {
        "/Users/gbolet/hecbench-roofline/src/gamma-correction-cuda/main.cpp": [
            "__global__ \nvoid gamma_correction(ImgPixel* pixel) {\n\tint i = blockIdx.x * blockDim.x + threadIdx.x;\n\n\t// Lambda to process image with gamma = 2\n\tconst float v = (0.3f * pixel[i].r + 0.59f * pixel[i].g + 0.11f * pixel[i].b) / 255.0;\n\n\tstd::uint8_t gamma_pixel = static_cast<std::uint8_t>(255 * v * v);\n\tif (gamma_pixel > 255) gamma_pixel = 255;\n\tpixel[i].set(gamma_pixel, gamma_pixel, gamma_pixel, gamma_pixel);\n}"
        ]
    },
    "haccmk-cuda": {
        "/Users/gbolet/hecbench-roofline/src/haccmk-cuda/haccmk.cu": [
            "inline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\ninline __host__ __device__ float4 sqrtf(float4 v)\n{\n    return make_float4(sqrtf(v.x), sqrtf(v.y), sqrtf(v.z), sqrtf(v.w));\n}\n\n__global__ void\nhaccmk_kernel (\n    const int n1,  // outer loop count\n    const int n2,  // inner loop count\n    const float *__restrict__ xx, \n    const float *__restrict__ yy,\n    const float *__restrict__ zz,\n    const float *__restrict__ mass,\n          float *__restrict__ vx2,\n          float *__restrict__ vy2,\n          float *__restrict__ vz2,\n    const float fsrmax,\n    const float mp_rsm,\n    const float fcoeff ) \n{\n  int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i >= n1) return;\n\n  const float ma0 = 0.269327f; \n  const float ma1 = -0.0750978f; \n  const float ma2 = 0.0114808f; \n  const float ma3 = -0.00109313f; \n  const float ma4 = 0.0000605491f; \n  const float ma5 = -0.00000147177f;\n\n  float dxc, dyc, dzc, m, r2, f, xi, yi, zi;\n\n  xi = 0.f; \n  yi = 0.f;\n  zi = 0.f;\n\n  float xxi = xx[i];\n  float yyi = yy[i];\n  float zzi = zz[i];\n\n  for ( int j = 0; j < n2; j++ ) {\n    dxc = xx[j] - xxi;\n    dyc = yy[j] - yyi;\n    dzc = zz[j] - zzi;\n\n    r2 = dxc * dxc + dyc * dyc + dzc * dzc;\n\n    if ( r2 < fsrmax ) m = mass[j]; else m = 0.f;\n\n    f = r2 + mp_rsm;\n    f = m * (1.f / (f * sqrtf(f)) - (ma0 + r2*(ma1 + r2*(ma2 + r2*(ma3 + r2*(ma4 + r2*ma5))))));\n\n    xi = xi + f * dxc;\n    yi = yi + f * dyc;\n    zi = zi + f * dzc;\n  }\n\n  vx2[i] += xi * fcoeff;\n  vy2[i] += yi * fcoeff;\n  vz2[i] += zi * fcoeff;\n}"
        ]
    },
    "sobol-cuda": {
        "/Users/gbolet/hecbench-roofline/src/sobol-cuda/sobol_gpu.cu": [
            "__global__\nvoid sobolGPU_kernel(unsigned n_vectors, unsigned n_dimensions,\n                     unsigned *__restrict__ d_directions,\n                     float *__restrict__ d_output)\n{\n    // Handle to thread block group\n    //cg::thread_block cta = cg::this_thread_block();\n    __shared__ unsigned int v[n_directions];\n\n    // Offset into the correct dimension as specified by the\n    // block y coordinate\n    d_directions += n_directions * blockIdx.y;\n    d_output += n_vectors * blockIdx.y;\n\n    // Copy the direction numbers for this dimension into shared\n    // memory - there are only 32 direction numbers so only the\n    // first 32 (n_directions) threads need participate.\n    if (threadIdx.x < n_directions)\n    {\n        v[threadIdx.x] = d_directions[threadIdx.x];\n    }\n\n    //cg::sync(cta);\n    __syncthreads();\n\n    // Set initial index (i.e. which vector this thread is\n    // computing first) and stride (i.e. step to the next vector\n    // for this thread)\n    int i0     = threadIdx.x + blockIdx.x * blockDim.x;\n    int stride = gridDim.x * blockDim.x;\n\n    // Get the gray code of the index\n    // c.f. Numerical Recipes in C, chapter 20\n    // http://www.nrbook.com/a/bookcpdf/c20-2.pdf\n    unsigned int g = i0 ^ (i0 >> 1);\n\n    // Initialisation for first point x[i0]\n    // In the Bratley and Fox paper this is equation (*), where\n    // we are computing the value for x[n] without knowing the\n    // value of x[n-1].\n    unsigned int X = 0;\n    unsigned int mask;\n\n    for (unsigned int k = 0 ; k < __ffs(stride) - 1 ; k++)\n    {\n        // We want X ^= g_k * v[k], where g_k is one or zero.\n        // We do this by setting a mask with all bits equal to\n        // g_k. In reality we keep shifting g so that g_k is the\n        // LSB of g. This way we avoid multiplication.\n        mask = - (g & 1);\n        X ^= mask & v[k];\n        g = g >> 1;\n    }\n\n    if (i0 < n_vectors)\n    {\n        d_output[i0] = (float)X * k_2powneg32;\n    }\n\n    // Now do rest of points, using the stride\n    // Here we want to generate x[i] from x[i-stride] where we\n    // don't have any of the x in between, therefore we have to\n    // revisit the equation (**), this is easiest with an example\n    // so assume stride is 16.\n    // From x[n] to x[n+16] there will be:\n    //   8 changes in the first bit\n    //   4 changes in the second bit\n    //   2 changes in the third bit\n    //   1 change in the fourth\n    //   1 change in one of the remaining bits\n    //\n    // What this means is that in the equation:\n    //   x[n+1] = x[n] ^ v[p]\n    //   x[n+2] = x[n+1] ^ v[q] = x[n] ^ v[p] ^ v[q]\n    //   ...\n    // We will apply xor with v[1] eight times, v[2] four times,\n    // v[3] twice, v[4] once and one other direction number once.\n    // Since two xors cancel out, we can skip even applications\n    // and just apply xor with v[4] (i.e. log2(16)) and with\n    // the current applicable direction number.\n    // Note that all these indices count from 1, so we need to\n    // subtract 1 from them all to account for C arrays counting\n    // from zero.\n    unsigned int v_log2stridem1 = v[__ffs(stride) - 2];\n    unsigned int v_stridemask = stride - 1;\n\n    for (unsigned int i = i0 + stride ; i < n_vectors ; i += stride)\n    {\n        // x[i] = x[i-stride] ^ v[b] ^ v[c]\n        //  where b is log2(stride) minus 1 for C array indexing\n        //  where c is the index of the rightmost zero bit in i,\n        //  not including the bottom log2(stride) bits, minus 1\n        //  for C array indexing\n        // In the Bratley and Fox paper this is equation (**)\n        X ^= v_log2stridem1 ^ v[__ffs(~((i - stride) | v_stridemask)) - 1];\n        d_output[i] = (float)X * k_2powneg32;\n    }\n}"
        ]
    },
    "vote-cuda": {
        "/Users/gbolet/hecbench-roofline/src/vote-cuda/kernels.h": [
            "__global__ void VoteAnyKernel1(const unsigned int *input, unsigned int *result,\n                               int repeat) {\n  int tx = threadIdx.x;\n  for (int i = 0; i < repeat; i++)\n    result[tx] = __any_sync(MASK, input[tx]);\n}",
            "__global__ void VoteAllKernel2(const unsigned int *input, unsigned int *result,\n                               int repeat) {\n  int tx = threadIdx.x;\n  for (int i = 0; i < repeat; i++)\n    result[tx] = __all_sync(MASK, input[tx]);\n}",
            "__global__ void VoteAnyKernel3(bool *info, int warp_size, int repeat) {\n  int tx = threadIdx.x;\n  for (int i = 0; i < repeat; i++) {\n    bool *offs = info + (tx * 3);\n\n    // The following should hold true for the second and third warp\n    *offs = __any_sync(MASK, (tx >= (warp_size * 3) / 2));\n\n    // The following should hold true for the \"upper half\" of the second warp,\n    // and all of the third warp\n    *(offs + 1) = (tx >= (warp_size * 3) / 2 ? true : false);\n\n    // The following should hold true for the third warp only\n    if (__all_sync(MASK, (tx >= (warp_size * 3) / 2))) {\n      *(offs + 2) = true;\n    }\n  }\n}"
        ]
    },
    "bilateral-cuda": {
        "/Users/gbolet/hecbench-roofline/src/bilateral-cuda/main.cu": [
            "inline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\ninline __host__ __device__ float4 expf(float4 v)\n{\n    return make_float4(expf(v.x), expf(v.y), expf(v.z), expf(v.w));\n}\n\n__global__ void bilateralFilter(\n    const float *__restrict__ in,\n    float *__restrict__ out,\n    int w, \n    int h, \n    float a_square,\n    float variance_I,\n    float variance_spatial)\n{\n  const int idx = blockIdx.x*blockDim.x + threadIdx.x;\n  const int idy = blockIdx.y*blockDim.y + threadIdx.y;\n\n  if(idx >= w || idy >= h) return;\n\n  int id = idy*w + idx;\n  float I = in[id];\n  float res = 0;\n  float normalization = 0;\n\n  // window centered at the coordinate (idx, idy)\n#ifdef LOOP_UNROLL\n  #pragma unroll\n#endif\n  for(int i = -R; i <= R; i++) {\n#ifdef LOOP_UNROLL\n    #pragma unroll\n#endif\n    for(int j = -R; j <= R; j++) {\n\n      int idk = idx+i;\n      int idl = idy+j;\n\n      // mirror edges\n      if( idk < 0) idk = -idk;\n      if( idl < 0) idl = -idl;\n      if( idk > w - 1) idk = w - 1 - i;\n      if( idl > h - 1) idl = h - 1 - j;\n\n      int id_w = idl*w + idk;\n      float I_w = in[id_w];\n\n      // range kernel for smoothing differences in intensities\n      float range = -(I-I_w) * (I-I_w) / (2.f * variance_I);\n\n      // spatial (or domain) kernel for smoothing differences in coordinates\n      float spatial = -((idk-idx)*(idk-idx) + (idl-idy)*(idl-idy)) /\n                      (2.f * variance_spatial);\n\n      // the weight is assigned using the spatial closeness (using the spatial kernel) \n      // and the intensity difference (using the range kernel)\n      float weight = a_square * expf(spatial + range);\n\n      normalization += weight;\n      res += (I_w * weight);\n    }\n  }\n  out[id] = res/normalization;\n}"
        ]
    },
    "streamPriority-cuda": {
        "/Users/gbolet/hecbench-roofline/src/streamPriority-cuda/main.cu": [
            "__global__ void memcpy_kernel(int *dst, int *src, size_t n, bool wait) {\n  int num = gridDim.x * blockDim.x;\n  int id = blockDim.x * blockIdx.x + threadIdx.x;\n\n  for (size_t i = id; i < n / sizeof(int); i += num) {\n    int v = src[i];\n    if (wait) {\n      while (v--) \n        dst[i] = v;\n    }\n    dst[i] = src[i];\n  }\n}"
        ]
    },
    "depixel-cuda": {
        "/Users/gbolet/hecbench-roofline/src/depixel-cuda/kernels.h": [
            "inline __host__ __device__ int4 make_int4(float4 a)\n{\n    return make_int4(int(a.x), int(a.y), int(a.z), int(a.w));\n}\n\ninline __host__ __device__ int4 abs(int4 v)\n{\n    return make_int4(abs(v.x), abs(v.y), abs(v.z), abs(v.w));\n}\n\n__device__ __forceinline__ bool isConnected(uint lnode, uint rnode)\n{\n  int ly = lnode & 0xff;\n  int lu = ((lnode>>8) & 0xff);\n  int lv = ((lnode>>16) & 0xff);\n  int ry = rnode & 0xff;\n  int ru = ((rnode>>8) & 0xff);\n  int rv = ((rnode>>16) & 0xff);\n  return !((abs(ly-ry) > 48) || (abs(lu-ru) > 7) || (abs(lv-rv) > 6));\n}\n\n__device__ __forceinline__ uint rgbToyuv(float3 rgba)\n{\n  float3 yuv;\n  yuv.x = 0.299f*rgba.x + 0.587f*rgba.y+0.114f*rgba.z;\n  yuv.y = 0.713f*(rgba.x - yuv.x) + 0.5f;\n  yuv.z = 0.564f*(rgba.z - yuv.x) + 0.5f;\n  yuv.x = __saturatef(yuv.x);\n  yuv.y = __saturatef(yuv.y);\n  yuv.z = __saturatef(yuv.z);\n  return (uint(255)<<24) | (uint(yuv.z*255.f) << 16) | (uint(yuv.y*255.f) << 8) | uint(yuv.x*255.f);\n}\n\n__global__\nvoid check_connect(\n  const float3 *__restrict__ rgba,\n          uint *__restrict__ connect,\n  const int w, const int h)\n{\n  unsigned int center = __umul24(blockIdx.x, blockDim.x) + threadIdx.x;\n  int row = center/w;\n  int column = center%w;\n  int neibor_row, neibor_column;\n  unsigned char con = 0;\n  uint yuv_c = rgbToyuv(rgba[center]);\n\n  //check 8 neiboughrs of one node for their connectivities.\n\n  //upper left\n  neibor_row = (row>0&&column>0)?(row-1):row;\n  neibor_column = (column>0&&row>0)?(column-1):column;\n  uint yuv_ul = rgbToyuv(rgba[neibor_row * w + neibor_column]);\n  con += (uint)(!((row==neibor_row) && (column==neibor_column)) && isConnected(yuv_c, yuv_ul));\n\n  //upper\n  neibor_row = (row>0)?(row-1):row;\n  neibor_column = column;\n  uint yuv_up = rgbToyuv(rgba[neibor_row * w + neibor_column]);\n  con += (uint)(!((row==neibor_row) && (column==neibor_column)) && isConnected(yuv_c, yuv_up))<<1;\n\n  //upper right\n  neibor_row = (row>0&&column<(w-1))?(row-1):row;\n  neibor_column = (column<(w-1)&&row>0)?(column+1):column;\n  uint yuv_ur = rgbToyuv(rgba[neibor_row * w + neibor_column]);\n  con += (uint)(!((row==neibor_row) && (column==neibor_column)) && isConnected(yuv_c, yuv_ur))<<2;\n\n  //right\n  neibor_row = row;\n  neibor_column = (column<(w-1))?(column+1):column;\n  uint yuv_rt = rgbToyuv(rgba[neibor_row * w + neibor_column]);\n  con += (uint)(!((row==neibor_row) && (column==neibor_column)) && isConnected(yuv_c, yuv_rt))<<3;\n\n  //lower right\n  neibor_row = (row<(h-1)&&column<(w-1))?(row+1):row;\n  neibor_column = (column<(w-1)&&row<(h-1))?(column+1):column;\n  uint yuv_lr = rgbToyuv(rgba[neibor_row * w + neibor_column]);\n  con += (uint)(!((row==neibor_row) && (column==neibor_column)) && isConnected(yuv_c, yuv_lr))<<4;\n\n  //lower\n  neibor_row = (row<(h-1))?(row+1):row;\n  neibor_column = column;\n  uint yuv_lw = rgbToyuv(rgba[neibor_row * w + neibor_column]);\n  con += (uint)(!((row==neibor_row) && (column==neibor_column)) && isConnected(yuv_c, yuv_lw))<<5;\n\n  //lower left\n  neibor_row = (row<(h-1)&&column>0)?(row+1):row;\n  neibor_column = (column>0&&row<(h-1))?(column-1):column;\n  uint yuv_ll = rgbToyuv(rgba[neibor_row * w + neibor_column]);\n  con += (uint)(!((row==neibor_row) && (column==neibor_column)) && isConnected(yuv_c, yuv_ll))<<6;\n\n  //left\n  neibor_row = row;\n  neibor_column = (column>0)?(column-1):column;\n  uint yuv_lt = rgbToyuv(rgba[neibor_row * w + neibor_column]);\n  con += (uint)(!((row==neibor_row) && (column==neibor_column)) && isConnected(yuv_c, yuv_lt))<<7;\n\n  connect[center] = (yuv_c>>16&0xFF)<<24 | (yuv_c>>8&0xFF)<<16 | (yuv_c&0xFF)<<8 | con;\n}",
            "inline __host__ __device__ int4 make_int4(float4 a)\n{\n    return make_int4(int(a.x), int(a.y), int(a.z), int(a.w));\n}\n\ninline __host__ __device__ int4 abs(int4 v)\n{\n    return make_int4(abs(v.x), abs(v.y), abs(v.z), abs(v.w));\n}\n\n__device__ __forceinline__ uint bitCount(uint v)\n{\n  uint c;\n  for (c = 0; v; ++c) v &= v - 1;\n  return c;\n}\n\n__device__ __forceinline__ bool isConnected(uint lnode, uint rnode)\n{\n  int ly = lnode & 0xff;\n  int lu = ((lnode>>8) & 0xff);\n  int lv = ((lnode>>16) & 0xff);\n  int ry = rnode & 0xff;\n  int ru = ((rnode>>8) & 0xff);\n  int rv = ((rnode>>16) & 0xff);\n  return !((abs(ly-ry) > 48) || (abs(lu-ru) > 7) || (abs(lv-rv) > 6));\n}\n\n__global__\nvoid eliminate_crosses(\n  const uint *__restrict__ id,\n        uint *__restrict__ od,\n  const int w, const int h)\n{\n  unsigned int center = __umul24(blockIdx.x, blockDim.x) + threadIdx.x;\n  int row = center/w;\n  int column = center%w;\n  int start_row = (row > 2)?row-3:0;\n  int start_column = (column > 2)?column-3:0;\n  int end_row = (row < w-4)?row+4:w-1;\n  int end_column = (column < h-4)?column+4:h-1;\n  int weight_l = 0;  //weight for left diagonal  \n  int weight_r = 0;  //weight for right diagonal\n  od[center] = 0;\n  if ((row<h-1) && (column<w-1))\n  {\n    od[center] = (id[center]&0x08)>>3 | \n                 ((id[center+w+1]&0x02)>>1)<<1 |\n                 ((id[center+w+1]&0x80)>>7)<<2 |\n                 ((id[center]&0x20)>>5)<<3;\n\n    if ((id[center]&0x10 && id[center+1]&0x40))\n    {\n      //if fully connected\n      if (id[center]&0x28 && id[center+1]&0xA0)\n      {\n        //eliminate cross (no cross needs to be added)\n        od[center] = ((id[center]>>8)&0xFFFFFF)<<8 | od[center];\n        return;\n      }\n\n      //island\n      if (id[center] == 0x10)\n      {\n        //island 1\n        //accumulate weight\n        weight_l += 5;\n      }\n      if (id[center+1] == 0x40)\n      {\n        //island 2\n        //accumulate weight\n        weight_r += 5;\n      }\n\n      //sparse judge\n      int sum_l = 0;\n      int sum_r = 0;\n      for ( int i = start_row; i <= end_row; ++i )\n      {\n        for ( int j = start_column; j <= end_column; ++j )\n        {\n          //compute connectivity\n          //accumulate weight\n          if (i*w+j!=center && i*w+j!=center+1)\n          {\n            sum_l += isConnected(id[center]>>8, id[i*w+j]>>8);\n            sum_r += isConnected(id[center+1]>>8, id[i*w+j]>>8);\n          }\n        }\n      }\n\n      weight_r += (sum_l > sum_r)?(sum_l-sum_r):0;\n      weight_l += (sum_l < sum_r)?(sum_r-sum_l):0;\n\n      //curve judge\n      int c_row = row;\n      int c_column = column;\n      uint curve_l = id[c_row*w+c_column]&0xFF;\n      uint edge_l = 16;\n      sum_l = 1;\n      while(bitCount(curve_l) == 2 && sum_l < w*h)\n      {\n        edge_l = curve_l - edge_l;\n        switch (edge_l)\n        {\n          case 1:\n            c_row -= 1;\n            c_column -= 1;\n            break;\n          case 2:\n            c_row -= 1;\n            break;\n          case 4:\n            c_row -= 1;\n            c_column += 1;\n            break;\n          case 8:\n            c_column += 1;\n            break;\n          case 16:\n            c_row += 1;\n            c_column += 1;\n            break;\n          case 32:\n            c_row += 1;\n            break;\n          case 64:\n            c_row += 1;\n            c_column -= 1;\n            break;\n          case 128:\n            c_column -= 1;\n            break;\n        }\n        edge_l = (edge_l > 8)?edge_l>>4:edge_l<<4;\n        curve_l = id[c_row*w+c_column]&0xFF;\n        ++sum_l;\n      }\n      c_row = row+1;\n      c_column = column+1;\n      curve_l = id[c_row*w+c_column]&0xFF;\n      edge_l = 1;\n      while(bitCount(curve_l) == 2 && sum_l < w*h)\n      {\n        edge_l = curve_l - edge_l;\n        switch (edge_l)\n        {\n          case 1:\n            c_row -= 1;\n            c_column -= 1;\n            break;\n          case 16:\n            c_row += 1;\n            c_column += 1;\n            break;\n          case 2:\n            c_row -= 1;\n            break;\n          case 4:\n            c_row -= 1;\n            c_column += 1;\n            break;\n          case 8:\n            c_column += 1;\n            break;\n          case 32:\n            c_row += 1;\n            break;\n          case 64:\n            c_row += 1;\n            c_column -= 1;\n            break;\n          case 128:\n            c_column -= 1;\n            break;\n        }\n        edge_l = (edge_l > 8)?edge_l>>4:edge_l<<4;\n        curve_l = id[c_row*w+c_column]&0xFF;\n        ++sum_l;\n      }\n      c_row = row;\n      c_column = column + 1;\n      uint curve_r = id[c_row*w+c_column]&0xFF;\n      uint edge_r = 64;\n      sum_r = 1;\n      while(bitCount(curve_r) == 2 && sum_r < w*h)\n      {\n        edge_r = curve_r - edge_r;\n        switch (edge_r)\n        {\n          case 64:\n            c_row += 1;\n            c_column -= 1;\n          case 1:\n            c_row -= 1;\n            c_column -= 1;\n            break;\n          case 2:\n            c_row -= 1;\n            break;\n          case 4:\n            c_row -= 1;\n            c_column += 1;\n            break;\n          case 8:\n            c_column += 1;\n            break;\n          case 32:\n            c_row += 1;\n            break;\n          case 16:\n            c_row += 1;\n            c_column += 1;\n            break;\n          case 128:\n            c_column -= 1;\n            break;\n        }\n        edge_r = (edge_r > 8)?edge_r>>4:edge_r<<4;\n        curve_r = id[c_row*w+c_column]&0xFF;\n        ++sum_r;\n      }\n      c_row = row+1;\n      c_column = column;\n      curve_r = id[c_row*w+c_column]&0xFF;\n      edge_r = 4;\n      while(bitCount(curve_r) == 2 && sum_r < w*h)\n      {  \n        edge_r = curve_r - edge_r;\n        \n        switch (edge_r)\n        {\n          case 4:\n            c_row -= 1;\n            c_column += 1;\n            break;\n          case 16:\n            c_row += 1;\n            c_column += 1;\n            break;\n          case 2:\n            c_row -= 1;\n            break;\n          case 1:\n            c_row -= 1;\n            c_column -= 1;\n            break;\n          case 8:\n            c_column += 1;\n            break;\n          case 32:\n            c_row += 1;\n            break;\n          case 64:\n            c_row += 1;\n            c_column -= 1;\n            break;\n          case 128:\n            c_column -= 1;\n            break;\n        }\n        edge_r = (edge_r > 8)?edge_r>>4:edge_r<<4;\n        curve_r = id[c_row*w+c_column]&0xFF;\n        ++sum_r;\n      }\n\n      weight_l += (sum_l > sum_r)?(sum_l-sum_r):0;\n      weight_r += (sum_l < sum_r)?(sum_r-sum_l):0;\n\n      //eliminate cross according to weight\n      if (weight_l > weight_r)\n      {\n        //add left diagonal\n        od[center] |= 0x10;\n        od[center] = ((id[center]>>8)&0xFFFFFF)<<8 | od[center];\n        return;\n      }\n      else\n      {\n        if(weight_r > weight_l)\n        {\n          //add right diagonal\n          od[center] |= 0x20;\n          od[center] = ((id[center]>>8)&0xFFFFFF)<<8 | od[center];\n          return;\n        }\n      }\n    }\n    od[center] = od[center] | (((id[center]&0x10)>>4)<<4) | (((id[center+1]&0x40)>>6)<<5);\n  }\n  od[center] = ((id[center]>>8)&0xFFFFFF)<<8 | od[center];\n}"
        ]
    },
    "phmm-cuda": {
        "/Users/gbolet/hecbench-roofline/src/phmm-cuda/kernel.h": [
            "__global__ void pair_HMM_forward(\n    const int cur_i,\n    const int cur_j,\n    //const double forward_matrix_in[x_dim+1][y_dim+1][batch][states-1],\n    const fArray *__restrict__ forward_matrix_in,\n    //const double transitions[x_dim+1][batch][states-1][states],\n    const tArray *__restrict__ transitions,\n    //const double emissions[x_dim+1][y_dim+1][batch][states-1],\n    const fArray *__restrict__ emissions,\n    // const double likelihood[2][2][batch][states-1],\n    const lArray *__restrict__ likelihood,\n    //const double start_transitions[batch][states-1],\n    const sArray *__restrict__ start_transitions,\n          //double forward_matrix_out[x_dim+1][y_dim+1][batch][states-1])\n          fArray *__restrict__ forward_matrix_out)\n{\n  int batch_id = blockIdx.x;\n  int states_id = threadIdx.x;\n\n  __shared__ double e[batch][states-1];\n  __shared__ double f01[1][batch][2];\n  __shared__ double mul_3d[1][batch][2];\n  __shared__ double mul_4d[4][batch][1][2];\n\n  e[batch_id][states_id] = emissions[cur_i][cur_j][batch_id][states_id];\n\n  double t[2][2][batch][2][2];\n  for (int k = 0; k < 2; k++) {\n    for (int l = 0; l < 2; l++) {\n      t[0][0][batch_id][k][l] = transitions[cur_i - 1][batch_id][k][l];\n      t[0][1][batch_id][k][l] = transitions[cur_i - 1][batch_id][k][l];\n      t[1][0][batch_id][k][l] = transitions[cur_i][batch_id][k][l];\n      t[1][1][batch_id][k][l] = transitions[cur_i][batch_id][k][l];\n    }\n  }\n  __syncthreads();\n\n  if (cur_i > 0 && cur_j == 0) {\n    if (cur_i == 1) {\n      forward_matrix_out[1][0][batch_id][states_id] = \n        start_transitions[batch_id][states_id] * e[0][states_id];\n    }\n    else {\n      double t01[batch][2][2];\n      for (int j = 0; j < 2; j++) {\n        for (int k = 0; k < 2; k++) {\n          t01[batch_id][j][k] = t[0][1][batch_id][j][k];\n        }\n      }\n\n      f01[0][batch_id][states_id] = \n        forward_matrix_in[cur_i - 1][cur_j][batch_id][states_id];\n\n      __syncthreads();\n\n      double s = 0.0;\n      for (int k = 0; k < 2; k++)\n        s += f01[0][batch_id][k] * t01[batch_id][k][states_id];\n      s *= (e[batch_id][states_id] * likelihood[0][1][batch_id][states_id]);\n      mul_3d[0][batch_id][states_id] = s;\n\n      __syncthreads();\n\n      forward_matrix_out[cur_i][0][batch_id][states_id] = mul_3d[0][batch_id][states_id];\n    }\n  }\n  else if (cur_i > 0 and cur_j > 0) {\n\n    double f[2][2][batch][1][2];\n    for (int i = 0; i < 2; i++) {\n      f[0][0][batch_id][0][i] = forward_matrix_in[cur_i-1][cur_j-1][batch_id][i];\n      f[0][1][batch_id][0][i] = forward_matrix_in[cur_i-1][cur_j][batch_id][i];\n      f[1][0][batch_id][0][i] = forward_matrix_in[cur_i][cur_j-1][batch_id][i];\n      f[1][1][batch_id][0][i] = forward_matrix_in[cur_i][cur_j][batch_id][i];\n    }\n    __syncthreads();\n\n    double s0 = 0.0;\n    double s1 = 0.0;\n    double s2 = 0.0;\n    double s3 = 0.0;\n\n    for (int k = 0; k < 2; k++) {\n      s0 += f[0][0][batch_id][0][k] * t[0][0][batch_id][k][states_id];\n      s1 += f[0][1][batch_id][0][k] * t[0][1][batch_id][k][states_id];\n      s2 += f[1][0][batch_id][0][k] * t[1][0][batch_id][k][states_id];\n      s3 += f[1][1][batch_id][0][k] * t[1][1][batch_id][k][states_id];\n    }\n    s0 *= likelihood[0][0][batch_id][states_id];\n    s1 *= likelihood[0][1][batch_id][states_id];\n    s2 *= likelihood[1][0][batch_id][states_id];\n    s3 *= likelihood[1][1][batch_id][states_id];\n    mul_4d[0][batch_id][0][states_id] = s0;\n    mul_4d[1][batch_id][0][states_id] = s1;\n    mul_4d[2][batch_id][0][states_id] = s2;\n    mul_4d[3][batch_id][0][states_id] = s3;\n\n    __syncthreads();\n\n    for (int j = 0; j < 2; j++) {\n      double summation = mul_4d[0][batch_id][0][j] + \n                         mul_4d[1][batch_id][0][j] +\n                         mul_4d[2][batch_id][0][j] +\n                         mul_4d[3][batch_id][0][j];\n\n      summation *= e[batch_id][j];\n\n      forward_matrix_out[cur_i][cur_j][batch_id][j] = summation;\n    }\n  }\n}"
        ]
    },
    "lif-cuda": {
        "/Users/gbolet/hecbench-roofline/src/lif-cuda/main.cu": [
            "inline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\ninline __host__ __device__ float4 fmaxf(float4 a, float4 b)\n{\n    return make_float4(fmaxf(a.x,b.x), fmaxf(a.y,b.y), fmaxf(a.z,b.z), fmaxf(a.w,b.w));\n}\n\n__global__ void lif (\n    int numNeurons, int neurons_per_item, float dt, \n    const float*__restrict__ encode_result,\n          float*__restrict__ voltage_array,\n          float*__restrict__ reftime_array,\n    float tau_rc, float tau_ref,\n    const float*__restrict__ bias,\n    const float*__restrict__ gain,\n          float*__restrict__ spikes)\n{\n  int i = threadIdx.x + blockDim.x * blockIdx.x;\n  if (i < numNeurons)\n  {\n    int neuron_index = i % neurons_per_item;\n    int item_index = (int)(i / neurons_per_item);\n\n    float voltage = voltage_array[i];\n    float ref_time = reftime_array[i];\n    float current = bias[neuron_index] + gain[neuron_index] * encode_result[item_index];\n    float dV, spike, mult;\n\n    dV = -expm1f(-dt / tau_rc) * (current - voltage);\n    voltage = fmaxf(voltage + dV, 0.f);\n\n    ref_time -= dt;\n\n    mult = ref_time;\n    mult *= -1.f / dt;\n    mult += 1.f;\n\n    mult = mult > 1.f ? 1.f : mult;\n    mult = mult < 0.f ? 0.f : mult;\n    \n    voltage *= mult;\n\n    if(voltage > 1.f){\n      spike = 1.f / dt;\n      ref_time = tau_ref + dt * (1.f - (voltage - 1.f) / dV);\n      voltage = 0.f;\n    }else{\n      spike = 0.f;\n    }\n\n    reftime_array[i] = ref_time;\n    voltage_array[i] = voltage;\n    spikes[i] = spike;\n  }\n}"
        ]
    },
    "nms-cuda": {
        "/Users/gbolet/hecbench-roofline/src/nms-cuda/main.cu": [
            "inline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\ninline __host__ __device__ float4 fmaxf(float4 a, float4 b)\n{\n    return make_float4(fmaxf(a.x,b.x), fmaxf(a.y,b.y), fmaxf(a.z,b.z), fmaxf(a.w,b.w));\n}\n\ninline  __host__ __device__ float4 fminf(float4 a, float4 b)\n{\n    return make_float4(fminf(a.x,b.x), fminf(a.y,b.y), fminf(a.z,b.z), fminf(a.w,b.w));\n}\n\n__global__\nvoid generate_nms_bitmap(const float4* rects, unsigned char* nmsbitmap, const float othreshold)\n{\n  const int i = blockIdx.x * blockDim.x + threadIdx.x;\n  const int j = blockIdx.y * blockDim.y + threadIdx.y;\n\n  if(rects[i].w < rects[j].w)\n  {\n    float area = (rects[j].z + 1.0f) * (rects[j].z + 1.0f);\n    float w = fmaxf(0.0f, fminf(rects[i].x + rects[i].z, rects[j].x + rects[j].z) - fmaxf(rects[i].x, rects[j].x) + 1.0f);\n    float h = fmaxf(0.0f, fminf(rects[i].y + rects[i].z, rects[j].y + rects[j].z) - fmaxf(rects[i].y, rects[j].y) + 1.0f);\n    nmsbitmap[i * MAX_DETECTIONS + j] = (((w * h) / area) < othreshold) && (rects[j].z != 0);\n  } \n}",
            "__device__ __inline__\nvoid compute_nms_point_mask(unsigned char* pointsbitmap, int cond, int idx, int ndetections)\n{\n  *pointsbitmap = __syncthreads_and(cond);\n}\n\n__global__ void reduce_nms_bitmap(unsigned char* nmsbitmap, unsigned char* pointsbitmap, int ndetections)\n{\n  int idx = blockIdx.x * MAX_DETECTIONS + threadIdx.x;\n\n  compute_nms_point_mask(&pointsbitmap[blockIdx.x], nmsbitmap[idx], idx, ndetections);\n\n  for(int i=0; i<(N_PARTITIONS-1); i++)\n  {\n    idx += MAX_DETECTIONS / N_PARTITIONS;\n    compute_nms_point_mask(&pointsbitmap[blockIdx.x], pointsbitmap[blockIdx.x] && nmsbitmap[idx], idx, ndetections);\n  }\n}"
        ]
    },
    "ddbp-cuda": {
        "/Users/gbolet/hecbench-roofline/src/ddbp-cuda/main.cu": [
            "__global__ void pad_projections_kernel(\n    double* d_img,\n    const int nDetXMap,\n    const int nDetYMap,\n    const int nElem,\n    const int np)\n{\n  const int gid = blockIdx.x * blockDim.x + threadIdx.x;\n  if (gid < nElem)\n    d_img[(np*nDetYMap *nDetXMap) + (gid*nDetYMap)] = 0;\n}",
            "__global__ void map_boudaries_kernel(\n    double* d_pBound,\n    const int nElem,\n    const double valueLeftBound,\n    const double sizeElem,\n    const double offset)\n{\n  const int gid = blockIdx.x * blockDim.x + threadIdx.x;\n  if (gid < nElem)\n    d_pBound[gid] = (gid - valueLeftBound) * sizeElem + offset;\n}",
            "__global__ void rot_detector_kernel(\n          double* __restrict__ d_pRdetY,\n          double* __restrict__ d_pRdetZ,\n    const double* __restrict__ d_pYcoord,\n    const double* __restrict__ d_pZcoord,\n    const double yOffset,\n    const double zOffset,\n    const double phi,\n    const int nElem)\n{\n  const int gid = blockIdx.x * blockDim.x + threadIdx.x;\n  if (gid < nElem) {\n    // cos and sin are in measured in radians.\n    d_pRdetY[gid] = ((d_pYcoord[gid] - yOffset) * cos(phi) - \n                     (d_pZcoord[gid] - zOffset) * sin(phi)) + yOffset;\n    d_pRdetZ[gid] = ((d_pYcoord[gid] - yOffset) * sin(phi) +\n                     (d_pZcoord[gid] - zOffset) * cos(phi)) + zOffset;\n  }\n}",
            "__global__ void mapDet2Slice_kernel(\n           double* __restrict__ const pXmapp,\n           double* __restrict__ const pYmapp,\n    double tubeX,\n    double tubeY,\n    double tubeZ,\n    const double* __restrict__ const pXcoord,\n    const double* __restrict__ const pYcoord,\n    const double* __restrict__ const pZcoord,\n    const double* __restrict__ const pZSlicecoord,\n    const int nDetXMap,\n    const int nDetYMap,\n    const int nz)\n{\n  const int px = blockIdx.x * blockDim.x + threadIdx.x;\n  const int py = blockIdx.y * blockDim.y + threadIdx.y;\n\n  if (px < nDetYMap && py < nDetXMap) {\n\n    const int pos = py * nDetYMap + px;\n\n    pXmapp[pos] = ((pXcoord[py] - tubeX)*(pZSlicecoord[nz] - pZcoord[px]) - \n        (pXcoord[py] * tubeZ) + (pXcoord[py] * pZcoord[px])) / (-tubeZ + pZcoord[px]);\n\n    if (py == 0)\n      pYmapp[px] = ((pYcoord[px] - tubeY)*(pZSlicecoord[nz] - pZcoord[px]) -\n          (pYcoord[px] * tubeZ) + (pYcoord[px] * pZcoord[px])) / (-tubeZ + pZcoord[px]);\n  }\n}",
            "__global__ void img_integration_kernel(\n    double* d_img,\n    const int nPixX,\n    const int nPixY,\n    const bool direction,\n    const int offsetX,\n    const int offsetY,\n    const int nSlices)\n{\n  /*\n     Integration of 2D slices over the whole volume\n\n     (S.1.Integration. - Liu et al(2017))\n\n     Perform an inclusive scan\n   */\n\n  const int tx = blockIdx.x * blockDim.x + threadIdx.x;\n  const int ty = blockIdx.y * blockDim.y + threadIdx.y;\n  const int px = tx + offsetX;\n  const int py = ty + offsetY;\n  const int pz = blockIdx.z * blockDim.z + threadIdx.z;\n\n  if (px >= nPixY || py >= nPixX || pz >= nSlices) return;\n\n  if (direction == integrateXcoord) {\n\n    for (int s = 1; s <= blockDim.y; s *= 2) {\n\n      int spot = ty - s;\n\n      double val = 0;\n\n      if (spot >= 0) {\n        val = d_img[(pz*nPixY*nPixX) + (offsetY + spot) * nPixY + px];\n      }\n\n      if (spot >= 0) {\n        d_img[(pz*nPixY*nPixX) + (py * nPixY) + px] += val;\n      }\n    }\n  }\n  else\n  {\n    for (int s = 1; s <= blockDim.x; s *= 2) {\n\n      int spot = tx - s;\n\n      double val = 0;\n\n      if (spot >= 0) {\n        val = d_img[(pz*nPixY*nPixX) + py * nPixY + spot + offsetX];\n      }\n\n      if (spot >= 0) {\n        d_img[(pz*nPixY*nPixX) + (py * nPixY) + px] += val;\n      }\n    }\n  }\n}",
            "#define T ((int)32)\n\n\n__host__ __device__ __forceinline__ constexpr T floor(T a, T b) {\n  return (a - b + 1) / b;\n}\n\n__global__ void bilinear_interpolation_kernel(\n          double* __restrict__ d_sliceI,\n    const double* __restrict__ d_pProj,\n    const double* __restrict__ d_pObjX,\n    const double* __restrict__ d_pObjY,\n    const double* __restrict__ d_pDetmX,\n    const double* __restrict__ d_pDetmY,\n    const int nPixXMap,\n    const int nPixYMap,\n    const int nDetXMap,\n    const int nDetYMap,\n    const int nDetX,\n    const int nDetY,\n    const int np) \n{\n  const int px = blockIdx.x * blockDim.x + threadIdx.x;\n  const int py = blockIdx.y * blockDim.y + threadIdx.y;\n\n  // Make sure we don't try and access memory outside the detector\n  // by having any threads mapped there return early\n  if (px >= nPixYMap || py >= nPixXMap) return;\n\n  //  S.2. Interpolation - Liu et al (2017)\n\n  // Adjust the mapped coordinates to cross the range of (0-nDetX).*duMap \n  // Divide by pixelSize to get a unitary pixel size\n  const double xNormData = nDetX - d_pObjX[py] / d_pDetmX[0];\n  const int    xData = floor(xNormData);\n  const double alpha = xNormData - xData;\n\n  // Adjust the mapped coordinates to cross the range of (0-nDetY).*dyMap  \n  // Divide by pixelSize to get a unitary pixel size\n  const double yNormData = (d_pObjY[px] / d_pDetmX[0]) - (d_pDetmY[0] / d_pDetmX[0]);\n  const int    yData = floor(yNormData);\n  const double beta = yNormData - yData;\n\n  double d00, d01, d10, d11;\n  if (((xNormData) >= 0) && ((xNormData) <= nDetX) && ((yNormData) >= 0) && ((yNormData) <= nDetY)) \n    d00 = d_pProj[(np*nDetYMap*nDetXMap) + (xData*nDetYMap + yData)];\n  else\n    d00 = 0.0;\n\n  if (((xData + 1) > 0) && ((xData + 1) <= nDetX) && ((yNormData) >= 0) && ((yNormData) <= nDetY))\n    d10 = d_pProj[(np*nDetYMap*nDetXMap) + ((xData + 1)*nDetYMap + yData)];\n  else\n    d10 = 0.0;\n\n  if (((xNormData) >= 0) && ((xNormData) <= nDetX) && ((yData + 1) > 0) && ((yData + 1) <= nDetY))\n    d01 = d_pProj[(np*nDetYMap*nDetXMap) + (xData*nDetYMap + yData + 1)];\n  else\n    d01 = 0.0;\n\n  if (((xData + 1) > 0) && ((xData + 1) <= nDetX) && ((yData + 1) > 0) && ((yData + 1) <= nDetY))\n    d11 = d_pProj[(np*nDetYMap*nDetXMap) + ((xData + 1)*nDetYMap + yData + 1)];\n  else\n    d11 = 0.0;\n\n  double result_temp1 = alpha * d10 + (-d00 * alpha + d00);\n  double result_temp2 = alpha * d11 + (-d01 * alpha + d01);\n\n  d_sliceI[py * nPixYMap + px] = beta * result_temp2 + (-result_temp1 * beta + result_temp1);\n}",
            "__global__ void differentiation_kernel(\n          double* __restrict__ d_pVolume,\n    const double* __restrict__ d_sliceI,\n    double tubeX,\n    double rtubeY,\n    double rtubeZ,\n    const double* __restrict__ const d_pObjX,\n    const double* __restrict__ const d_pObjY,\n    const double* __restrict__ const d_pObjZ,\n    const int nPixX,\n    const int nPixY,\n    const int nPixXMap,\n    const int nPixYMap,\n    const double du,\n    const double dv,\n    const double dx,\n    const double dy,\n    const double dz,\n    const int nz) \n{\n  const int px = blockIdx.x * blockDim.x + threadIdx.x;\n  const int py = blockIdx.y * blockDim.y + threadIdx.y;\n\n  /*\n     S.3. Differentiation - Eq. 24 - Liu et al (2017)\n\n     Detector integral projection\n     ___________\n     |_A_|_B_|___|\n     |_C_|_D_|___|\n     |___|___|___|\n\n\n     (px,py)\n     ________________\n     |_A_|__B__|_____|\n     |_C_|(0,0)|(0,1)|\n     |___|(1,0)|(1,1)|\n\n     Threads are lauched from D up to nPixX (py) and nPixY (px)\n     i.e., they are running on the detector image. Thread (0,0) is on D.\n\n     Coordinates on intergal projection:\n\n     A = py * nPixYMap + px\n     B = ((py+1) * nPixYMap) + px\n     C = py * nPixYMap + px + 1\n     D = ((py+1) * nPixYMap) + px + 1\n   */\n\n  if (px < nPixY && py < nPixX) {\n\n    const int pos = (nPixX*nPixY*nz) + (py * nPixY) + px;\n\n    int coordA = py * nPixYMap + px;\n    int coordB = ((py + 1) * nPixYMap) + px;\n    int coordC = coordA + 1;\n    int coordD = coordB + 1;\n\n    // x - ray angle in X coord\n    double gamma = atan((d_pObjX[py] + (dx / 2.0) - tubeX) / (rtubeZ - d_pObjZ[nz]));\n\n    // x - ray angle in Y coord\n    double alpha = atan((d_pObjY[px] + (dy / 2.0) - rtubeY) / (rtubeZ - d_pObjZ[nz]));\n\n    double dA, dB, dC, dD;\n\n    dA = d_sliceI[coordA];\n    dB = d_sliceI[coordB];\n    dC = d_sliceI[coordC];\n    dD = d_sliceI[coordD];\n\n    // Treat border of interpolated integral detector\n    if (dC == 0 && dD == 0) {\n      dC = dA;\n      dD = dB;\n    }\n\n    // S.3.Differentiation - Eq. 24 - Liu et al(2017)\n    d_pVolume[pos] += ((dD - dC - dB + dA)*(du*dv*dz / (cos(alpha)*cos(gamma)*dx*dy)));\n  }\n}",
            "__global__ void division_kernel(\n    double* d_img,\n    const int nPixX,\n    const int nPixY,\n    const int nSlices,\n    const int nProj)\n{\n  const int px = blockIdx.x * blockDim.x + threadIdx.x;\n  const int py = blockIdx.y * blockDim.y + threadIdx.y;\n  const int pz = blockIdx.z * blockDim.z + threadIdx.z;\n\n  if (px < nPixY && py < nPixX && pz < nSlices) {\n    const int pos = (nPixX*nPixY*pz) + (py * nPixY) + px;\n    d_img[pos] /= (double) nProj;\n  }\n}"
        ]
    },
    "atomicSystemWide-cuda": {
        "/Users/gbolet/hecbench-roofline/src/atomicSystemWide-cuda/main.cu": [
            "__global__ void atomicKernel(int *atom_arr, const int loop_num)\n{\n  int tid = blockDim.x * blockIdx.x + threadIdx.x;\n\n  for (int i=0; i < loop_num; i++)\n  {\n    // Atomic addition\n    atomicAdd_system(&atom_arr[0], 10);\n\n    // Atomic exchange\n    atomicExch_system(&atom_arr[1], tid);\n\n    // Atomic maximum\n    atomicMax_system(&atom_arr[2], tid);\n\n    // Atomic minimum\n    atomicMin_system(&atom_arr[3], tid);\n\n    // Atomic increment (modulo 17+1)\n    //atomicInc_system((unsigned int *)&atom_arr[4], 17);\n\n    // Atomic decrement\n    //atomicDec_system((unsigned int *)&atom_arr[5], 137);\n\n    // Atomic compare-and-swap\n    atomicCAS_system(&atom_arr[6], tid-1, tid);\n\n    // Bitwise atomic instructions\n\n    // Atomic AND\n    atomicAnd_system(&atom_arr[7], 2*tid+7);\n\n    // Atomic OR\n    atomicOr_system(&atom_arr[8], 1 << tid);\n\n    // Atomic XOR\n    atomicXor_system(&atom_arr[9], tid);\n  }\n}"
        ]
    },
    "intrinsics-simd-cuda": {
        "/Users/gbolet/hecbench-roofline/src/intrinsics-simd-cuda/main.cu": [
            "__global__\nvoid simd_intrinsics(const int n,\n                     const unsigned int* input,\n                           unsigned int* output)\n{\n  int i = threadIdx.x + blockIdx.x * blockDim.x;\n  if (i >= n) return;\n\n  unsigned int a = input[i];\n  unsigned int b = a + ((i % 2) ? 1 : -1);\n  unsigned int c = a ^ b; \n  unsigned int r;\n\n  r  = __vabs2(a);\n  r ^= __vabs4(a);\n  \n  r ^= __vabsdiffs2(a, b);\n  r ^= __vabsdiffs4(a, b);\n  r ^= __vabsdiffu2(a, b);\n  r ^= __vabsdiffu4(a, b);\n  \n  r ^= __vabsss2(a);\n  r ^= __vabsss4(a);\n  \n  r ^= __vadd2(a, b);\n  r ^= __vadd4(a, b);\n  \n  r ^= __vaddss2(a, b);\n  r ^= __vaddss4(a, b);\n  r ^= __vaddus2(a, b);\n  r ^= __vaddus4(a, b);\n  \n  r ^= __vavgs2(a, b);\n  r ^= __vavgs4(a, b);\n  r ^= __vavgu2(a, b);\n  r ^= __vavgu4(a, b);\n  \n  r ^= __vcmpeq2(a, b);\n  r ^= __vcmpeq4(a, b);\n  \n  r ^= __vcmpges2(a, b);\n  r ^= __vcmpges4(a, b);\n  r ^= __vcmpgeu2(a, b);\n  r ^= __vcmpgeu4(a, b);\n  \n  r ^= __vcmpgts2(a, b);\n  r ^= __vcmpgts4(a, b);\n  r ^= __vcmpgtu2(a, b);\n  r ^= __vcmpgtu4(a, b);\n  \n  r ^= __vcmples2(a, b);\n  r ^= __vcmples4(a, b);\n  r ^= __vcmpleu2(a, b);\n  r ^= __vcmpleu4(a, b);\n  \n  r ^= __vcmplts2(a, b);\n  r ^= __vcmplts4(a, b);\n  r ^= __vcmpltu2(a, b);\n  r ^= __vcmpltu4(a, b);\n  \n  r ^= __vcmpne2(a, b);\n  r ^= __vcmpne4(a, b);\n  \n  r ^= __vhaddu2(a, b);\n  r ^= __vhaddu4(a, b);\n  \n  r ^= __viaddmax_s16x2(a, b, c);\n  r ^= __viaddmax_s16x2_relu(a, b, c);\n  r ^= __viaddmax_s32(a, b, c);\n  r ^= __viaddmax_s32_relu(a, b, c);\n  r ^= __viaddmax_u16x2(a, b, c);\n  r ^= __viaddmax_u32(a, b, c);\n  \n  r ^= __viaddmin_s16x2(a, b, c);\n  r ^= __viaddmin_s16x2_relu(a, b, c);\n  r ^= __viaddmin_s32(a, b, c);\n  r ^= __viaddmin_s32_relu(a, b, c);\n  r ^= __viaddmin_u16x2(a, b, c);\n  r ^= __viaddmin_u32(a, b, c);\n  \n  bool pred   ;\n  bool pred_hi;\n  bool pred_lo;\n\n  r ^= __vibmax_s16x2(a, b, &pred_hi, &pred_lo);\n  r ^= __vibmax_s32(a, b, &pred);\n  r ^= __vibmax_u16x2(a, b, &pred_hi, &pred_lo);\n  r ^= __vibmax_u32(a, b, &pred);\n  \n  r ^= __vibmin_s16x2(a, b, &pred_hi, &pred_lo);\n  r ^= __vibmin_s32(a, b, &pred);\n  r ^= __vibmin_u16x2(a, b, &pred_hi, &pred_lo);\n  r ^= __vibmin_u32(a, b, &pred);\n  \n  r ^= __vimax3_s16x2(a, b, c);\n  r ^= __vimax3_s16x2_relu(a, b, c);\n  r ^= __vimax3_s32(a, b, c);\n  r ^= __vimax3_s32_relu(a, b, c);\n  r ^= __vimax3_u16x2(a, b, c);\n  r ^= __vimax3_u32(a, b, c);\n  \n  r ^= __vimax_s16x2_relu(a, b);\n  r ^= __vimax_s32_relu(a, b);\n  \n  r ^= __vimin3_s16x2(a, b, c);\n  r ^= __vimin3_s16x2_relu(a, b, c);\n  r ^= __vimin3_s32(a, b, c);\n  r ^= __vimin3_s32_relu(a, b, c);\n  r ^= __vimin3_u16x2(a, b, c);\n  r ^= __vimin3_u32(a, b, c);\n  \n  r ^= __vimin_s16x2_relu(a, b);\n  r ^= __vimin_s32_relu(a, b);\n  \n  r ^= __vmaxs2(a, b);\n  r ^= __vmaxs4(a, b);\n  r ^= __vmaxu2(a, b);\n  r ^= __vmaxu4(a, b);\n  \n  r ^= __vmins2(a, b);\n  r ^= __vmins4(a, b);\n  r ^= __vminu2(a, b);\n  r ^= __vminu4(a, b);\n  \n  r ^= __vneg2(a);\n  r ^= __vneg4(a);\n  r ^= __vnegss2(a);\n  r ^= __vnegss4(a);\n  \n  r ^= __vsads2(a, b);\n  r ^= __vsads4(a, b);\n  r ^= __vsadu2(a, b);\n  r ^= __vsadu4(a, b);\n  \n  r ^= __vseteq2(a, b);\n  r ^= __vseteq4(a, b);\n  \n  r ^= __vsetges2(a, b);\n  r ^= __vsetges4(a, b);\n  r ^= __vsetgeu2(a, b);\n  r ^= __vsetgeu4(a, b);\n  \n  r ^= __vsetgts2(a, b);\n  r ^= __vsetgts4(a, b);\n  r ^= __vsetgtu2(a, b);\n  r ^= __vsetgtu4(a, b);\n  \n  r ^= __vsetles2(a, b);\n  r ^= __vsetles4(a, b);\n  r ^= __vsetleu2(a, b);\n  r ^= __vsetleu4(a, b);\n  \n  r ^= __vsetlts2(a, b);\n  r ^= __vsetlts4(a, b);\n  r ^= __vsetltu2(a, b);\n  r ^= __vsetltu4(a, b);\n  \n  r ^= __vsetne2(a, b);\n  r ^= __vsetne4(a, b);\n  \n  r ^= __vsub2(a, b);\n  r ^= __vsub4(a, b);\n  r ^= __vsubss2(a, b);\n  r ^= __vsubss4(a, b);\n  r ^= __vsubus2(a, b);\n  r ^= __vsubus4(a, b);\n\n  output[i] = r;\n}"
        ]
    },
    "pathfinder-cuda": {
        "/Users/gbolet/hecbench-roofline/src/pathfinder-cuda/main.cu": [
            "inline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\ninline  __host__ __device__ float4 fminf(float4 a, float4 b)\n{\n    return make_float4(fminf(a.x,b.x), fminf(a.y,b.y), fminf(a.z,b.z), fminf(a.w,b.w));\n}\n\n__device__ __forceinline__ float MIN<float>(float in, float in2)\n{\n    return fminf(in, in2);\n}\n\n__global__ void pathfinder (\n    const int*__restrict__ gpuWall,\n    const int*__restrict__ gpuSrc,\n          int*__restrict__ gpuResult,\n          int*__restrict__ outputBuffer,\n    const int iteration,\n    const int theHalo,\n    const int borderCols,\n    const int cols,\n    const int t)\n{\n  int BLOCK_SIZE = blockDim.x;\n  int bx = blockIdx.x;\n  int tx = threadIdx.x;\n  __shared__ int prev[250];\n  __shared__ int result[250];\n\n  // Each block finally computes result for a small block\n  // after N iterations.\n  // it is the non-overlapping small blocks that cover\n  // all the input data\n\n  // calculate the small block size.\n  int small_block_cols = BLOCK_SIZE - (iteration*theHalo*2);\n\n  // calculate the boundary for the block according to\n  // the boundary of its small block\n  int blkX = (small_block_cols*bx) - borderCols;\n  int blkXmax = blkX+BLOCK_SIZE-1;\n\n  // calculate the global thread coordination\n  int xidx = blkX+tx;\n\n  // effective range within this block that falls within\n  // the valid range of the input data\n  // used to rule out computation outside the boundary.\n  int validXmin = (blkX < 0) ? -blkX : 0;\n  int validXmax = (blkXmax > cols-1) ? BLOCK_SIZE-1-(blkXmax-cols+1) : BLOCK_SIZE-1;\n\n  int W = tx-1;\n  int E = tx+1;\n\n  W = (W < validXmin) ? validXmin : W;\n  E = (E > validXmax) ? validXmax : E;\n\n  bool isValid = IN_RANGE(tx, validXmin, validXmax);\n\n  if(IN_RANGE(xidx, 0, cols-1))\n  {\n    prev[tx] = gpuSrc[xidx];\n  }\n\n  __syncthreads();\n\n  bool computed;\n  for (int i = 0; i < iteration; i++)\n  {\n    computed = false;\n\n    if( IN_RANGE(tx, i+1, BLOCK_SIZE-i-2) && isValid )\n    {\n      computed = true;\n      int left = prev[W];\n      int up = prev[tx];\n      int right = prev[E];\n      int shortest = MIN(left, up);\n      shortest = MIN(shortest, right);\n\n      int index = cols*(t+i)+xidx;\n      result[tx] = shortest + gpuWall[index];\n\n      // ===================================================================\n      // add debugging info to the debug output buffer...\n      if (tx==11 && i==0)\n      {\n        // set bufIndex to what value/range of values you want to know.\n        int bufIndex = gpuSrc[xidx];\n        // dont touch the line below.\n        outputBuffer[bufIndex] = 1;\n      }\n      // ===================================================================\n    }\n\n    __syncthreads();\n\n    if(i==iteration-1)\n    {\n      // we are on the last iteration, and thus don't need to \n      // compute for the next step.\n      break;\n    }\n\n    if(computed)\n    {\n      //Assign the computation range\n      prev[tx] = result[tx];\n    }\n    __syncthreads();\n  }\n\n  // update the global memory\n  // after the last iteration, only threads coordinated within the\n  // small block perform the calculation and switch on \"computed\"\n  if (computed)\n  {\n    gpuResult[xidx] = result[tx];\n  }\n}"
        ]
    },
    "dp-cuda": {
        "/Users/gbolet/hecbench-roofline/src/dp-cuda/main.cu": [
            "#define T ((int)32)\n\n\n__global__\nvoid dot_product(const T *__restrict__ a,\n                 const T *__restrict__ b,\n                       T *__restrict__ d,\n                 const size_t n)\n{\n  size_t iGID = blockIdx.x * blockDim.x + threadIdx.x;\n  T sum = 0;\n  for(size_t idx = iGID; idx < n; idx += gridDim.x * blockDim.x) {\n    size_t iInOffset = idx * 4;\n    sum += a[iInOffset    ] * b[iInOffset    ] +\n           a[iInOffset + 1] * b[iInOffset + 1] +\n           a[iInOffset + 2] * b[iInOffset + 2] +\n           a[iInOffset + 3] * b[iInOffset + 3];\n  }\n\n  using BlockReduce = cub::BlockReduce<T, 256>;\n  __shared__ typename BlockReduce::TempStorage temp_storage;\n  T aggregate = BlockReduce(temp_storage).Sum(sum);\n  if (threadIdx.x == 0) {\n    atomicAdd(d, aggregate);\n  }\n}"
        ]
    },
    "cmembench-cuda": {
        "/Users/gbolet/hecbench-roofline/src/cmembench-cuda/main.cu": [
            "__device__ __constant__ int constant_data[VECTOR_SIZE];\n\n#define T ((int)32)\n\n\ninline __host__ __device__ int4 make_int4(float4 a)\n{\n    return make_int4(int(a.x), int(a.y), int(a.z), int(a.w));\n}\n\ninline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\n__device__ void add_vector(int4 &target, const int4 &v) {\n  target.x += v.x;\n  target.y += v.y;\n  target.z += v.z;\n  target.w += v.w;\n}\n\n__device__ int4 init_vector(int v){\n  return make_int4(v, v, v, v);\n}\n\n__device__ float4 reduce_vector(float4 v1, float4 v2, float4 v3, float4 v4, float4 v5, float4 v6){\n  return make_float4(v1.x + v2.x + v3.x + v4.x + v5.x + v6.x, \n                     v1.y + v2.y + v3.y + v4.y + v5.y + v6.y,\n                     v1.z + v2.z + v3.z + v4.z + v5.z + v6.z,\n                     v1.w + v2.w + v3.w + v4.w + v5.w + v6.w);\n}\n\n__global__\nvoid benchmark_constant(int *output, int repeat)\n{\n  T* constant_data_p = (T*)constant_data;\n  T sum = init_vector<T>(0);\n\n  for(int i=0; i<4; i++){\n    for(int j=0; j<VECTOR_SIZE/(sizeof(T)/sizeof(int)); j+=4){\n      add_vector(sum, constant_data_p[j+i]);\n    }\n  }\n\n  if( threadIdx.x==0 && blockIdx.x==0 ) {\n    *output = reduce_vector(sum);\n  }\n}"
        ]
    },
    "jacobi-cuda": {
        "/Users/gbolet/hecbench-roofline/src/jacobi-cuda/main.cu": [
            "__global__ void jacobi_step (float*__restrict__ f, \n                             const float*__restrict__ f_old, \n                             float*__restrict__ error) {\n  __shared__ float f_old_tile[18][18];\n\n  int i = threadIdx.x + blockIdx.x * blockDim.x;\n  int j = threadIdx.y + blockIdx.y * blockDim.y;\n\n  // First read in the \"interior\" data, one value per thread\n  // Note the offset by 1, to reserve space for the \"left\"/\"bottom\" halo\n\n  f_old_tile[threadIdx.y+1][threadIdx.x+1] = f_old[IDX(i,j)];\n\n  // Now read in the halo data; we'll pick the \"closest\" thread\n  // to each element. When we do this, make sure we don't fall\n  // off the end of the global memory array. Note that this\n  // code does not fill the corners, as they are not used in\n  // this stencil.\n\n  if (threadIdx.x == 0 && i >= 1) {\n    f_old_tile[threadIdx.y+1][threadIdx.x+0] = f_old[IDX(i-1,j)];\n  }\n  if (threadIdx.x == 15 && i <= N-2) {\n    f_old_tile[threadIdx.y+1][threadIdx.x+2] = f_old[IDX(i+1,j)];\n  }\n  if (threadIdx.y == 0 && j >= 1) {\n    f_old_tile[threadIdx.y+0][threadIdx.x+1] = f_old[IDX(i,j-1)];\n  }\n  if (threadIdx.y == 15 && j <= N-2) {\n    f_old_tile[threadIdx.y+2][threadIdx.x+1] = f_old[IDX(i,j+1)];\n  }\n\n  // Synchronize all threads\n  __syncthreads();\n\n  float err = 0.0f;\n\n  if (j >= 1 && j <= N-2) {\n    if (i >= 1 && i <= N-2) {\n      // Perform the read from shared memory\n      f[IDX(i,j)] = 0.25f * (f_old_tile[threadIdx.y+1][threadIdx.x+2] + \n                             f_old_tile[threadIdx.y+1][threadIdx.x+0] + \n                             f_old_tile[threadIdx.y+2][threadIdx.x+1] + \n                             f_old_tile[threadIdx.y+0][threadIdx.x+1]);\n      float df = f[IDX(i,j)] - f_old_tile[threadIdx.y+1][threadIdx.x+1];\n      err = df * df;\n    }\n  }\n\n  // Sum over threads in the warp\n  // For simplicity, we do this outside the above conditional\n  // so that all threads participate\n  for (int offset = 8; offset > 0; offset /= 2) {\n    err += __shfl_down_sync(0xffffffff, err, offset);\n  }\n\n  // If we're thread 0 in the warp, update our value to shared memory\n  // Note that we're assuming exactly a 16x16 block and that the warp ID\n  // is equivalent to threadIdx.y. For the general case, we would have to\n  // write more careful code.\n  __shared__ float reduction_array[16];\n  if (threadIdx.x == 0) {\n    reduction_array[threadIdx.y] = err;\n  }\n\n  // Synchronize the block before reading any values from smem\n  __syncthreads();\n\n  // Using the first warp in the block, reduce over the partial sums\n  // in the shared memory array.\n  if (threadIdx.y == 0) {\n    err = reduction_array[threadIdx.x];\n    for (int offset = 8; offset > 0; offset /= 2) {\n      err += __shfl_down_sync(0xffffffff, err, offset);\n    }\n    if (threadIdx.x == 0) {\n      atomicAdd(error, err);\n    }\n  }\n}",
            "__global__ void swap_data (const float*__restrict__ f,\n                                 float*__restrict__ f_old) {\n  int i = threadIdx.x + blockIdx.x * blockDim.x;\n  int j = threadIdx.y + blockIdx.y * blockDim.y;\n\n  if (j >= 1 && j <= N-2) {\n    if (i >= 1 && i <= N-2) {\n      f_old[IDX(i,j)] = f[IDX(i,j)];\n    }\n  }\n}"
        ]
    },
    "ss-cuda": {
        "/Users/gbolet/hecbench-roofline/src/ss-cuda/kernels.cu": [
            "__device__\nint compare(const uchar* text, const uchar* pattern, uint length)\n{\n  for(uint l=0; l<length; ++l)\n  {\n    if (TOLOWER(text[l]) != pattern[l]) return 0;\n  }\n  return 1;\n}\n\n__global__ void \nStringSearchNaive (\n    const uchar* text,\n    const uint textLength,\n    const uchar* pattern,\n    const uint patternLength,\n    uint* resultBuffer,\n    uint* resultCountPerWG,\n    const uint maxSearchLength)\n{\n  extern __shared__ uchar localPattern[];\n  __shared__ uint groupSuccessCounter;\n\n  int localIdx = threadIdx.x;\n  int localSize = blockDim.x;\n  int groupIdx = blockIdx.x;\n\n  // Last search idx for all work items\n  uint lastSearchIdx = textLength - patternLength + 1;\n\n  // global idx for all work items in a WorkGroup\n  uint beginSearchIdx = groupIdx * maxSearchLength;\n  uint endSearchIdx = beginSearchIdx + maxSearchLength;\n  if(beginSearchIdx > lastSearchIdx) return;\n  if(endSearchIdx > lastSearchIdx) endSearchIdx = lastSearchIdx;\n\n  // Copy the pattern from global to local buffer\n  for(int idx = localIdx; idx < patternLength; idx+=localSize)\n  {\n    localPattern[idx] = TOLOWER(pattern[idx]);\n  }\n\n  if(localIdx == 0) groupSuccessCounter = 0;\n  __syncthreads();\n\n  // loop over positions in global buffer\n  for(uint stringPos=beginSearchIdx+localIdx; stringPos<endSearchIdx; stringPos+=localSize)\n  {\n    if (compare(text+stringPos, localPattern, patternLength) == 1)\n    {\n      int count = atomicAdd(&groupSuccessCounter, (uint)1);\n      resultBuffer[beginSearchIdx+count] = stringPos;\n    }\n  }\n\n  __syncthreads();\n  if(localIdx == 0) resultCountPerWG[groupIdx] = groupSuccessCounter;\n}",
            "__device__\nint compare(const uchar* text, const uchar* pattern, uint length)\n{\n  for(uint l=0; l<length; ++l)\n  {\n    if (TOLOWER(text[l]) != pattern[l]) return 0;\n  }\n  return 1;\n}\n\n__global__ void \nStringSearchLoadBalance (\n    const uchar* text,\n    const uint textLength,\n    const uchar* pattern,\n    const uint patternLength,\n    uint* resultBuffer,\n    uint* resultCountPerWG,\n    const uint maxSearchLength)\n{\n  extern __shared__ uchar localPattern[];\n  __shared__ uint stack1[LOCAL_SIZE*2];\n  __shared__ uint stack2[LOCAL_SIZE*2];\n  __shared__ uint stack1Counter;\n  __shared__ uint stack2Counter;\n  __shared__ uint groupSuccessCounter;\n\n  int localIdx = threadIdx.x;\n  int localSize = blockDim.x;\n  int groupIdx = blockIdx.x;\n\n  // Initialize the local variaables\n  if(localIdx == 0)\n  {\n    groupSuccessCounter = 0;\n    stack1Counter = 0;\n    stack2Counter = 0;\n  }\n\n  // Last search idx for all work items\n  uint lastSearchIdx = textLength - patternLength + 1;\n  uint stackSize = 0;\n\n  // global idx for all work items in a WorkGroup\n  uint beginSearchIdx = groupIdx * maxSearchLength;\n  uint endSearchIdx = beginSearchIdx + maxSearchLength;\n  if(beginSearchIdx > lastSearchIdx) return;\n  if(endSearchIdx > lastSearchIdx) endSearchIdx = lastSearchIdx;\n  uint searchLength = endSearchIdx - beginSearchIdx;\n\n  // Copy the pattern from global to local buffer\n  for(uint idx = localIdx; idx < patternLength; idx+=localSize)\n  {\n    localPattern[idx] = TOLOWER(pattern[idx]);\n  }\n\n  __syncthreads();\n\n  uchar first = localPattern[0];\n  uchar second = localPattern[1];\n  int stringPos = localIdx;\n  int stackPos = 0;\n  int revStackPos = 0;\n\n  while (true)    // loop over positions in global buffer\n  {\n\n    // Level-1 : Quick filter on 2 char match and store the good positions on stack1.\n    if(stringPos < searchLength)\n    {\n      // Queue the initial match positions. Make sure queue has sufficient positions for each work-item.\n      if ((first == TOLOWER(text[beginSearchIdx+stringPos])) && (second == TOLOWER(text[beginSearchIdx+stringPos+1])))\n      {\n        stackPos = atomicAdd(&stack1Counter, (uint)1);\n        stack1[stackPos] = stringPos;\n      }\n    }\n\n    stringPos += localSize;     // next search idx\n\n    __syncthreads();\n    stackSize = stack1Counter;\n    __syncthreads();\n\n    // continue until stack1 has sufficient good positions for proceed to next Level\n    if((stackSize < localSize) && ((((stringPos)/localSize)*localSize) < searchLength)) continue;\n\n\n#ifdef ENABLE_2ND_LEVEL_FILTER\n    // Level-2 : (Processing the stack1 and filling the stack2) For large patterns roll over\n    // another 8-bytes from the positions in stack1 and store the match positions in stack2.\n    if(localIdx < stackSize)\n    {\n      revStackPos = atomicSub(&stack1Counter, (uint)1);\n      uint pos = stack1[--revStackPos];\n      bool status = (localPattern[2] == TOLOWER(text[beginSearchIdx+pos+2]));\n      status = status && (localPattern[3] == TOLOWER(text[beginSearchIdx+pos+3]));\n      status = status && (localPattern[4] == TOLOWER(text[beginSearchIdx+pos+4]));\n      status = status && (localPattern[5] == TOLOWER(text[beginSearchIdx+pos+5]));\n      status = status && (localPattern[6] == TOLOWER(text[beginSearchIdx+pos+6]));\n      status = status && (localPattern[7] == TOLOWER(text[beginSearchIdx+pos+7]));\n      status = status && (localPattern[8] == TOLOWER(text[beginSearchIdx+pos+8]));\n      status = status && (localPattern[9] == TOLOWER(text[beginSearchIdx+pos+9]));\n\n      if (status)\n      {\n        stackPos = atomicAdd(&stack2Counter, (uint)1);\n        stack2[stackPos] = pos;\n      }\n    }\n\n    __syncthreads();\n    stackSize = stack2Counter;\n    __syncthreads();\n\n    // continue until stack2 has sufficient good positions proceed to next level\n    if((stackSize < localSize) && ((((stringPos)/localSize)*localSize) < searchLength)) continue;\n#endif\n\n\n    // Level-3 : (Processing stack1/stack2) Check the remaining positions.\n    if(localIdx < stackSize)\n    {\n#ifdef ENABLE_2ND_LEVEL_FILTER\n      revStackPos = atomicSub(&stack2Counter, (uint)1);\n      int pos = stack2[--revStackPos];\n      if (compare(text+beginSearchIdx+pos+10, localPattern+10, patternLength-10) == 1)\n#else\n      revStackPos = atomicSub(&stack1Counter, (uint)1);\n      int pos = stack1[--revStackPos];\n      if (compare(text+beginSearchIdx+pos+2, localPattern+2, patternLength-2) == 1)\n#endif\n      {\n        // Full match found\n        int count = atomicAdd(&groupSuccessCounter, (uint)1);\n        resultBuffer[beginSearchIdx+count] = beginSearchIdx+pos;\n      }\n    }\n\n    __syncthreads();\n    if((((stringPos/localSize)*localSize) >= searchLength) && \n        (stack1Counter <= 0) && (stack2Counter <= 0)) break;\n  }\n\n  if(localIdx == 0) resultCountPerWG[groupIdx] = groupSuccessCounter;\n}"
        ]
    },
    "heat2d-cuda": {
        "/Users/gbolet/hecbench-roofline/src/heat2d-cuda/main.cu": [
            "__global__ void\ndev_lapl_iter(float *out, const float *in, const float delta, const float norm, const int lx, const int ly)\n{\n  int i = blockIdx.x * blockDim.x + threadIdx.x;\n  int x = i % lx;\n  int y = i / lx;\n  int v00 = y*lx + x;\n  int v0p = y*lx + (x + 1)%lx;\n  int v0m = y*lx + (lx + x - 1)%lx;\n  int vp0 = ((y+1)%ly)*lx + x;\n  int vm0 = ((ly+y-1)%ly)*lx + x;\n  out[v00] = norm*in[v00]\n    + delta*(in[v0p] + in[v0m] + in[vp0] + in[vm0]);\n  return;\n}"
        ]
    },
    "bmf-cuda": {
        "/Users/gbolet/hecbench-roofline/src/bmf-cuda/src/bit_vector_kernels.cuh": [
            "__global__ void initFactor(\n  bit_vector_t * Ab,\n  const index_t height,\n  const uint8_t factorDim,\n  const uint32_t seed, \n  const float threshold)\n{\n  const index_t tid = threadIdx.x + blockIdx.x * blockDim.x;\n\n  if(tid < height) {\n    bit_vector_t Ai = 0;\n\n    const int randDepth = -log2f(threshold)+1;\n    // if threshold very small simply initilize as 0s (also catch threshold=0)\n    if(randDepth < 16) {\n      fast_kiss_state32_t state = get_initial_fast_kiss_state32(seed + tid);\n\n      Ai = ~bit_vector_t(0) >> (32-factorDim);\n      for(int d=0; d<randDepth; ++d)\n        Ai &= fast_kiss32(state);\n    }\n    Ab[tid] = Ai;\n  }\n}",
            "#define T ((int)32)\n\n\n__inline__ __device__ T warpReduceSum(T val)\n{\n#pragma unroll\n  for (int mask = 16; mask > 0; mask >>= 1)\n    val += __shfl_xor_sync(FINAL_MASK, val, mask, 32);\n  return val;\n}\n\n__inline__ __device__ T blockReduceSum(T val)\n{\n  static __shared__ T shared[32];\n  int lane = threadIdx.x & 0x1f;\n  int wid = threadIdx.x >> 5;\n\n  val = warpReduceSum<T>(val);\n\n  if (lane == 0)\n    shared[wid] = val;\n\n  __syncthreads();\n\n  // Modify from blockDim.x << 5 to blockDim.x / 32. to prevent\n  // blockDim.x is not divided by 32\n  val = (threadIdx.x < (blockDim.x / 32.f)) ? shared[lane] : (T)(0.0f);\n  val = warpReduceSum<T>(val);\n\n  return val;\n}\n\n__global__ void computeDistanceRowsShared(\n  const bit_factor_t * __restrict__ Ab,\n  const bit_factor_t * __restrict__ Bb,\n  const bit_matrix_t * __restrict__ Cb, \n  const index_t height,\n  const index_t width,\n  const index_t padded_width,\n  const uint8_t factorDim,\n  const error_t weight,\n  error_t *global_error)\n{\n  __shared__ bit_factor_t B_block[ 32 * WARPSPERBLOCK ];\n  __shared__ bit_matrix_t C_block[ 32 * WARPSPERBLOCK ];\n\n  const index_t warpIdIntern = threadIdx.x / warpSize;\n  const index_t warpId = blockIdx.x * WARPSPERBLOCK + warpIdIntern;\n  const index_t warpLane = threadIdx.x % warpSize;\n\n  const index_t blockSize = WARPSPERBLOCK*32;\n\n  const index_t i = warpId;\n  const bit_factor_t A_i = i < height ? Ab[i] : 0;\n\n  const index_t vecRow = i / 32;\n  const index_t vecFirst = vecRow * padded_width;\n  const index_t vecLane = i % 32;\n  const index_t col_in_tile = warpLane;\n  const index_t padded_width_blocks = SDIV(width, blockSize) * blockSize;\n  error_t error_thread = 0;\n  for (index_t j = threadIdx.x; j < padded_width_blocks; j += blockSize) {\n    B_block[threadIdx.x] = (j < width) ? Bb[j] : 0;\n    C_block[threadIdx.x] = (j < width) ? Cb[vecFirst + j] : 0;\n    __syncthreads();\n\n    if(i < height) {\n      #pragma unroll\n      for(index_t w = 0; w < WARPSPERBLOCK; ++w) {\n        const bit_factor_t B_j = B_block[w*warpSize + warpLane];\n\n        const int C_ij = (C_block[w*warpSize + col_in_tile] >> vecLane) & 1;\n\n        const int product = (B_j & A_i) ? 1 : 0;\n\n        error_thread += error_measure(product, C_ij, weight);\n      }\n    }\n    __syncthreads();\n  }\n\n  __shared__ error_t reductionArray[WARPSPERBLOCK];\n  const error_t error_block = blockReduceSum(error_thread, reductionArray);\n  // Thread with threadIdx.x==0 now has total error of block\n\n  if (threadIdx.x == 0)\n    atomicAdd(global_error, error_block);\n}",
            "#define T ((int)32)\n\n\n__inline__ __device__ T warpReduceSum(T val)\n{\n#pragma unroll\n  for (int mask = 16; mask > 0; mask >>= 1)\n    val += __shfl_xor_sync(FINAL_MASK, val, mask, 32);\n  return val;\n}\n\n__inline__ __device__ T blockReduceSum(T val)\n{\n  static __shared__ T shared[32];\n  int lane = threadIdx.x & 0x1f;\n  int wid = threadIdx.x >> 5;\n\n  val = warpReduceSum<T>(val);\n\n  if (lane == 0)\n    shared[wid] = val;\n\n  __syncthreads();\n\n  // Modify from blockDim.x << 5 to blockDim.x / 32. to prevent\n  // blockDim.x is not divided by 32\n  val = (threadIdx.x < (blockDim.x / 32.f)) ? shared[lane] : (T)(0.0f);\n  val = warpReduceSum<T>(val);\n\n  return val;\n}\n\n__global__ void computeDistanceRows(\n  const bit_factor_t * __restrict__ Ab,\n  const bit_factor_t * __restrict__ Bb,\n  const bit_matrix_t * __restrict__ Cb, \n  const index_t height, const index_t width,\n  const index_t padded_width,\n  const uint8_t factorDim,\n  const int weight,\n  error_t *global_error)\n{\n  const index_t warpId = (threadIdx.x + blockIdx.x * blockDim.x) / warpSize;\n  const index_t warpLane = threadIdx.x % warpSize;\n\n  const index_t i = warpId;\n  error_t error_thread = 0;\n  if (i < height) {\n    const bit_factor_t A_i = Ab[i];\n\n    for (index_t j = warpLane; j < width; j += warpSize) {\n      const int product = (A_i & Bb[j]) ? 1 : 0;\n\n      const index_t vecId = i / 32 * padded_width + j;\n      const index_t vecLane = i % 32;\n      const int C_ij = (Cb[vecId] >> vecLane) & 1;\n\n      error_thread += error_measure(product, C_ij, weight);\n    }\n  }\n\n  __shared__ error_t reductionArray[WARPSPERBLOCK];\n  const error_t error_block = blockReduceSum(error_thread, reductionArray);\n  // Thread with threadIdx.x==0 now has total error of block\n\n  if (threadIdx.x == 0)\n    atomicAdd(global_error, error_block);\n}",
            "#define T ((int)32)\n\n\n__inline__ __device__ T warpReduceSum(T val)\n{\n#pragma unroll\n  for (int mask = 16; mask > 0; mask >>= 1)\n    val += __shfl_xor_sync(FINAL_MASK, val, mask, 32);\n  return val;\n}\n\n__global__ void vectorMatrixMultCompareRowWarpShared(\n        bit_factor_t * __restrict__ A,\n  const bit_factor_t * __restrict__ B,\n  const bit_matrix_t * __restrict__ C,\n  const index_t height,\n  const index_t width,\n  const index_t padded_width,\n  const uint8_t factorDim,\n  const index_t startrow,\n  error_t *global_error,\n  const uint32_t seed, \n  const float temperature,\n  const float flipManyChance,\n  const uint32_t flipManyDepth,\n  const error_t weight)\n{\n  __shared__ bit_factor_t B_block[ 32 * WARPSPERBLOCK ];\n  __shared__ bit_matrix_t C_block[ 32 * WARPSPERBLOCK ];\n\n  const index_t warpId = blockIdx.x * WARPSPERBLOCK + threadIdx.x / warpSize;\n  const index_t warpLane = threadIdx.x % warpSize;\n\n  const index_t padded_height_blocks = SDIV(height, WARPSPERBLOCK) * WARPSPERBLOCK;\n  const index_t i = (startrow + warpId) % padded_height_blocks;\n\n  fast_kiss_state32_t state;\n\n  const bit_factor_t A_i = i < height ? A[i] : 0;\n  bit_factor_t A_i_changed = 0;\n  if (i < height) {\n    state = get_initial_fast_kiss_state32(seed + warpId);\n\n    A_i_changed = A_i ^ get_flip_mask(factorDim, state, flipManyChance, flipManyDepth);\n  }\n\n  const index_t vecRow = i / 32;\n  const index_t vecFirst = vecRow * padded_width;\n  const index_t vecLane = i % 32;\n  const index_t col_in_tile = warpLane;\n  const index_t padded_width_blocks = SDIV(width, WARPSPERBLOCK*32) * WARPSPERBLOCK*32;\n  error_t error_thread = 0;\n  for (index_t j = threadIdx.x; j < padded_width_blocks; j += WARPSPERBLOCK*32) {\n    B_block[threadIdx.x] = (j < width) ? B[j] : 0;\n    C_block[threadIdx.x] = (j < width) ? C[vecFirst + j] : 0;\n    __syncthreads();\n\n    if(i < height) {\n      #pragma unroll\n      for(index_t w = 0; w < WARPSPERBLOCK; ++w) {\n        const bit_factor_t B_j = B_block[w*warpSize + warpLane];\n        const int C_ij = (C_block[w*warpSize + col_in_tile] >> vecLane) & 1;\n\n        const int product_new = (B_j & A_i_changed) ? 1 : 0;\n        const int product_old = (B_j & A_i        ) ? 1 : 0;\n\n        error_thread += error_measure(product_new, C_ij, weight)\n          - error_measure(product_old, C_ij, weight);\n      }\n    }\n    __syncthreads();\n  }\n  if(i < height) {\n    const error_t error_warp = warpReduceSum(error_thread);\n    // Thread with warpLane==0 now has total error of warp\n\n    // Thread 0 checks if new low has been found and applies if necessary\n    if (warpLane == 0) {\n      // Metropolis\u2013Hastings algorithm\n      if (metro(state, error_warp, temperature, width)) {\n        A[i] = A_i_changed;\n        atomicAdd(global_error, error_warp);\n      }\n    }\n  }\n}",
            "#define T ((int)32)\n\n\n__inline__ __device__ T warpReduceSum(T val)\n{\n#pragma unroll\n  for (int mask = 16; mask > 0; mask >>= 1)\n    val += __shfl_xor_sync(FINAL_MASK, val, mask, 32);\n  return val;\n}\n\n__global__ void vectorMatrixMultCompareColWarpShared(\n  const bit_factor_t * __restrict__ A,\n  bit_factor_t * __restrict__ B,\n  const bit_matrix_t * __restrict__ C,\n  const index_t height,\n  const index_t width,\n  const index_t padded_width,\n  const uint8_t factorDim,\n  const index_t startcol,\n  error_t *global_error,\n  const uint32_t seed,\n  const float temperature,\n  const float flipManyChance,\n  const uint32_t flipManyDepth,\n  const error_t weight)\n{\n  __shared__ bit_factor_t A_block[32*WARPSPERBLOCK];\n  __shared__ bit_matrix_t C_block[32*WARPSPERBLOCK];\n\n  const index_t warpIdIntern = threadIdx.x / warpSize;\n  const index_t warpId = blockIdx.x * WARPSPERBLOCK + warpIdIntern;\n  const index_t warpLane = threadIdx.x % warpSize;\n\n  const index_t padded_width_blocks = SDIV(width, WARPSPERBLOCK) * WARPSPERBLOCK;\n  const index_t j = (startcol + warpId) % padded_width_blocks;\n\n  fast_kiss_state32_t state;\n\n  const bit_factor_t B_j = j < width ? B[j] : 0;\n  bit_factor_t B_j_changed = 0;\n  if (j < width) {\n    state = get_initial_fast_kiss_state32(seed + warpId);\n\n    B_j_changed = B_j ^ get_flip_mask(factorDim, state, flipManyChance, flipManyDepth);\n  }\n\n  error_t error_thread = 0;\n  const index_t vecLane = warpLane;\n  const index_t col_in_tile = j % 32;\n  const index_t colFirst = j / 32 * 32;\n  const index_t padded_height_blocks = SDIV(height, WARPSPERBLOCK*32) * WARPSPERBLOCK*32;\n  for (index_t i = threadIdx.x; i < padded_height_blocks; i += WARPSPERBLOCK*32) {\n    A_block[threadIdx.x] = (i < height) ? A[i] : 0;\n    const index_t vecRow = i / 32;\n    const index_t vecFirst = vecRow * padded_width + colFirst;\n    C_block[threadIdx.x] = (vecRow < SDIV(height,32)) ? C[vecFirst + warpLane] : 0;\n    __syncthreads();\n\n    if (j < width) {\n      #pragma unroll\n      for(index_t w = 0; w < WARPSPERBLOCK; ++w) {\n        const bit_factor_t A_i = A_block[w*warpSize + warpLane];\n        const int C_ij = (C_block[w*warpSize + col_in_tile] >> vecLane) & 1;\n\n        const int product_new = (A_i & B_j_changed) ? 1 : 0;\n        const int product_old = (A_i & B_j        ) ? 1 : 0;\n\n        error_thread += error_measure(product_new, C_ij, weight)\n          - error_measure(product_old, C_ij, weight);\n      }\n    }\n    __syncthreads();\n  }\n  if (j < width) {\n    const error_t error_warp = warpReduceSum(error_thread);\n    // Thread with warpLane==0 now has total error of warp\n\n    // Thread 0 checks if new low has been found and applies if necessary\n    if (warpLane == 0) {\n      // Metropolis\u2013Hastings algorithm\n      if (metro(state, error_warp, temperature, height)) {\n        B[j] = B_j_changed;\n        atomicAdd(global_error, error_warp);\n      }\n    }\n  }\n}"
        ]
    },
    "mt-cuda": {
        "/Users/gbolet/hecbench-roofline/src/mt-cuda/MT.cu": [
            "inline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\ninline __host__ __device__ float4 sqrtf(float4 v)\n{\n    return make_float4(sqrtf(v.x), sqrtf(v.y), sqrtf(v.z), sqrtf(v.w));\n}\n\n__device__\nvoid BoxMullerTrans(float *u1, float *u2)\n{\n  const float   r = sqrtf(-2.0f * logf(*u1));\n  const float phi = 2 * PI * (*u2);\n  *u1 = r * cosf(phi);\n  *u2 = r * sinf(phi);\n}\n\n__global__ void boxmuller (float* Rand, const int nPerRng) \n{\n  int globalID = blockDim.x * blockIdx.x + threadIdx.x;\n  for (int iOut = 0; iOut < nPerRng; iOut += 2) {\n    BoxMullerTrans(&Rand[globalID + (iOut + 0) * MT_RNG_COUNT],\n        &Rand[globalID + (iOut + 1) * MT_RNG_COUNT]);\n  }\n}",
            "__device__ __shared__ uint32 mt[MERS_N];\n\n__global__ void mt (const mt_struct_stripped* MT, float* Rand, const int nPerRng) \n{\n  int globalID = blockDim.x * blockIdx.x + threadIdx.x;\n\n  int iState, iState1, iStateM, iOut;\n  unsigned int mti, mti1, mtiM, x;\n  unsigned int mt[MT_NN], matrix_a, mask_b, mask_c; \n\n  //Load bit-vector Mersenne Twister parameters\n  matrix_a = MT[globalID].matrix_a;\n  mask_b   = MT[globalID].mask_b;\n  mask_c   = MT[globalID].mask_c;\n\n  //Initialize current state\n  mt[0] = MT[globalID].seed;\n  for (iState = 1; iState < MT_NN; iState++)\n    mt[iState] = (1812433253U * (mt[iState - 1] ^ (mt[iState - 1] >> 30)) + iState) & MT_WMASK;\n\n  iState = 0;\n  mti1 = mt[0];\n  for (iOut = 0; iOut < nPerRng; iOut++) {\n    iState1 = iState + 1;\n    iStateM = iState + MT_MM;\n    if(iState1 >= MT_NN) iState1 -= MT_NN;\n    if(iStateM >= MT_NN) iStateM -= MT_NN;\n    mti  = mti1;\n    mti1 = mt[iState1];\n    mtiM = mt[iStateM];\n\n    // MT recurrence\n    x = (mti & MT_UMASK) | (mti1 & MT_LMASK);\n    x = mtiM ^ (x >> 1) ^ ((x & 1) ? matrix_a : 0);\n\n    mt[iState] = x;\n    iState = iState1;\n\n    //Tempering transformation\n    x ^= (x >> MT_SHIFT0);\n    x ^= (x << MT_SHIFTB) & mask_b;\n    x ^= (x << MT_SHIFTC) & mask_c;\n    x ^= (x >> MT_SHIFT1);\n\n    //Convert to (0, 1] float and write to global memory\n    Rand[globalID + iOut * MT_RNG_COUNT] = ((float)x + 1.0f) / 4294967296.0f;\n  }\n}"
        ]
    },
    "mmcsf-cuda": {
        "/Users/gbolet/hecbench-roofline/src/mmcsf-cuda/kernels.cu": [
            "#define DTYPE float\n\n\n#define ITYPE size_t // if chnage to unsigned int change the grid.x and gID in cuda kernel computation to long\n\n\n__global__ void mttkrp_MIHCSR_kernel_slc_atomic_fbrLvlPar(\n  const DTYPE *__restrict__ vals,\n  const ITYPE *__restrict__ fbrLikeSlcInds,\n  const ITYPE *__restrict__ dInds2, \n  const ITYPE *__restrict__ fbrPtr0,\n  const ITYPE *__restrict__ fbrPtr1,\n  const ITYPE *__restrict__ fbrIdx1,\n  ITYPE nFibers,\n        DTYPE *__restrict__ dU0,\n  const DTYPE *__restrict__ dU1,\n  const DTYPE *__restrict__ dU2, \n  ITYPE  mode,\n  ITYPE R,\n  ITYPE warpPerSlice,\n  int logOfWPC,\n  int fbrPerWarp,\n  int logOfFPW)\n{\n\n  ITYPE tId = threadIdx.x;\n  ITYPE laneId = tId & 31;\n  ITYPE bdim = blockDim.x;\n  ITYPE gId = (blockIdx.x * bdim + tId);\n  ITYPE workId = (tId & ((1 << (5 + logOfWPC)) - 1)) >> 5;  //tId >> 5; //tId >> 5;//\n  ITYPE fbr = (gId >> (5 + logOfWPC)) << logOfFPW; // 5: minimum 1 WARP (2^5) // blockIdx.x ;//\n\n  DTYPE tmp = 0, tmp_val;\n\n  if(fbr < nFibers - 1){       \n\n    tmp_val = 0;\n    bool diffFiber = false;\n    unsigned int idx0;\n\n    for (int fr = 0; fr < fbrPerWarp && (fbr+fr) < (nFibers - 1); ++fr){\n\n      diffFiber = false;\n      unsigned int idx1 = fbrIdx1[fbr+fr];// dInds1[fbrPtr1[fbr]];  \n      idx0 = fbrLikeSlcInds[fbr+fr];//slc;  \n      tmp_val = 0;\n\n      for(unsigned int x = fbrPtr1[fbr+fr] + workId; x < fbrPtr1[fbr+fr+1]; x+=warpPerSlice) {\n\n        unsigned int idx2 = dInds2[x];                    \n\n        for(unsigned int r=laneId; r<R; r+=32) {\n          tmp_val += vals[x] * dU2[idx2 * R + r]; //2MR   \n        }       \n      }\n\n      for(unsigned int r=laneId; r<R; r+=32) { \n        tmp += tmp_val * dU1[idx1 * R + r] ; //2PR\n      } \n\n      if(fbrLikeSlcInds[fbr+fr] != fbrLikeSlcInds[fbr+fr+1]) {\n\n        diffFiber = true;\n        for(unsigned int r=laneId; r<R; r+=32) { \n          atomicAdd(&dU0[idx0 * R + r], tmp); //2PR\n        } \n        tmp = 0;\n      }\n    } \n\n    if(!diffFiber) {  \n      for(unsigned int r=laneId; r<R; r+=32) { \n        atomicAdd(&dU0[idx0 * R + r], tmp); \n      }  \n    }  \n  }\n}",
            "#define DTYPE float\n\n\n#define ITYPE size_t // if chnage to unsigned int change the grid.x and gID in cuda kernel computation to long\n\n\n__global__ void mttkrp_MIHCSR_kernel_slc_atomic_fbrLvlPar_4D(\n  const DTYPE *__restrict__ vals,\n  const ITYPE *__restrict__ fbrLikeSlcInds,\n  const ITYPE *__restrict__ dInds3, \n  const ITYPE *__restrict__ fbrPtr0,\n  const ITYPE *__restrict__ fbrPtr1,\n  const ITYPE *__restrict__ fbrIdx1,\n  const ITYPE *__restrict__ fbrPtr2,\n  const ITYPE *__restrict__ fbrIdx2,\n  ITYPE nFibers,\n        DTYPE *__restrict__ dU0, \n  const DTYPE *__restrict__ dU1,\n  const DTYPE *__restrict__ dU2,\n  const DTYPE *__restrict__ dU3,\n  ITYPE  mode,\n  ITYPE R,\n  ITYPE warpPerSlice,\n  int logOfWPC,\n  int fbrPerWarp,\n  int logOfFPW)\n{\n  ITYPE tId = threadIdx.x;\n  ITYPE laneId = tId & 31;\n  ITYPE bdim = blockDim.x;\n  ITYPE gId = (blockIdx.x * bdim + tId);\n  ITYPE workId = (tId & ((1 << (5 + logOfWPC)) - 1)) >> 5;  //tId >> 5; //tId >> 5;//\n  ITYPE fbrS = (gId >> (5 + logOfWPC)) << logOfFPW; // 5: minimum 1 WARP (2^5) // blockIdx.x ;//\n  DTYPE tmp = 0, tmp_val, tmp2= 0;\n\n  if(fbrS < nFibers - 1){       \n\n    tmp_val = 0;\n    bool diffFiber = false;\n    unsigned int idx0;\n\n    for (int fr = 0; fr < fbrPerWarp && (fbrS+fr) < (nFibers - 1); ++fr){\n\n      diffFiber = false;\n      unsigned int idx1 = fbrIdx1[fbrS+fr];// dInds1[fbrPtr1[fbr]];  \n      idx0 = fbrLikeSlcInds[fbrS+fr];//slc;  \n      tmp = 0;\n\n      for (int fbr = fbrPtr1[fbrS+fr] + workId; fbr < fbrPtr1[fbrS+fr+1]; fbr+=warpPerSlice){\n        ITYPE idx2 = fbrIdx2[fbr];\n        tmp_val = 0;\n\n        for(unsigned int x = fbrPtr2[fbr]; x < fbrPtr2[fbr+1]; x++) {\n\n          unsigned int idx3 = dInds3[x];\n\n          for(unsigned int r=laneId; r<R; r+=32) {\n            tmp_val += vals[x] * dU3[idx3 * R + r]; //2MR   \n          }       \n        }\n\n        for(unsigned int r=laneId; r<R; r+=32) { \n          tmp += tmp_val * dU2[idx2 * R + r] ;\n        } \n      }\n      for(unsigned int r=laneId; r<R; r+=32) { \n        tmp2 += tmp * dU1[idx1 * R + r] ;\n      } \n\n      if(fbrLikeSlcInds[fbrS+fr] != fbrLikeSlcInds[fbrS+fr+1]) {\n\n        diffFiber = true;\n        for(unsigned int r=laneId; r<R; r+=32) { \n          atomicAdd(&dU0[idx0 * R + r], tmp2); //2PR\n        } \n        tmp2 = 0;\n      }\n    }\n\n    if(!diffFiber) {  \n      for(unsigned int r=laneId; r<R; r+=32) \n        atomicAdd(&dU0[idx0 * R + r], tmp2); //2PR           \n    }  \n  }\n}",
            "#define DTYPE float\n\n\n#define ITYPE size_t // if chnage to unsigned int change the grid.x and gID in cuda kernel computation to long\n\n\n__global__ void mttkrp_MIHCSR_kernel_fbrS_atomic_fbrLvlPar_4D(\n  const DTYPE *__restrict__ vals,\n  const ITYPE *__restrict__ fbrLikeSlcInds,\n  const ITYPE *__restrict__ dInds3, \n  const ITYPE *__restrict__ fbrPtr0,\n  const ITYPE *__restrict__ fbrPtr1,\n  const ITYPE *__restrict__ fbrIdx1,\n  const ITYPE *__restrict__ fbrPtr2,\n  const ITYPE *__restrict__ fbrIdx2,\n  ITYPE nFibers,\n  DTYPE *__restrict__ dU0,\n  const DTYPE *__restrict__ dU1,\n  const DTYPE *__restrict__ dU2,\n  const DTYPE *__restrict__ dU3,\n  ITYPE mode,\n  ITYPE R,\n  ITYPE warpPerSlice,\n  int logOfWPC)\n{\n\n  ITYPE tId = threadIdx.x;\n  ITYPE laneId = tId & 31;\n  ITYPE bdim = blockDim.x;\n  ITYPE gId = (blockIdx.x * bdim + tId);\n  ITYPE workId = (tId & ((1 << (5 + logOfWPC)) - 1)) >> 5;  //tId >> 5; //tId >> 5;//\n  ITYPE fbrS = gId >> (5 + logOfWPC); // 5: minimum 1 WARP (2^5) // blockIdx.x ;//\n  DTYPE tmp = 0, tmp_val, tmp2 = 0;\n\n  if(fbrS < nFibers - 1){       \n\n    tmp = 0;\n    unsigned int idx0 = fbrIdx1[fbrS];// dInds1[fbrPtr1[fbr]];  \n    unsigned int idx3 = fbrLikeSlcInds[fbrS];//slc;  \n\n    for (int fbr = fbrPtr1[fbrS] + workId; fbr < fbrPtr1[fbrS+1]; fbr+=warpPerSlice){\n      unsigned int idx1 = fbrIdx2[fbr];\n      tmp_val = 0;\n\n      for(unsigned int x = fbrPtr2[fbr]; x < fbrPtr2[fbr+1]; ++x) {\n        unsigned int idx2 = dInds3[x];                    \n\n        for(unsigned int r=laneId; r<R; r+=32) \n          tmp_val += vals[x] * dU2[idx2 * R + r] ; //2MR\n      }\n      for(unsigned int r=laneId; r<R; r+=32)  \n        tmp += tmp_val * dU1[idx1 * R + r]  ;  \n    }     \n    for(unsigned int r=laneId; r<R; r+=32) { \n      tmp2 = tmp * dU3[idx3 * R + r];\n      atomicAdd(&dU0[idx0 * R + r], tmp2); //2PR\n    }    \n  }\n}",
            "#define DTYPE float\n\n\n#define ITYPE size_t // if chnage to unsigned int change the grid.x and gID in cuda kernel computation to long\n\n\n__global__ void mttkrp_MIHCSR_kernel_fbr_atomic_fbrLvlPar(\n  const DTYPE *__restrict__ vals,\n  const ITYPE *__restrict__ fbrLikeSlcInds,\n  const ITYPE *__restrict__ dInds2, \n  const ITYPE *__restrict__ fbrPtr0,\n  const ITYPE *__restrict__ fbrPtr1,\n  const ITYPE *__restrict__ fbrIdx1,\n  ITYPE nFibers,\n        DTYPE *__restrict__ dU0,\n  const DTYPE *__restrict__ dU1,\n  const DTYPE *__restrict__ dU2, \n  ITYPE mode,\n  ITYPE R,\n  ITYPE warpPerSlice,\n  int logOfWPC)\n{\n  ITYPE tId = threadIdx.x;\n  ITYPE laneId = tId & 31;\n  ITYPE bdim = blockDim.x;\n  ITYPE gId = (blockIdx.x * bdim + tId);\n  ITYPE workId = (tId & ((1 << (5 + logOfWPC)) - 1)) >> 5;  //tId >> 5; //tId >> 5;//\n  ITYPE fbr = gId >> (5 + logOfWPC); // 5: minimum 1 WARP (2^5) // blockIdx.x ;//\n  DTYPE tmp = 0, tmp_val;\n\n  if(fbr < nFibers - 1){       \n\n    tmp_val = 0;\n    unsigned int idx0 = fbrIdx1[fbr];// dInds1[fbrPtr1[fbr]];  \n    unsigned int idx2 = fbrLikeSlcInds[fbr];//slc; \n\n    for(unsigned int x = fbrPtr1[fbr] + workId; x < fbrPtr1[fbr+1]; x+=warpPerSlice) {\n\n      unsigned int idx1 = dInds2[x];                    \n\n      for(unsigned int r=laneId; r<R; r+=32) {\n        tmp_val += vals[x] * dU1[idx1 * R + r]; //2MR\n      }\n    }     \n    for(unsigned int r=laneId; r<R; r+=32) { \n      tmp = tmp_val * dU2[idx2 * R + r] ;\n      atomicAdd(&dU0[idx0 * R + r], tmp); //2PR\n    }    \n  }\n}",
            "#define DTYPE float\n\n\n#define ITYPE size_t // if chnage to unsigned int change the grid.x and gID in cuda kernel computation to long\n\n\n__global__ void mttkrp_MIHCSR_kernel_fbr_atomic_fbrLvlPar_4D(\n  const DTYPE *__restrict__ vals,\n  const ITYPE *__restrict__ fbrLikeSlcInds,\n  const ITYPE *__restrict__ dInds3, \n  const ITYPE *__restrict__ fbrPtr0,\n  const ITYPE *__restrict__ fbrPtr1,\n  const ITYPE *__restrict__ fbrIdx1,\n  const ITYPE *__restrict__ fbrPtr2,\n  const ITYPE *__restrict__ fbrIdx2,\n  ITYPE nFibers,\n        DTYPE *__restrict__ dU0,\n  const DTYPE *__restrict__ dU1,\n  const DTYPE *__restrict__ dU2,\n  const DTYPE *__restrict__ dU3,\n  ITYPE mode,\n  ITYPE R,\n  ITYPE warpPerSlice,\n  int logOfWPC)\n{\n  ITYPE tId = threadIdx.x;\n  ITYPE laneId = tId & 31;\n  ITYPE bdim = blockDim.x;\n  ITYPE gId = (blockIdx.x * bdim + tId);\n  ITYPE workId = (tId & ((1 << (5 + logOfWPC)) - 1)) >> 5;  //tId >> 5; //tId >> 5;//\n  ITYPE fbrS = gId >> (5 + logOfWPC); // 5: minimum 1 WARP (2^5) // blockIdx.x ;//\n  DTYPE tmp;\n\n  if(fbrS < nFibers - 1){       \n\n    unsigned int idx2 = fbrLikeSlcInds[fbrS];//slc;  \n    unsigned int idx3 = fbrIdx1[fbrS];// dInds1[fbrPtr1[fbr]];  \n\n    for (int fbr = fbrPtr1[fbrS] + workId; fbr < fbrPtr1[fbrS+1]; fbr+=warpPerSlice){\n      unsigned int idx0 = fbrIdx2[fbr];\n      tmp = 0;\n\n      for(unsigned int x = fbrPtr2[fbr]; x < fbrPtr2[fbr+1]; ++x) {\n        unsigned int idx1 = dInds3[x];                    \n\n        for(unsigned int r=laneId; r<R; r+=32) \n          tmp += vals[x] * dU1[idx1 * R + r]; //2MR\n      }\n      for(unsigned int r=laneId; r<R; r+=32)  {\n        atomicAdd(&dU0[idx0 * R + r], tmp * dU2[idx2 * R + r] * dU3[idx3 * R + r]) ;  \n      }\n    }            \n  }\n}",
            "#define DTYPE float\n\n\n#define ITYPE size_t // if chnage to unsigned int change the grid.x and gID in cuda kernel computation to long\n\n\n__global__ void mttkrp_MIHCSR_kernel_all_atomic_fbrLvlPar(\n  const DTYPE *__restrict__ vals,\n  const ITYPE *__restrict__ fbrLikeSlcInds,\n  const ITYPE *__restrict__ dInds2, \n  const ITYPE *__restrict__ fbrPtr0,\n  const ITYPE *__restrict__ fbrPtr1,\n  const ITYPE *__restrict__ fbrIdx1,\n  ITYPE nFibers,\n        DTYPE *__restrict__ dU0,\n  const DTYPE *__restrict__ dU1,\n  const DTYPE *__restrict__ dU2, \n  ITYPE mode,\n  ITYPE R,\n  ITYPE warpPerSlice,\n  int logOfWPC)\n{\n  ITYPE tId = threadIdx.x;\n  ITYPE laneId = tId & 31;\n  ITYPE bdim = blockDim.x;\n  ITYPE gId = (blockIdx.x * bdim + tId);\n  ITYPE workId = (tId & ((1 << (5 + logOfWPC)) - 1)) >> 5;  //tId >> 5; //tId >> 5;//\n  ITYPE fbr = gId >> (5 + logOfWPC); // 5: minimum 1 WARP (2^5) // blockIdx.x ;//\n  DTYPE tmp = 0, tmp_val;\n\n  if(fbr < nFibers - 1){       \n\n    tmp_val = 0;\n    unsigned int idx1 = fbrLikeSlcInds[fbr];//slc;  \n    unsigned int idx2 = fbrIdx1[fbr];// dInds1[fbrPtr1[fbr]];  \n\n    for(unsigned int r=laneId; r<R; r+=32) \n      tmp = dU1[idx1 * R + r] * dU2[idx2 * R + r] ; //1PR\n\n    for(unsigned int x = fbrPtr1[fbr] + workId; x < fbrPtr1[fbr+1]; x+=warpPerSlice) {\n\n      unsigned int idx0 = dInds2[x];                    \n\n      for(unsigned int r=laneId; r<R; r+=32) {\n        tmp_val = vals[x] * tmp;///dU1[idx1 * R + r] * dU2[idx2 * R + r] ; //2MR\n        atomicAdd(&dU0[idx0 * R + r], tmp_val);\n      }\n    }         \n  }\n}",
            "#define DTYPE float\n\n\n#define ITYPE size_t // if chnage to unsigned int change the grid.x and gID in cuda kernel computation to long\n\n\n__global__ void mttkrp_MIHCSR_kernel_all_atomic_fbrLvlPar_4D(\n  const DTYPE *__restrict__ vals,\n  const ITYPE *__restrict__ fbrLikeSlcInds,\n  const ITYPE *__restrict__ dInds3, \n  const ITYPE *__restrict__ fbrPtr0,\n  const ITYPE *__restrict__ fbrPtr1,\n  const ITYPE *__restrict__ fbrIdx1,\n  const ITYPE *__restrict__ fbrPtr2,\n  const ITYPE *__restrict__ fbrIdx2,\n  ITYPE nFibers,\n        DTYPE *__restrict__ dU0,\n  const DTYPE *__restrict__ dU1,\n  const DTYPE *__restrict__ dU2,\n  const DTYPE *__restrict__ dU3,\n  ITYPE mode,\n  ITYPE R,\n  ITYPE warpPerSlice,\n  int logOfWPC)\n{\n  ITYPE tId = threadIdx.x;\n  ITYPE laneId = tId & 31;\n  ITYPE bdim = blockDim.x;\n  ITYPE gId = (blockIdx.x * bdim + tId);\n  ITYPE workId = (tId & ((1 << (5 + logOfWPC)) - 1)) >> 5;  //tId >> 5; //tId >> 5;//\n  ITYPE fbrS = gId >> (5 + logOfWPC); // 5: minimum 1 WARP (2^5) // blockIdx.x ;//\n  DTYPE tmp = 0, tmp_val = 0;;\n\n  if(fbrS < nFibers - 1){       \n\n    tmp = 0;\n    unsigned int idx1 = fbrLikeSlcInds[fbrS];//slc;  \n    unsigned int idx2 = fbrIdx1[fbrS];// dInds1[fbrPtr1[fbr]];                \n\n    for(unsigned int r=laneId; r<R; r+=32) \n      tmp_val = dU1[idx1 * R + r] * dU2[idx2 * R + r] ; //1PR\n\n    for (int fbr = fbrPtr1[fbrS] + workId; fbr < fbrPtr1[fbrS+1]; fbr+=warpPerSlice){\n      ITYPE idx3 = fbrIdx2[fbr];\n\n      for(unsigned int x = fbrPtr2[fbr]; x < fbrPtr2[fbr+1]; ++x) {\n        unsigned int idx0 = dInds3[x];  \n\n        for(unsigned int r=laneId; r<R; r+=32) {\n          tmp = vals[x] * dU3[idx3 * R + r] * tmp_val;//2MR\n          atomicAdd(&dU0[idx0 * R + r], tmp);\n        }\n      }\n    }            \n  }\n}"
        ]
    },
    "tensorT-cuda": {
        "/Users/gbolet/hecbench-roofline/src/tensorT-cuda/main.cu": [
            "__global__ void tensor_transpose(\n    const int dim_input, \n    const int dim_output, \n    const int nblocks, \n    const int tile_size,\n    const int *shape_input, \n    const int *shape_output, \n    const float *shape_input_r, \n    const float *shape_output_r, \n    const int *stride_input,\n    const int *stride_output_local, \n    const int *stride_output_global,\n    const double *input, \n    double *output) \n{\n  __shared__ double tile[TILE_SIZE];\n\n  for (int block_idx = blockIdx.x; block_idx < nblocks; block_idx += gridDim.x) {\n    int it = block_idx, im = 0, offset1 = 0;\n    for (int i = 0; i < dim_input; i++) {\n      im = it * shape_input_r[i];  // replace division with multiplication\n      offset1 += stride_input[i] * (it - im * shape_input[i]);\n      it = im;\n    }\n\n    for (int i = threadIdx.x; i < tile_size; i += blockDim.x) {\n      tile[i] = input[i + block_idx * tile_size];\n    }\n\n    __syncthreads();\n\n    for (int i = threadIdx.x; i < tile_size; i += blockDim.x) {\n      it = i;\n      int offset2 = 0, local_offset = 0;\n      for (int j = 0; j < dim_output; j++) {\n        im = it * shape_output_r[j];  // replace division with multiplication\n        int tmp = it - im * shape_output[j];\n        offset2 += stride_output_global[j] * tmp;\n        local_offset += stride_output_local[j] * tmp;\n        it = im;\n      }\n      output[offset1 + offset2] = tile[local_offset];\n    }\n\n    __syncthreads();\n  }\n}"
        ]
    },
    "rowwiseMoments-cuda": {
        "/Users/gbolet/hecbench-roofline/src/rowwiseMoments-cuda/main.cu": [
            "#define T ((int)32)\n\n\n__global__\nvoid RowwiseMomentsKernel(\n    int64_t N,\n    T eps,\n    const T* X,\n    T* mean,\n    T* rstd) \n{\n  using T_ACC = T;\n  using WelfordType = WelfordData<T_ACC, int64_t>;\n  using WelfordOp =\n    WelfordOps<T_ACC, T_ACC, int64_t, thrust::pair<T_ACC, T_ACC>>;\n\n  const int64_t i = blockIdx.x;\n  WelfordOp welford_op = {/*correction=*/0, /*take_sqrt=*/false};\n  WelfordType val(0, 0, 0, 0);\n  for (int64_t j = threadIdx.x; j < N; j += blockDim.x) {\n    const int64_t index = i * N + j;\n    val = welford_op.reduce(val, static_cast<T_ACC>(X[index]), index);\n  }\n\n  // There will be a warning if we declare a __shared__ WelfordType array.\n  // https://github.com/pytorch/pytorch/pull/13967\n  __shared__ typename std::aligned_storage<\n    sizeof(WelfordType),\n    alignof(WelfordType)>::type val_shared[WARP_SIZE];\n\n  WelfordType* val_shared_ptr = reinterpret_cast<WelfordType*>(val_shared);\n  val = BlockReduce(\n      val,\n      welford_op,\n      /*identity_element=*/WelfordType(0, 0, 0, 0),\n      val_shared_ptr);\n\n  if (threadIdx.x == 0) {\n    T_ACC m1;\n    T_ACC m2;\n    thrust::tie(m1, m2) = welford_op.project(val);\n    rstd[i] = rsqrt(m1 + static_cast<T_ACC>(eps));\n    mean[i] = m2;\n  }\n}"
        ]
    },
    "marchingCubes-cuda": {
        "/Users/gbolet/hecbench-roofline/src/marchingCubes-cuda/main.cu": [
            "__inline__ __device__ float f(unsigned int x, unsigned int y, unsigned int z)\n{\n  constexpr float d(2.0f / N);\n  float xf((int(x - Nd2)) * d);//[-1, 1)\n  float yf((int(z - Nd2)) * d);\n  float zf((int(z - Nd2)) * d);\n  return 1.f - 16.f * xf * yf * zf - 4.f * (xf * xf + yf * yf + zf * zf);\n}\n\n__global__ void computeMinMaxLv1(float*__restrict__ minMax)\n{\n  __shared__ float sminMax[64];\n  constexpr unsigned int threadNum(voxelXLv1 * voxelYLv1);\n  constexpr unsigned int warpNum(threadNum / 32);\n  unsigned int x(blockIdx.x * (voxelXLv1 - 1) + threadIdx.x);\n  unsigned int y(blockIdx.y * (voxelYLv1 - 1) + threadIdx.y);\n  unsigned int z(blockIdx.z * (voxelZLv1 - 1));\n  unsigned int tid(threadIdx.x + voxelXLv1 * threadIdx.y);\n  unsigned int laneid = tid % 32;\n  unsigned int blockid(blockIdx.x + gridXLv1 * (blockIdx.y + gridYLv1 * blockIdx.z));\n  unsigned int warpid(tid >> 5);\n  float v(f(x, y, z));\n  float minV(v), maxV(v);\n  for (int c0(1); c0 < voxelZLv1; ++c0)\n  {\n    v = f(x, y, z + c0);\n    if (v < minV)minV = v;\n    if (v > maxV)maxV = v;\n  }\n#pragma unroll\n  for (int c0(16); c0 > 0; c0 /= 2)\n  {\n    float t0, t1;\n    t0 = __shfl_down_sync(0xffffffffu, minV, c0);\n    t1 = __shfl_down_sync(0xffffffffu, maxV, c0);\n    if (t0 < minV)minV = t0;\n    if (t1 > maxV)maxV = t1;\n  }\n  if (laneid == 0)\n  {\n    sminMax[warpid] = minV;\n    sminMax[warpid + warpNum] = maxV;\n  }\n  __syncthreads();\n  if (warpid == 0)\n  {\n    minV = sminMax[laneid];\n    maxV = sminMax[laneid + warpNum];\n#pragma unroll\n    for (int c0(warpNum / 2); c0 > 0; c0 /= 2)\n    {\n      float t0, t1;\n      t0 = __shfl_down_sync(0xffffffffu, minV, c0);\n      t1 = __shfl_down_sync(0xffffffffu, maxV, c0);\n      if (t0 < minV)minV = t0;\n      if (t1 > maxV)maxV = t1;\n    }\n    if (laneid == 0)\n    {\n      minMax[blockid * 2] = minV;\n      minMax[blockid * 2 + 1] = maxV;\n    }\n  }\n}",
            "__global__ void compactLv1(\n  float isoValue, \n  const float*__restrict__ minMax,\n  unsigned int*__restrict__ blockIndices,\n  unsigned int*__restrict__ countedBlockNum)\n{\n  __shared__ unsigned int sums[32];\n  constexpr unsigned int warpNum(countingThreadNumLv1 / 32);\n  unsigned int tid(threadIdx.x);\n  unsigned int laneid = tid % 32;\n  unsigned int bIdx(blockIdx.x * countingThreadNumLv1 + tid);\n  unsigned int warpid(tid >> 5);\n  unsigned int test;\n  if (minMax[2 * bIdx] <= isoValue && minMax[2 * bIdx + 1] >= isoValue)test = 1;\n  else test = 0;\n  unsigned int testSum(test);\n#pragma unroll\n  for (int c0(1); c0 < 32; c0 *= 2)\n  {\n    unsigned int tp(__shfl_up_sync(0xffffffffu, testSum, c0));\n    if (laneid >= c0)testSum += tp;\n  }\n  if (laneid == 31)sums[warpid] = testSum;\n  __syncthreads();\n  if (warpid == 0)\n  {\n    unsigned int warpSum = sums[laneid];\n#pragma unroll\n    for (int c0(1); c0 < warpNum; c0 *= 2)\n    {\n      unsigned int tp(__shfl_up_sync(0xffffffffu, warpSum, c0));\n      if (laneid >= c0) warpSum += tp;\n    }\n    sums[laneid] = warpSum;\n  }\n  __syncthreads();\n  if (warpid != 0)testSum += sums[warpid - 1];\n  if (tid == countingThreadNumLv1 - 1 && testSum != 0)\n    sums[31] = atomicAdd(countedBlockNum, testSum);\n  __syncthreads();\n  if (test)blockIndices[testSum + sums[31] - 1] = bIdx;\n}",
            "__inline__ __device__ float f(unsigned int x, unsigned int y, unsigned int z)\n{\n  constexpr float d(2.0f / N);\n  float xf((int(x - Nd2)) * d);//[-1, 1)\n  float yf((int(z - Nd2)) * d);\n  float zf((int(z - Nd2)) * d);\n  return 1.f - 16.f * xf * yf * zf - 4.f * (xf * xf + yf * yf + zf * zf);\n}\n\n__global__ void computeMinMaxLv2(\n  const unsigned int*__restrict__ blockIndicesLv1,\n  float*__restrict__ minMax)\n{\n  unsigned int tid(threadIdx.x);\n  unsigned int voxelOffset(threadIdx.y);\n  unsigned int blockIndex(blockIndicesLv1[blockIdx.x]);\n  unsigned int tp(blockIndex);\n  unsigned int x((blockIndex % gridXLv1) * (voxelXLv1 - 1) + (voxelOffset % 5) * (voxelXLv2 - 1) + (tid & 3));\n  tp /= gridXLv1;\n  unsigned int y((tp % gridYLv1) * (voxelYLv1 - 1) + (voxelOffset / 5) * (voxelYLv2 - 1) + (tid >> 2));\n  tp /= gridYLv1;\n  unsigned int z(tp * (voxelZLv1 - 1));\n  float v(f(x, y, z));\n  float minV(v), maxV(v);\n  unsigned int idx(2 * (voxelOffset + voxelNumLv2 * blockIdx.x));\n  for (int c0(0); c0 < blockZLv2; ++c0)\n  {\n    for (int c1(1); c1 < voxelZLv2; ++c1)\n    {\n      v = f(x, y, z + c1);\n      if (v < minV)minV = v;\n      if (v > maxV)maxV = v;\n    }\n    z += voxelZLv2 - 1;\n#pragma unroll\n    for (int c1(8); c1 > 0; c1 /= 2)\n    {\n      float t0, t1;\n      t0 = __shfl_down_sync(0xffffffffu, minV, c1);\n      t1 = __shfl_down_sync(0xffffffffu, maxV, c1);\n      if (t0 < minV)minV = t0;\n      if (t1 > maxV)maxV = t1;\n    }\n    if (tid == 0)\n    {\n      minMax[idx] = minV;\n      minMax[idx + 1] = maxV;\n      constexpr unsigned int offsetSize(2 * blockXLv2 * blockYLv2);\n      idx += offsetSize;\n    }\n    minV = v;\n    maxV = v;\n  }\n}",
            "__global__ void compactLv2(\n  float isoValue,\n  const float*__restrict__ minMax,\n  const unsigned int*__restrict__ blockIndicesLv1,\n  unsigned int*__restrict__ blockIndicesLv2,\n  unsigned int counterBlockNumLv1,\n  unsigned int*__restrict__ countedBlockNumLv2)\n{\n  __shared__ unsigned int sums[32];\n  constexpr unsigned int warpNum(countingThreadNumLv2 / 32);\n  unsigned int tid(threadIdx.x);\n  unsigned int laneid = tid % 32;\n  unsigned int warpid(tid >> 5);\n  unsigned int id0(tid + blockIdx.x * countingThreadNumLv2);\n  unsigned int id1(id0 / voxelNumLv2);\n  unsigned int test;\n  if (id1 < counterBlockNumLv1)\n  {\n    if (minMax[2 * id0] <= isoValue && minMax[2 * id0 + 1] >= isoValue)\n      test = 1;\n    else\n      test = 0;\n  }\n  else test = 0;\n  unsigned int testSum(test);\n#pragma unroll\n  for (int c0(1); c0 < 32; c0 *= 2)\n  {\n    unsigned int tp(__shfl_up_sync(0xffffffffu, testSum, c0));\n    if (laneid >= c0)testSum += tp;\n  }\n  if (laneid == 31)sums[warpid] = testSum;\n  __syncthreads();\n  if (warpid == 0)\n  {\n    unsigned warpSum = sums[laneid];\n#pragma unroll\n    for (int c0(1); c0 < warpNum; c0 *= 2)\n    {\n      unsigned int tp(__shfl_up_sync(0xffffffffu, warpSum, c0));\n      if (laneid >= c0)warpSum += tp;\n    }\n    sums[laneid] = warpSum;\n  }\n  __syncthreads();\n  if (warpid != 0)testSum += sums[warpid - 1];\n  if (tid == countingThreadNumLv2 - 1)\n    sums[31] = atomicAdd(countedBlockNumLv2, testSum);\n  __syncthreads();\n\n  if (test)\n  {\n    unsigned int bIdx1(blockIndicesLv1[id1]);\n    unsigned int bIdx2;\n    unsigned int x1, y1, z1;\n    unsigned int x2, y2, z2;\n    unsigned int tp1(bIdx1);\n    unsigned int tp2((tid + blockIdx.x * countingThreadNumLv2) % voxelNumLv2);\n    x1 = tp1 % gridXLv1;\n    x2 = tp2 % blockXLv2;\n    tp1 /= gridXLv1;\n    tp2 /= blockXLv2;\n    y1 = tp1 % gridYLv1;\n    y2 = tp2 % blockYLv2;\n    z1 = tp1 / gridYLv1;\n    z2 = tp2 / blockYLv2;\n    bIdx2 = x2 + blockXLv2 * (x1 + gridXLv1 * (y2 + blockYLv2 * (y1 + gridYLv1 * (z1 * blockZLv2 + z2))));\n    blockIndicesLv2[testSum + sums[31] - 1] = bIdx2;\n  }\n}",
            "__inline__ __device__ float f(unsigned int x, unsigned int y, unsigned int z)\n{\n  constexpr float d(2.0f / N);\n  float xf((int(x - Nd2)) * d);//[-1, 1)\n  float yf((int(z - Nd2)) * d);\n  float zf((int(z - Nd2)) * d);\n  return 1.f - 16.f * xf * yf * zf - 4.f * (xf * xf + yf * yf + zf * zf);\n}\n\n__inline__ __device__ float transformToCoord(unsigned int x)\n{\n  return (int(x) - int(Nd2)) * (2.0f / N);\n}\n\n__inline__ __device__ float zeroPoint(unsigned int x, float v0, float v1, float isoValue)\n{\n  return ((x * (v1 - isoValue) + (x + 1) * (isoValue - v0)) / (v1 - v0) - Nd2) * (2.0f / N);\n}\n\n__global__ void generatingTriangles(\n  float isoValue, \n  const unsigned int*__restrict__ blockIndicesLv2,\n  const unsigned short *__restrict__ distinctEdgesTable,\n  const int *__restrict__ triTable,\n  const uchar4 *__restrict__ edgeIDTable,\n  unsigned int*__restrict__ countedVerticesNum,\n  unsigned int*__restrict__ countedTrianglesNum,\n  unsigned long long*__restrict__ triangles,\n  float*__restrict__ coordX,\n  float*__restrict__ coordY,\n  float*__restrict__ coordZ,\n  float*__restrict__ coordZP)\n{\n  __shared__ unsigned short vertexIndices[voxelZLv2][voxelYLv2][voxelXLv2];\n  __shared__ float value[voxelZLv2 + 1][voxelYLv2 + 1][voxelXLv2 + 1];\n  __shared__ unsigned int sumsVertices[32];\n  __shared__ unsigned int sumsTriangles[32];\n\n  unsigned int blockId(blockIndicesLv2[blockIdx.x]);\n  unsigned int tp(blockId);\n  unsigned int x((tp % gridXLv2) * (voxelXLv2 - 1) + threadIdx.x);\n  tp /= gridXLv2;\n  unsigned int y((tp % gridYLv2) * (voxelYLv2 - 1) + threadIdx.y);\n  unsigned int z((tp / gridYLv2) * (voxelZLv2 - 1) + threadIdx.z);\n  unsigned int eds(7);\n  float v(value[threadIdx.z][threadIdx.y][threadIdx.x] = f(x, y, z));\n  if (threadIdx.x == voxelXLv2 - 1)\n  {\n    eds &= 6;\n    value[threadIdx.z][threadIdx.y][voxelXLv2] = f(x + 1, y, z);\n    if (threadIdx.y == voxelYLv2 - 1)\n      value[threadIdx.z][voxelYLv2][voxelXLv2] = f(x + 1, y + 1, z);\n  }\n  if (threadIdx.y == voxelYLv2 - 1)\n  {\n    eds &= 5;\n    value[threadIdx.z][voxelYLv2][threadIdx.x] = f(x, y + 1, z);\n    if (threadIdx.z == voxelZLv2 - 1)\n      value[voxelZLv2][voxelYLv2][threadIdx.x] = f(x, y + 1, z + 1);\n  }\n  if (threadIdx.z == voxelZLv2 - 1)\n  {\n    eds &= 3;\n    value[voxelZLv2][threadIdx.y][threadIdx.x] = f(x, y, z + 1);\n    if (threadIdx.x == voxelXLv2 - 1)\n      value[voxelZLv2][threadIdx.y][voxelXLv2] = f(x + 1, y, z + 1);\n  }\n  eds <<= 13;\n  __syncthreads();\n  unsigned int cubeCase(0);\n  if (value[threadIdx.z][threadIdx.y][threadIdx.x] < isoValue) cubeCase |= 1;\n  if (value[threadIdx.z][threadIdx.y][threadIdx.x + 1] < isoValue) cubeCase |= 2;\n  if (value[threadIdx.z][threadIdx.y + 1][threadIdx.x + 1] < isoValue) cubeCase |= 4;\n  if (value[threadIdx.z][threadIdx.y + 1][threadIdx.x] < isoValue) cubeCase |= 8;\n  if (value[threadIdx.z + 1][threadIdx.y][threadIdx.x] < isoValue) cubeCase |= 16;\n  if (value[threadIdx.z + 1][threadIdx.y][threadIdx.x + 1] < isoValue) cubeCase |= 32;\n  if (value[threadIdx.z + 1][threadIdx.y + 1][threadIdx.x + 1] < isoValue) cubeCase |= 64;\n  if (value[threadIdx.z + 1][threadIdx.y + 1][threadIdx.x] < isoValue) cubeCase |= 128;\n\n  unsigned int distinctEdges(eds ? distinctEdgesTable[cubeCase] : 0);\n  unsigned int numTriangles(eds != 0xe000 ? 0 : distinctEdges & 7);\n  unsigned int numVertices(__popc(distinctEdges &= eds));\n  unsigned int laneid = (threadIdx.x + voxelXLv2 * (threadIdx.y + voxelYLv2 * threadIdx.z)) % 32;\n  unsigned warpid((threadIdx.x + voxelXLv2 * (threadIdx.y + voxelYLv2 * threadIdx.z)) >> 5);\n  constexpr unsigned int threadNum(voxelXLv2 * voxelYLv2 * voxelZLv2);\n  constexpr unsigned int warpNum(threadNum / 32);\n  unsigned int sumVertices(numVertices);\n  unsigned int sumTriangles(numTriangles);\n\n#pragma unroll\n  for (int c0(1); c0 < 32; c0 *= 2)\n  {\n    unsigned int tp0(__shfl_up_sync(0xffffffffu, sumVertices, c0));\n    unsigned int tp1(__shfl_up_sync(0xffffffffu, sumTriangles, c0));\n    if (laneid >= c0)\n    {\n      sumVertices += tp0;\n      sumTriangles += tp1;\n    }\n  }\n  if (laneid == 31)\n  {\n    sumsVertices[warpid] = sumVertices;\n    sumsTriangles[warpid] = sumTriangles;\n  }\n  __syncthreads();\n  if (warpid == 0)\n  {\n    unsigned warpSumVertices = sumsVertices[laneid];\n    unsigned warpSumTriangles = sumsTriangles[laneid];\n#pragma unroll\n    for (int c0(1); c0 < warpNum; c0 *= 2)\n    {\n      unsigned int tp0(__shfl_up_sync(0xffffffffu, warpSumVertices, c0));\n      unsigned int tp1(__shfl_up_sync(0xffffffffu, warpSumTriangles, c0));\n      if (laneid >= c0)\n      {\n        warpSumVertices += tp0;\n        warpSumTriangles += tp1;\n      }\n    }\n    sumsVertices[laneid] = warpSumVertices;\n    sumsTriangles[laneid] = warpSumTriangles;\n  }\n  __syncthreads();\n  if (warpid != 0)\n  {\n    sumVertices += sumsVertices[warpid - 1];\n    sumTriangles += sumsTriangles[warpid - 1];\n  }\n  if (eds == 0)\n  {\n    sumsVertices[31] = atomicAdd(countedVerticesNum, sumVertices);\n    sumsTriangles[31] = atomicAdd(countedTrianglesNum, sumTriangles);\n  }\n\n  unsigned int interOffsetVertices(sumVertices - numVertices);\n  sumVertices = interOffsetVertices + sumsVertices[31];//exclusive offset\n  sumTriangles = sumTriangles + sumsTriangles[31] - numTriangles;//exclusive offset\n  vertexIndices[threadIdx.z][threadIdx.y][threadIdx.x] = interOffsetVertices | distinctEdges;\n  __syncthreads();\n\n  for (unsigned int c0(0); c0 < numTriangles; ++c0)\n  {\n#pragma unroll\n    for (unsigned int c1(0); c1 < 3; ++c1)\n    {\n      int edgeID(triTable[16 * cubeCase + 3 * c0 + c1]);\n      uchar4 edgePos(edgeIDTable[edgeID]);\n      unsigned short vertexIndex(vertexIndices[threadIdx.z + edgePos.z][threadIdx.y + edgePos.y][threadIdx.x + edgePos.x]);\n      unsigned int tp(__popc(vertexIndex >> (16 - edgePos.w)) + (vertexIndex & 0x1fff));\n      atomicAdd(triangles, (unsigned long long)(sumsVertices[31] + tp));\n    }\n  }\n\n  // sumVertices may be too large for a GPU memory\n  float zp = 0.f, cx = 0.f, cy = 0.f, cz = 0.f;\n\n  if (distinctEdges & (1 << 15))\n  {\n    zp = zeroPoint(x, v, value[threadIdx.z][threadIdx.y][threadIdx.x + 1], isoValue);\n    cy = transformToCoord(y);\n    cz = transformToCoord(z);\n  }\n  if (distinctEdges & (1 << 14))\n  {\n    cx = transformToCoord(x);\n    zp += zeroPoint(y, v, value[threadIdx.z][threadIdx.y + 1][threadIdx.x], isoValue);\n    cz += transformToCoord(z);\n  }\n  if (distinctEdges & (1 << 13))\n  {\n    cx += transformToCoord(x);\n    cy += transformToCoord(y);\n    zp += zeroPoint(z, v, value[threadIdx.z + 1][threadIdx.y][threadIdx.x], isoValue);\n  }\n  atomicAdd(coordX, cx);\n  atomicAdd(coordY, cy);\n  atomicAdd(coordZ, cz);\n  atomicAdd(coordZP, zp);\n}"
        ]
    },
    "minimap2-cuda": {
        "/Users/gbolet/hecbench-roofline/src/minimap2-cuda/device/device_kernel.cu": [
            "__device__\nscore_dt device_ilog2(const score_dt v)\n{\n  if (v < 2) return 0;\n  else if (v < 4) return 1;\n  else if (v < 8) return 2;\n  else if (v < 16) return 3;\n  else if (v < 32) return 4;\n  else if (v < 64) return 5;\n  else if (v < 128) return 6;\n  else if (v < 256) return 7;\n  else return 8;\n}\n\n__device__\nscore_dt chain_dp_score(\n  const anchor_dt *active,\n  const anchor_dt curr,\n  const float avg_qspan,\n  const int max_dist_x,\n  const int max_dist_y, \n  const int bw, const int id)\n{\n  anchor_dt act;\n  *((short4*)&act) = ((short4*)active)[id];\n\n  if (curr.tag != act.tag) return NEG_INF_SCORE_GPU;\n\n  score_dt dist_x = act.x - curr.x;\n  if (dist_x == 0 || dist_x > max_dist_x) return NEG_INF_SCORE_GPU;\n\n  score_dt dist_y = act.y - curr.y;\n  if (dist_y > max_dist_y || dist_y <= 0) return NEG_INF_SCORE_GPU;\n\n  score_dt dd = dist_x > dist_y ? dist_x - dist_y : dist_y - dist_x;\n  if (dd > bw) return NEG_INF_SCORE_GPU;\n\n  score_dt min_d = dist_y < dist_x ? dist_y : dist_x;\n  score_dt log_dd = device_ilog2(dd);\n\n  score_dt sc = min_d > act.w ? act.w : min_d;\n  sc -= (score_dt)(dd * (0.01f * avg_qspan)) + (log_dd >> 1);\n\n  return sc;\n}\n\n__global__\nvoid device_chain_tiled(\n  return_dt *__restrict__ ret,\n  const anchor_dt *__restrict__ a,\n  const control_dt *__restrict__ cont,\n  score_dt *__restrict__ max_tracker_g,\n  parent_dt *__restrict__ j_tracker_g,\n  const int max_dist_x, \n  const int max_dist_y,\n  const int bw)\n{\n  int block = blockIdx.x;\n  int id = threadIdx.x;\n  int ofs = block;\n  auto control = cont[ofs];\n\n  __shared__ anchor_dt active[BACK_SEARCH_COUNT_GPU];\n  __shared__ score_dt max_tracker[BACK_SEARCH_COUNT_GPU];\n  __shared__ parent_dt j_tracker[BACK_SEARCH_COUNT_GPU];\n\n  ((short4*)active)[id] = ((short4*)a)[ofs * TILE_SIZE_ACTUAL + id];\n  if (control.is_new_read) {\n    max_tracker[id] = 0;\n    j_tracker[id] = -1;\n  } else {\n    max_tracker[id] = max_tracker_g[ofs * BACK_SEARCH_COUNT_GPU + id];\n    j_tracker[id] = j_tracker_g[ofs * BACK_SEARCH_COUNT_GPU + id];\n  }\n\n  for (int i = BACK_SEARCH_COUNT_GPU, curr_idx = 0; curr_idx < TILE_SIZE; i++, curr_idx++) {\n\n    __syncthreads();\n    anchor_dt curr;\n    *((short4*)&curr) = ((short4*)active)[i % BACK_SEARCH_COUNT_GPU];\n    score_dt f_curr = max_tracker[i % BACK_SEARCH_COUNT_GPU];\n    parent_dt p_curr = j_tracker[i % BACK_SEARCH_COUNT_GPU];\n    if (curr.w >= f_curr) {\n      f_curr = curr.w;\n      p_curr = (parent_dt)-1;\n    }\n\n    /* read in new query anchor, put into active array*/\n    __syncthreads();\n    if (id == i % BACK_SEARCH_COUNT_GPU) {\n      ((short4*)active)[id] = ((short4*)a)[ofs * TILE_SIZE_ACTUAL + i];\n      max_tracker[id] = 0;\n      j_tracker[id] = -1;\n    }\n\n    __syncthreads();\n    score_dt sc = chain_dp_score(active, curr,\n        control.avg_qspan, max_dist_x, max_dist_y, bw, id);\n\n    __syncthreads();\n    if (sc + f_curr >= max_tracker[id]) {\n      max_tracker[id] = sc + f_curr;\n      j_tracker[id] = (parent_dt)curr_idx + (parent_dt)control.tile_num * TILE_SIZE;\n    }\n\n    __syncthreads();\n    if (id == curr_idx % BACK_SEARCH_COUNT_GPU) {\n      return_dt tmp;\n      tmp.score = f_curr;\n      tmp.parent = p_curr;\n      ((short4*)ret)[ofs * TILE_SIZE + curr_idx] = *((short4*)&tmp);\n    }\n  }\n\n  __syncthreads();\n  max_tracker_g[ofs * BACK_SEARCH_COUNT_GPU + id] = max_tracker[id];\n  j_tracker_g[ofs * BACK_SEARCH_COUNT_GPU + id] = j_tracker[id];\n}"
        ]
    },
    "lulesh-cuda": {
        "/Users/gbolet/hecbench-roofline/src/lulesh-cuda/lulesh.cu": [
            "#define Real_t float\n\n\n__global__ void fill_sig(\n    Real_t *__restrict__ sigxx,\n    Real_t *__restrict__ sigyy,\n    Real_t *__restrict__ sigzz,\n    const Real_t *__restrict__ p,\n    const Real_t *__restrict__ q,\n    const Index_t numElem )\n{\n  Index_t i = blockDim.x*blockIdx.x+threadIdx.x;\n  if (i >= numElem) return;\n  sigxx[i] = sigyy[i] = sigzz[i] = - p[i] - q[i] ;\n}",
            "#define Real_t float\n\n\n__device__ static inline\nvoid SumElemFaceNormal(Real_t *normalX0, Real_t *normalY0, Real_t *normalZ0,\n    Real_t *normalX1, Real_t *normalY1, Real_t *normalZ1,\n    Real_t *normalX2, Real_t *normalY2, Real_t *normalZ2,\n    Real_t *normalX3, Real_t *normalY3, Real_t *normalZ3,\n    const Real_t x0, const Real_t y0, const Real_t z0,\n    const Real_t x1, const Real_t y1, const Real_t z1,\n    const Real_t x2, const Real_t y2, const Real_t z2,\n    const Real_t x3, const Real_t y3, const Real_t z3)\n{\n  Real_t bisectX0 = Real_t(0.5) * (x3 + x2 - x1 - x0);\n  Real_t bisectY0 = Real_t(0.5) * (y3 + y2 - y1 - y0);\n  Real_t bisectZ0 = Real_t(0.5) * (z3 + z2 - z1 - z0);\n  Real_t bisectX1 = Real_t(0.5) * (x2 + x1 - x3 - x0);\n  Real_t bisectY1 = Real_t(0.5) * (y2 + y1 - y3 - y0);\n  Real_t bisectZ1 = Real_t(0.5) * (z2 + z1 - z3 - z0);\n  Real_t areaX = Real_t(0.25) * (bisectY0 * bisectZ1 - bisectZ0 * bisectY1);\n  Real_t areaY = Real_t(0.25) * (bisectZ0 * bisectX1 - bisectX0 * bisectZ1);\n  Real_t areaZ = Real_t(0.25) * (bisectX0 * bisectY1 - bisectY0 * bisectX1);\n\n  *normalX0 += areaX;\n  *normalX1 += areaX;\n  *normalX2 += areaX;\n  *normalX3 += areaX;\n\n  *normalY0 += areaY;\n  *normalY1 += areaY;\n  *normalY2 += areaY;\n  *normalY3 += areaY;\n\n  *normalZ0 += areaZ;\n  *normalZ1 += areaZ;\n  *normalZ2 += areaZ;\n  *normalZ3 += areaZ;\n}\n\n__device__ static inline\nvoid CalcElemNodeNormals(Real_t pfx[8],\n    Real_t pfy[8],\n    Real_t pfz[8],\n    const Real_t x[8],\n    const Real_t y[8],\n    const Real_t z[8])\n{\n  for (Index_t i = 0 ; i < 8 ; ++i) {\n    pfx[i] = Real_t(0.0);\n    pfy[i] = Real_t(0.0);\n    pfz[i] = Real_t(0.0);\n  }\n  /* evaluate face one: nodes 0, 1, 2, 3 */\n  SumElemFaceNormal(&pfx[0], &pfy[0], &pfz[0],\n      &pfx[1], &pfy[1], &pfz[1],\n      &pfx[2], &pfy[2], &pfz[2],\n      &pfx[3], &pfy[3], &pfz[3],\n      x[0], y[0], z[0], x[1], y[1], z[1],\n      x[2], y[2], z[2], x[3], y[3], z[3]);\n  /* evaluate face two: nodes 0, 4, 5, 1 */\n  SumElemFaceNormal(&pfx[0], &pfy[0], &pfz[0],\n      &pfx[4], &pfy[4], &pfz[4],\n      &pfx[5], &pfy[5], &pfz[5],\n      &pfx[1], &pfy[1], &pfz[1],\n      x[0], y[0], z[0], x[4], y[4], z[4],\n      x[5], y[5], z[5], x[1], y[1], z[1]);\n  /* evaluate face three: nodes 1, 5, 6, 2 */\n  SumElemFaceNormal(&pfx[1], &pfy[1], &pfz[1],\n      &pfx[5], &pfy[5], &pfz[5],\n      &pfx[6], &pfy[6], &pfz[6],\n      &pfx[2], &pfy[2], &pfz[2],\n      x[1], y[1], z[1], x[5], y[5], z[5],\n      x[6], y[6], z[6], x[2], y[2], z[2]);\n  /* evaluate face four: nodes 2, 6, 7, 3 */\n  SumElemFaceNormal(&pfx[2], &pfy[2], &pfz[2],\n      &pfx[6], &pfy[6], &pfz[6],\n      &pfx[7], &pfy[7], &pfz[7],\n      &pfx[3], &pfy[3], &pfz[3],\n      x[2], y[2], z[2], x[6], y[6], z[6],\n      x[7], y[7], z[7], x[3], y[3], z[3]);\n  /* evaluate face five: nodes 3, 7, 4, 0 */\n  SumElemFaceNormal(&pfx[3], &pfy[3], &pfz[3],\n      &pfx[7], &pfy[7], &pfz[7],\n      &pfx[4], &pfy[4], &pfz[4],\n      &pfx[0], &pfy[0], &pfz[0],\n      x[3], y[3], z[3], x[7], y[7], z[7],\n      x[4], y[4], z[4], x[0], y[0], z[0]);\n  /* evaluate face six: nodes 4, 7, 6, 5 */\n  SumElemFaceNormal(&pfx[4], &pfy[4], &pfz[4],\n      &pfx[7], &pfy[7], &pfz[7],\n      &pfx[6], &pfy[6], &pfz[6],\n      &pfx[5], &pfy[5], &pfz[5],\n      x[4], y[4], z[4], x[7], y[7], z[7],\n      x[6], y[6], z[6], x[5], y[5], z[5]);\n}\n\n__device__ static inline\nvoid CalcElemShapeFunctionDerivatives( Real_t const x[],\n    Real_t const y[],\n    Real_t const z[],\n    Real_t b[][8],\n    Real_t* const volume )\n{\n  const Real_t x0 = x[0] ;   const Real_t x1 = x[1] ;\n  const Real_t x2 = x[2] ;   const Real_t x3 = x[3] ;\n  const Real_t x4 = x[4] ;   const Real_t x5 = x[5] ;\n  const Real_t x6 = x[6] ;   const Real_t x7 = x[7] ;\n\n  const Real_t y0 = y[0] ;   const Real_t y1 = y[1] ;\n  const Real_t y2 = y[2] ;   const Real_t y3 = y[3] ;\n  const Real_t y4 = y[4] ;   const Real_t y5 = y[5] ;\n  const Real_t y6 = y[6] ;   const Real_t y7 = y[7] ;\n\n  const Real_t z0 = z[0] ;   const Real_t z1 = z[1] ;\n  const Real_t z2 = z[2] ;   const Real_t z3 = z[3] ;\n  const Real_t z4 = z[4] ;   const Real_t z5 = z[5] ;\n  const Real_t z6 = z[6] ;   const Real_t z7 = z[7] ;\n\n  Real_t fjxxi, fjxet, fjxze;\n  Real_t fjyxi, fjyet, fjyze;\n  Real_t fjzxi, fjzet, fjzze;\n  Real_t cjxxi, cjxet, cjxze;\n  Real_t cjyxi, cjyet, cjyze;\n  Real_t cjzxi, cjzet, cjzze;\n\n  fjxxi = Real_t(.125) * ( (x6-x0) + (x5-x3) - (x7-x1) - (x4-x2) );\n  fjxet = Real_t(.125) * ( (x6-x0) - (x5-x3) + (x7-x1) - (x4-x2) );\n  fjxze = Real_t(.125) * ( (x6-x0) + (x5-x3) + (x7-x1) + (x4-x2) );\n\n  fjyxi = Real_t(.125) * ( (y6-y0) + (y5-y3) - (y7-y1) - (y4-y2) );\n  fjyet = Real_t(.125) * ( (y6-y0) - (y5-y3) + (y7-y1) - (y4-y2) );\n  fjyze = Real_t(.125) * ( (y6-y0) + (y5-y3) + (y7-y1) + (y4-y2) );\n\n  fjzxi = Real_t(.125) * ( (z6-z0) + (z5-z3) - (z7-z1) - (z4-z2) );\n  fjzet = Real_t(.125) * ( (z6-z0) - (z5-z3) + (z7-z1) - (z4-z2) );\n  fjzze = Real_t(.125) * ( (z6-z0) + (z5-z3) + (z7-z1) + (z4-z2) );\n\n  /* compute cofactors */\n  cjxxi =    (fjyet * fjzze) - (fjzet * fjyze);\n  cjxet =  - (fjyxi * fjzze) + (fjzxi * fjyze);\n  cjxze =    (fjyxi * fjzet) - (fjzxi * fjyet);\n\n  cjyxi =  - (fjxet * fjzze) + (fjzet * fjxze);\n  cjyet =    (fjxxi * fjzze) - (fjzxi * fjxze);\n  cjyze =  - (fjxxi * fjzet) + (fjzxi * fjxet);\n\n  cjzxi =    (fjxet * fjyze) - (fjyet * fjxze);\n  cjzet =  - (fjxxi * fjyze) + (fjyxi * fjxze);\n  cjzze =    (fjxxi * fjyet) - (fjyxi * fjxet);\n\n  /* calculate partials :\n     this need only be done for l = 0,1,2,3   since , by symmetry ,\n     (6,7,4,5) = - (0,1,2,3) .\n   */\n  b[0][0] =   -  cjxxi  -  cjxet  -  cjxze;\n  b[0][1] =      cjxxi  -  cjxet  -  cjxze;\n  b[0][2] =      cjxxi  +  cjxet  -  cjxze;\n  b[0][3] =   -  cjxxi  +  cjxet  -  cjxze;\n  b[0][4] = -b[0][2];\n  b[0][5] = -b[0][3];\n  b[0][6] = -b[0][0];\n  b[0][7] = -b[0][1];\n\n  b[1][0] =   -  cjyxi  -  cjyet  -  cjyze;\n  b[1][1] =      cjyxi  -  cjyet  -  cjyze;\n  b[1][2] =      cjyxi  +  cjyet  -  cjyze;\n  b[1][3] =   -  cjyxi  +  cjyet  -  cjyze;\n  b[1][4] = -b[1][2];\n  b[1][5] = -b[1][3];\n  b[1][6] = -b[1][0];\n  b[1][7] = -b[1][1];\n\n  b[2][0] =   -  cjzxi  -  cjzet  -  cjzze;\n  b[2][1] =      cjzxi  -  cjzet  -  cjzze;\n  b[2][2] =      cjzxi  +  cjzet  -  cjzze;\n  b[2][3] =   -  cjzxi  +  cjzet  -  cjzze;\n  b[2][4] = -b[2][2];\n  b[2][5] = -b[2][3];\n  b[2][6] = -b[2][0];\n  b[2][7] = -b[2][1];\n\n  /* calculate jacobian determinant (volume) */\n  *volume = Real_t(8.) * ( fjxet * cjxet + fjyet * cjyet + fjzet * cjzet);\n}\n\n__device__ static inline\nvoid SumElemStressesToNodeForces( const Real_t B[][8],\n    const Real_t stress_xx,\n    const Real_t stress_yy,\n    const Real_t stress_zz,\n    Real_t fx[], Real_t fy[], Real_t fz[] )\n{\n  for(Index_t i = 0; i < 8; i++) {\n    fx[i] = -( stress_xx * B[0][i] );\n    fy[i] = -( stress_yy * B[1][i] );\n    fz[i] = -( stress_zz * B[2][i] );\n  }\n}\n\n__global__ void integrateStress (\n    Real_t *__restrict__ fx_elem,\n    Real_t *__restrict__ fy_elem,\n    Real_t *__restrict__ fz_elem,\n    const Real_t *__restrict__ x,\n    const Real_t *__restrict__ y,\n    const Real_t *__restrict__ z,\n    const Index_t *__restrict__ nodelist,\n    const Real_t *__restrict__ sigxx,\n    const Real_t *__restrict__ sigyy,\n    const Real_t *__restrict__ sigzz,\n    Real_t *__restrict__ determ,\n    const Index_t numElem) \n{\n  Index_t k = blockDim.x*blockIdx.x+threadIdx.x;\n  if (k >= numElem) return;\n\n  const Index_t* const elemToNode = nodelist + Index_t(8)*k;\n  Real_t B[3][8] ;// shape function derivatives\n  Real_t x_local[8] ;\n  Real_t y_local[8] ;\n  Real_t z_local[8] ;\n  determ[k] = Real_t(10.0);\n\n  // get nodal coordinates from global arrays and copy into local arrays.\n  Index_t nd0i = elemToNode[0] ;\n  Index_t nd1i = elemToNode[1] ;\n  Index_t nd2i = elemToNode[2] ;\n  Index_t nd3i = elemToNode[3] ;\n  Index_t nd4i = elemToNode[4] ;\n  Index_t nd5i = elemToNode[5] ;\n  Index_t nd6i = elemToNode[6] ;\n  Index_t nd7i = elemToNode[7] ;\n\n  x_local[0] = x[nd0i];\n  x_local[1] = x[nd1i];\n  x_local[2] = x[nd2i];\n  x_local[3] = x[nd3i];\n  x_local[4] = x[nd4i];\n  x_local[5] = x[nd5i];\n  x_local[6] = x[nd6i];\n  x_local[7] = x[nd7i];\n\n  y_local[0] = y[nd0i];\n  y_local[1] = y[nd1i];\n  y_local[2] = y[nd2i];\n  y_local[3] = y[nd3i];\n  y_local[4] = y[nd4i];\n  y_local[5] = y[nd5i];\n  y_local[6] = y[nd6i];\n  y_local[7] = y[nd7i];\n\n  z_local[0] = z[nd0i];\n  z_local[1] = z[nd1i];\n  z_local[2] = z[nd2i];\n  z_local[3] = z[nd3i];\n  z_local[4] = z[nd4i];\n  z_local[5] = z[nd5i];\n  z_local[6] = z[nd6i];\n  z_local[7] = z[nd7i];\n\n  // Volume calculation involves extra work for numerical consistency\n  CalcElemShapeFunctionDerivatives(x_local, y_local, z_local, B, &determ[k]);\n\n  CalcElemNodeNormals( B[0], B[1], B[2], x_local, y_local, z_local );\n\n  // Eliminate thread writing conflicts at the nodes by giving\n  // each element its own copy to write to\n  SumElemStressesToNodeForces( B, sigxx[k], sigyy[k], sigzz[k],\n      &fx_elem[k*8],\n      &fy_elem[k*8],\n      &fz_elem[k*8] ) ;\n}",
            "#define Real_t float\n\n\n__global__ void acc_final_force (\n    const Real_t *__restrict__ fx_elem,\n    const Real_t *__restrict__ fy_elem,\n    const Real_t *__restrict__ fz_elem,\n    Real_t *__restrict__ fx,\n    Real_t *__restrict__ fy,\n    Real_t *__restrict__ fz,\n    const Index_t *__restrict__ nodeElemStart,\n    const Index_t *__restrict__ nodeElemCornerList,\n    const Index_t numNode) \n{\n  Index_t gnode = blockDim.x*blockIdx.x+threadIdx.x;\n  if (gnode >= numNode) return;\n  // element count\n  const Index_t count = nodeElemStart[gnode+1] - nodeElemStart[gnode];//domain.nodeElemCount(gnode) ;\n  // list of all corners\n  const Index_t *cornerList = nodeElemCornerList + nodeElemStart[gnode];//domain.nodeElemCornerList(gnode) ;\n  Real_t fx_tmp = Real_t(0.0) ;\n  Real_t fy_tmp = Real_t(0.0) ;\n  Real_t fz_tmp = Real_t(0.0) ;\n  for (Index_t i=0 ; i < count ; ++i) {\n    Index_t elem = cornerList[i] ;\n    fx_tmp += fx_elem[elem] ;\n    fy_tmp += fy_elem[elem] ;\n    fz_tmp += fz_elem[elem] ;\n  }\n  fx[gnode] = fx_tmp ;\n  fy[gnode] = fy_tmp ;\n  fz[gnode] = fz_tmp ;\n}",
            "#define Real_t float\n\n\n__device__ static inline\nvoid VoluDer(const Real_t x0, const Real_t x1, const Real_t x2,\n    const Real_t x3, const Real_t x4, const Real_t x5,\n    const Real_t y0, const Real_t y1, const Real_t y2,\n    const Real_t y3, const Real_t y4, const Real_t y5,\n    const Real_t z0, const Real_t z1, const Real_t z2,\n    const Real_t z3, const Real_t z4, const Real_t z5,\n    Real_t* dvdx, Real_t* dvdy, Real_t* dvdz)\n{\n  const Real_t twelfth = Real_t(1.0) / Real_t(12.0) ;\n\n  *dvdx =\n    (y1 + y2) * (z0 + z1) - (y0 + y1) * (z1 + z2) +\n    (y0 + y4) * (z3 + z4) - (y3 + y4) * (z0 + z4) -\n    (y2 + y5) * (z3 + z5) + (y3 + y5) * (z2 + z5);\n  *dvdy =\n    - (x1 + x2) * (z0 + z1) + (x0 + x1) * (z1 + z2) -\n    (x0 + x4) * (z3 + z4) + (x3 + x4) * (z0 + z4) +\n    (x2 + x5) * (z3 + z5) - (x3 + x5) * (z2 + z5);\n\n  *dvdz =\n    - (y1 + y2) * (x0 + x1) + (y0 + y1) * (x1 + x2) -\n    (y0 + y4) * (x3 + x4) + (y3 + y4) * (x0 + x4) +\n    (y2 + y5) * (x3 + x5) - (y3 + y5) * (x2 + x5);\n\n  *dvdx *= twelfth;\n  *dvdy *= twelfth;\n  *dvdz *= twelfth;\n}\n\n__device__ static inline\nvoid CalcElemVolumeDerivative(Real_t dvdx[8],\n    Real_t dvdy[8],\n    Real_t dvdz[8],\n    const Real_t x[8],\n    const Real_t y[8],\n    const Real_t z[8])\n{\n  VoluDer(x[1], x[2], x[3], x[4], x[5], x[7],\n      y[1], y[2], y[3], y[4], y[5], y[7],\n      z[1], z[2], z[3], z[4], z[5], z[7],\n      &dvdx[0], &dvdy[0], &dvdz[0]);\n  VoluDer(x[0], x[1], x[2], x[7], x[4], x[6],\n      y[0], y[1], y[2], y[7], y[4], y[6],\n      z[0], z[1], z[2], z[7], z[4], z[6],\n      &dvdx[3], &dvdy[3], &dvdz[3]);\n  VoluDer(x[3], x[0], x[1], x[6], x[7], x[5],\n      y[3], y[0], y[1], y[6], y[7], y[5],\n      z[3], z[0], z[1], z[6], z[7], z[5],\n      &dvdx[2], &dvdy[2], &dvdz[2]);\n  VoluDer(x[2], x[3], x[0], x[5], x[6], x[4],\n      y[2], y[3], y[0], y[5], y[6], y[4],\n      z[2], z[3], z[0], z[5], z[6], z[4],\n      &dvdx[1], &dvdy[1], &dvdz[1]);\n  VoluDer(x[7], x[6], x[5], x[0], x[3], x[1],\n      y[7], y[6], y[5], y[0], y[3], y[1],\n      z[7], z[6], z[5], z[0], z[3], z[1],\n      &dvdx[4], &dvdy[4], &dvdz[4]);\n  VoluDer(x[4], x[7], x[6], x[1], x[0], x[2],\n      y[4], y[7], y[6], y[1], y[0], y[2],\n      z[4], z[7], z[6], z[1], z[0], z[2],\n      &dvdx[5], &dvdy[5], &dvdz[5]);\n  VoluDer(x[5], x[4], x[7], x[2], x[1], x[3],\n      y[5], y[4], y[7], y[2], y[1], y[3],\n      z[5], z[4], z[7], z[2], z[1], z[3],\n      &dvdx[6], &dvdy[6], &dvdz[6]);\n  VoluDer(x[6], x[5], x[4], x[3], x[2], x[0],\n      y[6], y[5], y[4], y[3], y[2], y[0],\n      z[6], z[5], z[4], z[3], z[2], z[0],\n      &dvdx[7], &dvdy[7], &dvdz[7]);\n}\n\n__global__ void hgc (\n    Real_t *__restrict__ dvdx,\n    Real_t *__restrict__ dvdy,\n    Real_t *__restrict__ dvdz,\n    Real_t *__restrict__ x8n,\n    Real_t *__restrict__ y8n,\n    Real_t *__restrict__ z8n,\n    Real_t *__restrict__ determ,\n\n    const Real_t *__restrict__ x,\n    const Real_t *__restrict__ y,\n    const Real_t *__restrict__ z,\n    const Index_t *__restrict__ nodelist,\n    const Real_t *__restrict__ volo,\n    const Real_t *__restrict__ v,\n    int *__restrict__ vol_error,\n    const Index_t numElem )\n{\n  Index_t i = blockDim.x*blockIdx.x+threadIdx.x;\n  if (i >= numElem) return;\n\n  Real_t  x1[8],  y1[8],  z1[8] ;\n  Real_t pfx[8], pfy[8], pfz[8] ;\n\n  const Index_t* elemToNode = nodelist + Index_t(8)*i;\n\n  // CollectDomainNodesToElemNodes(domain, elemToNode, x1, y1, z1);\n\n  // inline the function manually\n  Index_t nd0i = elemToNode[0] ;\n  Index_t nd1i = elemToNode[1] ;\n  Index_t nd2i = elemToNode[2] ;\n  Index_t nd3i = elemToNode[3] ;\n  Index_t nd4i = elemToNode[4] ;\n  Index_t nd5i = elemToNode[5] ;\n  Index_t nd6i = elemToNode[6] ;\n  Index_t nd7i = elemToNode[7] ;\n\n  x1[0] = x[nd0i];\n  x1[1] = x[nd1i];\n  x1[2] = x[nd2i];\n  x1[3] = x[nd3i];\n  x1[4] = x[nd4i];\n  x1[5] = x[nd5i];\n  x1[6] = x[nd6i];\n  x1[7] = x[nd7i];\n\n  y1[0] = y[nd0i];\n  y1[1] = y[nd1i];\n  y1[2] = y[nd2i];\n  y1[3] = y[nd3i];\n  y1[4] = y[nd4i];\n  y1[5] = y[nd5i];\n  y1[6] = y[nd6i];\n  y1[7] = y[nd7i];\n\n  z1[0] = z[nd0i];\n  z1[1] = z[nd1i];\n  z1[2] = z[nd2i];\n  z1[3] = z[nd3i];\n  z1[4] = z[nd4i];\n  z1[5] = z[nd5i];\n  z1[6] = z[nd6i];\n  z1[7] = z[nd7i];\n\n  CalcElemVolumeDerivative(pfx, pfy, pfz, x1, y1, z1);\n\n  /* load into temporary storage for FB Hour Glass control */\n  for(Index_t ii=0;ii<8;++ii){\n    Index_t jj=8*i+ii;\n\n    dvdx[jj] = pfx[ii];\n    dvdy[jj] = pfy[ii];\n    dvdz[jj] = pfz[ii];\n    x8n[jj]  = x1[ii];\n    y8n[jj]  = y1[ii];\n    z8n[jj]  = z1[ii];\n  }\n\n  determ[i] = volo[i] * v[i];\n\n  /* Do a check for negative volumes */\n  if ( v[i] <= Real_t(0.0) ) {\n    vol_error[0] = i;\n  }\n}",
            "#define Real_t float\n\n\n__global__ void fb (\n    const Real_t *__restrict__ dvdx,\n    const Real_t *__restrict__ dvdy,\n    const Real_t *__restrict__ dvdz,\n    const Real_t *__restrict__ x8n,\n    const Real_t *__restrict__ y8n,\n    const Real_t *__restrict__ z8n,\n    const Real_t *__restrict__ determ,\n    const Real_t *__restrict__ xd,\n    const Real_t *__restrict__ yd,\n    const Real_t *__restrict__ zd,\n    const Real_t *__restrict__ ss,\n    const Real_t *__restrict__ elemMass,\n    const Index_t *__restrict__ nodelist,\n    const Real_t *__restrict__ gamma,\n    Real_t *__restrict__ fx_elem,\n    Real_t *__restrict__ fy_elem,\n    Real_t *__restrict__ fz_elem,\n    Real_t hgcoef,\n    const Index_t numElem )\n{\n  Index_t i2 = blockDim.x*blockIdx.x+threadIdx.x;\n  if (i2 >= numElem) return;\n\n  Index_t i3 = 8*i2;\n\n  const Index_t* elemToNode = nodelist + i3;\n\n  Real_t hgfx[8], hgfy[8], hgfz[8] ;\n\n  Real_t coefficient;\n\n  Real_t hourgam[8][4];\n  Real_t xd1[8], yd1[8], zd1[8] ;\n\n  Real_t volinv = ONE/determ[i2];\n  Real_t ss1, mass1, volume13 ;\n\n  for(Index_t i1=0;i1<4;++i1) {\n\n    Real_t hourmodx =\n      x8n[i3]   * gamma[i1*8+0] + x8n[i3+1] * gamma[i1*8+1] +\n      x8n[i3+2] * gamma[i1*8+2] + x8n[i3+3] * gamma[i1*8+3] +\n      x8n[i3+4] * gamma[i1*8+4] + x8n[i3+5] * gamma[i1*8+5] +\n      x8n[i3+6] * gamma[i1*8+6] + x8n[i3+7] * gamma[i1*8+7];\n\n    Real_t hourmody =\n      y8n[i3]   * gamma[i1*8+0] + y8n[i3+1] * gamma[i1*8+1] +\n      y8n[i3+2] * gamma[i1*8+2] + y8n[i3+3] * gamma[i1*8+3] +\n      y8n[i3+4] * gamma[i1*8+4] + y8n[i3+5] * gamma[i1*8+5] +\n      y8n[i3+6] * gamma[i1*8+6] + y8n[i3+7] * gamma[i1*8+7];\n\n    Real_t hourmodz =\n      z8n[i3]   * gamma[i1*8+0] + z8n[i3+1] * gamma[i1*8+1] +\n      z8n[i3+2] * gamma[i1*8+2] + z8n[i3+3] * gamma[i1*8+3] +\n      z8n[i3+4] * gamma[i1*8+4] + z8n[i3+5] * gamma[i1*8+5] +\n      z8n[i3+6] * gamma[i1*8+6] + z8n[i3+7] * gamma[i1*8+7];\n\n    hourgam[0][i1] = gamma[i1*8+0] - volinv*(dvdx[i3  ] * hourmodx +\n        dvdy[i3  ] * hourmody +\n        dvdz[i3  ] * hourmodz );\n\n    hourgam[1][i1] = gamma[i1*8+1] - volinv*(dvdx[i3+1] * hourmodx +\n        dvdy[i3+1] * hourmody +\n        dvdz[i3+1] * hourmodz );\n\n    hourgam[2][i1] = gamma[i1*8+2] - volinv*(dvdx[i3+2] * hourmodx +\n        dvdy[i3+2] * hourmody +\n        dvdz[i3+2] * hourmodz );\n\n    hourgam[3][i1] = gamma[i1*8+3] - volinv*(dvdx[i3+3] * hourmodx +\n        dvdy[i3+3] * hourmody +\n        dvdz[i3+3] * hourmodz );\n\n    hourgam[4][i1] = gamma[i1*8+4] - volinv*(dvdx[i3+4] * hourmodx +\n        dvdy[i3+4] * hourmody +\n        dvdz[i3+4] * hourmodz );\n\n    hourgam[5][i1] = gamma[i1*8+5] - volinv*(dvdx[i3+5] * hourmodx +\n        dvdy[i3+5] * hourmody +\n        dvdz[i3+5] * hourmodz );\n\n    hourgam[6][i1] = gamma[i1*8+6] - volinv*(dvdx[i3+6] * hourmodx +\n        dvdy[i3+6] * hourmody +\n        dvdz[i3+6] * hourmodz );\n\n    hourgam[7][i1] = gamma[i1*8+7] - volinv*(dvdx[i3+7] * hourmodx +\n        dvdy[i3+7] * hourmody +\n        dvdz[i3+7] * hourmodz );\n\n  }\n\n  /* compute forces */\n  /* store forces into h arrays (force arrays) */\n\n  ss1 = ss[i2];\n  mass1 = elemMass[i2];\n  volume13 = cbrt(determ[i2]);\n\n  Index_t n0si2 = elemToNode[0];\n  Index_t n1si2 = elemToNode[1];\n  Index_t n2si2 = elemToNode[2];\n  Index_t n3si2 = elemToNode[3];\n  Index_t n4si2 = elemToNode[4];\n  Index_t n5si2 = elemToNode[5];\n  Index_t n6si2 = elemToNode[6];\n  Index_t n7si2 = elemToNode[7];\n\n  xd1[0] = xd[n0si2];\n  xd1[1] = xd[n1si2];\n  xd1[2] = xd[n2si2];\n  xd1[3] = xd[n3si2];\n  xd1[4] = xd[n4si2];\n  xd1[5] = xd[n5si2];\n  xd1[6] = xd[n6si2];\n  xd1[7] = xd[n7si2];\n\n  yd1[0] = yd[n0si2];\n  yd1[1] = yd[n1si2];\n  yd1[2] = yd[n2si2];\n  yd1[3] = yd[n3si2];\n  yd1[4] = yd[n4si2];\n  yd1[5] = yd[n5si2];\n  yd1[6] = yd[n6si2];\n  yd1[7] = yd[n7si2];\n\n  zd1[0] = zd[n0si2];\n  zd1[1] = zd[n1si2];\n  zd1[2] = zd[n2si2];\n  zd1[3] = zd[n3si2];\n  zd1[4] = zd[n4si2];\n  zd1[5] = zd[n5si2];\n  zd1[6] = zd[n6si2];\n  zd1[7] = zd[n7si2];\n\n  coefficient = hgcoef * Real_t(-0.01) * ss1 * mass1 / volume13;\n\n  Real_t hxx[4], hyy[4], hzz[4];\n\n  for(Index_t i = 0; i < 4; i++) {\n    hxx[i] = hourgam[0][i] * xd1[0] + hourgam[1][i] * xd1[1] +\n      hourgam[2][i] * xd1[2] + hourgam[3][i] * xd1[3] +\n      hourgam[4][i] * xd1[4] + hourgam[5][i] * xd1[5] +\n      hourgam[6][i] * xd1[6] + hourgam[7][i] * xd1[7];\n  }\n  for(Index_t i = 0; i < 8; i++) {\n    hgfx[i] = coefficient *\n      (hourgam[i][0] * hxx[0] + hourgam[i][1] * hxx[1] +\n       hourgam[i][2] * hxx[2] + hourgam[i][3] * hxx[3]);\n  }\n  for(Index_t i = 0; i < 4; i++) {\n    hyy[i] = hourgam[0][i] * yd1[0] + hourgam[1][i] * yd1[1] +\n      hourgam[2][i] * yd1[2] + hourgam[3][i] * yd1[3] +\n      hourgam[4][i] * yd1[4] + hourgam[5][i] * yd1[5] +\n      hourgam[6][i] * yd1[6] + hourgam[7][i] * yd1[7];\n  }\n  for(Index_t i = 0; i < 8; i++) {\n    hgfy[i] = coefficient *\n      (hourgam[i][0] * hyy[0] + hourgam[i][1] * hyy[1] +\n       hourgam[i][2] * hyy[2] + hourgam[i][3] * hyy[3]);\n  }\n  for(Index_t i = 0; i < 4; i++) {\n    hzz[i] = hourgam[0][i] * zd1[0] + hourgam[1][i] * zd1[1] +\n      hourgam[2][i] * zd1[2] + hourgam[3][i] * zd1[3] +\n      hourgam[4][i] * zd1[4] + hourgam[5][i] * zd1[5] +\n      hourgam[6][i] * zd1[6] + hourgam[7][i] * zd1[7];\n  }\n  for(Index_t i = 0; i < 8; i++) {\n    hgfz[i] = coefficient *\n      (hourgam[i][0] * hzz[0] + hourgam[i][1] * hzz[1] +\n       hourgam[i][2] * hzz[2] + hourgam[i][3] * hzz[3]);\n  }\n\n  // With the threaded version, we write into local arrays per elem\n  // so we don't have to worry about race conditions\n\n  Real_t *fx_local = fx_elem + i3 ;\n  fx_local[0] = hgfx[0];\n  fx_local[1] = hgfx[1];\n  fx_local[2] = hgfx[2];\n  fx_local[3] = hgfx[3];\n  fx_local[4] = hgfx[4];\n  fx_local[5] = hgfx[5];\n  fx_local[6] = hgfx[6];\n  fx_local[7] = hgfx[7];\n\n  Real_t *fy_local = fy_elem + i3 ;\n  fy_local[0] = hgfy[0];\n  fy_local[1] = hgfy[1];\n  fy_local[2] = hgfy[2];\n  fy_local[3] = hgfy[3];\n  fy_local[4] = hgfy[4];\n  fy_local[5] = hgfy[5];\n  fy_local[6] = hgfy[6];\n  fy_local[7] = hgfy[7];\n\n  Real_t *fz_local = fz_elem + i3 ;\n  fz_local[0] = hgfz[0];\n  fz_local[1] = hgfz[1];\n  fz_local[2] = hgfz[2];\n  fz_local[3] = hgfz[3];\n  fz_local[4] = hgfz[4];\n  fz_local[5] = hgfz[5];\n  fz_local[6] = hgfz[6];\n  fz_local[7] = hgfz[7];\n}",
            "#define Real_t float\n\n\n__global__ void collect_final_force (\n    const Real_t *__restrict__ fx_elem,\n    const Real_t *__restrict__ fy_elem,\n    const Real_t *__restrict__ fz_elem,\n    Real_t *__restrict__ fx,\n    Real_t *__restrict__ fy,\n    Real_t *__restrict__ fz,\n    const Index_t *__restrict__ nodeElemStart,\n    const Index_t *__restrict__ nodeElemCornerList,\n    const Index_t numNode )\n{\n  Index_t gnode = blockDim.x*blockIdx.x+threadIdx.x;\n  if (gnode >= numNode) return;\n  // element count\n  const Index_t count = nodeElemStart[gnode+1] - nodeElemStart[gnode];//domain.nodeElemCount(gnode) ;\n  // list of all corners\n  const Index_t *cornerList = nodeElemCornerList + nodeElemStart[gnode];//domain.nodeElemCornerList(gnode) ;\n  Real_t fx_tmp = Real_t(0.0) ;\n  Real_t fy_tmp = Real_t(0.0) ;\n  Real_t fz_tmp = Real_t(0.0) ;\n  for (Index_t i=0 ; i < count ; ++i) {\n    Index_t elem = cornerList[i] ;\n    fx_tmp += fx_elem[elem] ;\n    fy_tmp += fy_elem[elem] ;\n    fz_tmp += fz_elem[elem] ;\n  }\n  fx[gnode] = fx_tmp ;\n  fy[gnode] = fy_tmp ;\n  fz[gnode] = fz_tmp ;\n}",
            "#define Real_t float\n\n\n__global__  void accelerationForNode (\n    const Real_t *__restrict__ fx,\n    const Real_t *__restrict__ fy,\n    const Real_t *__restrict__ fz,\n    const Real_t *__restrict__ nodalMass,\n    Real_t *__restrict__ xdd,\n    Real_t *__restrict__ ydd,\n    Real_t *__restrict__ zdd,\n    const Index_t numNode)\n{\n  Index_t i = blockDim.x*blockIdx.x+threadIdx.x;\n  if (i >= numNode) return;\n  Real_t one_over_nMass = Real_t(1.) / nodalMass[i];\n  xdd[i] = fx[i] * one_over_nMass;\n  ydd[i] = fy[i] * one_over_nMass;\n  zdd[i] = fz[i] * one_over_nMass;\n}",
            "#define Real_t float\n\n\n__global__ void applyAccelerationBoundaryConditionsForNodes (\n    const Index_t *__restrict__ symmX,\n    const Index_t *__restrict__ symmY,\n    const Index_t *__restrict__ symmZ,\n    Real_t *__restrict__ xdd,\n    Real_t *__restrict__ ydd,\n    Real_t *__restrict__ zdd,\n    const Index_t s1,\n    const Index_t s2,\n    const Index_t s3,\n    const Index_t numNodeBC ) \n{\n  Index_t i = blockDim.x*blockIdx.x+threadIdx.x;\n  if (i >= numNodeBC) return;\n  if (s1 == 0) \n    xdd[symmX[i]] = Real_t(0.0);\n  if (s2 == 0) ydd[symmY[i]] = Real_t(0.0);\n  if (s3 == 0) zdd[symmZ[i]] = Real_t(0.0);\n}",
            "#define Real_t float\n\n\ninline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\ninline __host__ __device__ float4 fabs(float4 v)\n{\n    return make_float4(fabs(v.x), fabs(v.y), fabs(v.z), fabs(v.w));\n}\n\n__global__ void  calcVelocityForNodes (\n    Real_t *__restrict__ xd,\n    Real_t *__restrict__ yd,\n    Real_t *__restrict__ zd,\n    const Real_t *__restrict__ xdd,\n    const Real_t *__restrict__ ydd,\n    const Real_t *__restrict__ zdd,\n    const Real_t deltaTime,\n    const Real_t u_cut,\n    const Index_t numNode )\n{\n  Index_t i = blockDim.x*blockIdx.x+threadIdx.x;\n  if (i >= numNode) return;\n\n  Real_t xdtmp = xd[i] + xdd[i] * deltaTime;\n  // FABS is not compiled with target regions in mind\n  // To get around this, compute the absolute value manually:\n  // if( xdtmp > Real_t(0.0) && xdtmp < u_cut || Real_t(-1.0) * xdtmp < u_cut)\n  if( fabs(xdtmp) < u_cut ) xdtmp = Real_t(0.0);\n  xd[i] = xdtmp ;\n\n  Real_t ydtmp = yd[i] + ydd[i] * deltaTime;\n  if( fabs(ydtmp) < u_cut ) ydtmp = Real_t(0.0);\n  yd[i] = ydtmp ;\n\n  Real_t zdtmp = zd[i] + zdd[i] * deltaTime;\n  if( fabs(zdtmp) < u_cut ) zdtmp = Real_t(0.0);\n  zd[i] = zdtmp ;\n}",
            "#define Real_t float\n\n\n__global__ void calcPositionForNodes (\n    Real_t *__restrict__ x,\n    Real_t *__restrict__ y,\n    Real_t *__restrict__ z,\n    const Real_t *__restrict__ xd,\n    const Real_t *__restrict__ yd,\n    const Real_t *__restrict__ zd,\n    const Real_t deltaTime,\n    const Index_t numNode) \n{\n  Index_t i = blockDim.x*blockIdx.x+threadIdx.x;\n  if (i >= numNode) return;\n  x[i] += xd[i] * deltaTime;\n  y[i] += yd[i] * deltaTime;\n  z[i] += zd[i] * deltaTime;\n}",
            "#define Real_t float\n\n\ninline __host__ __device__ uint4 make_uint4(int4 a)\n{\n    return make_uint4(uint(a.x), uint(a.y), uint(a.z), uint(a.w));\n}\n\ninline __host__ __device__ uint4 max(uint4 a, uint4 b)\n{\n    return make_uint4(max(a.x,b.x), max(a.y,b.y), max(a.z,b.z), max(a.w,b.w));\n}\n\n__device__ static inline\nReal_t AreaFace( const Real_t x0, const Real_t x1,\n    const Real_t x2, const Real_t x3,\n    const Real_t y0, const Real_t y1,\n    const Real_t y2, const Real_t y3,\n    const Real_t z0, const Real_t z1,\n    const Real_t z2, const Real_t z3)\n{\n  Real_t fx = (x2 - x0) - (x3 - x1);\n  Real_t fy = (y2 - y0) - (y3 - y1);\n  Real_t fz = (z2 - z0) - (z3 - z1);\n  Real_t gx = (x2 - x0) + (x3 - x1);\n  Real_t gy = (y2 - y0) + (y3 - y1);\n  Real_t gz = (z2 - z0) + (z3 - z1);\n  Real_t area =\n    (fx * fx + fy * fy + fz * fz) *\n    (gx * gx + gy * gy + gz * gz) -\n    (fx * gx + fy * gy + fz * gz) *\n    (fx * gx + fy * gy + fz * gz);\n  return area ;\n}\n\n__host__ __device__ static inline\nReal_t calcElemVolume( const Real_t x0, const Real_t x1,\n    const Real_t x2, const Real_t x3,\n    const Real_t x4, const Real_t x5,\n    const Real_t x6, const Real_t x7,\n    const Real_t y0, const Real_t y1,\n    const Real_t y2, const Real_t y3,\n    const Real_t y4, const Real_t y5,\n    const Real_t y6, const Real_t y7,\n    const Real_t z0, const Real_t z1,\n    const Real_t z2, const Real_t z3,\n    const Real_t z4, const Real_t z5,\n    const Real_t z6, const Real_t z7 )\n{\n  Real_t twelveth = Real_t(1.0)/Real_t(12.0);\n\n  Real_t dx61 = x6 - x1;\n  Real_t dy61 = y6 - y1;\n  Real_t dz61 = z6 - z1;\n\n  Real_t dx70 = x7 - x0;\n  Real_t dy70 = y7 - y0;\n  Real_t dz70 = z7 - z0;\n\n  Real_t dx63 = x6 - x3;\n  Real_t dy63 = y6 - y3;\n  Real_t dz63 = z6 - z3;\n\n  Real_t dx20 = x2 - x0;\n  Real_t dy20 = y2 - y0;\n  Real_t dz20 = z2 - z0;\n\n  Real_t dx50 = x5 - x0;\n  Real_t dy50 = y5 - y0;\n  Real_t dz50 = z5 - z0;\n\n  Real_t dx64 = x6 - x4;\n  Real_t dy64 = y6 - y4;\n  Real_t dz64 = z6 - z4;\n\n  Real_t dx31 = x3 - x1;\n  Real_t dy31 = y3 - y1;\n  Real_t dz31 = z3 - z1;\n\n  Real_t dx72 = x7 - x2;\n  Real_t dy72 = y7 - y2;\n  Real_t dz72 = z7 - z2;\n\n  Real_t dx43 = x4 - x3;\n  Real_t dy43 = y4 - y3;\n  Real_t dz43 = z4 - z3;\n\n  Real_t dx57 = x5 - x7;\n  Real_t dy57 = y5 - y7;\n  Real_t dz57 = z5 - z7;\n\n  Real_t dx14 = x1 - x4;\n  Real_t dy14 = y1 - y4;\n  Real_t dz14 = z1 - z4;\n\n  Real_t dx25 = x2 - x5;\n  Real_t dy25 = y2 - y5;\n  Real_t dz25 = z2 - z5;\n\n#define TRIPLE_PRODUCT(x1, y1, z1, x2, y2, z2, x3, y3, z3) \\\n  ((x1)*((y2)*(z3) - (z2)*(y3)) + (x2)*((z1)*(y3) - (y1)*(z3)) + (x3)*((y1)*(z2) - (z1)*(y2)))\n\n  Real_t volume =\n    TRIPLE_PRODUCT(dx31 + dx72, dx63, dx20,\n        dy31 + dy72, dy63, dy20,\n        dz31 + dz72, dz63, dz20) +\n    TRIPLE_PRODUCT(dx43 + dx57, dx64, dx70,\n        dy43 + dy57, dy64, dy70,\n        dz43 + dz57, dz64, dz70) +\n    TRIPLE_PRODUCT(dx14 + dx25, dx61, dx50,\n        dy14 + dy25, dy61, dy50,\n        dz14 + dz25, dz61, dz50);\n\n#undef TRIPLE_PRODUCT\n\n  volume *= twelveth;\n\n  return volume ;\n}\n\n__device__ static inline\nReal_t CalcElemCharacteristicLength( const Real_t x[8],\n    const Real_t y[8],\n    const Real_t z[8],\n    const Real_t volume)\n{\n  Real_t a, charLength = Real_t(0.0);\n\n  a = AreaFace(x[0],x[1],x[2],x[3],\n      y[0],y[1],y[2],y[3],\n      z[0],z[1],z[2],z[3]) ;\n  charLength = max(a,charLength) ;\n\n  a = AreaFace(x[4],x[5],x[6],x[7],\n      y[4],y[5],y[6],y[7],\n      z[4],z[5],z[6],z[7]) ;\n  charLength = max(a,charLength) ;\n\n  a = AreaFace(x[0],x[1],x[5],x[4],\n      y[0],y[1],y[5],y[4],\n      z[0],z[1],z[5],z[4]) ;\n  charLength = max(a,charLength) ;\n\n  a = AreaFace(x[1],x[2],x[6],x[5],\n      y[1],y[2],y[6],y[5],\n      z[1],z[2],z[6],z[5]) ;\n  charLength = max(a,charLength) ;\n\n  a = AreaFace(x[2],x[3],x[7],x[6],\n      y[2],y[3],y[7],y[6],\n      z[2],z[3],z[7],z[6]) ;\n  charLength = max(a,charLength) ;\n\n  a = AreaFace(x[3],x[0],x[4],x[7],\n      y[3],y[0],y[4],y[7],\n      z[3],z[0],z[4],z[7]) ;\n  charLength = max(a,charLength) ;\n\n  charLength = Real_t(4.0) * volume / sqrt(charLength);\n\n  return charLength;\n}\n\n__device__ static inline\nvoid CalcElemShapeFunctionDerivatives( Real_t const x[],\n    Real_t const y[],\n    Real_t const z[],\n    Real_t b[][8],\n    Real_t* const volume )\n{\n  const Real_t x0 = x[0] ;   const Real_t x1 = x[1] ;\n  const Real_t x2 = x[2] ;   const Real_t x3 = x[3] ;\n  const Real_t x4 = x[4] ;   const Real_t x5 = x[5] ;\n  const Real_t x6 = x[6] ;   const Real_t x7 = x[7] ;\n\n  const Real_t y0 = y[0] ;   const Real_t y1 = y[1] ;\n  const Real_t y2 = y[2] ;   const Real_t y3 = y[3] ;\n  const Real_t y4 = y[4] ;   const Real_t y5 = y[5] ;\n  const Real_t y6 = y[6] ;   const Real_t y7 = y[7] ;\n\n  const Real_t z0 = z[0] ;   const Real_t z1 = z[1] ;\n  const Real_t z2 = z[2] ;   const Real_t z3 = z[3] ;\n  const Real_t z4 = z[4] ;   const Real_t z5 = z[5] ;\n  const Real_t z6 = z[6] ;   const Real_t z7 = z[7] ;\n\n  Real_t fjxxi, fjxet, fjxze;\n  Real_t fjyxi, fjyet, fjyze;\n  Real_t fjzxi, fjzet, fjzze;\n  Real_t cjxxi, cjxet, cjxze;\n  Real_t cjyxi, cjyet, cjyze;\n  Real_t cjzxi, cjzet, cjzze;\n\n  fjxxi = Real_t(.125) * ( (x6-x0) + (x5-x3) - (x7-x1) - (x4-x2) );\n  fjxet = Real_t(.125) * ( (x6-x0) - (x5-x3) + (x7-x1) - (x4-x2) );\n  fjxze = Real_t(.125) * ( (x6-x0) + (x5-x3) + (x7-x1) + (x4-x2) );\n\n  fjyxi = Real_t(.125) * ( (y6-y0) + (y5-y3) - (y7-y1) - (y4-y2) );\n  fjyet = Real_t(.125) * ( (y6-y0) - (y5-y3) + (y7-y1) - (y4-y2) );\n  fjyze = Real_t(.125) * ( (y6-y0) + (y5-y3) + (y7-y1) + (y4-y2) );\n\n  fjzxi = Real_t(.125) * ( (z6-z0) + (z5-z3) - (z7-z1) - (z4-z2) );\n  fjzet = Real_t(.125) * ( (z6-z0) - (z5-z3) + (z7-z1) - (z4-z2) );\n  fjzze = Real_t(.125) * ( (z6-z0) + (z5-z3) + (z7-z1) + (z4-z2) );\n\n  /* compute cofactors */\n  cjxxi =    (fjyet * fjzze) - (fjzet * fjyze);\n  cjxet =  - (fjyxi * fjzze) + (fjzxi * fjyze);\n  cjxze =    (fjyxi * fjzet) - (fjzxi * fjyet);\n\n  cjyxi =  - (fjxet * fjzze) + (fjzet * fjxze);\n  cjyet =    (fjxxi * fjzze) - (fjzxi * fjxze);\n  cjyze =  - (fjxxi * fjzet) + (fjzxi * fjxet);\n\n  cjzxi =    (fjxet * fjyze) - (fjyet * fjxze);\n  cjzet =  - (fjxxi * fjyze) + (fjyxi * fjxze);\n  cjzze =    (fjxxi * fjyet) - (fjyxi * fjxet);\n\n  /* calculate partials :\n     this need only be done for l = 0,1,2,3   since , by symmetry ,\n     (6,7,4,5) = - (0,1,2,3) .\n   */\n  b[0][0] =   -  cjxxi  -  cjxet  -  cjxze;\n  b[0][1] =      cjxxi  -  cjxet  -  cjxze;\n  b[0][2] =      cjxxi  +  cjxet  -  cjxze;\n  b[0][3] =   -  cjxxi  +  cjxet  -  cjxze;\n  b[0][4] = -b[0][2];\n  b[0][5] = -b[0][3];\n  b[0][6] = -b[0][0];\n  b[0][7] = -b[0][1];\n\n  b[1][0] =   -  cjyxi  -  cjyet  -  cjyze;\n  b[1][1] =      cjyxi  -  cjyet  -  cjyze;\n  b[1][2] =      cjyxi  +  cjyet  -  cjyze;\n  b[1][3] =   -  cjyxi  +  cjyet  -  cjyze;\n  b[1][4] = -b[1][2];\n  b[1][5] = -b[1][3];\n  b[1][6] = -b[1][0];\n  b[1][7] = -b[1][1];\n\n  b[2][0] =   -  cjzxi  -  cjzet  -  cjzze;\n  b[2][1] =      cjzxi  -  cjzet  -  cjzze;\n  b[2][2] =      cjzxi  +  cjzet  -  cjzze;\n  b[2][3] =   -  cjzxi  +  cjzet  -  cjzze;\n  b[2][4] = -b[2][2];\n  b[2][5] = -b[2][3];\n  b[2][6] = -b[2][0];\n  b[2][7] = -b[2][1];\n\n  /* calculate jacobian determinant (volume) */\n  *volume = Real_t(8.) * ( fjxet * cjxet + fjyet * cjyet + fjzet * cjzet);\n}\n\n__device__ static inline\nvoid CalcElemVelocityGradient( const Real_t* const xvel,\n    const Real_t* const yvel,\n    const Real_t* const zvel,\n    const Real_t b[][8],\n    const Real_t detJ,\n    Real_t* const d )\n{\n  const Real_t inv_detJ = Real_t(1.0) / detJ ;\n  Real_t dyddx, dxddy, dzddx, dxddz, dzddy, dyddz;\n  const Real_t* const pfx = b[0];\n  const Real_t* const pfy = b[1];\n  const Real_t* const pfz = b[2];\n\n  d[0] = inv_detJ * ( pfx[0] * (xvel[0]-xvel[6])\n      + pfx[1] * (xvel[1]-xvel[7])\n      + pfx[2] * (xvel[2]-xvel[4])\n      + pfx[3] * (xvel[3]-xvel[5]) );\n\n  d[1] = inv_detJ * ( pfy[0] * (yvel[0]-yvel[6])\n      + pfy[1] * (yvel[1]-yvel[7])\n      + pfy[2] * (yvel[2]-yvel[4])\n      + pfy[3] * (yvel[3]-yvel[5]) );\n\n  d[2] = inv_detJ * ( pfz[0] * (zvel[0]-zvel[6])\n      + pfz[1] * (zvel[1]-zvel[7])\n      + pfz[2] * (zvel[2]-zvel[4])\n      + pfz[3] * (zvel[3]-zvel[5]) );\n\n  dyddx  = inv_detJ * ( pfx[0] * (yvel[0]-yvel[6])\n      + pfx[1] * (yvel[1]-yvel[7])\n      + pfx[2] * (yvel[2]-yvel[4])\n      + pfx[3] * (yvel[3]-yvel[5]) );\n\n  dxddy  = inv_detJ * ( pfy[0] * (xvel[0]-xvel[6])\n      + pfy[1] * (xvel[1]-xvel[7])\n      + pfy[2] * (xvel[2]-xvel[4])\n      + pfy[3] * (xvel[3]-xvel[5]) );\n\n  dzddx  = inv_detJ * ( pfx[0] * (zvel[0]-zvel[6])\n      + pfx[1] * (zvel[1]-zvel[7])\n      + pfx[2] * (zvel[2]-zvel[4])\n      + pfx[3] * (zvel[3]-zvel[5]) );\n\n  dxddz  = inv_detJ * ( pfz[0] * (xvel[0]-xvel[6])\n      + pfz[1] * (xvel[1]-xvel[7])\n      + pfz[2] * (xvel[2]-xvel[4])\n      + pfz[3] * (xvel[3]-xvel[5]) );\n\n  dzddy  = inv_detJ * ( pfy[0] * (zvel[0]-zvel[6])\n      + pfy[1] * (zvel[1]-zvel[7])\n      + pfy[2] * (zvel[2]-zvel[4])\n      + pfy[3] * (zvel[3]-zvel[5]) );\n\n  dyddz  = inv_detJ * ( pfz[0] * (yvel[0]-yvel[6])\n      + pfz[1] * (yvel[1]-yvel[7])\n      + pfz[2] * (yvel[2]-yvel[4])\n      + pfz[3] * (yvel[3]-yvel[5]) );\n  d[5]  = Real_t( .5) * ( dxddy + dyddx );\n  d[4]  = Real_t( .5) * ( dxddz + dzddx );\n  d[3]  = Real_t( .5) * ( dzddy + dyddz );\n}\n\n__host__  __device__\nReal_t CalcElemVolume( const Real_t x[8], const Real_t y[8], const Real_t z[8] )\n{\n  return calcElemVolume(x[0], x[1], x[2], x[3], x[4], x[5], x[6], x[7],\n      y[0], y[1], y[2], y[3], y[4], y[5], y[6], y[7],\n      z[0], z[1], z[2], z[3], z[4], z[5], z[6], z[7]);\n}\n\n__global__ void calcKinematicsForElems ( \n    const Real_t *__restrict__ xd,\n    const Real_t *__restrict__ yd,\n    const Real_t *__restrict__ zd,\n    const Real_t *__restrict__ x,\n    const Real_t *__restrict__ y,\n    const Real_t *__restrict__ z,\n    const Index_t *__restrict__ nodeList,\n    const Real_t *__restrict__ volo,\n    const Real_t *__restrict__ v,\n    Real_t *__restrict__ delv,\n    Real_t *__restrict__ arealg,\n    Real_t *__restrict__ dxx,\n    Real_t *__restrict__ dyy,\n    Real_t *__restrict__ dzz,\n    Real_t *__restrict__ vnew,\n    const Real_t deltaTime,\n    const Index_t numElem )\n{\n  Index_t k = blockDim.x*blockIdx.x+threadIdx.x;\n  if (k >= numElem) return;\n\n  Real_t B[3][8] ; // shape function derivatives \n  Real_t D[6] ;\n  Real_t x_local[8] ;\n  Real_t y_local[8] ;\n  Real_t z_local[8] ;\n  Real_t xd_local[8] ;\n  Real_t yd_local[8] ;\n  Real_t zd_local[8] ;\n  Real_t detJ = Real_t(0.0) ;\n\n  Real_t volume ;\n  Real_t relativeVolume ;\n  const Index_t* elemToNode = nodeList + Index_t(8)*k;\n\n  // get nodal coordinates from global arrays and copy into local arrays.\n\n  Index_t nd0i = elemToNode[0] ;\n  Index_t nd1i = elemToNode[1] ;\n  Index_t nd2i = elemToNode[2] ;\n  Index_t nd3i = elemToNode[3] ;\n  Index_t nd4i = elemToNode[4] ;\n  Index_t nd5i = elemToNode[5] ;\n  Index_t nd6i = elemToNode[6] ;\n  Index_t nd7i = elemToNode[7] ;\n\n  x_local[0] = x[nd0i];\n  x_local[1] = x[nd1i];\n  x_local[2] = x[nd2i];\n  x_local[3] = x[nd3i];\n  x_local[4] = x[nd4i];\n  x_local[5] = x[nd5i];\n  x_local[6] = x[nd6i];\n  x_local[7] = x[nd7i];\n\n  y_local[0] = y[nd0i];\n  y_local[1] = y[nd1i];\n  y_local[2] = y[nd2i];\n  y_local[3] = y[nd3i];\n  y_local[4] = y[nd4i];\n  y_local[5] = y[nd5i];\n  y_local[6] = y[nd6i];\n  y_local[7] = y[nd7i];\n\n  z_local[0] = z[nd0i];\n  z_local[1] = z[nd1i];\n  z_local[2] = z[nd2i];\n  z_local[3] = z[nd3i];\n  z_local[4] = z[nd4i];\n  z_local[5] = z[nd5i];\n  z_local[6] = z[nd6i];\n  z_local[7] = z[nd7i];\n\n  // volume calculations\n  volume = CalcElemVolume(x_local, y_local, z_local );\n  relativeVolume = volume / volo[k] ;\n  vnew[k] = relativeVolume ;\n  delv[k] = relativeVolume - v[k] ;\n\n  // set characteristic length\n  arealg[k] = CalcElemCharacteristicLength(x_local, y_local, z_local,\n      volume);\n\n  // get nodal velocities from global array and copy into local arrays.\n  for( Index_t lnode=0 ; lnode<8 ; ++lnode )\n  {\n    Index_t gnode = elemToNode[lnode];\n    xd_local[lnode] = xd[gnode];\n    yd_local[lnode] = yd[gnode];\n    zd_local[lnode] = zd[gnode];\n  }\n\n  Real_t dt2 = Real_t(0.5) * deltaTime;\n  for ( Index_t j=0 ; j<8 ; ++j )\n  {\n    x_local[j] -= dt2 * xd_local[j];\n    y_local[j] -= dt2 * yd_local[j];\n    z_local[j] -= dt2 * zd_local[j];\n  }\n\n  CalcElemShapeFunctionDerivatives( x_local, y_local, z_local,\n      B, &detJ );\n\n  CalcElemVelocityGradient( xd_local, yd_local, zd_local,\n      B, detJ, D );\n\n  // put velocity gradient quantities into their global arrays.\n  dxx[k] = D[0];\n  dyy[k] = D[1];\n  dzz[k] = D[2];\n}",
            "#define Real_t float\n\n\n__global__ void calcStrainRates(\n    Real_t *__restrict__ dxx,\n    Real_t *__restrict__ dyy,\n    Real_t *__restrict__ dzz,\n    const Real_t *__restrict__ vnew,\n    Real_t *__restrict__ vdov,\n    int *__restrict__ vol_error,\n    const Index_t numElem )\n{\n  Index_t k = blockDim.x*blockIdx.x+threadIdx.x;\n  if (k >= numElem) return;\n\n  // calc strain rate and apply as constraint (only done in FB element)\n  Real_t vvdov = dxx[k] + dyy[k] + dzz[k] ;\n  Real_t vdovthird = vvdov/Real_t(3.0) ;\n\n  // make the rate of deformation tensor deviatoric\n  vdov[k] = vvdov;\n  dxx[k] -= vdovthird ;  //LG:   why to update dxx?  it is deallocated right after\n  dyy[k] -= vdovthird ;\n  dzz[k] -= vdovthird ;\n\n  // See if any volumes are negative, and take appropriate action.\n  if (vnew[k] <= Real_t(0.0))\n  {\n    vol_error[0] = k;\n  }\n}",
            "#define Real_t float\n\n\n__global__ void calcMonotonicQGradientsForElems (\n    const Real_t *__restrict__ xd,\n    const Real_t *__restrict__ yd,\n    const Real_t *__restrict__ zd,\n    const Real_t *__restrict__ x,\n    const Real_t *__restrict__ y,\n    const Real_t *__restrict__ z,\n    const Index_t *__restrict__ nodelist,\n    const Real_t *__restrict__ volo,\n    Real_t *__restrict__ delv_eta,\n    Real_t *__restrict__ delx_eta,\n    Real_t *__restrict__ delv_zeta,\n    Real_t *__restrict__ delx_zeta,\n    Real_t *__restrict__ delv_xi,\n    Real_t *__restrict__ delx_xi,\n    const Real_t *__restrict__ vnew,\n    const Index_t numElem )\n{\n  Index_t i = blockDim.x*blockIdx.x+threadIdx.x;\n  if (i >= numElem) return;\n\n  Real_t ax,ay,az ;\n  Real_t dxv,dyv,dzv ;\n\n  const Index_t *elemToNode = nodelist + Index_t(8) * i;\n  Index_t n0 = elemToNode[0] ;\n  Index_t n1 = elemToNode[1] ;\n  Index_t n2 = elemToNode[2] ;\n  Index_t n3 = elemToNode[3] ;\n  Index_t n4 = elemToNode[4] ;\n  Index_t n5 = elemToNode[5] ;\n  Index_t n6 = elemToNode[6] ;\n  Index_t n7 = elemToNode[7] ;\n\n  Real_t x0 = x[n0] ;\n  Real_t x1 = x[n1] ;\n  Real_t x2 = x[n2] ;\n  Real_t x3 = x[n3] ;\n  Real_t x4 = x[n4] ;\n  Real_t x5 = x[n5] ;\n  Real_t x6 = x[n6] ;\n  Real_t x7 = x[n7] ;\n\n  Real_t y0 = y[n0] ;\n  Real_t y1 = y[n1] ;\n  Real_t y2 = y[n2] ;\n  Real_t y3 = y[n3] ;\n  Real_t y4 = y[n4] ;\n  Real_t y5 = y[n5] ;\n  Real_t y6 = y[n6] ;\n  Real_t y7 = y[n7] ;\n\n  Real_t z0 = z[n0] ;\n  Real_t z1 = z[n1] ;\n  Real_t z2 = z[n2] ;\n  Real_t z3 = z[n3] ;\n  Real_t z4 = z[n4] ;\n  Real_t z5 = z[n5] ;\n  Real_t z6 = z[n6] ;\n  Real_t z7 = z[n7] ;\n\n  Real_t xv0 = xd[n0] ;\n  Real_t xv1 = xd[n1] ;\n  Real_t xv2 = xd[n2] ;\n  Real_t xv3 = xd[n3] ;\n  Real_t xv4 = xd[n4] ;\n  Real_t xv5 = xd[n5] ;\n  Real_t xv6 = xd[n6] ;\n  Real_t xv7 = xd[n7] ;\n\n  Real_t yv0 = yd[n0] ;\n  Real_t yv1 = yd[n1] ;\n  Real_t yv2 = yd[n2] ;\n  Real_t yv3 = yd[n3] ;\n  Real_t yv4 = yd[n4] ;\n  Real_t yv5 = yd[n5] ;\n  Real_t yv6 = yd[n6] ;\n  Real_t yv7 = yd[n7] ;\n\n  Real_t zv0 = zd[n0] ;\n  Real_t zv1 = zd[n1] ;\n  Real_t zv2 = zd[n2] ;\n  Real_t zv3 = zd[n3] ;\n  Real_t zv4 = zd[n4] ;\n  Real_t zv5 = zd[n5] ;\n  Real_t zv6 = zd[n6] ;\n  Real_t zv7 = zd[n7] ;\n\n  Real_t vol = volo[i] * vnew[i] ;\n  Real_t norm = Real_t(1.0) / ( vol + PTINY ) ;\n\n  Real_t dxj = Real_t(-0.25)*((x0+x1+x5+x4) - (x3+x2+x6+x7)) ;\n  Real_t dyj = Real_t(-0.25)*((y0+y1+y5+y4) - (y3+y2+y6+y7)) ;\n  Real_t dzj = Real_t(-0.25)*((z0+z1+z5+z4) - (z3+z2+z6+z7)) ;\n\n  Real_t dxi = Real_t( 0.25)*((x1+x2+x6+x5) - (x0+x3+x7+x4)) ;\n  Real_t dyi = Real_t( 0.25)*((y1+y2+y6+y5) - (y0+y3+y7+y4)) ;\n  Real_t dzi = Real_t( 0.25)*((z1+z2+z6+z5) - (z0+z3+z7+z4)) ;\n\n  Real_t dxk = Real_t( 0.25)*((x4+x5+x6+x7) - (x0+x1+x2+x3)) ;\n  Real_t dyk = Real_t( 0.25)*((y4+y5+y6+y7) - (y0+y1+y2+y3)) ;\n  Real_t dzk = Real_t( 0.25)*((z4+z5+z6+z7) - (z0+z1+z2+z3)) ;\n\n  /* find delvk and delxk ( i cross j ) */\n\n  ax = dyi*dzj - dzi*dyj ;\n  ay = dzi*dxj - dxi*dzj ;\n  az = dxi*dyj - dyi*dxj ;\n\n  delx_zeta[i] = vol / sqrt(ax*ax + ay*ay + az*az + PTINY) ;\n\n  ax *= norm ;\n  ay *= norm ;\n  az *= norm ;\n\n  dxv = Real_t(0.25)*((xv4+xv5+xv6+xv7) - (xv0+xv1+xv2+xv3)) ;\n  dyv = Real_t(0.25)*((yv4+yv5+yv6+yv7) - (yv0+yv1+yv2+yv3)) ;\n  dzv = Real_t(0.25)*((zv4+zv5+zv6+zv7) - (zv0+zv1+zv2+zv3)) ;\n\n  delv_zeta[i] = ax*dxv + ay*dyv + az*dzv ;\n\n  /* find delxi and delvi ( j cross k ) */\n\n  ax = dyj*dzk - dzj*dyk ;\n  ay = dzj*dxk - dxj*dzk ;\n  az = dxj*dyk - dyj*dxk ;\n\n  delx_xi[i] = vol / sqrt(ax*ax + ay*ay + az*az + PTINY) ;\n\n  ax *= norm ;\n  ay *= norm ;\n  az *= norm ;\n\n  dxv = Real_t(0.25)*((xv1+xv2+xv6+xv5) - (xv0+xv3+xv7+xv4)) ;\n  dyv = Real_t(0.25)*((yv1+yv2+yv6+yv5) - (yv0+yv3+yv7+yv4)) ;\n  dzv = Real_t(0.25)*((zv1+zv2+zv6+zv5) - (zv0+zv3+zv7+zv4)) ;\n\n  delv_xi[i] = ax*dxv + ay*dyv + az*dzv ;\n\n  /* find delxj and delvj ( k cross i ) */\n\n  ax = dyk*dzi - dzk*dyi ;\n  ay = dzk*dxi - dxk*dzi ;\n  az = dxk*dyi - dyk*dxi ;\n\n  delx_eta[i] = vol / sqrt(ax*ax + ay*ay + az*az + PTINY) ;\n\n  ax *= norm ;\n  ay *= norm ;\n  az *= norm ;\n\n  dxv = Real_t(-0.25)*((xv0+xv1+xv5+xv4) - (xv3+xv2+xv6+xv7)) ;\n  dyv = Real_t(-0.25)*((yv0+yv1+yv5+yv4) - (yv3+yv2+yv6+yv7)) ;\n  dzv = Real_t(-0.25)*((zv0+zv1+zv5+zv4) - (zv3+zv2+zv6+zv7)) ;\n\n  delv_eta[i] = ax*dxv + ay*dyv + az*dzv ;\n}",
            "#define Real_t float\n\n\n__global__ void calcMonotonicQForElems (\n    const Index_t *__restrict__ elemBC,\n    const Real_t *__restrict__ elemMass,\n    Real_t *__restrict__ ql,\n    Real_t *__restrict__ qq,\n    const Real_t *__restrict__ vdov,\n    const Real_t *__restrict__ volo,\n    const Real_t *__restrict__ delv_eta,\n    const Real_t *__restrict__ delx_eta,\n    const Real_t *__restrict__ delv_zeta,\n    const Real_t *__restrict__ delx_zeta,\n    const Real_t *__restrict__ delv_xi,\n    const Real_t *__restrict__ delx_xi,\n    const Index_t *__restrict__ lxim,\n    const Index_t *__restrict__ lxip,\n    const Index_t *__restrict__ lzetam,\n    const Index_t *__restrict__ lzetap,\n    const Index_t *__restrict__ letap,\n    const Index_t *__restrict__ letam,\n    const Real_t *__restrict__ vnew,\n    const Real_t monoq_limiter_mult,\n    const Real_t monoq_max_slope,\n    const Real_t qlc_monoq,\n    const Real_t qqc_monoq,\n    const Index_t numElem )\n{\n  Index_t i = blockDim.x*blockIdx.x+threadIdx.x;\n  if (i >= numElem) return;\n\n  Real_t qlin, qquad ;\n  Real_t phixi, phieta, phizeta ;\n  Int_t bcMask = elemBC[i] ;\n  Real_t delvm = 0.0, delvp =0.0;\n\n  /*  phixi     */\n  Real_t norm = Real_t(1.) / (delv_xi[i]+ PTINY ) ;\n\n  switch (bcMask & XI_M) {\n    case XI_M_COMM: /* needs comm data */\n    case 0:         delvm = delv_xi[lxim[i]]; break ;\n    case XI_M_SYMM: delvm = delv_xi[i] ;       break ;\n    case XI_M_FREE: delvm = Real_t(0.0) ;      break ;\n    default: //fprintf(stderr, \"Error in switch at %s line %d\\n\", __FILE__, __LINE__);\n        delvm = 0; /* ERROR - but quiets the compiler */\n        break;\n  }\n  switch (bcMask & XI_P) {\n    case XI_P_COMM: /* needs comm data */\n    case 0:         delvp = delv_xi[lxip[i]] ; break ;\n    case XI_P_SYMM: delvp = delv_xi[i] ;       break ;\n    case XI_P_FREE: delvp = Real_t(0.0) ;      break ;\n    default: //fprintf(stderr, \"Error in switch at %s line %d\\n\", __FILE__, __LINE__);\n        delvp = 0; /* ERROR - but quiets the compiler */\n        break;\n  }\n\n  delvm = delvm * norm ;\n  delvp = delvp * norm ;\n\n  phixi = Real_t(.5) * ( delvm + delvp ) ;\n\n  delvm *= monoq_limiter_mult ;\n  delvp *= monoq_limiter_mult ;\n\n  if ( delvm < phixi ) phixi = delvm ;\n  if ( delvp < phixi ) phixi = delvp ;\n  if ( phixi < Real_t(0.)) phixi = Real_t(0.) ;\n  if ( phixi > monoq_max_slope) phixi = monoq_max_slope;\n\n\n  /*  phieta     */\n  norm = Real_t(1.) / ( delv_eta[i] + PTINY ) ;\n\n  switch (bcMask & ETA_M) {\n    case ETA_M_COMM: /* needs comm data */\n    case 0:          delvm = delv_eta[letam[i]] ; break ;\n    case ETA_M_SYMM: delvm = delv_eta[i] ;        break ;\n    case ETA_M_FREE: delvm = Real_t(0.0) ;        break ;\n    default: //fprintf(stderr, \"Error in switch at %s line %d\\n\", __FILE__, __LINE__);\n         delvm = 0; /* ERROR - but quiets the compiler */\n         break;\n  }\n  switch (bcMask & ETA_P) {\n    case ETA_P_COMM: /* needs comm data */\n    case 0:          delvp = delv_eta[letap[i]] ; break ;\n    case ETA_P_SYMM: delvp = delv_eta[i] ;        break ;\n    case ETA_P_FREE: delvp = Real_t(0.0) ;        break ;\n    default: \n         delvp = 0; /* ERROR - but quiets the compiler */\n         break;\n  }\n\n  delvm = delvm * norm ;\n  delvp = delvp * norm ;\n\n  phieta = Real_t(.5) * ( delvm + delvp ) ;\n\n  delvm *= monoq_limiter_mult ;\n  delvp *= monoq_limiter_mult ;\n\n  if ( delvm  < phieta ) phieta = delvm ;\n  if ( delvp  < phieta ) phieta = delvp ;\n  if ( phieta < Real_t(0.)) phieta = Real_t(0.) ;\n  if ( phieta > monoq_max_slope)  phieta = monoq_max_slope;\n\n  /*  phizeta     */\n  norm = Real_t(1.) / ( delv_zeta[i] + PTINY ) ;\n\n  switch (bcMask & ZETA_M) {\n    case ZETA_M_COMM: /* needs comm data */\n    case 0:           delvm = delv_zeta[lzetam[i]] ; break ;\n    case ZETA_M_SYMM: delvm = delv_zeta[i] ;         break ;\n    case ZETA_M_FREE: delvm = Real_t(0.0) ;          break ;\n    default: \n          delvm = 0; /* ERROR - but quiets the compiler */\n          break;\n  }\n  switch (bcMask & ZETA_P) {\n    case ZETA_P_COMM: /* needs comm data */\n    case 0:           delvp = delv_zeta[lzetap[i]] ; break ;\n    case ZETA_P_SYMM: delvp = delv_zeta[i] ;         break ;\n    case ZETA_P_FREE: delvp = Real_t(0.0) ;          break ;\n    default:\n          delvp = 0; /* ERROR - but quiets the compiler */\n          break;\n  }\n\n  delvm = delvm * norm ;\n  delvp = delvp * norm ;\n\n  phizeta = Real_t(.5) * ( delvm + delvp ) ;\n\n  delvm *= monoq_limiter_mult ;\n  delvp *= monoq_limiter_mult ;\n\n  if ( delvm   < phizeta ) phizeta = delvm ;\n  if ( delvp   < phizeta ) phizeta = delvp ;\n  if ( phizeta < Real_t(0.)) phizeta = Real_t(0.);\n  if ( phizeta > monoq_max_slope  ) phizeta = monoq_max_slope;\n\n  /* Remove length scale */\n\n  if ( vdov[i] > Real_t(0.) )  {\n    qlin  = Real_t(0.) ;\n    qquad = Real_t(0.) ;\n  }\n  else {\n    Real_t delvxxi   = delv_xi[i]   * delx_xi[i]   ;\n    Real_t delvxeta  = delv_eta[i]  * delx_eta[i]  ;\n    Real_t delvxzeta = delv_zeta[i] * delx_zeta[i] ;\n\n    if ( delvxxi   > Real_t(0.) ) delvxxi   = Real_t(0.) ;\n    if ( delvxeta  > Real_t(0.) ) delvxeta  = Real_t(0.) ;\n    if ( delvxzeta > Real_t(0.) ) delvxzeta = Real_t(0.) ;\n\n    Real_t rho = elemMass[i] / (volo[i] * vnew[i]) ;\n\n    qlin = -qlc_monoq * rho *\n      (  delvxxi   * (Real_t(1.) - phixi) +\n         delvxeta  * (Real_t(1.) - phieta) +\n         delvxzeta * (Real_t(1.) - phizeta)  ) ;\n\n    qquad = qqc_monoq * rho *\n      (  delvxxi*delvxxi     * (Real_t(1.) - phixi*phixi) +\n         delvxeta*delvxeta   * (Real_t(1.) - phieta*phieta) +\n         delvxzeta*delvxzeta * (Real_t(1.) - phizeta*phizeta)  ) ;\n  }\n\n  qq[i] = qquad ;\n  ql[i] = qlin  ;\n}",
            "#define Real_t float\n\n\ninline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\ninline __host__ __device__ float4 fabs(float4 v)\n{\n    return make_float4(fabs(v.x), fabs(v.y), fabs(v.z), fabs(v.w));\n}\n\n__global__ void applyMaterialPropertiesForElems(\n    const Real_t *__restrict__ ql,\n    const Real_t *__restrict__ qq,\n    const Real_t *__restrict__ delv,\n    const Index_t *__restrict__ elemRep,\n    const Index_t *__restrict__ elemElem,\n    Real_t *__restrict__ q,\n    Real_t *__restrict__ p,\n    Real_t *__restrict__ e,\n    Real_t *__restrict__ ss,\n    Real_t *__restrict__ v,\n    Real_t *__restrict__ vnewc,\n    const Real_t  e_cut,\n    const Real_t  p_cut,\n    const Real_t  ss4o3,\n    const Real_t  q_cut,\n    const Real_t  v_cut,\n\n    const Real_t eosvmax,\n    const Real_t eosvmin,\n    const Real_t pmin,\n    const Real_t emin,\n    const Real_t rho0,\n    const Index_t numElem )\n{\n  Index_t elem = blockDim.x*blockIdx.x+threadIdx.x;\n  if (elem >= numElem) return;\n  Index_t rep = elemRep[elem];\n  Real_t e_old, delvc, p_old, q_old, qq_old, ql_old;\n  Real_t p_new, q_new, e_new;\n  Real_t work, compression, compHalfStep, bvc, pbvc, pHalfStep;\n  Real_t vchalf ;\n  Real_t vhalf ;\n  Real_t ssc ;\n  Real_t q_tilde ;\n  Real_t ssTmp ;\n\n  if (eosvmin != ZERO) {\n    if (vnewc[elem] < eosvmin)\n      vnewc[elem] = eosvmin ;\n  }\n\n  if (eosvmax != ZERO) {\n    if (vnewc[elem] > eosvmax)\n      vnewc[elem] = eosvmax ;\n  }\n\n  // This check may not make perfect sense in LULESH, but\n  // it's representative of something in the full code -\n  // just leave it in, please\n  Real_t vc = v[elem] ;\n  if (eosvmin != ZERO) {\n    if (vc < eosvmin)\n      vc = eosvmin ;\n  }\n  if (eosvmax != ZERO) {\n    if (vc > eosvmax)\n      vc = eosvmax ;\n  }\n\n  Real_t vnewc_t = vnewc[elem];\n\n  Real_t e_temp    =    e[elem];\n  Real_t delv_temp = delv[elem];\n  Real_t p_temp    =    p[elem];\n  Real_t q_temp    =    q[elem];\n  Real_t qq_temp   =   qq[elem];\n  Real_t ql_temp   =   ql[elem];\n  for(Index_t j = 0; j < rep; j++) {\n\n    e_old  =    e_temp ;\n    delvc  = delv_temp ;\n    p_old  =    p_temp ;\n    q_old  =    q_temp ;\n    qq_old =   qq_temp ;\n    ql_old =   ql_temp ;\n\n    compression = ONE / vnewc_t - ONE;\n    vchalf = vnewc_t - delvc * HALF;\n    compHalfStep = ONE / vchalf - ONE;\n    if (vnewc_t <= eosvmin) { /* impossible due to calling func? */\n      compHalfStep = compression ;\n    }\n    if (vnewc_t >= eosvmax) { /* impossible due to calling func? */\n      p_old        = ZERO ;\n      compression  = ZERO ;\n      compHalfStep = ZERO ;\n    }\n    work = ZERO ;\n\n    e_new = e_old - HALF * delvc * (p_old + q_old)\n      + HALF * work;\n\n    if (e_new  < emin ) {\n      e_new = emin ;\n    }\n\n    bvc = C1S * (compHalfStep + ONE);\n    pbvc = C1S;\n\n    pHalfStep = bvc * e_new ;\n\n    if    (fabs(pHalfStep) <  p_cut   )\n      pHalfStep = ZERO ;\n\n    if    ( vnewc_t >= eosvmax ) /* impossible condition here? */\n      pHalfStep = ZERO ;\n\n    if    (pHalfStep      <  pmin)\n      pHalfStep   = pmin ;\n\n    vhalf = ONE / (ONE + compHalfStep) ;\n\n    if ( delvc > ZERO ) {\n      q_new /* = qq_old[elem] = ql_old[elem] */ = ZERO ;\n    } else {\n      ssc = ( pbvc * e_new + vhalf * vhalf * bvc * pHalfStep ) / rho0 ;\n\n      if ( ssc <= C1 ) {\n        ssc = C2 ;\n      } else {\n        ssc = sqrt(ssc) ;\n      }\n\n      q_new = (ssc*ql_old + qq_old) ;\n    }\n\n    e_new = e_new + HALF * delvc\n      * (THREE*(p_old     + q_old)\n          - FOUR*(pHalfStep + q_new)) ;\n\n    e_new += HALF * work;\n\n    if (fabs(e_new) < e_cut) {\n      e_new = ZERO  ;\n    }\n    if (     e_new  < emin ) {\n      e_new = emin ;\n    }\n\n    bvc = C1S * (compression + ONE);\n    pbvc = C1S;\n\n    p_new = bvc * e_new ;\n\n    if    (fabs(p_new) <  p_cut   )\n      p_new = ZERO ;\n\n    if    ( vnewc_t >= eosvmax ) /* impossible condition here? */\n      p_new = ZERO ;\n\n    if    (p_new  <  pmin)\n      p_new   = pmin ;\n\n\n    if (delvc > ZERO) {\n      q_tilde = ZERO ;\n    }\n    else {\n      Real_t ssc = ( pbvc * e_new + vnewc_t * vnewc_t * bvc * p_new ) / rho0 ;\n\n      if ( ssc <= C1 ) {\n        ssc = C2 ;\n      } else {\n        ssc = sqrt(ssc) ;\n      }\n\n      q_tilde = (ssc * ql_old + qq_old) ;\n    }\n\n    e_new = e_new - (  SEVEN*(p_old     + q_old)\n        - EIGHT*(pHalfStep + q_new)\n        + (p_new + q_tilde)) * delvc*SIXTH ;\n\n    if (fabs(e_new) < e_cut) {\n      e_new = ZERO  ;\n    }\n    if (e_new < emin) {\n      e_new = emin ;\n    }\n\n    bvc = C1S * (compression + ONE);\n    pbvc = C1S;\n\n    p_new = bvc * e_new ;\n\n    if ( fabs(p_new) <  p_cut )\n      p_new = ZERO ;\n\n    if ( vnewc_t >= eosvmax ) /* impossible condition here? */\n      p_new = ZERO ;\n\n    if (p_new < pmin)\n      p_new = pmin ;\n    if ( delvc <= ZERO ) {\n      ssc = ( pbvc * e_new + vnewc_t * vnewc_t * bvc * p_new ) / rho0 ;\n\n      if ( ssc <= C1 ) {\n        ssc = C2 ;\n      } else {\n        ssc = sqrt(ssc) ;\n      }\n\n      q_new = (ssc*ql_old + qq_old) ;\n\n      if (fabs(q_new) < q_cut) q_new = ZERO ;\n    }\n  } //this is the end of the rep loop\n\n  p[elem] = p_new ;\n  e[elem] = e_new ;\n  q[elem] = q_new ;\n\n  ssTmp = (pbvc * e_new + vnewc_t * vnewc_t * bvc * p_new) / rho0;\n  if (ssTmp <= C1) {\n    ssTmp = C2;\n  } else {\n    ssTmp = sqrt(ssTmp);\n  }\n  ss[elem] = ssTmp ;\n\n  if ( fabs(vnewc_t - ONE) < v_cut )\n    vnewc_t = ONE ;\n\n  v[elem] = vnewc_t ;\n}"
        ]
    },
    "swish-cuda": {
        "/Users/gbolet/hecbench-roofline/src/swish-cuda/main.cu": [
            "#define N\t\t\t\t\t  6\n\n\n#define T ((int)32)\n\n\n__global__\nvoid SwishKernel(const int N, const T* X, T* Y)\n{\n  KERNEL_LOOP(i, N) {\n    Y[i] = __ldg(X + i) / (T(1) + exp(-__ldg(X + i)));\n  }\n}",
            "#define N\t\t\t\t\t  6\n\n\n#define T ((int)32)\n\n\n__global__\nvoid SwishGradientKernel(\n    const int N,\n    const T* X,\n    const T* Y,\n    const T* dY,\n          T* dX)\n{\n  KERNEL_LOOP(i, N) {\n    dX[i] = __ldg(dY + i) *\n            (__ldg(Y + i) + (T(1) - __ldg(Y + i)) / (T(1) + exp(-__ldg(X + i))));\n  }\n}"
        ]
    },
    "sph-cuda": {
        "/Users/gbolet/hecbench-roofline/src/sph-cuda/fluid.cu": [
            "__device__\ndouble del_W(double3 p_pos, double3 q_pos, double h)\n{\n    double r = sqrt((p_pos.x-q_pos.x)*(p_pos.x-q_pos.x)\n                  + (p_pos.y-q_pos.y)*(p_pos.y-q_pos.y)\n                  + (p_pos.z-q_pos.z)*(p_pos.z-q_pos.z));\n    double C = 1.0/(M_PI * h*h*h);\n    double u = r/h;\n    double val = 0.0;\n    if(u >= 2.0)\n        return val;\n    else if(u < 1.0 )\n        val = -1.0/(h*h) * (3.0 - 9.0/4.0*u);\n    else if(u >= 1.0 && u < 2.0)\n        val = -3.0/(4.0*h*r) * pow(2.0-u,2.0);\n\n    val *= C;\n    return val;\n}\n\n__device__\ndouble computeDensity(double3 p_pos, double3 p_v, double3 q_pos, double3 q_v,\n                      const param *params)\n{\n    double v_x = (p_v.x - q_v.x);\n    double v_y = (p_v.y - q_v.y);\n    double v_z = (p_v.z - q_v.z);\n\n    double density = params->mass_particle * del_W(p_pos,q_pos,\n                                                   params->smoothing_radius);\n    double density_x = density * v_x * (p_pos.x - q_pos.x);\n    double density_y = density * v_y * (p_pos.y - q_pos.y);\n    double density_z = density * v_z * (p_pos.z - q_pos.z);\n\n    density = (density_x + density_y + density_z)*params->time_step;\n\n    return density;\n}\n\n__device__\ndouble computePressure(double p_density, const param *params)\n{\n    double gam = 7.0;\n    double B = params->rest_density * params->speed_sound*params->speed_sound / gam;\n    double pressure =  B * (pow((p_density/params->rest_density),gam) - 1.0);\n\n    return pressure;\n}\n\n__global__\nvoid updatePressures(fluid_particle *__restrict__ fluid_particles,\n                     const param *__restrict__ params)\n{\n    int num_fluid_particles = params->number_fluid_particles;\n    int i = blockDim.x * blockIdx.x + threadIdx.x;\n    if (i >= num_fluid_particles) return;\n    double3 p_pos = fluid_particles[i].pos;\n    double3 p_v   = fluid_particles[i].v;\n    double density = fluid_particles[i].density;\n\n    for(int j=0; j< num_fluid_particles; j++) {\n        double3 q_pos = fluid_particles[j].pos;\n        double3 q_v   = fluid_particles[j].v;\n        density += computeDensity(p_pos,p_v,q_pos,q_v, params);\n    }\n    fluid_particles[i].density = density;\n    fluid_particles[i].pressure = computePressure(density, params);\n}",
            "__device__\ndouble W(double3 p_pos, double3 q_pos, double h)\n{\n    double r = sqrt((p_pos.x-q_pos.x)*(p_pos.x-q_pos.x)\n                  + (p_pos.y-q_pos.y)*(p_pos.y-q_pos.y)\n                  + (p_pos.z-q_pos.z)*(p_pos.z-q_pos.z));\n    double C = 1.0/(M_PI*h*h*h);\n    double u = r/h;\n    double val = 0.0;\n    if(u >= 2.0)\n        return val;\n    else if(u < 1.0 )\n        val = 1.0 - (3.0/2.0)*u*u + (3.0/4.0)*u*u*u;\n    else if(u >= 1.0 && u < 2.0)\n        val = (1.0/4.0) * pow(2.0-u,3.0);\n\n    val *= C;\n    return val;\n}\n\n__device__\ndouble del_W(double3 p_pos, double3 q_pos, double h)\n{\n    double r = sqrt((p_pos.x-q_pos.x)*(p_pos.x-q_pos.x)\n                  + (p_pos.y-q_pos.y)*(p_pos.y-q_pos.y)\n                  + (p_pos.z-q_pos.z)*(p_pos.z-q_pos.z));\n    double C = 1.0/(M_PI * h*h*h);\n    double u = r/h;\n    double val = 0.0;\n    if(u >= 2.0)\n        return val;\n    else if(u < 1.0 )\n        val = -1.0/(h*h) * (3.0 - 9.0/4.0*u);\n    else if(u >= 1.0 && u < 2.0)\n        val = -3.0/(4.0*h*r) * pow(2.0-u,2.0);\n\n    val *= C;\n    return val;\n}\n\n__device__\ndouble3 computeAcceleration(double3 p_pos, double3 p_v, double p_density,\n                            double p_pressure, double3 q_pos, double3 q_v,\n                            double q_density, double q_pressure, const param *const params)\n{\n    double3 a;\n    double accel;\n    double h = params->smoothing_radius;\n    double alpha = params->alpha;\n    double speed_sound = params->speed_sound;\n    double mass_particle = params->mass_particle;\n    double surface_tension = params->surface_tension;\n\n    // Pressure force\n    accel = (p_pressure/(p_density*p_density) + q_pressure/(q_density*q_density))\n            * mass_particle * del_W(p_pos,q_pos,h);\n    a.x = -accel * (p_pos.x - q_pos.x);\n    a.y = -accel * (p_pos.y - q_pos.y);\n    a.z = -accel * (p_pos.z - q_pos.z);\n\n    // Viscosity force\n    double VdotR = (p_v.x-q_v.x)*(p_pos.x-q_pos.x)\n                 + (p_v.y-q_v.y)*(p_pos.y-q_pos.y)\n                 + (p_v.z-q_v.z)*(p_pos.z-q_pos.z);\n    if(VdotR < 0.0)\n    {\n        double nu = 2.0 * alpha * h * speed_sound / (p_density + q_density);\n        double r2 = (p_pos.x-q_pos.x)*(p_pos.x-q_pos.x)\n                  + (p_pos.y-q_pos.y)*(p_pos.y-q_pos.y)\n                  + (p_pos.z-q_pos.z)*(p_pos.z-q_pos.z);\n        double eps = h/10.0;\n        double stress = nu * VdotR / (r2 + eps*h*h);\n        accel = mass_particle * stress * del_W(p_pos, q_pos, h);\n        a.x += accel * (p_pos.x - q_pos.x);\n        a.y += accel * (p_pos.y - q_pos.y);\n        a.z += accel * (p_pos.z - q_pos.z);\n    }\n\n    //Surface tension\n    // BT 07 http://cg.informatik.uni-freiburg.de/publications/2011_GRAPP_airBubbles.pdf\n    accel = surface_tension * W(p_pos,q_pos,h);\n    a.x += accel * (p_pos.x - q_pos.x);\n    a.y += accel * (p_pos.y - q_pos.y);\n    a.z += accel * (p_pos.z - q_pos.z);\n\n    return a;\n}\n\n__global__\nvoid updateAccelerationsFP(fluid_particle *__restrict__ fluid_particles,\n                           const param *__restrict__ params)\n{\n    int num_fluid_particles = params->number_fluid_particles;\n\n    int i = blockDim.x * blockIdx.x + threadIdx.x;\n    if (i >= num_fluid_particles) return;\n\n        double ax = 0.0;\n        double ay = 0.0;\n        double az = -9.8;\n\n        double3 p_pos = fluid_particles[i].pos;\n        double3 p_v   = fluid_particles[i].v;\n        double p_density = fluid_particles[i].density;\n        double p_pressure = fluid_particles[i].pressure;\n\n        for(int j=0; j<num_fluid_particles; j++) {\n            if (i!=j) {\n                double3 q_pos = fluid_particles[j].pos;\n                double3 q_v   = fluid_particles[j].v;\n                double q_density = fluid_particles[j].density;\n                double q_pressure = fluid_particles[j].pressure;\n                double3 tmp_a = computeAcceleration(p_pos, p_v, p_density,\n                                                    p_pressure, q_pos, q_v,\n                                                    q_density, q_pressure, params);\n\n                ax += tmp_a.x;\n                ay += tmp_a.y;\n                az += tmp_a.z;\n            }\n        }\n\n        fluid_particles[i].a.x = ax;\n        fluid_particles[i].a.y = ay;\n        fluid_particles[i].a.z = az;\n}",
            "__device__\ndouble boundaryGamma(double3 p_pos, double3 k_pos, double3 k_n, double h, double speed_sound)\n{\n    // Radial distance between p,q\n    double r = sqrt((p_pos.x-k_pos.x)*(p_pos.x-k_pos.x)\n                  + (p_pos.y-k_pos.y)*(p_pos.y-k_pos.y)\n                  + (p_pos.z-k_pos.z)*(p_pos.z-k_pos.z));\n    // Distance to p normal to surface particle\n    double y = sqrt((p_pos.x-k_pos.x)*(p_pos.x-k_pos.x)*(k_n.x*k_n.x)\n                  + (p_pos.y-k_pos.y)*(p_pos.y-k_pos.y)*(k_n.y*k_n.y)\n                  + (p_pos.z-k_pos.z)*(p_pos.z-k_pos.z)*(k_n.z*k_n.z));\n    // Tangential distance\n    double x = r-y;\n\n    double u = y/h;\n    double xi = (1-x/h)?x<h:0.0;\n    double C = xi*2.0*0.02 * speed_sound * speed_sound / y;\n    double val = 0.0;\n\n    if(u > 0.0 && u < 2.0/3.0)\n        val = 2.0/3.0;\n    else if(u < 1.0 && u > 2.0/3.0 )\n        val = (2*u - 3.0/2.0*u*u);\n    else if (u < 2.0 && u > 1.0)\n        val = 0.5*(2.0-u)*(2.0-u);\n    else\n        val = 0.0;\n\n    val *= C;\n\n    return val;\n}\n\n__device__\ndouble3 computeBoundaryAcceleration(double3 p_pos, double3 k_pos, double3 k_n,\n                                    double h, double speed_sound)\n{\n    double3 p_a;\n    double bGamma = boundaryGamma(p_pos,k_pos,k_n,h,speed_sound);\n    p_a.x = bGamma * k_n.x;\n    p_a.y = bGamma * k_n.y;\n    p_a.z = bGamma * k_n.z;\n\n    return p_a;\n}\n\n__global__\nvoid updateAccelerationsBP(fluid_particle *__restrict__ fluid_particles,\n                           const boundary_particle *__restrict__ boundary_particles, \n                           const param *__restrict__ params)\n{\n    int num_fluid_particles = params->number_fluid_particles;\n    int num_boundary_particles = params->number_boundary_particles;\n    int i = blockDim.x * blockIdx.x + threadIdx.x;\n    if (i >= num_fluid_particles) return;\n\n    double ax = fluid_particles[i].a.x;\n    double ay = fluid_particles[i].a.y;\n    double az = fluid_particles[i].a.z;\n    double3 p_pos = fluid_particles[i].pos;\n\n    for (int j=0; j<num_boundary_particles; j++) {\n      double3 k_pos = boundary_particles[j].pos;\n      double3 k_n   = boundary_particles[j].n;\n      double3 tmp_a = computeBoundaryAcceleration(p_pos,k_pos,k_n,\n          params->smoothing_radius,\n          params->speed_sound);\n      ax += tmp_a.x;\n      ay += tmp_a.y;\n      az += tmp_a.z;\n    }\n\n    fluid_particles[i].a.x = ax;\n    fluid_particles[i].a.y = ay;\n    fluid_particles[i].a.z = az;\n}",
            "__global__\nvoid updatePositions(fluid_particle *__restrict__ fluid_particles,\n                     const param *__restrict__ params)\n{\n    double dt = params->time_step;\n\n    int num_fluid_particles = params->number_fluid_particles;\n    int i = blockDim.x * blockIdx.x + threadIdx.x;\n    if (i >= num_fluid_particles) return;\n\n    // Velocity at t + dt/2\n    double3 v_half = fluid_particles[i].v_half;\n    double3 v      = fluid_particles[i].v;\n    double3 pos    = fluid_particles[i].pos;\n    double3 a      = fluid_particles[i].a;\n\n    v_half.x = v_half.x + dt * a.x;\n    v_half.y = v_half.y + dt * a.y;\n    v_half.z = v_half.z + dt * a.z;\n\n    // Velocity at t + dt, must estimate for foce calc\n    v.x = v_half.x + a.x * (dt / 2.0);\n    v.y = v_half.y + a.y * (dt / 2.0);\n    v.z = v_half.z + a.z * (dt / 2.0);\n\n    // Position at time t + dt\n    pos.x = pos.x + dt * v_half.x;\n    pos.y = pos.y + dt * v_half.y;\n    pos.z = pos.z + dt * v_half.z;\n\n    fluid_particles[i].v_half = v_half;\n    fluid_particles[i].v      = v;\n    fluid_particles[i].pos    = pos;\n}"
        ]
    },
    "che-cuda": {
        "/Users/gbolet/hecbench-roofline/src/che-cuda/kernels.h": [
            "__device__ double Laplacian(const double c[][DATAYSIZE][DATAXSIZE],\n                            double dx, double dy, double dz, int x, int y, int z)\n{\n  int xp, xn, yp, yn, zp, zn;\n\n  int nx = (int)DATAXSIZE - 1;\n  int ny = (int)DATAYSIZE - 1;\n  int nz = (int)DATAZSIZE - 1;\n\n  xp = x+1;\n  xn = x-1;\n  yp = y+1;\n  yn = y-1;\n  zp = z+1;\n  zn = z-1;\n\n  if (xp > nx) xp = 0;\n  if (yp > ny) yp = 0;\n  if (zp > nz) zp = 0;\n  if (xn < 0)  xn = nx;\n  if (yn < 0)  yn = ny;\n  if (zn < 0)  zn = nz;\n\n  double cxx = (c[z][y][xp] + c[z][y][xn] - 2.0*c[z][y][x]) / (dx*dx);\n  double cyy = (c[z][yp][x] + c[z][yn][x] - 2.0*c[z][y][x]) / (dy*dy);\n  double czz = (c[zp][y][x] + c[zn][y][x] - 2.0*c[z][y][x]) / (dz*dz);\n\n  return cxx + cyy + czz;\n}\n\n__global__ void chemicalPotential(\n    const double c[][DATAYSIZE][DATAXSIZE], \n    double mu[][DATAYSIZE][DATAXSIZE], \n    double dx,\n    double dy,\n    double dz,\n    double gamma,\n    double e_AA,\n    double e_BB,\n    double e_AB)\n{\n  unsigned idx = blockIdx.x*blockDim.x + threadIdx.x;\n  unsigned idy = blockIdx.y*blockDim.y + threadIdx.y;\n  unsigned idz = blockIdx.z*blockDim.z + threadIdx.z;\n\n  if ((idx < DATAXSIZE) && (idy < DATAYSIZE) && (idz < DATAZSIZE)) {\n\n    mu[idz][idy][idx] = 4.5 * ( ( c[idz][idy][idx] + 1.0 ) * e_AA + \n        ( c[idz][idy][idx] - 1 ) * e_BB - 2.0 * c[idz][idy][idx] * e_AB ) + \n      3.0 * c[idz][idy][idx] + c[idz][idy][idx] * c[idz][idy][idx] * c[idz][idy][idx] - \n      gamma * Laplacian(c,dx,dy,dz,idx,idy,idz);\n  }\n}",
            "__device__ double GradientX(const double phi[][DATAYSIZE][DATAXSIZE], \n                            double dx, double dy, double dz, int x, int y, int z)\n{\n  int nx = (int)DATAXSIZE - 1;\n  int xp = x+1;\n  int xn = x-1;\n\n  if (xp > nx) xp = 0;\n  if (xn < 0)  xn = nx;\n\n  return (phi[z][y][xp] - phi[z][y][xn]) / (2.0*dx);\n}\n\n__device__ double GradientY(const double phi[][DATAYSIZE][DATAXSIZE], \n                            double dx, double dy, double dz, int x, int y, int z)\n{\n  int ny = (int)DATAYSIZE - 1;\n  int yp = y+1;\n  int yn = y-1;\n\n  if (yp > ny) yp = 0;\n  if (yn < 0)  yn = ny;\n\n  return (phi[z][yp][x] - phi[z][yn][x]) / (2.0*dy);\n}\n\n__device__ double GradientZ(const double phi[][DATAYSIZE][DATAXSIZE],\n                            double dx, double dy, double dz, int x, int y, int z)\n{\n  int nz = (int)DATAZSIZE - 1;\n  int zp = z+1;\n  int zn = z-1;\n\n  if (zp > nz) zp = 0;\n  if (zn < 0)  zn = nz;\n\n  return (phi[zp][y][x] - phi[zn][y][x]) / (2.0*dz);\n}\n\n__device__ double freeEnergy(double c, double e_AA, double e_BB, double e_AB)\n{\n  return (((9.0 / 4.0) * ((c*c+2.0*c+1.0)*e_AA+(c*c-2.0*c+1.0)*e_BB+\n          2.0*(1.0-c*c)*e_AB)) + ((3.0/2.0) * c * c) + ((3.0/12.0) * c * c * c * c));\n}\n\n__global__ void localFreeEnergyFunctional(\n    const double c[][DATAYSIZE][DATAXSIZE],\n    double f[][DATAYSIZE][DATAXSIZE], \n    double dx,\n    double dy,\n    double dz,\n    double gamma,\n    double e_AA,\n    double e_BB,\n    double e_AB)\n{\n  unsigned idx = blockIdx.x*blockDim.x + threadIdx.x;\n  unsigned idy = blockIdx.y*blockDim.y + threadIdx.y;\n  unsigned idz = blockIdx.z*blockDim.z + threadIdx.z;\n\n  if ((idx < DATAXSIZE) && (idy < DATAYSIZE) && (idz < DATAZSIZE)) {\n\n    f[idz][idy][idx] = freeEnergy(c[idz][idy][idx],e_AA,e_BB,e_AB) + (gamma / 2.0) * (\n        GradientX(c,dx,dy,dz,idx,idy,idz) * GradientX(c,dx,dy,dz,idx,idy,idz) + \n        GradientY(c,dx,dy,dz,idx,idy,idz) * GradientY(c,dx,dy,dz,idx,idy,idz) + \n        GradientZ(c,dx,dy,dz,idx,idy,idz) * GradientZ(c,dx,dy,dz,idx,idy,idz));\n  }\n}",
            "__device__ double Laplacian(const double c[][DATAYSIZE][DATAXSIZE],\n                            double dx, double dy, double dz, int x, int y, int z)\n{\n  int xp, xn, yp, yn, zp, zn;\n\n  int nx = (int)DATAXSIZE - 1;\n  int ny = (int)DATAYSIZE - 1;\n  int nz = (int)DATAZSIZE - 1;\n\n  xp = x+1;\n  xn = x-1;\n  yp = y+1;\n  yn = y-1;\n  zp = z+1;\n  zn = z-1;\n\n  if (xp > nx) xp = 0;\n  if (yp > ny) yp = 0;\n  if (zp > nz) zp = 0;\n  if (xn < 0)  xn = nx;\n  if (yn < 0)  yn = ny;\n  if (zn < 0)  zn = nz;\n\n  double cxx = (c[z][y][xp] + c[z][y][xn] - 2.0*c[z][y][x]) / (dx*dx);\n  double cyy = (c[z][yp][x] + c[z][yn][x] - 2.0*c[z][y][x]) / (dy*dy);\n  double czz = (c[zp][y][x] + c[zn][y][x] - 2.0*c[z][y][x]) / (dz*dz);\n\n  return cxx + cyy + czz;\n}\n\n__global__ void cahnHilliard(\n    double cnew[][DATAYSIZE][DATAXSIZE], \n    const double cold[][DATAYSIZE][DATAXSIZE], \n    const double mu[][DATAYSIZE][DATAXSIZE],\n    double D,\n    double dt,\n    double dx,\n    double dy,\n    double dz)\n{\n  unsigned idx = blockIdx.x*blockDim.x + threadIdx.x;\n  unsigned idy = blockIdx.y*blockDim.y + threadIdx.y;\n  unsigned idz = blockIdx.z*blockDim.z + threadIdx.z;\n  if ((idx < DATAXSIZE) && (idy < DATAYSIZE) && (idz < DATAZSIZE)) {\n    cnew[idz][idy][idx] = cold[idz][idy][idx] + dt * D * Laplacian(mu,dx,dy,dz,idx,idy,idz);\n  }\n}",
            "__global__ void Swap(double cnew[][DATAYSIZE][DATAXSIZE], double cold[][DATAYSIZE][DATAXSIZE])\n{\n  unsigned idx = blockIdx.x*blockDim.x + threadIdx.x;\n  unsigned idy = blockIdx.y*blockDim.y + threadIdx.y;\n  unsigned idz = blockIdx.z*blockDim.z + threadIdx.z;\n  double tmp;    \n\n  if ((idx < DATAXSIZE) && (idy < DATAYSIZE) && (idz < DATAZSIZE)) {\n    tmp = cnew[idz][idy][idx];\n    cnew[idz][idy][idx] = cold[idz][idy][idx];\n    cold[idz][idy][idx] = tmp;\n  }\n}"
        ]
    },
    "lr-cuda": {
        "/Users/gbolet/hecbench-roofline/src/lr-cuda/kernel.h": [
            "__global__\nvoid linear_regression(\n  const float2 *__restrict__ dataset,\n        float4 *__restrict__ result)\n{\n  extern __shared__ float4 interns[];\n\n  size_t loc_id   = threadIdx.x;\n  size_t loc_size = blockDim.x; \n  size_t glob_id  = blockIdx.x * loc_size + loc_id;\n\n  /* Initialize local buffer */\n  interns[loc_id].x = dataset[glob_id].x;\n  interns[loc_id].y = dataset[glob_id].y;\n  interns[loc_id].z = (dataset[glob_id].x * dataset[glob_id].y);\n  interns[loc_id].w = (dataset[glob_id].x * dataset[glob_id].x);\n  \n  __syncthreads();\n\n  for (size_t i = (loc_size / 2), old_i = loc_size; i > 0; old_i = i, i /= 2)\n  {\n    if (loc_id < i) {\n      // Only first half of workitems on each workgroup\n      interns[loc_id] += interns[loc_id + i];\n      if (loc_id == (i - 1) && old_i % 2 != 0) {\n        // If there is an odd number of data\n        interns[loc_id] += interns[old_i - 1];\n      }\n    }\n    __syncthreads();\n  }\n\n  if (loc_id == 0) result[blockIdx.x] = interns[0];\n}",
            "__global__\nvoid rsquared(\n  const float2 *__restrict__ dataset,\n  const float mean,\n  const float2 equation, // [a0,a1]\n  float2 *__restrict__ result)\n{\n  extern __shared__ float2 dist[];\n\n  size_t loc_id   = threadIdx.x;\n  size_t loc_size = blockDim.x; \n  size_t glob_id  = blockIdx.x * loc_size + loc_id;\n\n  dist[loc_id].x = powf((dataset[glob_id].y - mean), 2.f);\n\n  float y_estimated = dataset[glob_id].x * equation.y + equation.x;\n  dist[loc_id].y = powf((y_estimated - mean), 2.f);\n\n  __syncthreads();\n\n  for (size_t i = (loc_size / 2), old_i = loc_size; i > 0; old_i = i, i /= 2)\n  {\n    if (loc_id < i) {\n      // Only first half of workitems on each workgroup\n      dist[loc_id] += dist[loc_id + i];\n      if (loc_id == (i - 1) && old_i % 2 != 0) {\n        // If there is an odd number of data\n        dist[loc_id] += dist[old_i - 1];\n      }\n    }\n    __syncthreads();\n  }\n\n  if (loc_id == 0) result[blockIdx.x] = dist[0];\n}"
        ]
    },
    "gc-cuda": {
        "/Users/gbolet/hecbench-roofline/src/gc-cuda/main.cu": [
            "inline __host__ __device__ uint4 make_uint4(int4 a)\n{\n    return make_uint4(uint(a.x), uint(a.y), uint(a.z), uint(a.w));\n}\n\ninline __host__ __device__ uint4 max(uint4 a, uint4 b)\n{\n    return make_uint4(max(a.x,b.x), max(a.y,b.y), max(a.z,b.z), max(a.w,b.w));\n}\n\n__global__\nvoid runLarge(const int nodes, \n    const int* const __restrict__ nidx,\n    const int* const __restrict__ nlist,\n    int* const __restrict__ posscol,\n    int* const __restrict__ posscol2,\n    volatile int* const __restrict__ color,\n    const int* const __restrict__ wl,\n    const int* __restrict__ wlsize)\n{\n  const int stop = *wlsize;\n  if (stop != 0) {\n    const int lane = threadIdx.x % WS;\n    const int thread = threadIdx.x + blockIdx.x * ThreadsPerBlock;\n    const int threads = gridDim.x * ThreadsPerBlock;\n    bool again;\n    do {\n      again = false;\n      for (int w = thread; __any_sync(Warp, w < stop); w += threads) {\n        bool shortcut, done, cond = false;\n        int v, data, range, beg, pcol;\n        if (w < stop) {\n          v = wl[w];\n          data = color[v];\n          range = data >> (WS / 2);\n          if (range > 0) {\n            beg = nidx[v];\n            pcol = posscol[v];\n            cond = true;\n          }\n        }\n\n        int bal = __ballot_sync(Warp, cond);\n        while (bal != 0) {\n          const int who = __ffs(bal) - 1;\n          bal &= bal - 1;\n          const int wdata = __shfl_sync(Warp, data, who);\n          const int wrange = wdata >> (WS / 2);\n          const int wbeg = __shfl_sync(Warp, beg, who);\n          const int wmincol = wdata & Mask;\n          const int wmaxcol = wmincol + wrange;\n          const int wend = wbeg + wmaxcol;\n          const int woffs = wbeg / WS;\n          int wpcol = __shfl_sync(Warp, pcol, who);\n\n          bool wshortcut = true;\n          bool wdone = true;\n          for (int i = wbeg + lane; __any_sync(Warp, i < wend); i += WS) {\n            int nei, neidata, neirange;\n            if (i < wend) {\n              nei = nlist[i];\n              neidata = color[nei];\n              neirange = neidata >> (WS / 2);\n              const bool neidone = (neirange == 0);\n              wdone &= neidone; //consolidated below\n              if (neidone) {\n                const int neicol = neidata;\n                if (neicol < WS) {\n                  wpcol &= ~((unsigned int)MSB >> neicol); //consolidated below\n                } else {\n                  if ((wmincol <= neicol) && (neicol < wmaxcol) && ((posscol2[woffs + neicol / WS] << (neicol % WS)) < 0)) {\n                    atomicAnd((int*)&posscol2[woffs + neicol / WS], ~((unsigned int)MSB >> (neicol % WS)));\n                  }\n                }\n              } else {\n                const int neimincol = neidata & Mask;\n                const int neimaxcol = neimincol + neirange;\n                if ((neimincol <= wmincol) && (neimaxcol >= wmincol)) wshortcut = false; //consolidated below\n              }\n            }\n          }\n          wshortcut = __all_sync(Warp, wshortcut);\n          wdone = __all_sync(Warp, wdone);\n          wpcol &= __shfl_xor_sync(Warp, wpcol, 1);\n          wpcol &= __shfl_xor_sync(Warp, wpcol, 2);\n          wpcol &= __shfl_xor_sync(Warp, wpcol, 4);\n          wpcol &= __shfl_xor_sync(Warp, wpcol, 8);\n          wpcol &= __shfl_xor_sync(Warp, wpcol, 16);\n          if (who == lane) pcol = wpcol;\n          if (who == lane) done = wdone;\n          if (who == lane) shortcut = wshortcut;\n        }\n\n        if (w < stop) {\n          if (range > 0) {\n            const int mincol = data & Mask;\n            int val = pcol, mc = 0;\n            if (pcol == 0) {\n              const int offs = beg / WS;\n              mc = max(1, mincol / WS);\n              while ((val = posscol2[offs + mc]) == 0) mc++;\n            }\n            int newmincol = mc * WS + __clz(val);\n            if (mincol != newmincol) shortcut = false;\n            if (shortcut || done) {\n              pcol = (newmincol < WS) ? ((unsigned int)MSB >> newmincol) : 0;\n            } else {\n              const int maxcol = mincol + range;\n              const int range = maxcol - newmincol;\n              newmincol = (range << (WS / 2)) | newmincol;\n              again = true;\n            }\n            posscol[v] = pcol;\n            color[v] = newmincol;\n          }\n        }\n      }\n    } while (__any_sync(Warp, again));\n  }\n}",
            "__global__ \nvoid runSmall(const int nodes,\n    const int* const __restrict__ nidx,\n    const int* const __restrict__ nlist,\n    volatile int* const __restrict__ posscol,\n    int* const __restrict__ color)\n    //int* __restrict__ wlsize)\n{\n  const int thread = threadIdx.x + blockIdx.x * ThreadsPerBlock;\n  const int threads = gridDim.x * ThreadsPerBlock;\n\n  bool again;\n  do {\n    again = false;\n    for (int v = thread; v < nodes; v += threads) {\n      int pcol = posscol[v];\n      if (__popc(pcol) > 1) {\n        const int beg = nidx[v];\n        int active = color[v];\n        int allnei = 0;\n        int keep = active;\n        do {\n          const int old = active;\n          active &= active - 1;\n          const int curr = old ^ active;\n          const int i = beg + __clz(curr);\n          const int nei = nlist[i];\n          const int neipcol = posscol[nei];\n          allnei |= neipcol;\n          if ((pcol & neipcol) == 0) {\n            pcol &= pcol - 1;\n            keep ^= curr;\n          } else if (__popc(neipcol) == 1) {\n            pcol ^= neipcol;\n            keep ^= curr;\n          }\n        } while (active != 0);\n        if (keep != 0) {\n          const int best = (unsigned int)MSB >> __clz(pcol);\n          if ((best & ~allnei) != 0) {\n            pcol = best;\n            keep = 0;\n          }\n        }\n        again |= keep;\n        if (keep == 0) keep = __clz(pcol);\n        color[v] = keep;\n        posscol[v] = pcol;\n      }\n    }\n  } while (again);\n}"
        ]
    },
    "damage-cuda": {
        "/Users/gbolet/hecbench-roofline/src/damage-cuda/kernel.h": [
            "__global__ void damage_of_node(\n  const int n,\n  const int *__restrict__ nlist,\n  const int *__restrict__ family,\n        int *__restrict__ n_neigh,\n     double *__restrict__ damage)\n{\n  extern __shared__ int local_cache[];\n\n  const int local_id = threadIdx.x;\n  const int local_size = blockDim.x;\n  const int nid = blockIdx.x;\n  const int global_id = nid * local_size + local_id;\n  if (global_id >= n) return;\n\n  //Copy values into local memory \n  local_cache[local_id] = nlist[global_id] != -1 ? 1 : 0; \n\n  //Wait for all threads\n  __syncthreads();\n\n  for (int i = local_size/2; i > 0; i /= 2) {\n    if(local_id < i){\n      local_cache[local_id] += local_cache[local_id + i];\n    } \n    //Wait for all threads\n    __syncthreads();\n  }\n\n  if (local_id == 0) {\n    // Update damage and n_neigh\n    int neighbours = local_cache[0];\n    n_neigh[nid] = neighbours;\n    damage[nid] = 1.0 - (double) neighbours / (double) (family[nid]);\n  }\n}"
        ]
    },
    "match-cuda": {
        "/Users/gbolet/hecbench-roofline/src/match-cuda/main.cu": [
            "__global__ void Match1(const float *__restrict d_pts1, \n                       const float *__restrict d_pts2,\n                             float *__restrict d_score,\n                               int *__restrict d_index)\n{\n  int p1 = threadIdx.x + M1W*blockIdx.x;\n  float max_score = 0.0f;\n  int index = -1;\n  \n  for (int p2=0;p2<NPTS;p2++) {\n    float score = 0.0f;\n    for (int d=0;d<NDIM;d++)\n      score += d_pts1[p1*NDIM + d]*d_pts2[p2*NDIM + d];\n    if (score>max_score) {\n      max_score = score;\n      index = p2;\n    }\n  }\n  \n  d_score[p1] = max_score;\n  d_index[p1] = index;\n}",
            "__global__ void Match2(const float *__restrict d_pts1, \n                       const float *__restrict d_pts2,\n                             float *__restrict d_score,\n                               int *__restrict d_index)\n{\n  __shared__ float buffer1[M2W*NDIM];  \n  __shared__ float buffer2[M2H*NDIM];  \n  __shared__ float scores[M2W*M2H];    \n  int tx = threadIdx.x;\n  int ty = threadIdx.y;\n  int idx = tx + M2W*ty;\n  int bp1 = M2W*blockIdx.x;\n  if (ty<M2W)\n    for (int d=tx;d<NDIM;d+=M2W)\n      for (int j=ty;j<M2W;j+=M2H)\n\tbuffer1[j*NDIM + d] = d_pts1[(bp1 + j)*NDIM + d];   \n  __syncthreads();\n  \n  float max_score = 0.0f;\n  int index = -1;\n  for (int bp2=0;bp2<NPTS;bp2+=M2H) {\n    for (int d=tx;d<NDIM;d+=M2W)\n      buffer2[ty*NDIM + d] = d_pts2[(bp2 + ty)*NDIM + d]; \n    __syncthreads();\n\n    float score = 0.0f;\n    for (int d=0;d<NDIM;d++) \n      score += buffer1[tx*NDIM + d]*buffer2[ty*NDIM + d];   \n    scores[idx] = score;\n    __syncthreads();\n    \n    if (ty==0) {\n      for (int i=0;i<M2H;i++) {\n\tif (scores[i*M2W + tx]>max_score) {\n\t  max_score = scores[i*M2W + tx];\n\t  index = bp2 + i;\n\t}\n      }\n    }\n    __syncthreads();\n  }\n  \n  if (ty==0) {\n    d_score[bp1 + tx] = max_score;\n    d_index[bp1 + tx] = index;\n  }\n}",
            "__global__ void Match3(const float *__restrict d_pts1, \n                       const float *__restrict d_pts2,\n                             float *__restrict d_score,\n                               int *__restrict d_index)\n{\n  __shared__ float buffer1[M2W*(NDIM + 1)]; \n  __shared__ float buffer2[M2H*NDIM];\n  __shared__ float scores[M2W*M2H];\n  int tx = threadIdx.x;\n  int ty = threadIdx.y;\n  int idx = tx + M2W*ty;\n  int bp1 = M2W*blockIdx.x;\n  if (ty<M2W)\n    for (int d=tx;d<NDIM;d+=M2W)\n      for (int j=ty;j<M2W;j+=M2H)\n\tbuffer1[j*(NDIM + 1) + d] = d_pts1[(bp1 + j)*NDIM + d]; \n  __syncthreads();\n  \n  float max_score = 0.0f;\n  int index = -1;\n  for (int bp2=0;bp2<NPTS;bp2+=M2H) {\n    for (int d=tx;d<NDIM;d+=M2W)\n      buffer2[ty*NDIM + d] = d_pts2[(bp2 + ty)*NDIM + d];\n    __syncthreads();\n\n    float score = 0.0f;\n    for (int d=0;d<NDIM;d++) \n      score += buffer1[tx*(NDIM + 1) + d]*buffer2[ty*NDIM + d]; \n    scores[idx] = score;\n    __syncthreads();\n    \n    if (ty==0) {\n      for (int i=0;i<M2H;i++) {\n\tif (scores[i*M2W + tx]>max_score) {\n\t  max_score = scores[i*M2W + tx];\n\t  index = bp2 + i;\n\t}\n      }\n    }\n    __syncthreads();\n  }\n  \n  if (ty==0) {\n    d_score[bp1 + tx] = max_score;\n    d_index[bp1 + tx] = index;\n  }\n}",
            "__global__ void Match4(const float *__restrict d_pts1, \n                       const float *__restrict d_pts2,\n                             float *__restrict d_score,\n                               int *__restrict d_index)\n{\n  __shared__ float4 buffer1[M2W*(NDIM/4 + 1)];  \n  __shared__ float4 buffer2[M2H*NDIM/4];        \n  __shared__ float scores[M2W*M2H];\n  int tx = threadIdx.x;\n  int ty = threadIdx.y;\n  int idx = tx + M2W*ty;\n  int bp1 = M2W*blockIdx.x;\n  if (ty<M2W)\n    for (int d=tx;d<NDIM/4;d+=M2W)\n      for (int j=ty;j<M2W;j+=M2H)\n\tbuffer1[j*(NDIM/4 + 1) + d] = ((float4*)d_pts1)[(bp1 + j)*(NDIM/4) + d]; \n  __syncthreads();\n  \n  float max_score = 0.0f;\n  int index = -1;\n  for (int bp2=0;bp2<NPTS;bp2+=M2H) {\n    for (int d=tx;d<NDIM/4;d+=M2W)\n      buffer2[ty*NDIM/4 + d] = ((float4*)d_pts2)[(bp2 + ty)*(NDIM/4) + d]; \n    __syncthreads();\n\n    float score = 0.0f;\n    for (int d=0;d<NDIM/4;d++) {\n      float4 v1 = buffer1[tx*(NDIM/4 + 1) + d]; \n      float4 v2 = buffer2[ty*(NDIM/4) + d];     \n      score += v1.x*v2.x; score += v1.y*v2.y;\n      score += v1.z*v2.z; score += v1.w*v2.w;\n    }\n    scores[idx] = score;\n    __syncthreads();\n    \n    if (ty==0) {\n      for (int i=0;i<M2H;i++) {\n\tif (scores[i*M2W + tx]>max_score) {\n\t  max_score = scores[i*M2W + tx];\n\t  index = bp2 + i;\n\t}\n      }\n    }\n    __syncthreads();\n  }\n  \n  if (ty==0) {\n    d_score[bp1 + tx] = max_score;\n    d_index[bp1 + tx] = index;\n  }\n}",
            "__global__ void Match5(const float *__restrict d_pts1, \n                       const float *__restrict d_pts2,\n                             float *__restrict d_score,\n                               int *__restrict d_index)\n{\n  __shared__ float4 buffer1[M5W*(NDIM/4 + 1)]; \n  __shared__ float4 buffer2[M5H*NDIM/4];       \n  __shared__ float scores[M5W*M5H];\n  int tx = threadIdx.x;\n  int ty = threadIdx.y;\n  int bp1 = M5W*blockIdx.x;\n  if (ty<M5W)\n    for (int d=tx;d<NDIM/4;d+=M5W)\n      for (int j=ty;j<M5W;j+=M5H)\n\tbuffer1[j*(NDIM/4 + 1) + d] = ((float4*)d_pts1)[(bp1 + j)*(NDIM/4) + d];\n  __syncthreads();\n  \n  float max_score = 0.0f;\n  int index = -1;\n  for (int bp2=0;bp2<NPTS;bp2+=M5H) {\n    for (int d=tx;d<NDIM/4;d+=M5W)\n      buffer2[ty*NDIM/4 + d] = ((float4*)d_pts2)[(bp2 + ty)*(NDIM/4) + d];\n    __syncthreads();\n\n    if (ty<M5H/M5R) {  \n      float score[M5R];                                    \n      for (int dy=0;dy<M5R;dy++)\n\tscore[dy] = 0.0f;\n      for (int d=0;d<NDIM/4;d++) {\n\tfloat4 v1 = buffer1[tx*(NDIM/4 + 1) + d];\n\tfor (int dy=0;dy<M5R;dy++) {\n\t  float4 v2 = buffer2[(M5R*ty + dy)*(NDIM/4) + d];    \n\t  score[dy] += v1.x*v2.x; score[dy] += v1.y*v2.y;\n\t  score[dy] += v1.z*v2.z; score[dy] += v1.w*v2.w;\n\t}\n      }\n      for (int dy=0;dy<M5R;dy++)\n\tscores[tx + M5W*(M5R*ty + dy)] = score[dy];\n    }\n    __syncthreads();\n    \n    if (ty==0) {\n      for (int i=0;i<M5H;i++) {\n\tif (scores[i*M2W + tx]>max_score) {\n\t  max_score = scores[i*M5W + tx];\n\t  index = bp2 + i;\n\t}\n      }\n    }\n    __syncthreads();\n  }\n\n  if (ty==0) {\n    d_score[bp1 + tx] = max_score;\n    d_index[bp1 + tx] = index;\n  }\n}",
            "__global__ void Match6(const float *__restrict d_pts1, \n                       const float *__restrict d_pts2,\n                             float *__restrict d_score,\n                               int *__restrict d_index)\n{\n  __shared__ float4 buffer1[M5W*(NDIM/4 + 1)]; \n  __shared__ float4 buffer2[M5H*NDIM/4];       \n  int tx = threadIdx.x;\n  int ty = threadIdx.y;\n  int bp1 = M5W*blockIdx.x;\n  if (ty<M5W)\n    for (int d=tx;d<NDIM/4;d+=M5W)\n      for (int j=ty;j<M5W;j+=M5H)\n\tbuffer1[j*(NDIM/4 + 1) + d] = ((float4*)d_pts1)[(bp1 + j)*(NDIM/4) + d];\n  \n  float max_score = 0.0f;\n  int index = -1;    \n  for (int bp2=0;bp2<NPTS;bp2+=M5H) {\n    for (int d=tx;d<NDIM/4;d+=M5W)\n      buffer2[ty*NDIM/4 + d] = ((float4*)d_pts2)[(bp2 + ty)*(NDIM/4) + d];\n    __syncthreads();\n\n    if (ty<M5H/M5R) {  \n      float score[M5R];                                    \n      for (int dy=0;dy<M5R;dy++)\n\tscore[dy] = 0.0f;\n      for (int d=0;d<NDIM/4;d++) {\n\tfloat4 v1 = buffer1[tx*(NDIM/4 + 1) + d];\n\tfor (int dy=0;dy<M5R;dy++) {\n\t  float4 v2 = buffer2[(M5R*ty + dy)*(NDIM/4) + d];    \n\t  score[dy] += v1.x*v2.x; score[dy] += v1.y*v2.y;\n\t  score[dy] += v1.z*v2.z; score[dy] += v1.w*v2.w;\n\t}\n      }\n      for (int dy=0;dy<M5R;dy++) {\n\tif (score[dy]>max_score) {   \n\t  max_score = score[dy];     \n\t  index = bp2 + M5R*ty + dy;               \n\t}\n      }\n    }\n    __syncthreads();\n  }\n\n  float *scores = (float*)buffer1;\n  int *indices = (int*)&scores[M5W*M5H/M5R];\n  if (ty<M5H/M5R) {\n    scores[ty*M5W + tx] = max_score;  \n    indices[ty*M5W + tx] = index;     \n  }\n  __syncthreads();\n  \n  if (ty==0) {\n    max_score = scores[tx];\n    index = indices[tx];\n    for (int y=0;y<M5H/M5R;y++)\n      if (scores[y*M5W + tx]>max_score) {\n\tmax_score = scores[y*M5W + tx]; \n\tindex = indices[y*M5W + tx];    \n      }\n    d_score[bp1 + tx] = max_score;\n    d_index[bp1 + tx] = index;\n  }\n}",
            "__global__ void Match7(const float *__restrict d_pts1, \n                       const float *__restrict d_pts2,\n                             float *__restrict d_score,\n                               int *__restrict d_index)\n{\n  __shared__ float4 buffer1[M7W*NDIM/4]; \n  __shared__ float4 buffer2[M7H*NDIM/4];       \n  int tx = threadIdx.x;\n  int ty = threadIdx.y;\n  int bp1 = M7W*blockIdx.x;\n  for (int d=tx;d<NDIM/4;d+=M7W)\n    for (int j=ty;j<M7W;j+=M7H/M7R)      \n      buffer1[j*NDIM/4 + (d + j)%(NDIM/4)] = ((float4*)d_pts1)[(bp1 + j)*(NDIM/4) + d];\n  \n  float max_score = 0.0f;\n  int index = -1;    \n  for (int bp2=0;bp2<NPTS;bp2+=M7H) {\n    for (int d=tx;d<NDIM/4;d+=M7W)\n      for (int j=ty;j<M7H;j+=M7H/M7R)       \n\tbuffer2[j*NDIM/4 + d] = ((float4*)d_pts2)[(bp2 + j)*(NDIM/4) + d];\n    __syncthreads();\n\n    float score[M7R];                                    \n    for (int dy=0;dy<M7R;dy++)\n      score[dy] = 0.0f;\n    for (int d=0;d<NDIM/4;d++) {\n      float4 v1 = buffer1[tx*NDIM/4 + (d + tx)%(NDIM/4)];\n      for (int dy=0;dy<M7R;dy++) {\n\tfloat4 v2 = buffer2[(M7R*ty + dy)*(NDIM/4) + d];    \n\tscore[dy] += v1.x*v2.x;\n        score[dy] += v1.y*v2.y;\n\tscore[dy] += v1.z*v2.z;\n        score[dy] += v1.w*v2.w;\n      }\n    }\n    for (int dy=0;dy<M7R;dy++) {\n      if (score[dy]>max_score) {   \n\tmax_score = score[dy];     \n\tindex = bp2 + M7R*ty + dy;               \n      }\n    }\n    __syncthreads();\n  }\n\n  float *scores = (float*)buffer1;\n  int *indices = (int*)&scores[M7W*M7H/M7R];\n  scores[ty*M7W + tx] = max_score;  \n  indices[ty*M7W + tx] = index;     \n  __syncthreads();\n  \n  if (ty==0) {\n    max_score = scores[tx];\n    index = indices[tx];\n    for (int y=0;y<M7H/M7R;y++)\n      if (scores[y*M7W + tx]>max_score) {\n\tmax_score = scores[y*M7W + tx]; \n\tindex = indices[y*M7W + tx];    \n      }\n    d_score[bp1 + tx] = max_score;\n    d_index[bp1 + tx] = index;\n  }\n}",
            "__global__ void Match8(const float *__restrict d_pts1, \n                       const float *__restrict d_pts2,\n                             float *__restrict d_score,\n                               int *__restrict d_index)\n{\n  __shared__ float4 buffer1[M7W*NDIM/4]; \n  __shared__ float4 buffer2[M7H*NDIM/4];       \n  int tx = threadIdx.x;\n  int ty = threadIdx.y;\n  int bp1 = M7W*blockIdx.x;\n  for (int d=tx;d<NDIM/4;d+=M7W)\n    for (int j=ty;j<M7W;j+=M7H/M7R)     \n      buffer1[j*NDIM/4 + (d + j)%(NDIM/4)] = ((float4*)d_pts1)[(bp1 + j)*(NDIM/4) + d];\n\n#define NRX 2\n  float max_score[NRX];\n  int index[NRX];\n  for (int i=0;i<NRX;i++) {\n    max_score[i] = 0.0f;\n    index[i] = -1;\n  }\n  int idx = ty*M7W + tx;\n  int ix = idx%(M7W/NRX);\n  int iy = idx/(M7W/NRX);\n  for (int bp2=0;bp2<NPTS;bp2+=M7H) {\n    for (int d=tx;d<NDIM/4;d+=M7W)\n      for (int j=ty;j<M7H;j+=M7H/M7R)       \n\tbuffer2[j*NDIM/4 + d] = ((float4*)d_pts2)[(bp2 + j)*(NDIM/4) + d];\n    __syncthreads();\n\n    if (idx<M7W*M7H/M7R/NRX) {\n      float score[M7R][NRX];                                    \n      for (int dy=0;dy<M7R;dy++)\n\tfor (int i=0;i<NRX;i++)\n\t  score[dy][i] = 0.0f;\n      for (int d=0;d<NDIM/4;d++) {\n\tfloat4 v1[NRX];\n\tfor (int i=0;i<NRX;i++) \n\t  v1[i] = buffer1[((M7W/NRX)*i + ix)*NDIM/4 + (d + (M7W/NRX)*i + ix)%(NDIM/4)];\n\tfor (int dy=0;dy<M7R;dy++) {\n\t  float4 v2 = buffer2[(M7R*iy + dy)*(NDIM/4) + d];    \n\t  for (int i=0;i<NRX;i++) {\n\t    score[dy][i] += v1[i].x*v2.x;\n\t    score[dy][i] += v1[i].y*v2.y;\n\t    score[dy][i] += v1[i].z*v2.z;\n\t    score[dy][i] += v1[i].w*v2.w;\n\t  }\n\t}\n      }\n      for (int dy=0;dy<M7R;dy++) {\n\tfor (int i=0;i<NRX;i++) {\n\t  if (score[dy][i]>max_score[i]) {\n\t    max_score[i] = score[dy][i];     \n\t    index[i] = bp2 + M7R*iy + dy;\n\t  }\n\t}\n      }\n    }\n    __syncthreads();\n  }\n\n  float *scores = (float*)buffer1;\n  int *indices = (int*)&scores[M7W*M7H/M7R];\n  if (idx<M7W*M7H/M7R/NRX) {\n    for (int i=0;i<NRX;i++) {\n      scores[iy*M7W + (M7W/NRX)*i + ix] = max_score[i];  \n      indices[iy*M7W + (M7W/NRX)*i + ix] = index[i];\n    }\n  }\n  __syncthreads();\n  \n  if (ty==0) {\n    float max_score = scores[tx];\n    int index = indices[tx];\n    for (int y=0;y<M7H/M7R;y++)\n      if (scores[y*M7W + tx]>max_score) {\n\tmax_score = scores[y*M7W + tx]; \n\tindex = indices[y*M7W + tx];    \n      }\n    d_score[bp1 + tx] = max_score;\n    d_index[bp1 + tx] = index;\n  }\n}",
            "__global__ void Match9(const float *__restrict d_pts1, \n                       const float *__restrict d_pts2,\n                             float *__restrict d_score,\n                               int *__restrict d_index)\n{\n#define NRX 2\n  __shared__ float4 buffer1[M7W*NDIM/4]; \n  __shared__ float4 buffer2[M7H*NDIM/4];       \n  int tx = threadIdx.x;\n  int ty = threadIdx.y;\n  int bp1 = M7W*blockIdx.x;\n  for (int d=tx;d<NDIM/4;d+=M7W)\n    for (int j=ty;j<M7W;j+=M7H/M7R/NRX)     \n      buffer1[j*NDIM/4 + (d + j)%(NDIM/4)] = ((float4*)d_pts1)[(bp1 + j)*(NDIM/4) + d];\n\n  float max_score[NRX];\n  int index[NRX];\n  for (int i=0;i<NRX;i++) {\n    max_score[i] = 0.0f;\n    index[i] = -1;\n  }\n  int idx = ty*M7W + tx;\n  int ix = idx%(M7W/NRX);\n  int iy = idx/(M7W/NRX);\n  for (int bp2=0;bp2<NPTS;bp2+=M7H) {\n    for (int d=tx;d<NDIM/4;d+=M7W)\n      for (int j=ty;j<M7H;j+=M7H/M7R/NRX)       \n\tbuffer2[j*NDIM/4 + d] = ((float4*)d_pts2)[(bp2 + j)*(NDIM/4) + d];\n    __syncthreads();\n\n    float score[M7R][NRX];                                    \n    for (int dy=0;dy<M7R;dy++)\n      for (int i=0;i<NRX;i++)\n\tscore[dy][i] = 0.0f;\n    for (int d=0;d<NDIM/4;d++) {\n      float4 v1[NRX];\n      for (int i=0;i<NRX;i++) \n\tv1[i] = buffer1[((M7W/NRX)*i + ix)*NDIM/4 + (d + (M7W/NRX)*i + ix)%(NDIM/4)];\n      for (int dy=0;dy<M7R;dy++) {\n\tfloat4 v2 = buffer2[(M7R*iy + dy)*(NDIM/4) + d];    \n\tfor (int i=0;i<NRX;i++) {\n\t  score[dy][i] += v1[i].x*v2.x;\n\t  score[dy][i] += v1[i].y*v2.y;\n\t  score[dy][i] += v1[i].z*v2.z;\n\t  score[dy][i] += v1[i].w*v2.w;\n\t}\n      }\n    }\n    for (int dy=0;dy<M7R;dy++) {\n      for (int i=0;i<NRX;i++) {\n\tif (score[dy][i]>max_score[i]) {\n\t  max_score[i] = score[dy][i];     \n\t  index[i] = bp2 + M7R*iy + dy;\n\t}\n      }\n    }\n    __syncthreads();\n  }\n\n  float *scores = (float*)buffer1;\n  int *indices = (int*)&scores[M7W*M7H/M7R];\n  if (idx<M7W*M7H/M7R/NRX) {\n    for (int i=0;i<NRX;i++) {\n      scores[iy*M7W + (M7W/NRX)*i + ix] = max_score[i];  \n      indices[iy*M7W + (M7W/NRX)*i + ix] = index[i];\n    }\n  }\n  __syncthreads();\n  \n  if (ty==0) {\n    float max_score = scores[tx];\n    int index = indices[tx];\n    for (int y=0;y<M7H/M7R;y++)\n      if (scores[y*M7W + tx]>max_score) {\n\tmax_score = scores[y*M7W + tx]; \n\tindex = indices[y*M7W + tx];    \n      }\n    d_score[bp1 + tx] = max_score;\n    d_index[bp1 + tx] = index;\n  }\n}",
            "__global__ void Match10(const float *__restrict d_pts1, \n                        const float *__restrict d_pts2,\n                              float *__restrict d_score,\n                                int *__restrict d_index)\n{\n#define NRX 2\n#define NUM (NRX*M7R)                       // 32*8 threads\n  __shared__ float4 buffer1[M7W*NDIM/4];    // 32*32\n  __shared__ float4 buffer2[M7H*NUM];       // 32*8\n  int tx = threadIdx.x;\n  int ty = threadIdx.y;\n  int bp1 = M7W*blockIdx.x;\n  for (int d=tx;d<NDIM/4;d+=M7W)\n    for (int j=ty;j<M7W;j+=M7H/M7R)     \n      buffer1[j*NDIM/4 + (d + j)%(NDIM/4)] = ((float4*)d_pts1)[(bp1 + j)*(NDIM/4) + d];\n\n  float max_score[NRX];\n  int index[NRX];\n  for (int i=0;i<NRX;i++) {\n    max_score[i] = 0.0f;\n    index[i] = -1;\n  }\n  int idx = ty*M7W + tx;\n  int ix = idx%(M7W/NRX);\n  int iy = idx/(M7W/NRX);\n  for (int bp2=0;bp2<NPTS;bp2+=M7H) {\n    float score[M7R][NRX];                                    \n    for (int dy=0;dy<M7R;dy++)\n      for (int i=0;i<NRX;i++)\n\tscore[dy][i] = 0.0f;\n\n    int d = (idx%NUM);\n    int j = (idx/NUM);\n    buffer2[j*NUM + d] = ((float4*)d_pts2)[(bp2 + j)*(NDIM/4) + d];\n    __syncthreads();\n    for (int dp=0;dp<NDIM/4;dp+=NUM) {\n      float4 temp;\n      if (dp<(NDIM/4-NUM))\n\ttemp = ((float4*)d_pts2)[(bp2 + j)*(NDIM/4) + dp + d + NUM];\n\n      if (idx<M7W*M7H/M7R/NRX) {\n\tfor (int d=0;d<NUM;d++) {\n\t  float4 v1[NRX];\n#pragma unroll\n\t  for (int i=0;i<NRX;i++) \n\t    v1[i] = buffer1[(((M7W/NRX)*i + ix)<<5) + ((dp + d + (M7W/NRX)*i + ix)&31)];\n\t  //v1[i] = buffer1[((M7W/NRX)*i + ix)*NDIM/4 + (dp + d + (M7W/NRX)*i + ix)%(NDIM/4)];\n#pragma unroll\n\t  for (int dy=0;dy<M7R;dy++) {\n\t    float4 v2 = buffer2[(M7R*iy + dy)*NUM + d];    \n#pragma unroll\n\t    for (int i=0;i<NRX;i++) {\n\t      score[dy][i] += v1[i].x*v2.x;\n\t      score[dy][i] += v1[i].y*v2.y;\n\t      score[dy][i] += v1[i].z*v2.z;\n\t      score[dy][i] += v1[i].w*v2.w;\n\t    }\n\t  }\n\t}\n      }\n      __syncthreads();\n\n      if (dp<(NDIM/4-NUM)) {\n\tbuffer2[j*NUM + d] = temp;\n\t__syncthreads();\n      }\n    }\n    for (int dy=0;dy<M7R;dy++) {\n      for (int i=0;i<NRX;i++) {\n\tif (score[dy][i]>max_score[i]) {\n\t  max_score[i] = score[dy][i];     \n\t  index[i] = bp2 + M7R*iy + dy;\n\t}\n      }\n    }\n    __syncthreads();\n  }\n\n  float *scores = (float*)buffer1;\n  int *indices = (int*)&scores[M7W*M7H/M7R];\n  if (idx<M7W*M7H/M7R/NRX) {\n    for (int i=0;i<NRX;i++) {\n      scores[iy*M7W + (M7W/NRX)*i + ix] = max_score[i];  \n      indices[iy*M7W + (M7W/NRX)*i + ix] = index[i];\n    }\n  }\n  __syncthreads();\n  \n  if (ty==0) {\n    float max_score = scores[tx];\n    int index = indices[tx];\n    for (int y=0;y<M7H/M7R;y++)\n      if (scores[y*M7W + tx]>max_score) {\n\tmax_score = scores[y*M7W + tx]; \n\tindex = indices[y*M7W + tx];    \n      }\n    d_score[bp1 + tx] = max_score;\n    d_index[bp1 + tx] = index;\n  }\n}"
        ]
    },
    "linearprobing-cuda": {
        "/Users/gbolet/hecbench-roofline/src/linearprobing-cuda/linearprobing.cu": [
            "__device__ __forceinline__ uint32 hash(int x) {\n    return hash((uint32)x);\n}\n\n__global__ void\nk_hashtable_insert(KeyValue*__restrict__ hashtable,\n                   const KeyValue*__restrict__ kvs,\n                   unsigned int numkvs)\n{\n  unsigned int tid = blockIdx.x * blockDim.x + threadIdx.x;\n  if (tid < numkvs)\n  {\n    uint32_t key = kvs[tid].key;\n    uint32_t value = kvs[tid].value;\n    uint32_t slot = hash(key);\n\n    while (true)\n    {\n      uint32_t prev = atomicCAS(&hashtable[slot].key, kEmpty, key);\n      if (prev == kEmpty || prev == key)\n      {\n        hashtable[slot].value = value;\n        return;\n      }\n\n      slot = (slot + 1) & (kHashTableCapacity-1);\n    }\n  }\n}",
            "__device__ __forceinline__ uint32 hash(int x) {\n    return hash((uint32)x);\n}\n\n__global__ void \nk_hashtable_delete(KeyValue*__restrict__ hashtable, \n                   const KeyValue*__restrict__ kvs,\n                   unsigned int numkvs)\n{\n  unsigned int tid = blockIdx.x * blockDim.x + threadIdx.x;\n  if (tid < numkvs)\n  {\n    uint32_t key = kvs[tid].key;\n    uint32_t slot = hash(key);\n\n    while (true)\n    {\n      if (hashtable[slot].key == key)\n      {\n        hashtable[slot].value = kEmpty;\n        return;\n      }\n      if (hashtable[slot].key == kEmpty)\n      {\n        return;\n      }\n      slot = (slot + 1) & (kHashTableCapacity - 1);\n    }\n  }\n}",
            "__global__ void\nk_iterate_hashtable(KeyValue*__restrict__ pHashTable,\n                    KeyValue*__restrict__ kvs,\n                    uint32_t* kvs_size)\n{\n  unsigned int tid = blockIdx.x * blockDim.x + threadIdx.x;\n  if (tid < kHashTableCapacity) \n  {\n    if (pHashTable[tid].key != kEmpty) \n    {\n      uint32_t value = pHashTable[tid].value;\n      if (value != kEmpty)\n      {\n        uint32_t size = atomicAdd(kvs_size, 1);\n        kvs[size] = pHashTable[tid];\n      }\n    }\n  }\n}"
        ]
    },
    "fpdc-cuda": {
        "/Users/gbolet/hecbench-roofline/src/fpdc-cuda/kernels.h": [
            "#define ull unsigned long long\n\n\n__global__ void CompressionKernel(\n  const int dimensionalityd,\n  const ull *__restrict__ cbufd,\n  char *__restrict__ dbufd,\n  const int *__restrict__ cutd,\n  int *__restrict__ offd)\n{\n  register int offset, code, bcount, tmp, off, beg, end, lane, warp, iindex, lastidx, start, term;\n  register ull diff, prev;\n  __shared__ int ibufs[32 * (3 * WARPSIZE / 2)]; // shared space for prefix sum\n\n  // index within this warp\n  lane = threadIdx.x & 31;\n  // index within shared prefix sum array\n  iindex = threadIdx.x / WARPSIZE * (3 * WARPSIZE / 2) + lane;\n  ibufs[iindex] = 0;\n  iindex += WARPSIZE / 2;\n  lastidx = (threadIdx.x / WARPSIZE + 1) * (3 * WARPSIZE / 2) - 1;\n  // warp id\n  warp = (threadIdx.x + blockIdx.x * blockDim.x) / WARPSIZE;\n  // prediction index within previous subchunk\n  offset = WARPSIZE - (dimensionalityd - lane % dimensionalityd) - lane;\n\n  // determine start and end of chunk to compress\n  start = 0;\n  if (warp > 0) start = cutd[warp-1];\n  term = cutd[warp];\n  off = ((start+1)/2*17);\n\n  prev = 0;\n  for (int i = start + lane; i < term; i += WARPSIZE) {\n    // calculate delta between value to compress and prediction\n    // and negate if negative\n    diff = cbufd[i] - prev;\n    code = (diff >> 60) & 8;\n    if (code != 0) {\n      diff = -diff;\n    }\n\n    // count leading zeros in positive delta\n    bcount = 8 - (__clzll(diff) >> 3);\n    if (bcount == 2) bcount = 3; // encode 6 lead-zero bytes as 5\n\n    // prefix sum to determine start positions of non-zero delta bytes\n    ibufs[iindex] = bcount;\n    __threadfence_block();\n    ibufs[iindex] += ibufs[iindex-1];\n    __threadfence_block();\n    ibufs[iindex] += ibufs[iindex-2];\n    __threadfence_block();\n    ibufs[iindex] += ibufs[iindex-4];\n    __threadfence_block();\n    ibufs[iindex] += ibufs[iindex-8];\n    __threadfence_block();\n    ibufs[iindex] += ibufs[iindex-16];\n    __threadfence_block();\n\n    // write out non-zero bytes of delta to compressed buffer\n    beg = off + (WARPSIZE/2) + ibufs[iindex-1];\n    end = beg + bcount;\n    for (; beg < end; beg++) {\n      dbufd[beg] = diff;\n      diff >>= 8;\n    }\n\n    if (bcount >= 3) bcount--; // adjust byte count for the dropped encoding\n    tmp = ibufs[lastidx];\n    code |= bcount;\n    ibufs[iindex] = code;\n    __threadfence_block();\n\n    // write out half-bytes of sign and leading-zero-byte count (every other thread\n    // writes its half-byte and neighbor's half-byte)\n    if ((lane & 1) != 0) {\n      dbufd[off + (lane >> 1)] = ibufs[iindex-1] | (code << 4);\n    }\n    off += tmp + (WARPSIZE/2);\n\n    // save prediction value from this subchunk (based on provided dimensionality)\n    // for use in next subchunk\n    prev = cbufd[i + offset];\n  }\n\n  // save final value of off, which is total bytes of compressed output for this chunk\n  if (lane == 31) offd[warp] = off;\n}"
        ]
    },
    "hypterm-cuda": {
        "/Users/gbolet/hecbench-roofline/src/hypterm-cuda/kernels.cu": [
            "inline __host__ __device__ uint4 make_uint4(int4 a)\n{\n    return make_uint4(uint(a.x), uint(a.y), uint(a.z), uint(a.w));\n}\n\ninline __host__ __device__ uint4 max(uint4 a, uint4 b)\n{\n    return make_uint4(max(a.x,b.x), max(a.y,b.y), max(a.z,b.z), max(a.w,b.w));\n}\n\n__global__\nvoid hypterm_1 (double * __restrict__ flux_0,\n                double * __restrict__ flux_1,\n                double * __restrict__ flux_2,\n                double * __restrict__ flux_3,\n                double * __restrict__ flux_4,\n                const double * __restrict__ cons_1,\n                const double * __restrict__ cons_2,\n                const double * __restrict__ cons_3,\n                const double * __restrict__ cons_4, \n                const double * __restrict__ q_1,\n                const double * __restrict__ q_2,\n                const double * __restrict__ q_3,\n                const double * __restrict__ q_4,\n                double dxinv0, double dxinv1, double dxinv2,\n                int L, int M, int N)\n{\n  //Determing the block's indices\n  int blockdim_i= (int)(blockDim.x);\n  int i0 = (int)(blockIdx.x)*(blockdim_i);\n  int i = max (i0, 0) + (int)(threadIdx.x);\n  int blockdim_j= (int)(blockDim.y);\n  int j0 = (int)(blockIdx.y)*(blockdim_j);\n  int j = max (j0, 0) + (int)(threadIdx.y);\n  int blockdim_k= (int)(blockDim.z);\n  int k0 = (int)(blockIdx.z)*(blockdim_k);\n  int k = max (k0, 0) + (int)(threadIdx.z);\n\n  if (i>=4 & j>=4 & k>=4 & i<=N-5 & j<=N-5 & k<=N-5) {\n  \tflux_0[k*M*N+j*N+i] = -((0.8f*(cons_1[k*M*N+j*N+i+1]-cons_1[k*M*N+j*N+i-1])-0.2f*(cons_1[k*M*N+j*N+i+2]-cons_1[k*M*N+j*N+i-2])+0.038f*(cons_1[k*M*N+j*N+i+3]-cons_1[k*M*N+j*N+i-3])-0.0035f*(cons_1[k*M*N+j*N+i+4]-cons_1[k*M*N+j*N+i-4]))*dxinv0);\n  \tflux_1[k*M*N+j*N+i] = -((0.8f*(cons_1[k*M*N+j*N+i+1]*q_1[k*M*N+j*N+i+1]-cons_1[k*M*N+j*N+i-1]*q_1[k*M*N+j*N+i-1]+(q_4[k*M*N+j*N+i+1]-q_4[k*M*N+j*N+i-1]))-0.2f*(cons_1[k*M*N+j*N+i+2]*q_1[k*M*N+j*N+i+2]-cons_1[k*M*N+j*N+i-2]*q_1[k*M*N+j*N+i-2]+(q_4[k*M*N+j*N+i+2]-q_4[k*M*N+j*N+i-2]))+0.038f*(cons_1[k*M*N+j*N+i+3]*q_1[k*M*N+j*N+i+3]-cons_1[k*M*N+j*N+i-3]*q_1[k*M*N+j*N+i-3]+(q_4[k*M*N+j*N+i+3]-q_4[k*M*N+j*N+i-3]))-0.0035f*(cons_1[k*M*N+j*N+i+4]*q_1[k*M*N+j*N+i+4]-cons_1[k*M*N+j*N+i-4]*q_1[k*M*N+j*N+i-4]+(q_4[k*M*N+j*N+i+4]-q_4[k*M*N+j*N+i-4])))*dxinv0);\n  \t flux_2[k*M*N+j*N+i] = -((0.8f*(cons_2[k*M*N+j*N+i+1]*q_1[k*M*N+j*N+i+1]-cons_2[k*M*N+j*N+i-1]*q_1[k*M*N+j*N+i-1])-0.2f*(cons_2[k*M*N+j*N+i+2]*q_1[k*M*N+j*N+i+2]-cons_2[k*M*N+j*N+i-2]*q_1[k*M*N+j*N+i-2])+0.038f*(cons_2[k*M*N+j*N+i+3]*q_1[k*M*N+j*N+i+3]-cons_2[k*M*N+j*N+i-3]*q_1[k*M*N+j*N+i-3])-0.0035f*(cons_2[k*M*N+j*N+i+4]*q_1[k*M*N+j*N+i+4]-cons_2[k*M*N+j*N+i-4]*q_1[k*M*N+j*N+i-4]))*dxinv0);\n  \tflux_3[k*M*N+j*N+i] = -((0.8f*(cons_3[k*M*N+j*N+i+1]*q_1[k*M*N+j*N+i+1]-cons_3[k*M*N+j*N+i-1]*q_1[k*M*N+j*N+i-1])-0.2f*(cons_3[k*M*N+j*N+i+2]*q_1[k*M*N+j*N+i+2]-cons_3[k*M*N+j*N+i-2]*q_1[k*M*N+j*N+i-2])+0.038f*(cons_3[k*M*N+j*N+i+3]*q_1[k*M*N+j*N+i+3]-cons_3[k*M*N+j*N+i-3]*q_1[k*M*N+j*N+i-3])-0.0035f*(cons_3[k*M*N+j*N+i+4]*q_1[k*M*N+j*N+i+4]-cons_3[k*M*N+j*N+i-4]*q_1[k*M*N+j*N+i-4]))*dxinv0);\n  \tflux_4[k*M*N+j*N+i] = -((0.8f*(cons_4[k*M*N+j*N+i+1]*q_1[k*M*N+j*N+i+1]-cons_4[k*M*N+j*N+i-1]*q_1[k*M*N+j*N+i-1]+(q_4[k*M*N+j*N+i+1]*q_1[k*M*N+j*N+i+1]-q_4[k*M*N+j*N+i-1]*q_1[k*M*N+j*N+i-1]))-0.2f*(cons_4[k*M*N+j*N+i+2]*q_1[k*M*N+j*N+i+2]-cons_4[k*M*N+j*N+i-2]*q_1[k*M*N+j*N+i-2]+(q_4[k*M*N+j*N+i+2]*q_1[k*M*N+j*N+i+2]-q_4[k*M*N+j*N+i-2]*q_1[k*M*N+j*N+i-2]))+0.038f*(cons_4[k*M*N+j*N+i+3]*q_1[k*M*N+j*N+i+3]-cons_4[k*M*N+j*N+i-3]*q_1[k*M*N+j*N+i-3]+(q_4[k*M*N+j*N+i+3]*q_1[k*M*N+j*N+i+3]-q_4[k*M*N+j*N+i-3]*q_1[k*M*N+j*N+i-3]))-0.0035f*(cons_4[k*M*N+j*N+i+4]*q_1[k*M*N+j*N+i+4]-cons_4[k*M*N+j*N+i-4]*q_1[k*M*N+j*N+i-4]+(q_4[k*M*N+j*N+i+4]*q_1[k*M*N+j*N+i+4]-q_4[k*M*N+j*N+i-4]*q_1[k*M*N+j*N+i-4])))*dxinv0);\n  } \n}",
            "inline __host__ __device__ uint4 make_uint4(int4 a)\n{\n    return make_uint4(uint(a.x), uint(a.y), uint(a.z), uint(a.w));\n}\n\ninline __host__ __device__ uint4 max(uint4 a, uint4 b)\n{\n    return make_uint4(max(a.x,b.x), max(a.y,b.y), max(a.z,b.z), max(a.w,b.w));\n}\n\n__global__\nvoid hypterm_2 (double * __restrict__ flux_0,\n                double * __restrict__ flux_1,\n                double * __restrict__ flux_2,\n                double * __restrict__ flux_3,\n                double * __restrict__ flux_4,\n                const double * __restrict__ cons_1,\n                const double * __restrict__ cons_2,\n                const double * __restrict__ cons_3,\n                const double * __restrict__ cons_4, \n                const double * __restrict__ q_1,\n                const double * __restrict__ q_2,\n                const double * __restrict__ q_3,\n                const double * __restrict__ q_4,\n                double dxinv0, double dxinv1, double dxinv2,\n                int L, int M, int N)\n{\n  //Determing the block's indices\n  int blockdim_i= (int)(blockDim.x);\n  int i0 = (int)(blockIdx.x)*(blockdim_i);\n  int i = max (i0, 0) + (int)(threadIdx.x);\n  int blockdim_j= (int)(blockDim.y);\n  int j0 = (int)(blockIdx.y)*(blockdim_j);\n  int j = max (j0, 0) + (int)(threadIdx.y);\n  int blockdim_k= (int)(blockDim.z);\n  int k0 = (int)(blockIdx.z)*(blockdim_k);\n  int k = max (k0, 0) + (int)(threadIdx.z);\n\n  if (i>=4 & j>=4 & k>=4 & i<=N-5 & j<=N-5 & k<=N-5) {\n  \tflux_0[k*M*N+j*N+i] -= (0.8f*(cons_2[k*M*N+(j+1)*N+i]-cons_2[k*M*N+(j-1)*N+i])-0.2f*(cons_2[k*M*N+(j+2)*N+i]-cons_2[k*M*N+(j-2)*N+i])+0.038f*(cons_2[k*M*N+(j+3)*N+i]-cons_2[k*M*N+(j-3)*N+i])-0.0035f*(cons_2[k*M*N+(j+4)*N+i]-cons_2[k*M*N+(j-4)*N+i]))*dxinv1;\n  \tflux_1[k*M*N+j*N+i] -= (0.8f*(cons_1[k*M*N+(j+1)*N+i]*q_2[k*M*N+(j+1)*N+i]-cons_1[k*M*N+(j-1)*N+i]*q_2[k*M*N+(j-1)*N+i])-0.2f*(cons_1[k*M*N+(j+2)*N+i]*q_2[k*M*N+(j+2)*N+i]-cons_1[k*M*N+(j-2)*N+i]*q_2[k*M*N+(j-2)*N+i])+0.038f*(cons_1[k*M*N+(j+3)*N+i]*q_2[k*M*N+(j+3)*N+i]-cons_1[k*M*N+(j-3)*N+i]*q_2[k*M*N+(j-3)*N+i])-0.0035f*(cons_1[k*M*N+(j+4)*N+i]*q_2[k*M*N+(j+4)*N+i]-cons_1[k*M*N+(j-4)*N+i]*q_2[k*M*N+(j-4)*N+i]))*dxinv1;\n  \tflux_2[k*M*N+j*N+i] -= (0.8f*(cons_2[k*M*N+(j+1)*N+i]*q_2[k*M*N+(j+1)*N+i]-cons_2[k*M*N+(j-1)*N+i]*q_2[k*M*N+(j-1)*N+i]+(q_4[k*M*N+(j+1)*N+i]-q_4[k*M*N+(j-1)*N+i]))-0.2f*(cons_2[k*M*N+(j+2)*N+i]*q_2[k*M*N+(j+2)*N+i]-cons_2[k*M*N+(j-2)*N+i]*q_2[k*M*N+(j-2)*N+i]+(q_4[k*M*N+(j+2)*N+i]-q_4[k*M*N+(j-2)*N+i]))+0.038f*(cons_2[k*M*N+(j+3)*N+i]*q_2[k*M*N+(j+3)*N+i]-cons_2[k*M*N+(j-3)*N+i]*q_2[k*M*N+(j-3)*N+i]+(q_4[k*M*N+(j+3)*N+i]-q_4[k*M*N+(j-3)*N+i]))-0.0035f*(cons_2[k*M*N+(j+4)*N+i]*q_2[k*M*N+(j+4)*N+i]-cons_2[k*M*N+(j-4)*N+i]*q_2[k*M*N+(j-4)*N+i]+(q_4[k*M*N+(j+4)*N+i]-q_4[k*M*N+(j-4)*N+i])))*dxinv1;\n  \tflux_3[k*M*N+j*N+i] -= (0.8f*(cons_3[k*M*N+(j+1)*N+i]*q_2[k*M*N+(j+1)*N+i]-cons_3[k*M*N+(j-1)*N+i]*q_2[k*M*N+(j-1)*N+i])-0.2f*(cons_3[k*M*N+(j+2)*N+i]*q_2[k*M*N+(j+2)*N+i]-cons_3[k*M*N+(j-2)*N+i]*q_2[k*M*N+(j-2)*N+i])+0.038f*(cons_3[k*M*N+(j+3)*N+i]*q_2[k*M*N+(j+3)*N+i]-cons_3[k*M*N+(j-3)*N+i]*q_2[k*M*N+(j-3)*N+i])-0.0035f*(cons_3[k*M*N+(j+4)*N+i]*q_2[k*M*N+(j+4)*N+i]-cons_3[k*M*N+(j-4)*N+i]*q_2[k*M*N+(j-4)*N+i]))*dxinv1;\n  \tflux_4[k*M*N+j*N+i] -= (0.8f*(cons_4[(k+1)*M*N+j*N+i]*q_3[(k+1)*M*N+j*N+i]-cons_4[(k-1)*M*N+j*N+i]*q_3[(k-1)*M*N+j*N+i]+(q_4[(k+1)*M*N+j*N+i]*q_3[(k+1)*M*N+j*N+i]-q_4[(k-1)*M*N+j*N+i]*q_3[(k-1)*M*N+j*N+i]))-0.2f*(cons_4[(k+2)*M*N+j*N+i]*q_3[(k+2)*M*N+j*N+i]-cons_4[(k-2)*M*N+j*N+i]*q_3[(k-2)*M*N+j*N+i]+(q_4[(k+2)*M*N+j*N+i]*q_3[(k+2)*M*N+j*N+i]-q_4[(k-2)*M*N+j*N+i]*q_3[(k-2)*M*N+j*N+i]))+0.038f*(cons_4[(k+3)*M*N+j*N+i]*q_3[(k+3)*M*N+j*N+i]-cons_4[(k-3)*M*N+j*N+i]*q_3[(k-3)*M*N+j*N+i]+(q_4[(k+3)*M*N+j*N+i]*q_3[(k+3)*M*N+j*N+i]-q_4[(k-3)*M*N+j*N+i]*q_3[(k-3)*M*N+j*N+i]))-0.0035f*(cons_4[(k+4)*M*N+j*N+i]*q_3[(k+4)*M*N+j*N+i]-cons_4[(k-4)*M*N+j*N+i]*q_3[(k-4)*M*N+j*N+i]+(q_4[(k+4)*M*N+j*N+i]*q_3[(k+4)*M*N+j*N+i]-q_4[(k-4)*M*N+j*N+i]*q_3[(k-4)*M*N+j*N+i])))*dxinv2;\n  } \n}",
            "inline __host__ __device__ uint4 make_uint4(int4 a)\n{\n    return make_uint4(uint(a.x), uint(a.y), uint(a.z), uint(a.w));\n}\n\ninline __host__ __device__ uint4 max(uint4 a, uint4 b)\n{\n    return make_uint4(max(a.x,b.x), max(a.y,b.y), max(a.z,b.z), max(a.w,b.w));\n}\n\n__global__\nvoid hypterm_3 (double * __restrict__ flux_0,\n                double * __restrict__ flux_1,\n                double * __restrict__ flux_2,\n                double * __restrict__ flux_3,\n                double * __restrict__ flux_4,\n                const double * __restrict__ cons_1,\n                const double * __restrict__ cons_2,\n                const double * __restrict__ cons_3,\n                const double * __restrict__ cons_4, \n                const double * __restrict__ q_1,\n                const double * __restrict__ q_2,\n                const double * __restrict__ q_3,\n                const double * __restrict__ q_4,\n                double dxinv0, double dxinv1, double dxinv2,\n                int L, int M, int N)\n{\n  //Determing the block's indices\n  int blockdim_i= (int)(blockDim.x);\n  int i0 = (int)(blockIdx.x)*(blockdim_i);\n  int i = max (i0, 0) + (int)(threadIdx.x);\n  int blockdim_j= (int)(blockDim.y);\n  int j0 = (int)(blockIdx.y)*(blockdim_j);\n  int j = max (j0, 0) + (int)(threadIdx.y);\n  int blockdim_k= (int)(blockDim.z);\n  int k0 = (int)(blockIdx.z)*(blockdim_k);\n  int k = max (k0, 0) + (int)(threadIdx.z);\n\n  if (i>=4 & j>=4 & k>=4 & i<=N-5 & j<=N-5 & k<=N-5) {\n  \tflux_0[k*M*N+j*N+i] -= (0.8f*(cons_3[(k+1)*M*N+j*N+i]-cons_3[(k-1)*M*N+j*N+i])-0.2f*(cons_3[(k+2)*M*N+j*N+i]-cons_3[(k-2)*M*N+j*N+i])+0.038f*(cons_3[(k+3)*M*N+j*N+i]-cons_3[(k-3)*M*N+j*N+i])-0.0035f*(cons_3[(k+4)*M*N+j*N+i]-cons_3[(k-4)*M*N+j*N+i]))*dxinv2;\n  \tflux_1[k*M*N+j*N+i] -= (0.8f*(cons_1[(k+1)*M*N+j*N+i]*q_3[(k+1)*M*N+j*N+i]-cons_1[(k-1)*M*N+j*N+i]*q_3[(k-1)*M*N+j*N+i])-0.2f*(cons_1[(k+2)*M*N+j*N+i]*q_3[(k+2)*M*N+j*N+i]-cons_1[(k-2)*M*N+j*N+i]*q_3[(k-2)*M*N+j*N+i])+0.038f*(cons_1[(k+3)*M*N+j*N+i]*q_3[(k+3)*M*N+j*N+i]-cons_1[(k-3)*M*N+j*N+i]*q_3[(k-3)*M*N+j*N+i])-0.0035f*(cons_1[(k+4)*M*N+j*N+i]*q_3[(k+4)*M*N+j*N+i]-cons_1[(k-4)*M*N+j*N+i]*q_3[(k-4)*M*N+j*N+i]))*dxinv2;\n  \tflux_2[k*M*N+j*N+i] -= (0.8f*(cons_2[(k+1)*M*N+j*N+i]*q_3[(k+1)*M*N+j*N+i]-cons_2[(k-1)*M*N+j*N+i]*q_3[(k-1)*M*N+j*N+i])-0.2f*(cons_2[(k+2)*M*N+j*N+i]*q_3[(k+2)*M*N+j*N+i]-cons_2[(k-2)*M*N+j*N+i]*q_3[(k-2)*M*N+j*N+i])+0.038f*(cons_2[(k+3)*M*N+j*N+i]*q_3[(k+3)*M*N+j*N+i]-cons_2[(k-3)*M*N+j*N+i]*q_3[(k-3)*M*N+j*N+i])-0.0035f*(cons_2[(k+4)*M*N+j*N+i]*q_3[(k+4)*M*N+j*N+i]-cons_2[(k-4)*M*N+j*N+i]*q_3[(k-4)*M*N+j*N+i]))*dxinv2;\n  \tflux_3[k*M*N+j*N+i] -= (0.8f*(cons_3[(k+1)*M*N+j*N+i]*q_3[(k+1)*M*N+j*N+i]-cons_3[(k-1)*M*N+j*N+i]*q_3[(k-1)*M*N+j*N+i]+(q_4[(k+1)*M*N+j*N+i]-q_4[(k-1)*M*N+j*N+i]))-0.2f*(cons_3[(k+2)*M*N+j*N+i]*q_3[(k+2)*M*N+j*N+i]-cons_3[(k-2)*M*N+j*N+i]*q_3[(k-2)*M*N+j*N+i]+(q_4[(k+2)*M*N+j*N+i]-q_4[(k-2)*M*N+j*N+i]))+0.038f*(cons_3[(k+3)*M*N+j*N+i]*q_3[(k+3)*M*N+j*N+i]-cons_3[(k-3)*M*N+j*N+i]*q_3[(k-3)*M*N+j*N+i]+(q_4[(k+3)*M*N+j*N+i]-q_4[(k-3)*M*N+j*N+i]))-0.0035f*(cons_3[(k+4)*M*N+j*N+i]*q_3[(k+4)*M*N+j*N+i]-cons_3[(k-4)*M*N+j*N+i]*q_3[(k-4)*M*N+j*N+i]+(q_4[(k+4)*M*N+j*N+i]-q_4[(k-4)*M*N+j*N+i])))*dxinv2;\n  \tflux_4[k*M*N+j*N+i] -= (0.8f*(cons_4[k*M*N+(j+1)*N+i]*q_2[k*M*N+(j+1)*N+i]-cons_4[k*M*N+(j-1)*N+i]*q_2[k*M*N+(j-1)*N+i]+(q_4[k*M*N+(j+1)*N+i]*q_2[k*M*N+(j+1)*N+i]-q_4[k*M*N+(j-1)*N+i]*q_2[k*M*N+(j-1)*N+i]))-0.2f*(cons_4[k*M*N+(j+2)*N+i]*q_2[k*M*N+(j+2)*N+i]-cons_4[k*M*N+(j-2)*N+i]*q_2[k*M*N+(j-2)*N+i]+(q_4[k*M*N+(j+2)*N+i]*q_2[k*M*N+(j+2)*N+i]-q_4[k*M*N+(j-2)*N+i]*q_2[k*M*N+(j-2)*N+i]))+0.038f*(cons_4[k*M*N+(j+3)*N+i]*q_2[k*M*N+(j+3)*N+i]-cons_4[k*M*N+(j-3)*N+i]*q_2[k*M*N+(j-3)*N+i]+(q_4[k*M*N+(j+3)*N+i]*q_2[k*M*N+(j+3)*N+i]-q_4[k*M*N+(j-3)*N+i]*q_2[k*M*N+(j-3)*N+i]))-0.0035f*(cons_4[k*M*N+(j+4)*N+i]*q_2[k*M*N+(j+4)*N+i]-cons_4[k*M*N+(j-4)*N+i]*q_2[k*M*N+(j-4)*N+i]+(q_4[k*M*N+(j+4)*N+i]*q_2[k*M*N+(j+4)*N+i]-q_4[k*M*N+(j-4)*N+i]*q_2[k*M*N+(j-4)*N+i])))*dxinv1;\n  } \n}"
        ]
    },
    "svd3x3-cuda": {
        "/Users/gbolet/hecbench-roofline/src/svd3x3-cuda/kernels.cu": [
            "inline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\ninline __host__ __device__ float4 fmaxf(float4 a, float4 b)\n{\n    return make_float4(fmaxf(a.x,b.x), fmaxf(a.y,b.y), fmaxf(a.z,b.z), fmaxf(a.w,b.w));\n}\n\n__device__ __host__ __forceinline__\nvoid svd(\n    float a11, float a12, float a13, float a21, float a22, float a23, float a31, float a32, float a33,      // input A     \n    float &u11, float &u12, float &u13, float &u21, float &u22, float &u23, float &u31, float &u32, float &u33,  // output U      \n    float &s11, \n    //float &s12, float &s13, float &s21, \n    float &s22, \n    //float &s23, float &s31, float &s32, \n    float &s33,  // output S\n    float &v11, float &v12, float &v13, float &v21, float &v22, float &v23, float &v31, float &v32, float &v33  // output V\n  )\n{\n  un Sa11, Sa21, Sa31, Sa12, Sa22, Sa32, Sa13, Sa23, Sa33;\n  un Su11, Su21, Su31, Su12, Su22, Su32, Su13, Su23, Su33;\n  un Sv11, Sv21, Sv31, Sv12, Sv22, Sv32, Sv13, Sv23, Sv33;\n  un Sc, Ss, Sch, Ssh;\n  un Stmp1, Stmp2, Stmp3, Stmp4, Stmp5;\n  un Ss11, Ss21, Ss31, Ss22, Ss32, Ss33;\n  un Sqvs, Sqvvx, Sqvvy, Sqvvz; \n\n  Sa11.f = a11; Sa12.f = a12; Sa13.f = a13;\n  Sa21.f = a21; Sa22.f = a22; Sa23.f = a23;\n  Sa31.f = a31; Sa32.f = a32; Sa33.f = a33;\n\n  //###########################################################\n  // Compute normal equations matrix\n  //###########################################################\n\n  Ss11.f = Sa11.f*Sa11.f;                  \n  Stmp1.f = Sa21.f*Sa21.f;                \n  Ss11.f = __fadd_rn(Stmp1.f, Ss11.f);          \n  Stmp1.f = Sa31.f*Sa31.f;                \n  Ss11.f = __fadd_rn(Stmp1.f, Ss11.f);          \n\n  Ss21.f = Sa12.f*Sa11.f;                  \n  Stmp1.f = Sa22.f*Sa21.f;                \n  Ss21.f = __fadd_rn(Stmp1.f, Ss21.f);          \n  Stmp1.f = Sa32.f*Sa31.f;                \n  Ss21.f = __fadd_rn(Stmp1.f, Ss21.f);          \n\n  Ss31.f = Sa13.f*Sa11.f;                  \n  Stmp1.f = Sa23.f*Sa21.f;                \n  Ss31.f = __fadd_rn(Stmp1.f, Ss31.f);          \n  Stmp1.f = Sa33.f*Sa31.f;                \n  Ss31.f = __fadd_rn(Stmp1.f, Ss31.f);          \n\n  Ss22.f = Sa12.f*Sa12.f;                  \n  Stmp1.f = Sa22.f*Sa22.f;                \n  Ss22.f = __fadd_rn(Stmp1.f, Ss22.f);          \n  Stmp1.f = Sa32.f*Sa32.f;                \n  Ss22.f = __fadd_rn(Stmp1.f, Ss22.f);          \n\n  Ss32.f = Sa13.f*Sa12.f;                  \n  Stmp1.f = Sa23.f*Sa22.f;                \n  Ss32.f = __fadd_rn(Stmp1.f, Ss32.f);          \n  Stmp1.f = Sa33.f*Sa32.f;                \n  Ss32.f = __fadd_rn(Stmp1.f, Ss32.f);          \n\n  Ss33.f = Sa13.f*Sa13.f;                  \n  Stmp1.f = Sa23.f*Sa23.f;                \n  Ss33.f = __fadd_rn(Stmp1.f, Ss33.f);          \n  Stmp1.f = Sa33.f*Sa33.f;                \n  Ss33.f = __fadd_rn(Stmp1.f, Ss33.f);          \n\n  Sqvs.f = 1.f; Sqvvx.f = 0.f; Sqvvy.f = 0.f; Sqvvz.f = 0.f;\n\n  //###########################################################\n  // Solve symmetric eigenproblem using Jacobi iteration\n  //###########################################################\n  for (int i = 0; i < 4; i++)\n  {\n    Ssh.f = Ss21.f * 0.5f;                  \n    Stmp5.f = __fsub_rn(Ss11.f, Ss22.f);                 \n\n    Stmp2.f = Ssh.f*Ssh.f;                                         \n    Stmp1.ui = (Stmp2.f >= gtiny_number) ? 0xffffffff : 0;     \n    Ssh.ui = Stmp1.ui&Ssh.ui;                                      \n    Sch.ui = Stmp1.ui&Stmp5.ui;                                     \n    Stmp2.ui = ~Stmp1.ui&gone;                                \n    Sch.ui = Sch.ui | Stmp2.ui;                                     \n\n    Stmp1.f = Ssh.f*Ssh.f;                         \n    Stmp2.f = Sch.f*Sch.f;                         \n    Stmp3.f = __fadd_rn(Stmp1.f, Stmp2.f);                 \n    Stmp4.f = __frsqrt_rn(Stmp3.f);                     \n\n    Ssh.f = Stmp4.f*Ssh.f;                         \n    Sch.f = Stmp4.f*Sch.f;                         \n    Stmp1.f = gfour_gamma_squared*Stmp1.f;             \n    Stmp1.ui = (Stmp2.f <= Stmp1.f) ? 0xffffffff : 0;           \n\n    Stmp2.ui = gsine_pi_over_eight&Stmp1.ui;               \n    Ssh.ui = ~Stmp1.ui&Ssh.ui;                       \n    Ssh.ui = Ssh.ui | Stmp2.ui;                       \n    Stmp2.ui = gcosine_pi_over_eight&Stmp1.ui;           \n    Sch.ui = ~Stmp1.ui&Sch.ui;                       \n    Sch.ui = Sch.ui | Stmp2.ui;                       \n\n    Stmp1.f = Ssh.f * Ssh.f;                       \n    Stmp2.f = Sch.f * Sch.f;                \n    Sc.f = __fsub_rn(Stmp2.f, Stmp1.f);            \n    Ss.f = Sch.f * Ssh.f;                         \n    Ss.f = __fadd_rn(Ss.f, Ss.f);                     \n\n#ifdef DEBUG_JACOBI_CONJUGATE\n    printf(\"GPU s %.20g, c %.20g, sh %.20g, ch %.20g\\n\", Ss.f, Sc.f, Ssh.f, Sch.f);\n#endif\n    //###########################################################\n    // Perform the actual Givens conjugation\n    //###########################################################\n\n    Stmp3.f = __fadd_rn(Stmp1.f, Stmp2.f);              \n    Ss33.f = Ss33.f * Stmp3.f;                    \n    Ss31.f = Ss31.f * Stmp3.f;                    \n    Ss32.f = Ss32.f * Stmp3.f;                    \n    Ss33.f = Ss33.f * Stmp3.f;                    \n\n    Stmp1.f = Ss.f * Ss31.f;                                                    \n    Stmp2.f = Ss.f * Ss32.f;                                                    \n    Ss31.f = Sc.f * Ss31.f;                                                      \n    Ss32.f = Sc.f * Ss32.f;                                                      \n    Ss31.f = __fadd_rn(Stmp2.f, Ss31.f);                                              \n    Ss32.f = __fsub_rn(Ss32.f, Stmp1.f);                                              \n\n    Stmp2.f = Ss.f*Ss.f;                                                      \n    Stmp1.f = Ss22.f*Stmp2.f;                                                    \n    Stmp3.f = Ss11.f*Stmp2.f;                                                    \n    Stmp4.f = Sc.f*Sc.f;                                                      \n    Ss11.f = Ss11.f*Stmp4.f;                                                    \n    Ss22.f = Ss22.f*Stmp4.f;                                                    \n    Ss11.f = __fadd_rn(Ss11.f, Stmp1.f);                                              \n    Ss22.f = __fadd_rn(Ss22.f, Stmp3.f);                                              \n    Stmp4.f = __fsub_rn(Stmp4.f, Stmp2.f);                                              \n    Stmp2.f = __fadd_rn(Ss21.f, Ss21.f);                                              \n    Ss21.f = Ss21.f*Stmp4.f;                                                    \n    Stmp4.f = Sc.f*Ss.f;                                                      \n    Stmp2.f = Stmp2.f*Stmp4.f;                                                    \n    Stmp5.f = Stmp5.f*Stmp4.f;                                                    \n    Ss11.f = __fadd_rn(Ss11.f, Stmp2.f);                                              \n    Ss21.f = __fsub_rn(Ss21.f, Stmp5.f);                                              \n    Ss22.f = __fsub_rn(Ss22.f, Stmp2.f);                                              \n\n#ifdef DEBUG_JACOBI_CONJUGATE\n    printf(\"%.20g\\n\", Ss11.f);\n    printf(\"%.20g %.20g\\n\", Ss21.f, Ss22.f);\n    printf(\"%.20g %.20g %.20g\\n\", Ss31.f, Ss32.f, Ss33.f);\n#endif\n\n    //###########################################################\n    // Compute the cumulative rotation, in quaternion form\n    //###########################################################\n\n    Stmp1.f = Ssh.f*Sqvvx.f;                                        \n    Stmp2.f = Ssh.f*Sqvvy.f;                                        \n    Stmp3.f = Ssh.f*Sqvvz.f;                                        \n    Ssh.f = Ssh.f*Sqvs.f;                                          \n\n    Sqvs.f = Sch.f*Sqvs.f;                                          \n    Sqvvx.f = Sch.f*Sqvvx.f;                                            \n    Sqvvy.f = Sch.f*Sqvvy.f;                                            \n    Sqvvz.f = Sch.f*Sqvvz.f;                                            \n\n    Sqvvz.f = __fadd_rn(Sqvvz.f, Ssh.f);                                              \n    Sqvs.f = __fsub_rn(Sqvs.f, Stmp3.f);                                              \n    Sqvvx.f = __fadd_rn(Sqvvx.f, Stmp2.f);                                              \n    Sqvvy.f = __fsub_rn(Sqvvy.f, Stmp1.f);                                          \n\n#ifdef DEBUG_JACOBI_CONJUGATE\n    printf(\"GPU q %.20g %.20g %.20g %.20g\\n\", Sqvvx.f, Sqvvy.f, Sqvvz.f, Sqvs.f);\n#endif\n\n    //////////////////////////////////////////////////////////////////////////\n    // (1->3)\n    //////////////////////////////////////////////////////////////////////////\n    Ssh.f = Ss32.f * 0.5f;                   \n    Stmp5.f = __fsub_rn(Ss22.f, Ss33.f);                                   \n\n    Stmp2.f = Ssh.f * Ssh.f;                                         \n    Stmp1.ui = (Stmp2.f >= gtiny_number) ? 0xffffffff : 0;       \n    Ssh.ui = Stmp1.ui&Ssh.ui;                                        \n    Sch.ui = Stmp1.ui&Stmp5.ui;                                   \n    Stmp2.ui = ~Stmp1.ui&gone;                                  \n    Sch.ui = Sch.ui | Stmp2.ui;                                   \n\n    Stmp1.f = Ssh.f * Ssh.f;                             \n    Stmp2.f = Sch.f * Sch.f;                             \n    Stmp3.f = __fadd_rn(Stmp1.f, Stmp2.f);                           \n    Stmp4.f = __frsqrt_rn(Stmp3.f);                           \n\n    Ssh.f = Stmp4.f * Ssh.f;                             \n    Sch.f = Stmp4.f * Sch.f;                             \n    Stmp1.f = gfour_gamma_squared * Stmp1.f;                 \n    Stmp1.ui = (Stmp2.f <= Stmp1.f) ? 0xffffffff : 0;             \n\n    Stmp2.ui = gsine_pi_over_eight&Stmp1.ui;             \n    Ssh.ui = ~Stmp1.ui&Ssh.ui;                         \n    Ssh.ui = Ssh.ui | Stmp2.ui;                     \n    Stmp2.ui = gcosine_pi_over_eight&Stmp1.ui;             \n    Sch.ui = ~Stmp1.ui&Sch.ui;                         \n    Sch.ui = Sch.ui | Stmp2.ui;                     \n\n    Stmp1.f = Ssh.f * Ssh.f;                             \n    Stmp2.f = Sch.f * Sch.f;                             \n    Sc.f = __fsub_rn(Stmp2.f, Stmp1.f);                 \n    Ss.f = Sch.f*Ssh.f;                         \n    Ss.f = __fadd_rn(Ss.f, Ss.f);                               \n\n#ifdef DEBUG_JACOBI_CONJUGATE\n    printf(\"GPU s %.20g, c %.20g, sh %.20g, ch %.20g\\n\", Ss.f, Sc.f, Ssh.f, Sch.f);\n#endif\n\n    //###########################################################\n    // Perform the actual Givens conjugation\n    //###########################################################\n\n    Stmp3.f = __fadd_rn(Stmp1.f, Stmp2.f);            \n    Ss11.f = Ss11.f * Stmp3.f;                  \n    Ss21.f = Ss21.f * Stmp3.f;                  \n    Ss31.f = Ss31.f * Stmp3.f;                  \n    Ss11.f = Ss11.f * Stmp3.f;                  \n\n    Stmp1.f = Ss.f*Ss21.f;                                  \n    Stmp2.f = Ss.f*Ss31.f;                                  \n    Ss21.f = Sc.f*Ss21.f;                                      \n    Ss31.f = Sc.f*Ss31.f;                                      \n    Ss21.f = __fadd_rn(Stmp2.f, Ss21.f);                                          \n    Ss31.f = __fsub_rn(Ss31.f, Stmp1.f);                                          \n\n    Stmp2.f = Ss.f*Ss.f;                                  \n    Stmp1.f = Ss33.f*Stmp2.f;                                \n    Stmp3.f = Ss22.f*Stmp2.f;                                \n    Stmp4.f = Sc.f * Sc.f;                                  \n    Ss22.f = Ss22.f * Stmp4.f;                                    \n    Ss33.f = Ss33.f * Stmp4.f;                                    \n    Ss22.f = __fadd_rn(Ss22.f, Stmp1.f);                                          \n    Ss33.f = __fadd_rn(Ss33.f, Stmp3.f);                                          \n    Stmp4.f = __fsub_rn(Stmp4.f, Stmp2.f);                                  \n    Stmp2.f = __fadd_rn(Ss32.f, Ss32.f);                                          \n    Ss32.f = Ss32.f*Stmp4.f;                                    \n    Stmp4.f = Sc.f*Ss.f;                                  \n    Stmp2.f = Stmp2.f*Stmp4.f;                                \n    Stmp5.f = Stmp5.f*Stmp4.f;                                \n    Ss22.f = __fadd_rn(Ss22.f, Stmp2.f);                                          \n    Ss32.f = __fsub_rn(Ss32.f, Stmp5.f);                              \n    Ss33.f = __fsub_rn(Ss33.f, Stmp2.f);                              \n\n#ifdef DEBUG_JACOBI_CONJUGATE\n    printf(\"%.20g\\n\", Ss11.f);\n    printf(\"%.20g %.20g\\n\", Ss21.f, Ss22.f);\n    printf(\"%.20g %.20g %.20g\\n\", Ss31.f, Ss32.f, Ss33.f);\n#endif\n\n    //###########################################################\n    // Compute the cumulative rotation, in quaternion form\n    //###########################################################\n\n    Stmp1.f = Ssh.f*Sqvvx.f;                                          \n    Stmp2.f = Ssh.f*Sqvvy.f;                                          \n    Stmp3.f = Ssh.f*Sqvvz.f;                                          \n    Ssh.f = Ssh.f*Sqvs.f;                                            \n\n    Sqvs.f = Sch.f*Sqvs.f;                                            \n    Sqvvx.f = Sch.f*Sqvvx.f;                                              \n    Sqvvy.f = Sch.f*Sqvvy.f;                                              \n    Sqvvz.f = Sch.f*Sqvvz.f;                                              \n\n    Sqvvx.f = __fadd_rn(Sqvvx.f, Ssh.f);                                                \n    Sqvs.f = __fsub_rn(Sqvs.f, Stmp1.f);                                                \n    Sqvvy.f = __fadd_rn(Sqvvy.f, Stmp3.f);                                                \n    Sqvvz.f = __fsub_rn(Sqvvz.f, Stmp2.f);               \n\n#ifdef DEBUG_JACOBI_CONJUGATE\n    printf(\"GPU q %.20g %.20g %.20g %.20g\\n\", Sqvvx.f, Sqvvy.f, Sqvvz.f, Sqvs.f);\n#endif\n#if 1\n    //////////////////////////////////////////////////////////////////////////\n    // 1 -> 2\n    //////////////////////////////////////////////////////////////////////////\n\n    Ssh.f = Ss31.f * 0.5f;                    \n    Stmp5.f = __fsub_rn(Ss33.f, Ss11.f);                                    \n\n    Stmp2.f = Ssh.f*Ssh.f;                                            \n    Stmp1.ui = (Stmp2.f >= gtiny_number) ? 0xffffffff : 0;        \n    Ssh.ui = Stmp1.ui&Ssh.ui;                                         \n    Sch.ui = Stmp1.ui&Stmp5.ui;                                    \n    Stmp2.ui = ~Stmp1.ui&gone;                                   \n    Sch.ui = Sch.ui | Stmp2.ui;                                    \n\n    Stmp1.f = Ssh.f*Ssh.f;                            \n    Stmp2.f = Sch.f*Sch.f;                            \n    Stmp3.f = __fadd_rn(Stmp1.f, Stmp2.f);                                \n    Stmp4.f = __frsqrt_rn(Stmp3.f);                            \n\n    Ssh.f = Stmp4.f*Ssh.f;                            \n    Sch.f = Stmp4.f*Sch.f;                            \n    Stmp1.f = gfour_gamma_squared*Stmp1.f;                \n    Stmp1.ui = (Stmp2.f <= Stmp1.f) ? 0xffffffff : 0;              \n\n    Stmp2.ui = gsine_pi_over_eight&Stmp1.ui;              \n    Ssh.ui = ~Stmp1.ui&Ssh.ui;                          \n    Ssh.ui = Ssh.ui | Stmp2.ui;                      \n    Stmp2.ui = gcosine_pi_over_eight&Stmp1.ui;              \n    Sch.ui = ~Stmp1.ui&Sch.ui;                          \n    Sch.ui = Sch.ui | Stmp2.ui;                      \n\n    Stmp1.f = Ssh.f*Ssh.f;                            \n    Stmp2.f = Sch.f*Sch.f;                            \n    Sc.f = __fsub_rn(Stmp2.f, Stmp1.f);                  \n    Ss.f = Sch.f*Ssh.f;                          \n    Ss.f = __fadd_rn(Ss.f, Ss.f);                                \n\n#ifdef DEBUG_JACOBI_CONJUGATE\n    printf(\"GPU s %.20g, c %.20g, sh %.20g, ch %.20g\\n\", Ss.f, Sc.f, Ssh.f, Sch.f);\n#endif\n\n    //###########################################################\n    // Perform the actual Givens conjugation\n    //###########################################################\n\n    Stmp3.f = __fadd_rn(Stmp1.f, Stmp2.f);              \n    Ss22.f = Ss22.f * Stmp3.f;                    \n    Ss32.f = Ss32.f * Stmp3.f;                    \n    Ss21.f = Ss21.f * Stmp3.f;                    \n    Ss22.f = Ss22.f * Stmp3.f;                    \n\n    Stmp1.f = Ss.f*Ss32.f;                                      \n    Stmp2.f = Ss.f*Ss21.f;                                      \n    Ss32.f = Sc.f*Ss32.f;                                          \n    Ss21.f = Sc.f*Ss21.f;                                          \n    Ss32.f = __fadd_rn(Stmp2.f, Ss32.f);                                              \n    Ss21.f = __fsub_rn(Ss21.f, Stmp1.f);                                              \n\n    Stmp2.f = Ss.f*Ss.f;                                      \n    Stmp1.f = Ss11.f*Stmp2.f;                                    \n    Stmp3.f = Ss33.f*Stmp2.f;                                    \n    Stmp4.f = Sc.f*Sc.f;                                      \n    Ss33.f = Ss33.f*Stmp4.f;                                        \n    Ss11.f = Ss11.f*Stmp4.f;                                        \n    Ss33.f = __fadd_rn(Ss33.f, Stmp1.f);                                              \n    Ss11.f = __fadd_rn(Ss11.f, Stmp3.f);                                              \n    Stmp4.f = __fsub_rn(Stmp4.f, Stmp2.f);                                      \n    Stmp2.f = __fadd_rn(Ss31.f, Ss31.f);                                              \n    Ss31.f = Ss31.f*Stmp4.f;                                        \n    Stmp4.f = Sc.f*Ss.f;                                      \n    Stmp2.f = Stmp2.f*Stmp4.f;                                    \n    Stmp5.f = Stmp5.f*Stmp4.f;                                    \n    Ss33.f = __fadd_rn(Ss33.f, Stmp2.f);                                              \n    Ss31.f = __fsub_rn(Ss31.f, Stmp5.f);                                              \n    Ss11.f = __fsub_rn(Ss11.f, Stmp2.f);                                              \n\n#ifdef DEBUG_JACOBI_CONJUGATE\n    printf(\"%.20g\\n\", Ss11.f);\n    printf(\"%.20g %.20g\\n\", Ss21.f, Ss22.f);\n    printf(\"%.20g %.20g %.20g\\n\", Ss31.f, Ss32.f, Ss33.f);\n#endif\n\n    //###########################################################\n    // Compute the cumulative rotation, in quaternion form\n    //###########################################################\n\n    Stmp1.f = Ssh.f*Sqvvx.f;                                            \n    Stmp2.f = Ssh.f*Sqvvy.f;                                            \n    Stmp3.f = Ssh.f*Sqvvz.f;                                            \n    Ssh.f = Ssh.f*Sqvs.f;                                              \n\n    Sqvs.f = Sch.f*Sqvs.f;                                              \n    Sqvvx.f = Sch.f*Sqvvx.f;                                                \n    Sqvvy.f = Sch.f*Sqvvy.f;                                                \n    Sqvvz.f = Sch.f*Sqvvz.f;                                                \n\n    Sqvvy.f = __fadd_rn(Sqvvy.f, Ssh.f);                                                  \n    Sqvs.f = __fsub_rn(Sqvs.f, Stmp2.f);                                      \n    Sqvvz.f = __fadd_rn(Sqvvz.f, Stmp1.f);                                                  \n    Sqvvx.f = __fsub_rn(Sqvvx.f, Stmp3.f);              \n#endif\n  }\n\n  //###########################################################\n  // Normalize quaternion for matrix V\n  //###########################################################\n\n  Stmp2.f = Sqvs.f*Sqvs.f;\n  Stmp1.f = Sqvvx.f*Sqvvx.f;\n  Stmp2.f = __fadd_rn(Stmp1.f, Stmp2.f);\n  Stmp1.f = Sqvvy.f*Sqvvy.f; \n  Stmp2.f = __fadd_rn(Stmp1.f, Stmp2.f);\n  Stmp1.f = Sqvvz.f*Sqvvz.f; \n  Stmp2.f = __fadd_rn(Stmp1.f, Stmp2.f);\n\n  Stmp1.f = __frsqrt_rn(Stmp2.f);\n  Stmp4.f = Stmp1.f*0.5f;\n  Stmp3.f = Stmp1.f*Stmp4.f;\n  Stmp3.f = Stmp1.f*Stmp3.f;\n  Stmp3.f = Stmp2.f*Stmp3.f;\n  Stmp1.f = __fadd_rn(Stmp1.f, Stmp4.f);\n  Stmp1.f = __fsub_rn(Stmp1.f, Stmp3.f);\n\n  Sqvs.f = Sqvs.f*Stmp1.f;\n  Sqvvx.f = Sqvvx.f*Stmp1.f;\n  Sqvvy.f = Sqvvy.f*Stmp1.f;\n  Sqvvz.f = Sqvvz.f*Stmp1.f;\n\n  //###########################################################\n  // Transform quaternion to matrix V\n  //###########################################################\n\n  Stmp1.f = Sqvvx.f*Sqvvx.f;\n  Stmp2.f = Sqvvy.f*Sqvvy.f;\n  Stmp3.f = Sqvvz.f*Sqvvz.f;\n  Sv11.f = Sqvs.f*Sqvs.f;\n  Sv22.f = __fsub_rn(Sv11.f, Stmp1.f);\n  Sv33.f = __fsub_rn(Sv22.f, Stmp2.f);\n  Sv33.f = __fadd_rn(Sv33.f, Stmp3.f);\n  Sv22.f = __fadd_rn(Sv22.f, Stmp2.f);\n  Sv22.f = __fsub_rn(Sv22.f, Stmp3.f);\n  Sv11.f = __fadd_rn(Sv11.f, Stmp1.f);\n  Sv11.f = __fsub_rn(Sv11.f, Stmp2.f);\n  Sv11.f = __fsub_rn(Sv11.f, Stmp3.f);\n  Stmp1.f = __fadd_rn(Sqvvx.f, Sqvvx.f);\n  Stmp2.f = __fadd_rn(Sqvvy.f, Sqvvy.f);\n  Stmp3.f = __fadd_rn(Sqvvz.f, Sqvvz.f);\n  Sv32.f = Sqvs.f*Stmp1.f;\n  Sv13.f = Sqvs.f*Stmp2.f;\n  Sv21.f = Sqvs.f*Stmp3.f;\n  Stmp1.f = Sqvvy.f*Stmp1.f;\n  Stmp2.f = Sqvvz.f*Stmp2.f;\n  Stmp3.f = Sqvvx.f*Stmp3.f;\n  Sv12.f = __fsub_rn(Stmp1.f, Sv21.f);\n  Sv23.f = __fsub_rn(Stmp2.f, Sv32.f);\n  Sv31.f = __fsub_rn(Stmp3.f, Sv13.f);\n  Sv21.f = __fadd_rn(Stmp1.f, Sv21.f);\n  Sv32.f = __fadd_rn(Stmp2.f, Sv32.f);\n  Sv13.f = __fadd_rn(Stmp3.f, Sv13.f);\n\n  ///###########################################################\n  // Multiply (from the right) with V\n  //###########################################################\n\n  Stmp2.f = Sa12.f;\n  Stmp3.f = Sa13.f;\n  Sa12.f = Sv12.f*Sa11.f;\n  Sa13.f = Sv13.f*Sa11.f;\n  Sa11.f = Sv11.f*Sa11.f;\n  Stmp1.f = Sv21.f*Stmp2.f;\n  Sa11.f = __fadd_rn(Sa11.f, Stmp1.f);\n  Stmp1.f = Sv31.f*Stmp3.f;\n  Sa11.f = __fadd_rn(Sa11.f, Stmp1.f);\n  Stmp1.f = Sv22.f*Stmp2.f;\n  Sa12.f = __fadd_rn(Sa12.f, Stmp1.f);\n  Stmp1.f = Sv32.f*Stmp3.f;\n  Sa12.f = __fadd_rn(Sa12.f, Stmp1.f);\n  Stmp1.f = Sv23.f*Stmp2.f;\n  Sa13.f = __fadd_rn(Sa13.f, Stmp1.f);\n  Stmp1.f = Sv33.f*Stmp3.f;\n  Sa13.f = __fadd_rn(Sa13.f, Stmp1.f);\n\n  Stmp2.f = Sa22.f;\n  Stmp3.f = Sa23.f;\n  Sa22.f = Sv12.f*Sa21.f;\n  Sa23.f = Sv13.f*Sa21.f;\n  Sa21.f = Sv11.f*Sa21.f;\n  Stmp1.f = Sv21.f*Stmp2.f;\n  Sa21.f = __fadd_rn(Sa21.f, Stmp1.f);\n  Stmp1.f = Sv31.f*Stmp3.f;\n  Sa21.f = __fadd_rn(Sa21.f, Stmp1.f);\n  Stmp1.f = Sv22.f*Stmp2.f;\n  Sa22.f = __fadd_rn(Sa22.f, Stmp1.f);\n  Stmp1.f = Sv32.f*Stmp3.f;\n  Sa22.f = __fadd_rn(Sa22.f, Stmp1.f);\n  Stmp1.f = Sv23.f*Stmp2.f;\n  Sa23.f = __fadd_rn(Sa23.f, Stmp1.f);\n  Stmp1.f = Sv33.f*Stmp3.f;\n  Sa23.f = __fadd_rn(Sa23.f, Stmp1.f);\n\n  Stmp2.f = Sa32.f;\n  Stmp3.f = Sa33.f;\n  Sa32.f = Sv12.f*Sa31.f;\n  Sa33.f = Sv13.f*Sa31.f;\n  Sa31.f = Sv11.f*Sa31.f;\n  Stmp1.f = Sv21.f*Stmp2.f;\n  Sa31.f = __fadd_rn(Sa31.f, Stmp1.f);\n  Stmp1.f = Sv31.f*Stmp3.f;\n  Sa31.f = __fadd_rn(Sa31.f, Stmp1.f);\n  Stmp1.f = Sv22.f*Stmp2.f;\n  Sa32.f = __fadd_rn(Sa32.f, Stmp1.f);\n  Stmp1.f = Sv32.f*Stmp3.f;\n  Sa32.f = __fadd_rn(Sa32.f, Stmp1.f);\n  Stmp1.f = Sv23.f*Stmp2.f;\n  Sa33.f = __fadd_rn(Sa33.f, Stmp1.f);\n  Stmp1.f = Sv33.f*Stmp3.f;\n  Sa33.f = __fadd_rn(Sa33.f, Stmp1.f);\n\n  //###########################################################\n  // Permute columns such that the singular values are sorted\n  //###########################################################\n\n  Stmp1.f = Sa11.f*Sa11.f;                \n  Stmp4.f = Sa21.f*Sa21.f;                \n  Stmp1.f = __fadd_rn(Stmp1.f, Stmp4.f);          \n  Stmp4.f = Sa31.f*Sa31.f;                \n  Stmp1.f = __fadd_rn(Stmp1.f, Stmp4.f);          \n\n  Stmp2.f = Sa12.f*Sa12.f;                \n  Stmp4.f = Sa22.f*Sa22.f;                \n  Stmp2.f = __fadd_rn(Stmp2.f, Stmp4.f);          \n  Stmp4.f = Sa32.f*Sa32.f;                \n  Stmp2.f = __fadd_rn(Stmp2.f, Stmp4.f);          \n\n  Stmp3.f = Sa13.f*Sa13.f;                \n  Stmp4.f = Sa23.f*Sa23.f;                \n  Stmp3.f = __fadd_rn(Stmp3.f, Stmp4.f);          \n  Stmp4.f = Sa33.f*Sa33.f;                \n  Stmp3.f = __fadd_rn(Stmp3.f, Stmp4.f);          \n\n  // Swap columns 1-2 if necessary\n\n  Stmp4.ui = (Stmp1.f < Stmp2.f) ? 0xffffffff : 0;  \n  Stmp5.ui = Sa11.ui^Sa12.ui;                \n  Stmp5.ui = Stmp5.ui&Stmp4.ui;              \n  Sa11.ui = Sa11.ui^Stmp5.ui;                \n  Sa12.ui = Sa12.ui^Stmp5.ui;                \n\n  Stmp5.ui = Sa21.ui^Sa22.ui;                \n  Stmp5.ui = Stmp5.ui&Stmp4.ui;              \n  Sa21.ui = Sa21.ui^Stmp5.ui;                \n  Sa22.ui = Sa22.ui^Stmp5.ui;                \n\n  Stmp5.ui = Sa31.ui^Sa32.ui;                \n  Stmp5.ui = Stmp5.ui&Stmp4.ui;              \n  Sa31.ui = Sa31.ui^Stmp5.ui;                \n  Sa32.ui = Sa32.ui^Stmp5.ui;                \n\n  Stmp5.ui = Sv11.ui^Sv12.ui;                \n  Stmp5.ui = Stmp5.ui&Stmp4.ui;              \n  Sv11.ui = Sv11.ui^Stmp5.ui;                \n  Sv12.ui = Sv12.ui^Stmp5.ui;                \n\n  Stmp5.ui = Sv21.ui^Sv22.ui;                \n  Stmp5.ui = Stmp5.ui&Stmp4.ui;              \n  Sv21.ui = Sv21.ui^Stmp5.ui;                \n  Sv22.ui = Sv22.ui^Stmp5.ui;                \n\n  Stmp5.ui = Sv31.ui^Sv32.ui;                \n  Stmp5.ui = Stmp5.ui&Stmp4.ui;              \n  Sv31.ui = Sv31.ui^Stmp5.ui;                \n  Sv32.ui = Sv32.ui^Stmp5.ui;                \n\n  Stmp5.ui = Stmp1.ui^Stmp2.ui;              \n  Stmp5.ui = Stmp5.ui&Stmp4.ui;              \n  Stmp1.ui = Stmp1.ui^Stmp5.ui;              \n  Stmp2.ui = Stmp2.ui^Stmp5.ui;              \n\n  // If columns 1-2 have been swapped, negate 2nd column of A and V so that V is still a rotation\n\n  Stmp5.f = -2.f;                      \n  Stmp5.ui = Stmp5.ui&Stmp4.ui;              \n  Stmp4.f = 1.f;                      \n  Stmp4.f = __fadd_rn(Stmp4.f, Stmp5.f);          \n\n  Sa12.f = Sa12.f*Stmp4.f;                \n  Sa22.f = Sa22.f*Stmp4.f;                \n  Sa32.f = Sa32.f*Stmp4.f;                \n\n  Sv12.f = Sv12.f*Stmp4.f;                \n  Sv22.f = Sv22.f*Stmp4.f;                \n  Sv32.f = Sv32.f*Stmp4.f;                \n\n  // Swap columns 1-3 if necessary\n\n  Stmp4.ui = (Stmp1.f < Stmp3.f) ? 0xffffffff : 0;    \n  Stmp5.ui = Sa11.ui^Sa13.ui;                \n  Stmp5.ui = Stmp5.ui&Stmp4.ui;              \n  Sa11.ui = Sa11.ui^Stmp5.ui;                \n  Sa13.ui = Sa13.ui^Stmp5.ui;                \n\n  Stmp5.ui = Sa21.ui^Sa23.ui;                \n  Stmp5.ui = Stmp5.ui&Stmp4.ui;              \n  Sa21.ui = Sa21.ui^Stmp5.ui;                \n  Sa23.ui = Sa23.ui^Stmp5.ui;                \n\n  Stmp5.ui = Sa31.ui^Sa33.ui;                \n  Stmp5.ui = Stmp5.ui&Stmp4.ui;              \n  Sa31.ui = Sa31.ui^Stmp5.ui;                \n  Sa33.ui = Sa33.ui^Stmp5.ui;                \n\n  Stmp5.ui = Sv11.ui^Sv13.ui;                \n  Stmp5.ui = Stmp5.ui&Stmp4.ui;              \n  Sv11.ui = Sv11.ui^Stmp5.ui;                \n  Sv13.ui = Sv13.ui^Stmp5.ui;                \n\n  Stmp5.ui = Sv21.ui^Sv23.ui;                \n  Stmp5.ui = Stmp5.ui&Stmp4.ui;              \n  Sv21.ui = Sv21.ui^Stmp5.ui;                \n  Sv23.ui = Sv23.ui^Stmp5.ui;                \n\n  Stmp5.ui = Sv31.ui^Sv33.ui;                \n  Stmp5.ui = Stmp5.ui&Stmp4.ui;              \n  Sv31.ui = Sv31.ui^Stmp5.ui;                \n  Sv33.ui = Sv33.ui^Stmp5.ui;                \n\n  Stmp5.ui = Stmp1.ui^Stmp3.ui;              \n  Stmp5.ui = Stmp5.ui&Stmp4.ui;              \n  Stmp1.ui = Stmp1.ui^Stmp5.ui;              \n  Stmp3.ui = Stmp3.ui^Stmp5.ui;              \n\n  // If columns 1-3 have been swapped, negate 1st column of A and V so that V is still a rotation\n\n  Stmp5.f = -2.f;                      \n  Stmp5.ui = Stmp5.ui&Stmp4.ui;              \n  Stmp4.f = 1.f;                      \n  Stmp4.f = __fadd_rn(Stmp4.f, Stmp5.f);          \n\n  Sa11.f = Sa11.f*Stmp4.f;                \n  Sa21.f = Sa21.f*Stmp4.f;                \n  Sa31.f = Sa31.f*Stmp4.f;                \n\n  Sv11.f = Sv11.f*Stmp4.f;                \n  Sv21.f = Sv21.f*Stmp4.f;                \n  Sv31.f = Sv31.f*Stmp4.f;                \n\n  // Swap columns 2-3 if necessary\n\n  Stmp4.ui = (Stmp2.f < Stmp3.f) ? 0xffffffff : 0;    \n  Stmp5.ui = Sa12.ui^Sa13.ui;                \n  Stmp5.ui = Stmp5.ui&Stmp4.ui;              \n  Sa12.ui = Sa12.ui^Stmp5.ui;                \n  Sa13.ui = Sa13.ui^Stmp5.ui;                \n\n  Stmp5.ui = Sa22.ui^Sa23.ui;                \n  Stmp5.ui = Stmp5.ui&Stmp4.ui;              \n  Sa22.ui = Sa22.ui^Stmp5.ui;                \n  Sa23.ui = Sa23.ui^Stmp5.ui;                \n\n  Stmp5.ui = Sa32.ui^Sa33.ui;                \n  Stmp5.ui = Stmp5.ui&Stmp4.ui;              \n  Sa32.ui = Sa32.ui^Stmp5.ui;                \n  Sa33.ui = Sa33.ui^Stmp5.ui;                \n\n  Stmp5.ui = Sv12.ui^Sv13.ui;                \n  Stmp5.ui = Stmp5.ui&Stmp4.ui;              \n  Sv12.ui = Sv12.ui^Stmp5.ui;                \n  Sv13.ui = Sv13.ui^Stmp5.ui;                \n\n  Stmp5.ui = Sv22.ui^Sv23.ui;                \n  Stmp5.ui = Stmp5.ui&Stmp4.ui;              \n  Sv22.ui = Sv22.ui^Stmp5.ui;                \n  Sv23.ui = Sv23.ui^Stmp5.ui;                \n\n  Stmp5.ui = Sv32.ui^Sv33.ui;                \n  Stmp5.ui = Stmp5.ui&Stmp4.ui;              \n  Sv32.ui = Sv32.ui^Stmp5.ui;                \n  Sv33.ui = Sv33.ui^Stmp5.ui;                \n\n  Stmp5.ui = Stmp2.ui^Stmp3.ui;              \n  Stmp5.ui = Stmp5.ui&Stmp4.ui;              \n  Stmp2.ui = Stmp2.ui^Stmp5.ui;              \n  Stmp3.ui = Stmp3.ui^Stmp5.ui;              \n\n  // If columns 2-3 have been swapped, negate 3rd column of A and V so that V is still a rotation\n\n  Stmp5.f = -2.f;                      \n  Stmp5.ui = Stmp5.ui&Stmp4.ui;              \n  Stmp4.f = 1.f;                      \n  Stmp4.f = __fadd_rn(Stmp4.f, Stmp5.f);          \n\n  Sa13.f = Sa13.f*Stmp4.f;                \n  Sa23.f = Sa23.f*Stmp4.f;                \n  Sa33.f = Sa33.f*Stmp4.f;                \n\n  Sv13.f = Sv13.f*Stmp4.f;                \n  Sv23.f = Sv23.f*Stmp4.f;                \n  Sv33.f = Sv33.f*Stmp4.f;                \n\n  //###########################################################\n  // Construct QR factorization of A*V (=U*D) using Givens rotations\n  //###########################################################\n\n  Su11.f = 1.f; Su12.f = 0.f; Su13.f = 0.f;\n  Su21.f = 0.f; Su22.f = 1.f; Su23.f = 0.f;\n  Su31.f = 0.f; Su32.f = 0.f; Su33.f = 1.f;\n\n  Ssh.f = Sa21.f*Sa21.f;                \n  Ssh.ui = (Ssh.f >= gsmall_number) ? 0xffffffff : 0;  \n  Ssh.ui = Ssh.ui&Sa21.ui;              \n\n  Stmp5.f = 0.f;                    \n  Sch.f = __fsub_rn(Stmp5.f, Sa11.f);          \n  Sch.f = fmaxf(Sch.f, Sa11.f);              \n  Sch.f = fmaxf(Sch.f, gsmall_number);          \n  Stmp5.ui = (Sa11.f >= Stmp5.f) ? 0xffffffff : 0;  \n\n  Stmp1.f = Sch.f*Sch.f;                \n  Stmp2.f = Ssh.f*Ssh.f;                \n  Stmp2.f = __fadd_rn(Stmp1.f, Stmp2.f);        \n  Stmp1.f = __frsqrt_rn(Stmp2.f);            \n\n  Stmp4.f = Stmp1.f*0.5f;              \n  Stmp3.f = Stmp1.f*Stmp4.f;            \n  Stmp3.f = Stmp1.f*Stmp3.f;            \n  Stmp3.f = Stmp2.f*Stmp3.f;            \n  Stmp1.f = __fadd_rn(Stmp1.f, Stmp4.f);      \n  Stmp1.f = __fsub_rn(Stmp1.f, Stmp3.f);      \n  Stmp1.f = Stmp1.f*Stmp2.f;            \n\n  Sch.f = __fadd_rn(Sch.f, Stmp1.f);        \n\n  Stmp1.ui = ~Stmp5.ui&Ssh.ui;          \n  Stmp2.ui = ~Stmp5.ui&Sch.ui;          \n  Sch.ui = Stmp5.ui&Sch.ui;            \n  Ssh.ui = Stmp5.ui&Ssh.ui;            \n  Sch.ui = Sch.ui | Stmp1.ui;            \n  Ssh.ui = Ssh.ui | Stmp2.ui;            \n\n  Stmp1.f = Sch.f*Sch.f;              \n  Stmp2.f = Ssh.f*Ssh.f;              \n  Stmp2.f = __fadd_rn(Stmp1.f, Stmp2.f);      \n  Stmp1.f = __frsqrt_rn(Stmp2.f);          \n\n  Stmp4.f = Stmp1.f*0.5f;              \n  Stmp3.f = Stmp1.f*Stmp4.f;            \n  Stmp3.f = Stmp1.f*Stmp3.f;            \n  Stmp3.f = Stmp2.f*Stmp3.f;            \n  Stmp1.f = __fadd_rn(Stmp1.f, Stmp4.f);      \n  Stmp1.f = __fsub_rn(Stmp1.f, Stmp3.f);      \n\n  Sch.f = Sch.f*Stmp1.f;              \n  Ssh.f = Ssh.f*Stmp1.f;              \n\n  Sc.f = Sch.f*Sch.f;                \n  Ss.f = Ssh.f*Ssh.f;                \n  Sc.f = __fsub_rn(Sc.f, Ss.f);          \n  Ss.f = Ssh.f*Sch.f;                \n  Ss.f = __fadd_rn(Ss.f, Ss.f);          \n\n  //###########################################################\n  // Rotate matrix A\n  //###########################################################\n\n  Stmp1.f = Ss.f*Sa11.f;                  \n  Stmp2.f = Ss.f*Sa21.f;                  \n  Sa11.f = Sc.f*Sa11.f;                  \n  Sa21.f = Sc.f*Sa21.f;                  \n  Sa11.f = __fadd_rn(Sa11.f, Stmp2.f);          \n  Sa21.f = __fsub_rn(Sa21.f, Stmp1.f);          \n\n  Stmp1.f = Ss.f*Sa12.f;                  \n  Stmp2.f = Ss.f*Sa22.f;                  \n  Sa12.f = Sc.f*Sa12.f;                  \n  Sa22.f = Sc.f*Sa22.f;                  \n  Sa12.f = __fadd_rn(Sa12.f, Stmp2.f);          \n  Sa22.f = __fsub_rn(Sa22.f, Stmp1.f);          \n\n  Stmp1.f = Ss.f*Sa13.f;                  \n  Stmp2.f = Ss.f*Sa23.f;                  \n  Sa13.f = Sc.f*Sa13.f;                  \n  Sa23.f = Sc.f*Sa23.f;                  \n  Sa13.f = __fadd_rn(Sa13.f, Stmp2.f);          \n  Sa23.f = __fsub_rn(Sa23.f, Stmp1.f);          \n\n  //###########################################################\n  // Update matrix U\n  //###########################################################\n\n  Stmp1.f = Ss.f*Su11.f;\n  Stmp2.f = Ss.f*Su12.f;\n  Su11.f = Sc.f*Su11.f;\n  Su12.f = Sc.f*Su12.f;\n  Su11.f = __fadd_rn(Su11.f, Stmp2.f);\n  Su12.f = __fsub_rn(Su12.f, Stmp1.f);\n\n  Stmp1.f = Ss.f*Su21.f;\n  Stmp2.f = Ss.f*Su22.f;\n  Su21.f = Sc.f*Su21.f;\n  Su22.f = Sc.f*Su22.f;\n  Su21.f = __fadd_rn(Su21.f, Stmp2.f);\n  Su22.f = __fsub_rn(Su22.f, Stmp1.f);\n\n  Stmp1.f = Ss.f*Su31.f;                \n  Stmp2.f = Ss.f*Su32.f;                \n  Su31.f = Sc.f*Su31.f;\n  Su32.f = Sc.f*Su32.f;\n  Su31.f = __fadd_rn(Su31.f, Stmp2.f);\n  Su32.f = __fsub_rn(Su32.f, Stmp1.f);\n\n  // Second Givens rotation\n\n  Ssh.f = Sa31.f*Sa31.f;                \n  Ssh.ui = (Ssh.f >= gsmall_number) ? 0xffffffff : 0;  \n  Ssh.ui = Ssh.ui&Sa31.ui;              \n\n  Stmp5.f = 0.f;                    \n  Sch.f = __fsub_rn(Stmp5.f, Sa11.f);          \n  Sch.f = fmaxf(Sch.f, Sa11.f);              \n  Sch.f = fmaxf(Sch.f, gsmall_number);          \n  Stmp5.ui = (Sa11.f >= Stmp5.f) ? 0xffffffff : 0;  \n\n  Stmp1.f = Sch.f*Sch.f;                \n  Stmp2.f = Ssh.f*Ssh.f;                \n  Stmp2.f = __fadd_rn(Stmp1.f, Stmp2.f);        \n  Stmp1.f = __frsqrt_rn(Stmp2.f);            \n\n  Stmp4.f = Stmp1.f*0.5;              \n  Stmp3.f = Stmp1.f*Stmp4.f;            \n  Stmp3.f = Stmp1.f*Stmp3.f;            \n  Stmp3.f = Stmp2.f*Stmp3.f;            \n  Stmp1.f = __fadd_rn(Stmp1.f, Stmp4.f);      \n  Stmp1.f = __fsub_rn(Stmp1.f, Stmp3.f);      \n  Stmp1.f = Stmp1.f*Stmp2.f;            \n\n  Sch.f = __fadd_rn(Sch.f, Stmp1.f);        \n\n  Stmp1.ui = ~Stmp5.ui&Ssh.ui;          \n  Stmp2.ui = ~Stmp5.ui&Sch.ui;          \n  Sch.ui = Stmp5.ui&Sch.ui;            \n  Ssh.ui = Stmp5.ui&Ssh.ui;            \n  Sch.ui = Sch.ui | Stmp1.ui;            \n  Ssh.ui = Ssh.ui | Stmp2.ui;            \n\n  Stmp1.f = Sch.f*Sch.f;              \n  Stmp2.f = Ssh.f*Ssh.f;              \n  Stmp2.f = __fadd_rn(Stmp1.f, Stmp2.f);      \n  Stmp1.f = __frsqrt_rn(Stmp2.f);          \n\n  Stmp4.f = Stmp1.f*0.5f;                  \n  Stmp3.f = Stmp1.f*Stmp4.f;                \n  Stmp3.f = Stmp1.f*Stmp3.f;                \n  Stmp3.f = Stmp2.f*Stmp3.f;                \n  Stmp1.f = __fadd_rn(Stmp1.f, Stmp4.f);          \n  Stmp1.f = __fsub_rn(Stmp1.f, Stmp3.f);          \n\n  Sch.f = Sch.f*Stmp1.f;                  \n  Ssh.f = Ssh.f*Stmp1.f;                  \n\n  Sc.f = Sch.f*Sch.f;                    \n  Ss.f = Ssh.f*Ssh.f;                    \n  Sc.f = __fsub_rn(Sc.f, Ss.f);              \n  Ss.f = Ssh.f*Sch.f;                    \n  Ss.f = __fadd_rn(Ss.f, Ss.f);              \n\n  //###########################################################\n  // Rotate matrix A\n  //###########################################################\n\n  Stmp1.f = Ss.f*Sa11.f;                  \n  Stmp2.f = Ss.f*Sa31.f;                  \n  Sa11.f = Sc.f*Sa11.f;                  \n  Sa31.f = Sc.f*Sa31.f;                  \n  Sa11.f = __fadd_rn(Sa11.f, Stmp2.f);          \n  Sa31.f = __fsub_rn(Sa31.f, Stmp1.f);          \n\n  Stmp1.f = Ss.f*Sa12.f;                  \n  Stmp2.f = Ss.f*Sa32.f;                  \n  Sa12.f = Sc.f*Sa12.f;                  \n  Sa32.f = Sc.f*Sa32.f;                  \n  Sa12.f = __fadd_rn(Sa12.f, Stmp2.f);          \n  Sa32.f = __fsub_rn(Sa32.f, Stmp1.f);          \n\n  Stmp1.f = Ss.f*Sa13.f;                  \n  Stmp2.f = Ss.f*Sa33.f;                  \n  Sa13.f = Sc.f*Sa13.f;                  \n  Sa33.f = Sc.f*Sa33.f;                  \n  Sa13.f = __fadd_rn(Sa13.f, Stmp2.f);          \n  Sa33.f = __fsub_rn(Sa33.f, Stmp1.f);          \n\n  //###########################################################\n  // Update matrix U\n  //###########################################################\n\n  Stmp1.f = Ss.f*Su11.f;\n  Stmp2.f = Ss.f*Su13.f;\n  Su11.f = Sc.f*Su11.f;\n  Su13.f = Sc.f*Su13.f;\n  Su11.f = __fadd_rn(Su11.f, Stmp2.f);\n  Su13.f = __fsub_rn(Su13.f, Stmp1.f);\n\n  Stmp1.f = Ss.f*Su21.f;\n  Stmp2.f = Ss.f*Su23.f;\n  Su21.f = Sc.f*Su21.f;\n  Su23.f = Sc.f*Su23.f;\n  Su21.f = __fadd_rn(Su21.f, Stmp2.f);\n  Su23.f = __fsub_rn(Su23.f, Stmp1.f);\n\n  Stmp1.f = Ss.f*Su31.f;\n  Stmp2.f = Ss.f*Su33.f;\n  Su31.f = Sc.f*Su31.f;\n  Su33.f = Sc.f*Su33.f;\n  Su31.f = __fadd_rn(Su31.f, Stmp2.f);\n  Su33.f = __fsub_rn(Su33.f, Stmp1.f);\n\n  // Third Givens Rotation\n\n  Ssh.f = Sa32.f*Sa32.f;                \n  Ssh.ui = (Ssh.f >= gsmall_number) ? 0xffffffff : 0;  \n  Ssh.ui = Ssh.ui&Sa32.ui;              \n\n  Stmp5.f = 0.f;                    \n  Sch.f = __fsub_rn(Stmp5.f, Sa22.f);          \n  Sch.f = fmaxf(Sch.f, Sa22.f);              \n  Sch.f = fmaxf(Sch.f, gsmall_number);          \n  Stmp5.ui = (Sa22.f >= Stmp5.f) ? 0xffffffff : 0;  \n\n  Stmp1.f = Sch.f*Sch.f;                \n  Stmp2.f = Ssh.f*Ssh.f;                \n  Stmp2.f = __fadd_rn(Stmp1.f, Stmp2.f);        \n  Stmp1.f = __frsqrt_rn(Stmp2.f);            \n\n  Stmp4.f = Stmp1.f*0.5f;              \n  Stmp3.f = Stmp1.f*Stmp4.f;            \n  Stmp3.f = Stmp1.f*Stmp3.f;            \n  Stmp3.f = Stmp2.f*Stmp3.f;            \n  Stmp1.f = __fadd_rn(Stmp1.f, Stmp4.f);      \n  Stmp1.f = __fsub_rn(Stmp1.f, Stmp3.f);      \n  Stmp1.f = Stmp1.f*Stmp2.f;            \n\n  Sch.f = __fadd_rn(Sch.f, Stmp1.f);        \n\n  Stmp1.ui = ~Stmp5.ui&Ssh.ui;          \n  Stmp2.ui = ~Stmp5.ui&Sch.ui;          \n  Sch.ui = Stmp5.ui&Sch.ui;            \n  Ssh.ui = Stmp5.ui&Ssh.ui;            \n  Sch.ui = Sch.ui | Stmp1.ui;            \n  Ssh.ui = Ssh.ui | Stmp2.ui;            \n\n  Stmp1.f = Sch.f*Sch.f;              \n  Stmp2.f = Ssh.f*Ssh.f;              \n  Stmp2.f = __fadd_rn(Stmp1.f, Stmp2.f);      \n  Stmp1.f = __frsqrt_rn(Stmp2.f);          \n\n  Stmp4.f = Stmp1.f*0.5f;              \n  Stmp3.f = Stmp1.f*Stmp4.f;            \n  Stmp3.f = Stmp1.f*Stmp3.f;            \n  Stmp3.f = Stmp2.f*Stmp3.f;            \n  Stmp1.f = __fadd_rn(Stmp1.f, Stmp4.f);      \n  Stmp1.f = __fsub_rn(Stmp1.f, Stmp3.f);      \n\n  Sch.f = Sch.f*Stmp1.f;              \n  Ssh.f = Ssh.f*Stmp1.f;              \n\n  Sc.f = Sch.f*Sch.f;                \n  Ss.f = Ssh.f*Ssh.f;                \n  Sc.f = __fsub_rn(Sc.f, Ss.f);          \n  Ss.f = Ssh.f*Sch.f;                \n  Ss.f = __fadd_rn(Ss.f, Ss.f);          \n\n  //###########################################################\n  // Rotate matrix A\n  //###########################################################\n\n  Stmp1.f = Ss.f*Sa21.f;                  \n  Stmp2.f = Ss.f*Sa31.f;                  \n  Sa21.f = Sc.f*Sa21.f;                  \n  Sa31.f = Sc.f*Sa31.f;                  \n  Sa21.f = __fadd_rn(Sa21.f, Stmp2.f);          \n  Sa31.f = __fsub_rn(Sa31.f, Stmp1.f);          \n\n  Stmp1.f = Ss.f*Sa22.f;                  \n  Stmp2.f = Ss.f*Sa32.f;                  \n  Sa22.f = Sc.f*Sa22.f;                  \n  Sa32.f = Sc.f*Sa32.f;                  \n  Sa22.f = __fadd_rn(Sa22.f, Stmp2.f);          \n  Sa32.f = __fsub_rn(Sa32.f, Stmp1.f);          \n\n  Stmp1.f = Ss.f*Sa23.f;                  \n  Stmp2.f = Ss.f*Sa33.f;                  \n  Sa23.f = Sc.f*Sa23.f;                  \n  Sa33.f = Sc.f*Sa33.f;                  \n  Sa23.f = __fadd_rn(Sa23.f, Stmp2.f);          \n  Sa33.f = __fsub_rn(Sa33.f, Stmp1.f);          \n\n  //###########################################################\n  // Update matrix U\n  //###########################################################\n\n  Stmp1.f = Ss.f*Su12.f;\n  Stmp2.f = Ss.f*Su13.f;\n  Su12.f = Sc.f*Su12.f;                  \n  Su13.f = Sc.f*Su13.f;                  \n  Su12.f = __fadd_rn(Su12.f, Stmp2.f);\n  Su13.f = __fsub_rn(Su13.f, Stmp1.f);\n\n  Stmp1.f = Ss.f*Su22.f;\n  Stmp2.f = Ss.f*Su23.f;\n  Su22.f = Sc.f*Su22.f;\n  Su23.f = Sc.f*Su23.f;\n  Su22.f = __fadd_rn(Su22.f, Stmp2.f);\n  Su23.f = __fsub_rn(Su23.f, Stmp1.f);\n\n  Stmp1.f = Ss.f*Su32.f;\n  Stmp2.f = Ss.f*Su33.f;\n  Su32.f = Sc.f*Su32.f;\n  Su33.f = Sc.f*Su33.f;\n  Su32.f = __fadd_rn(Su32.f, Stmp2.f);          \n  Su33.f = __fsub_rn(Su33.f, Stmp1.f);          \n\n  v11 = Sv11.f; v12 = Sv12.f; v13 = Sv13.f;\n  v21 = Sv21.f; v22 = Sv22.f; v23 = Sv23.f;\n  v31 = Sv31.f; v32 = Sv32.f; v33 = Sv33.f;\n\n  u11 = Su11.f; u12 = Su12.f; u13 = Su13.f;\n  u21 = Su21.f; u22 = Su22.f; u23 = Su23.f;\n  u31 = Su31.f; u32 = Su32.f; u33 = Su33.f;\n\n  s11 = Sa11.f; \n  //s12 = Sa12.f; s13 = Sa13.f; s21 = Sa21.f; \n  s22 = Sa22.f; \n  //s23 = Sa23.f; s31 = Sa31.f; s32 = Sa32.f; \n  s33 = Sa33.f;\n}\n\n__global__ void svd3_SOA(const float*__restrict__ input,\n                               float*__restrict__ output,\n                         const int testsize)\n{\n  int tid = blockDim.x * blockIdx.x + threadIdx.x;\n  if (tid >= testsize) return;\n\n  svd(\n      input[tid + 0 * testsize], input[tid + 1 * testsize], input[tid + 2 * testsize],\n      input[tid + 3 * testsize], input[tid + 4 * testsize], input[tid + 5 * testsize],\n      input[tid + 6 * testsize], input[tid + 7 * testsize], input[tid + 8 * testsize],\n\n      output[tid + 0 * testsize], output[tid + 1 * testsize], output[tid + 2 * testsize],\n      output[tid + 3 * testsize], output[tid + 4 * testsize], output[tid + 5 * testsize],\n      output[tid + 6 * testsize], output[tid + 7 * testsize], output[tid + 8 * testsize],\n\n      output[tid + 9 * testsize], output[tid + 10 * testsize], output[tid + 11 * testsize],\n\n      output[tid + 12 * testsize], output[tid + 13 * testsize], output[tid + 14 * testsize],\n      output[tid + 15 * testsize], output[tid + 16 * testsize], output[tid + 17 * testsize],\n      output[tid + 18 * testsize], output[tid + 19 * testsize], output[tid + 20 * testsize]\n     );\n}"
        ]
    },
    "extrema-cuda": {
        "/Users/gbolet/hecbench-roofline/src/extrema-cuda/main.cu": [
            "#define T ((int)32)\n\n\n__host__ __device__ __forceinline__\nvoid clip_minus( const bool &clip, const int &n, int &minus ) {\n  if ( clip ) {\n    if ( minus < 0 ) {\n      minus = 0;\n    }\n  } else {\n    if ( minus < 0 ) {\n      minus += n;\n    }\n  }\n}\n\n__host__ __device__ __forceinline__\nvoid clip_plus( const bool &clip, const int &n, int &plus ) {\n  if ( clip ) {\n    if ( plus >= n ) {\n      plus = n - 1;\n    }\n  } else {\n    if ( plus >= n ) {\n      plus -= n;\n    }\n  }\n}\n\n__global__ void relextrema_1D(\n  const int  n,\n  const int  order,\n  const bool clip,\n  const T *__restrict__ inp,\n  bool *__restrict__ results)\n{\n  const int tx = blockIdx.x * blockDim.x + threadIdx.x;\n  const int stride = blockDim.x * gridDim.x;\n\n  for ( int tid = tx; tid < n; tid += stride ) {\n\n    const T data = inp[tid];\n    bool    temp = true;\n\n    for ( int o = 1; o < ( order + 1 ); o++ ) {\n      int plus = tid + o;\n      int minus = tid - o;\n\n      clip_plus( clip, n, plus );\n      clip_minus( clip, n, minus );\n\n      temp &= data > inp[plus];\n      temp &= data >= inp[minus];\n    }\n    results[tid] = temp;\n  }\n}",
            "#define T ((int)32)\n\n\n__host__ __device__ __forceinline__\nvoid clip_minus( const bool &clip, const int &n, int &minus ) {\n  if ( clip ) {\n    if ( minus < 0 ) {\n      minus = 0;\n    }\n  } else {\n    if ( minus < 0 ) {\n      minus += n;\n    }\n  }\n}\n\n__host__ __device__ __forceinline__\nvoid clip_plus( const bool &clip, const int &n, int &plus ) {\n  if ( clip ) {\n    if ( plus >= n ) {\n      plus = n - 1;\n    }\n  } else {\n    if ( plus >= n ) {\n      plus -= n;\n    }\n  }\n}\n\n__global__ void relextrema_2D(\n  const int  in_x,\n  const int  in_y,\n  const int  order,\n  const bool clip,\n  const int  axis,\n  const T *__restrict__ inp,\n  bool *__restrict__ results) \n{\n  const int ty = blockIdx.x * blockDim.x + threadIdx.x;\n  const int tx = blockIdx.y * blockDim.y + threadIdx.y;\n\n  if ( ( tx < in_y ) && ( ty < in_x ) ) {\n    int tid = tx * in_x + ty ;\n\n    const T data = inp[tid] ;\n    bool    temp = true ;\n\n    for ( int o = 1; o < ( order + 1 ); o++ ) {\n\n      int plus;\n      int minus;\n\n      if ( axis == 0 ) {\n        plus  = tx + o;\n        minus = tx - o;\n\n        clip_plus( clip, in_y, plus );\n        clip_minus( clip, in_y, minus );\n\n        plus  = plus * in_x + ty;\n        minus = minus * in_x + ty;\n      } else {\n        plus  = ty + o;\n        minus = ty - o;\n\n        clip_plus( clip, in_x, plus );\n        clip_minus( clip, in_x, minus );\n\n        plus  = tx * in_x + plus;\n        minus = tx * in_x + minus;\n      }\n\n      temp &= data > inp[plus] ;\n      temp &= data >= inp[minus] ;\n    }\n    results[tid] = temp;\n  }\n}"
        ]
    },
    "concurrentKernels-cuda": {
        "/Users/gbolet/hecbench-roofline/src/concurrentKernels-cuda/main.cu": [
            "__global__ void clock_block(long *d_o, long clock_count) {\n  long clock_offset = 0;\n  for (int i = 0; i < clock_count; i++)\n    clock_offset += i % 3;\n  d_o[0] = clock_offset;\n}"
        ]
    },
    "threadfence-cuda": {
        "/Users/gbolet/hecbench-roofline/src/threadfence-cuda/main.cu": [
            "__global__ void sum (\n    const float*__restrict__ array,\n    const int N,\n    unsigned int *__restrict__ count,\n    volatile float*__restrict__ result)\n{\n  __shared__ bool isLastBlockDone;\n  __shared__ float partialSum;\n\n  // Each block sums a subset of the input array.\n  unsigned int bid = blockIdx.x;\n  unsigned int num_blocks = gridDim.x;\n  unsigned int block_size = blockDim.x;\n  unsigned int lid = threadIdx.x;\n  unsigned int gid = bid * block_size + lid;\n\n  if (lid == 0) partialSum = 0;\n  __syncthreads();\n\n  if (gid < N)\n    atomicAdd(&partialSum, array[gid]);\n\n  __syncthreads();\n\n  if (lid == 0) {\n\n    // Thread 0 of each block stores the partial sum\n    // to global memory. The compiler will use \n    // a store operation that bypasses the L1 cache\n    // since the \"result\" variable is declared as\n    // volatile. This ensures that the threads of\n    // the last block will read the correct partial\n    // sums computed by all other blocks.\n    result[bid] = partialSum;\n\n    // Thread 0 makes sure that the incrementation\n    // of the \"count\" variable is only performed after\n    // the partial sum has been written to global memory.\n    __threadfence();\n\n    // Thread 0 signals that it is done.\n    unsigned int value = atomicAdd(count, 1);\n\n    // Thread 0 determines if its block is the last\n    // block to be done.\n    isLastBlockDone = (value == (num_blocks - 1));\n  }\n\n  // Synchronize to make sure that each thread reads\n  // the correct value of isLastBlockDone.\n  __syncthreads();\n\n  if (isLastBlockDone) {\n\n    // The last block sums the partial sums\n    // stored in result[0 .. num_blocks-1]\n    if (lid == 0) partialSum = 0;\n    __syncthreads();\n\n    for (int i = lid; i < num_blocks; i += block_size)\n      atomicAdd(&partialSum, result[i]);\n\n    __syncthreads();\n\n    if (lid == 0) {\n      // Thread 0 of last block stores the total sum\n      // to global memory and resets the count\n      // varialble, so that the next kernel call\n      // works properly.\n      result[0] = partialSum;\n      *count = 0;\n    }\n  }\n}"
        ]
    },
    "triad-cuda": {
        "/Users/gbolet/hecbench-roofline/src/triad-cuda/triad.cu": [
            "__global__ void triad(const float*__restrict__ A,\n                      const float*__restrict__ B,\n                            float*__restrict__ C,\n                            float s)\n{\n  int gid = threadIdx.x + (blockIdx.x * blockDim.x);\n  C[gid] = A[gid] + s*B[gid];\n}"
        ]
    },
    "goulash-cuda": {
        "/Users/gbolet/hecbench-roofline/src/goulash-cuda/main.cu": [
            "__global__ \nvoid gate(double* __restrict__ m_gate, const long nCells, const double* __restrict__ Vm) \n{\n  long ii = blockIdx.x*blockDim.x + threadIdx.x;\n  if (ii >= nCells) return;\n\n  double sum1,sum2;\n  const double x = Vm[ii];\n  const int Mhu_l = 10;\n  const int Mhu_m = 5;\n  const double Mhu_a[] = { 9.9632117206253790e-01,  4.0825738726469545e-02,  6.3401613233199589e-04,  4.4158436861700431e-06,  1.1622058324043520e-08,  1.0000000000000000e+00,  4.0568375699663400e-02,  6.4216825832642788e-04,  4.2661664422410096e-06,  1.3559930396321903e-08, -1.3573468728873069e-11, -4.2594802366702580e-13,  7.6779952208246166e-15,  1.4260675804433780e-16, -2.6656212072499249e-18};\n\n  sum1 = 0;\n  for (int j = Mhu_m-1; j >= 0; j--)\n    sum1 = Mhu_a[j] + x*sum1;\n  sum2 = 0;\n  int k = Mhu_m + Mhu_l - 1;\n  for (int j = k; j >= Mhu_m; j--)\n    sum2 = Mhu_a[j] + x * sum2;\n  double mhu = sum1/sum2;\n\n  const int Tau_m = 18;\n  const double Tau_a[] = {1.7765862602413648e+01*0.02,  5.0010202770602419e-02*0.02, -7.8002064070783474e-04*0.02, -6.9399661775931530e-05*0.02,  1.6936588308244311e-06*0.02,  5.4629017090963798e-07*0.02, -1.3805420990037933e-08*0.02, -8.0678945216155694e-10*0.02,  1.6209833004622630e-11*0.02,  6.5130101230170358e-13*0.02, -6.9931705949674988e-15*0.02, -3.1161210504114690e-16*0.02,  5.0166191902609083e-19*0.02,  7.8608831661430381e-20*0.02,  4.3936315597226053e-22*0.02, -7.0535966258003289e-24*0.02, -9.0473475495087118e-26*0.02, -2.9878427692323621e-28*0.02,  1.0000000000000000e+00};\n\n  sum1 = 0;\n  for (int j = Tau_m-1; j >= 0; j--)\n    sum1 = Tau_a[j] + x*sum1;\n  double tauR = sum1;\n  m_gate[ii] += (mhu - m_gate[ii])*(1-exp(-tauR));\n}"
        ]
    },
    "pcc-cuda": {
        "/Users/gbolet/hecbench-roofline/src/pcc-cuda/device.cu": [
            "__global__ void ker2(const float * cormat, float * upper, int n1, int n)\n{\n  size_t idx = blockDim.x*blockIdx.x+threadIdx.x;\n  if (idx < (size_t)n1 * n) {\n    size_t i = idx/n;\n    size_t j = idx%n;\n    if(i<j && i<n1)\n    {\n      size_t t = n * i - i * (i+1) / 2 + j - i - 1;\n      upper[t]=cormat[j*n1+i];\n    }\n  }\n}"
        ]
    },
    "minibude-cuda": {
        "/Users/gbolet/hecbench-roofline/src/minibude-cuda/kernel.cu": [
            "inline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\ninline __host__ __device__ float4 fabs(float4 v)\n{\n    return make_float4(fabs(v.x), fabs(v.y), fabs(v.z), fabs(v.w));\n}\n\n__global__ void fasten_main(\n    //    size_t posesPerWI,\n    const size_t ntypes,\n    const size_t nposes,\n    const size_t natlig,\n    const size_t natpro,\n    const Atom *__restrict protein_molecule,\n    const Atom *__restrict ligand_molecule,\n    const float *__restrict transforms_0,\n    const float *__restrict transforms_1,\n    const float *__restrict transforms_2,\n    const float *__restrict transforms_3,\n    const float *__restrict transforms_4,\n    const float *__restrict transforms_5,\n    const FFParams *__restrict forcefield,\n    float *__restrict etotals) \n{\n\n  extern __shared__ FFParams local_forcefield[];\n  const size_t lid = threadIdx.x;\n  const size_t gid = blockIdx.x;\n  const size_t lrange = blockDim.x;\n\n  float etot[NUM_TD_PER_THREAD];\n  float3 lpos[NUM_TD_PER_THREAD];\n  float4 transform[NUM_TD_PER_THREAD][3];\n\n  size_t ix = gid * lrange * NUM_TD_PER_THREAD + lid;\n  ix = ix < nposes ? ix : nposes - NUM_TD_PER_THREAD;\n\n  for (int i = lid; i < ntypes; i += lrange) local_forcefield[i] = forcefield[i];\n\n  // Compute transformation matrix to private memory\n  for (size_t i = 0; i < NUM_TD_PER_THREAD; i++) {\n    size_t index = ix + i * lrange;\n\n    const float sx = sin(transforms_0[index]);\n    const float cx = cos(transforms_0[index]);\n    const float sy = sin(transforms_1[index]);\n    const float cy = cos(transforms_1[index]);\n    const float sz = sin(transforms_2[index]);\n    const float cz = cos(transforms_2[index]);\n\n    transform[i][0].x = cy * cz;\n    transform[i][0].y = sx * sy * cz - cx * sz;\n    transform[i][0].z = cx * sy * cz + sx * sz;\n    transform[i][0].w = transforms_3[index];\n    transform[i][1].x = cy * sz;\n    transform[i][1].y = sx * sy * sz + cx * cz;\n    transform[i][1].z = cx * sy * sz - sx * cz;\n    transform[i][1].w = transforms_4[index];\n    transform[i][2].x = -sy;\n    transform[i][2].y = sx * cy;\n    transform[i][2].z = cx * cy;\n    transform[i][2].w = transforms_5[index];\n\n    etot[i] = ZERO;\n  }\n\n  __syncthreads();\n\n  // Loop over ligand atoms\n  size_t il = 0;\n  do {\n    // Load ligand atom data\n    const Atom l_atom = ligand_molecule[il];\n    const FFParams l_params = local_forcefield[l_atom.type];\n    const bool lhphb_ltz = l_params.hphb < ZERO;\n    const bool lhphb_gtz = l_params.hphb > ZERO;\n\n    const float4 linitpos = make_float4(l_atom.x, l_atom.y, l_atom.z, ONE);\n    for (size_t i = 0; i < NUM_TD_PER_THREAD; i++) {\n      // Transform ligand atom\n      lpos[i].x = transform[i][0].w +\n        linitpos.x * transform[i][0].x +\n        linitpos.y * transform[i][0].y +\n        linitpos.z * transform[i][0].z;\n      lpos[i].y = transform[i][1].w +\n        linitpos.x * transform[i][1].x +\n        linitpos.y * transform[i][1].y +\n        linitpos.z * transform[i][1].z;\n      lpos[i].z = transform[i][2].w +\n        linitpos.x * transform[i][2].x +\n        linitpos.y * transform[i][2].y +\n        linitpos.z * transform[i][2].z;\n    }\n\n    // Loop over protein atoms\n    size_t ip = 0;\n    do {\n      // Load protein atom data\n      const Atom p_atom = protein_molecule[ip];\n      const FFParams p_params = local_forcefield[p_atom.type];\n\n      const float radij = p_params.radius + l_params.radius;\n      const float r_radij = 1.f / (radij);\n\n      const float elcdst = (p_params.hbtype == HBTYPE_F && l_params.hbtype == HBTYPE_F) ? FOUR : TWO;\n      const float elcdst1 = (p_params.hbtype == HBTYPE_F && l_params.hbtype == HBTYPE_F) ? QUARTER : HALF;\n      const bool type_E = ((p_params.hbtype == HBTYPE_E || l_params.hbtype == HBTYPE_E));\n\n      const bool phphb_ltz = p_params.hphb < ZERO;\n      const bool phphb_gtz = p_params.hphb > ZERO;\n      const bool phphb_nz = p_params.hphb != ZERO;\n      const float p_hphb = p_params.hphb * (phphb_ltz && lhphb_gtz ? -ONE : ONE);\n      const float l_hphb = l_params.hphb * (phphb_gtz && lhphb_ltz ? -ONE : ONE);\n      const float distdslv = (phphb_ltz ? (lhphb_ltz ? NPNPDIST : NPPDIST) : (lhphb_ltz ? NPPDIST : -FLT_MAX));\n      const float r_distdslv = 1.f / (distdslv);\n\n      const float chrg_init = l_params.elsc * p_params.elsc;\n      const float dslv_init = p_hphb + l_hphb;\n\n      for (size_t i = 0; i < NUM_TD_PER_THREAD; i++) {\n        // Calculate distance between atoms\n        const float x = lpos[i].x - p_atom.x;\n        const float y = lpos[i].y - p_atom.y;\n        const float z = lpos[i].z - p_atom.z;\n\n        const float distij = sqrt(x * x + y * y + z * z);\n\n        // Calculate the sum of the sphere radii\n        const float distbb = distij - radij;\n        const bool zone1 = (distbb < ZERO);\n\n        // Calculate steric energy\n        etot[i] += (ONE - (distij * r_radij)) * (zone1 ? 2 * HARDNESS : ZERO);\n\n        // Calculate formal and dipole charge interactions\n        float chrg_e = chrg_init * ((zone1 ? 1 : (ONE - distbb * elcdst1)) * (distbb < elcdst ? 1 : ZERO));\n        const float neg_chrg_e = -fabs(chrg_e);\n        chrg_e = type_E ? neg_chrg_e : chrg_e;\n        etot[i] += chrg_e * CNSTNT;\n\n        // Calculate the two cases for Nonpolar-Polar repulsive interactions\n        const float coeff = (ONE - (distbb * r_distdslv));\n        float dslv_e = dslv_init * ((distbb < distdslv && phphb_nz) ? 1 : ZERO);\n        dslv_e *= (zone1 ? 1 : coeff);\n        etot[i] += dslv_e;\n      }\n    } while (++ip < natpro); // loop over protein atoms\n  } while (++il < natlig); // loop over ligand atoms\n\n  // Write results\n  const size_t td_base = gid * lrange * NUM_TD_PER_THREAD + lid;\n\n  if (td_base < nposes) {\n    for (size_t i = 0; i < NUM_TD_PER_THREAD; i++) {\n      etotals[td_base + i * lrange] = etot[i] * HALF;\n    }\n  }\n}"
        ]
    },
    "stsg-cuda": {
        "/Users/gbolet/hecbench-roofline/src/stsg-cuda/Filter.cu": [
            "__global__ void Short_to_Float(const short *imgNDVI, const unsigned char *imgQA,\n                               int n_X, int n_Y, int n_B, int n_Years,\n                               float *__restrict__ img_NDVI,\n                               float *__restrict__ img_QA)\n{\n  int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i >= n_X)\n    return;\n  int j = blockIdx.y * blockDim.y + threadIdx.y;\n  if (j >= n_Y)\n    return;\n\n  for (int k = 0; k < n_B; k++)\n  {\n    for (int y = 0; y < n_Years; y++)\n    {\n      int idx = i + j*n_X + k*n_X*n_Y + y*n_X*n_Y*n_B;\n      img_NDVI[idx] = float(imgNDVI[idx]) / 10000.f;\n        img_QA[idx] = float(  imgQA[idx]);\n\n      if (img_NDVI[idx] < -0.2f || img_NDVI[idx] > 1.f ||\n            img_QA[idx] < -1.f || img_QA[idx] > 3.f)\n        img_QA[idx] = -1.f;\n    }\n  }\n}",
            "__global__ void Generate_NDVI_reference(float cosyear, int win_NDVI,\n                                        const float *__restrict__ img_NDVI,\n                                        const float *__restrict__ img_QA,\n                                        int n_X, int n_Y, int n_B, int n_Years, \n                                        float *__restrict__ reference_data,\n                                        float *__restrict__ d_res_3,\n                                        int *__restrict__ d_res_vec_res1)\n{\n  int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i >= n_X)\n    return;\n  int j = blockIdx.y * blockDim.y + threadIdx.y;\n  if (j >= n_Y)\n    return;\n\n  //calculating cosine similarity\n  float* res_cosyear = &d_res_3[(i + j * n_X) * (n_Years + 1) * n_Years];\n  for (int y_1 = 0; y_1 < n_Years; y_1++)\n  {\n    for (int y_2 = y_1 + 1; y_2 < n_Years; y_2++)\n    {\n      double xy_sum = 0;\n      double x2_sum = 0;\n      double y2_sum = 0;\n      for (int k = 0; k < n_B; k++)\n      {\n        int idx1 = i + j * n_X + k * n_X * n_Y + y_1 * n_X * n_Y * n_B; \n        int idx2 = i + j * n_X + k * n_X * n_Y + y_2 * n_X * n_Y * n_B; \n        if ((img_QA[idx1] == 0.f || img_QA[idx1] == 1.f) &&\n            (img_QA[idx2] == 0.f || img_QA[idx2] == 1.f))\n        {\n          xy_sum += img_NDVI[idx1] * img_NDVI[idx2];\n          x2_sum += img_NDVI[idx1] * img_NDVI[idx1];\n          y2_sum += img_NDVI[idx2] * img_NDVI[idx2];\n        }\n      }\n      if (x2_sum != 0 && y2_sum != 0)\n      {\n        res_cosyear[y_2 + y_1 * n_Years] = xy_sum / sqrt(x2_sum * y2_sum);\n        res_cosyear[y_1 + y_2 * n_Years] = xy_sum / sqrt(x2_sum * y2_sum);\n        res_cosyear[y_1 + y_1 * n_Years] += xy_sum / sqrt(x2_sum * y2_sum);\n        res_cosyear[y_2 + y_2 * n_Years] += xy_sum / sqrt(x2_sum * y2_sum);\n      }\n    }\n    res_cosyear[y_1 + y_1 * n_Years] = res_cosyear[y_1 + y_1 * n_Years] / (n_Years - 1);\n    res_cosyear[n_Years * n_Years] += res_cosyear[y_1 + y_1 * n_Years];\n  }\n  res_cosyear[n_Years * n_Years] = res_cosyear[n_Years * n_Years] / n_Years;\n\n  for (int y = 0; y < n_Years; y++)\n    res_cosyear[n_Years * n_Years + 1] += (res_cosyear[y + y * n_Years] - res_cosyear[n_Years * n_Years]) * \n                                          (res_cosyear[y + y * n_Years] - res_cosyear[n_Years * n_Years]);\n  res_cosyear[n_Years * n_Years + 1] = sqrt(res_cosyear[n_Years * n_Years + 1] / n_Years);\n  res_cosyear[n_Years * n_Years] = res_cosyear[n_Years * n_Years] - res_cosyear[n_Years * n_Years + 1];\n\n  //window\n  for (int y_1 = 0; y_1 < n_Years; y_1++)\n    res_cosyear[y_1] = res_cosyear[y_1 + y_1 * n_Years];\n  res_cosyear[n_Years] = res_cosyear[n_Years * n_Years];\n  for (int y = n_Years + 1; y < ((n_Years + 1) * n_Years); y++)\n    res_cosyear[y] = 0;\n\n  for (int k = 0; k < n_B; k++)\n  {\n    int count_img_QA = 0;\n    float mean_img_QA = 0;\n    for (int y = 0; y < n_Years; y++)\n    {\n      if (res_cosyear[y] >= res_cosyear[n_Years] || res_cosyear[y] >= cosyear)\n      {\n        int idx = i + j * n_X + k * n_X * n_Y + y * n_X * n_Y * n_B; \n        if (img_QA[idx] == 0.f || img_QA[idx] == 1.f)\n        {\n          count_img_QA++;\n          mean_img_QA += img_NDVI[idx];\n        }\n      }\n    }\n    if (count_img_QA >= 1)\n      reference_data[i + j * n_X + k * n_X * n_Y] = mean_img_QA / count_img_QA;\n  }\n\n  int n_dissimilar_year = 0;\n  for (int y_1 = 0; y_1 < n_Years; y_1++)\n  {\n    if (res_cosyear[y_1] < res_cosyear[n_Years] &&res_cosyear[y_1] < cosyear)\n    {\n      for (int k2 = -2; k2 <= 2; k2++)\n      {\n        if (k2 == 0)\n          continue;\n        double xy_sum = 0;\n        double x2_sum = 0;\n        double y2_sum = 0;\n        for (int k = 0; k < n_B; k++)\n        {\n          if ((k + k2) < 0 || k + k2 >= n_B)\n            continue;\n\n          int idx = i + j * n_X + (k + k2) * n_X * n_Y + y_1 * n_X * n_Y * n_B; \n          int ridx = i + j * n_X + k * n_X * n_Y; \n          if ((img_QA[idx] == 0.f || img_QA[idx] == 1.f)\n              && (reference_data[ridx] != 0.f))\n          {\n            xy_sum += img_NDVI[idx] * reference_data[ridx];\n            x2_sum += img_NDVI[idx] * img_NDVI[idx];\n            y2_sum += reference_data[ridx] * reference_data[ridx];\n          }\n        }\n        if (x2_sum != 0 && y2_sum != 0)\n        {\n          res_cosyear[n_Years + 1] = xy_sum / sqrt(x2_sum * y2_sum);\n          if (res_cosyear[n_Years + 1] > res_cosyear[y_1])\n          {\n            res_cosyear[y_1] = res_cosyear[n_Years + 1];\n          }\n        }\n      }\n      if (res_cosyear[y_1] >= res_cosyear[n_Years] || res_cosyear[y_1] >= cosyear)\n        continue;\n\n      for (int c_k = 0; c_k < n_B; c_k++)\n      {\n        int n_similar_year = 0;\n        for (int y_2 = 0; y_2 < n_Years; y_2++)\n        {\n          if (res_cosyear[y_2] >= res_cosyear[n_Years] || (res_cosyear[y_2] >= cosyear\n              && (img_QA[i + j * n_X + c_k * n_X * n_Y + y_1 * n_X * n_Y * n_B] == 0.f || img_QA[i + j * n_X + c_k * n_X * n_Y + y_1 * n_X * n_Y * n_B] == 1.f)\n              && (img_QA[i + j * n_X + c_k * n_X * n_Y + y_2 * n_X * n_Y * n_B] == 0.f || img_QA[i + j * n_X + c_k * n_X * n_Y + y_2 * n_X * n_Y * n_B] == 1.f)))\n          {\n            double xy_sum = 0;\n            double x2_sum = 0;\n            double y2_sum = 0;\n            for (int k = 1, n = 0; n < win_NDVI * 2 + 1&&((c_k + k)<n_B || (c_k - k) >= 0); k++)\n            {\n              if ((c_k - k) >= 0\n                  && (img_QA[i + j * n_X + (c_k - k) * n_X * n_Y + y_1 * n_X * n_Y * n_B] == 0.f || img_QA[i + j * n_X + (c_k - k) * n_X * n_Y + y_1 * n_X * n_Y * n_B] == 1.f)\n                  && (img_QA[i + j * n_X + (c_k - k) * n_X * n_Y + y_2 * n_X * n_Y * n_B] == 0.f || img_QA[i + j * n_X + (c_k - k) * n_X * n_Y + y_2 * n_X * n_Y * n_B] == 1.f))\n              {\n                xy_sum += img_NDVI[i + j * n_X + (c_k - k) * n_X * n_Y + y_1 * n_X * n_Y * n_B] * img_NDVI[i + j * n_X + (c_k - k) * n_X * n_Y + y_2 * n_X * n_Y * n_B];\n                x2_sum += img_NDVI[i + j * n_X + (c_k - k) * n_X * n_Y + y_1 * n_X * n_Y * n_B] * img_NDVI[i + j * n_X + (c_k - k) * n_X * n_Y + y_1 * n_X * n_Y * n_B];\n                y2_sum += img_NDVI[i + j * n_X + (c_k - k) * n_X * n_Y + y_2 * n_X * n_Y * n_B] * img_NDVI[i + j * n_X + (c_k - k) * n_X * n_Y + y_2 * n_X * n_Y * n_B];\n                n++;\n              }\n              if ((c_k + k) < n_B\n                  && (img_QA[i + j * n_X + (c_k + k) * n_X * n_Y + y_1 * n_X * n_Y * n_B] == 0.f || img_QA[i + j * n_X + (c_k + k) * n_X * n_Y + y_1 * n_X * n_Y * n_B] == 1.f)\n                  && (img_QA[i + j * n_X + (c_k + k) * n_X * n_Y + y_2 * n_X * n_Y * n_B] == 0.f || img_QA[i + j * n_X + (c_k + k) * n_X * n_Y + y_2 * n_X * n_Y * n_B] == 1.f))\n              {\n                xy_sum += img_NDVI[i + j * n_X + (c_k + k) * n_X * n_Y + y_1 * n_X * n_Y * n_B] * img_NDVI[i + j * n_X + (c_k + k) * n_X * n_Y + y_2 * n_X * n_Y * n_B];\n                x2_sum += img_NDVI[i + j * n_X + (c_k + k) * n_X * n_Y + y_1 * n_X * n_Y * n_B] * img_NDVI[i + j * n_X + (c_k + k) * n_X * n_Y + y_1 * n_X * n_Y * n_B];\n                y2_sum += img_NDVI[i + j * n_X + (c_k + k) * n_X * n_Y + y_2 * n_X * n_Y * n_B] * img_NDVI[i + j * n_X + (c_k + k) * n_X * n_Y + y_2 * n_X * n_Y * n_B];\n                n++;\n              }\n            }\n            if (x2_sum != 0 && y2_sum != 0)\n            {\n              res_cosyear[c_k + n_dissimilar_year * n_B + n_Years] += xy_sum / sqrt(x2_sum * y2_sum);\n              n_similar_year++;\n            }\n          }\n        }\n        if (n_similar_year != 0)\n          res_cosyear[c_k + n_dissimilar_year * n_B + n_Years+2] = res_cosyear[c_k + n_dissimilar_year * n_B + n_Years + 2] / n_similar_year;\n      }\n      n_dissimilar_year++;\n    }\n  }\n\n  int count_vec_res1 = 0;\n  int* res_vec_res1 = &d_res_vec_res1[(i + j * n_X) * n_B];\n  for (int k = 0; k < n_B; k++)\n  {\n    int count_img_QA = 0;\n    float mean_img_QA = 0;\n    n_dissimilar_year = 0;\n    for (int y = 0; y < n_Years; y++)\n    {\n      if (res_cosyear[y] >= res_cosyear[n_Years] ||res_cosyear[y] >= cosyear)\n      {\n        if (img_QA[i + j * n_X + k * n_X * n_Y + y * n_X * n_Y * n_B] == 0.f || \n            img_QA[i + j * n_X + k * n_X * n_Y + y * n_X * n_Y * n_B] == 1.f)\n        {\n          count_img_QA++;\n          mean_img_QA += img_NDVI[i + j * n_X + k * n_X * n_Y + y * n_X * n_Y * n_B];\n        }\n      }\n      else\n      {\n        if ((res_cosyear[k + n_dissimilar_year * n_B + n_Years + 2] >= res_cosyear[n_Years] ||\n             res_cosyear[k + n_dissimilar_year * n_B + n_Years + 2] >= cosyear) &&\n            (img_QA[i + j * n_X + k * n_X * n_Y + y * n_X * n_Y * n_B] == 0.f || \n            img_QA[i + j * n_X + k * n_X * n_Y + y * n_X * n_Y * n_B] == 1.f))\n        {\n          count_img_QA++;\n          mean_img_QA += img_NDVI[i + j * n_X + k * n_X * n_Y + y * n_X * n_Y * n_B];\n        }\n        n_dissimilar_year++;\n      }\n    }\n    if (count_img_QA >= 1)\n    {\n      reference_data[i + j*n_X + k*n_X*n_Y] = mean_img_QA / count_img_QA;\n      res_vec_res1[count_vec_res1++] = k;\n    }\n  }\n\n  if (count_vec_res1 < n_B && count_vec_res1 > 1)\n  {\n    double k;\n    int x = 0;\n    int l = 0;\n    if (res_vec_res1[count_vec_res1 - 1] != n_B - 1)\n    {\n      k = (reference_data[i + j*n_X + res_vec_res1[count_vec_res1 - 1] * n_X*n_Y] - reference_data[i + j*n_X + res_vec_res1[count_vec_res1 - 2] * n_X*n_Y]) / (res_vec_res1[count_vec_res1 - 1] - res_vec_res1[count_vec_res1 - 2]);\n      reference_data[i + j*n_X + (n_B - 1)*n_X*n_Y] = reference_data[i + j*n_X + (res_vec_res1[count_vec_res1 - 1])*n_X*n_Y] + k*(n_B - 1 - res_vec_res1[count_vec_res1 - 1]);\n      res_vec_res1[count_vec_res1++] = n_B - 1;\n    }\n    int count_res_vec_res1 = count_vec_res1;\n    while (count_vec_res1 < n_B&&x < count_res_vec_res1 - 1)\n    {\n      l = res_vec_res1[x + 1] - res_vec_res1[x];\n      int n = 1;\n      while (l > 1)\n      {\n        k = (reference_data[i + j*n_X + res_vec_res1[x + 1] * n_X*n_Y] - reference_data[i + j*n_X + res_vec_res1[x] * n_X*n_Y]) / (res_vec_res1[x + 1] - res_vec_res1[x]);\n        reference_data[i + j*n_X + (res_vec_res1[x] + n)*n_X*n_Y] = reference_data[i + j*n_X + res_vec_res1[x] * n_X*n_Y] + k*n;\n        count_vec_res1++;\n        n++;\n        l--;\n      }\n      x++;\n    }\n    if (res_vec_res1[0] != 0)\n    {\n      k = (reference_data[i + j*n_X + res_vec_res1[1] * n_X*n_Y] - reference_data[i + j*n_X + res_vec_res1[0] * n_X*n_Y]) / (res_vec_res1[1] - res_vec_res1[0]);\n      l = res_vec_res1[0];\n      int n = 0;\n      do\n      {\n        reference_data[i + j*n_X + n*n_X*n_Y] = reference_data[i + j*n_X + res_vec_res1[0] * n_X*n_Y] - k*l;\n        n++;\n        l--;\n      } while (l >= 1);\n    }\n  }\n}",
            "__global__ void Compute_d_res(const float *img_NDVI, const float*img_QA, const float *reference_data,\nint StartY, int TotalY, int Buffer_Up, int Buffer_Dn, int n_X, int n_Y, int n_B, int n_Years, int win, float *d_res)\n{\n  int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i >= n_X)\n    return;\n  int j = blockIdx.y * blockDim.y + threadIdx.y;\n  if (j >= n_Y - Buffer_Dn- Buffer_Up )\n    return;\n\n  float *corr_res = &d_res[(i + j*n_X)*(2 * win + 1)*(2 * win + 1) * 4];\n  float *Slope_res = &d_res[(i + j*n_X)*(2 * win + 1)*(2 * win + 1) * 4 + (2 * win + 1)*(2 * win + 1)];\n  float *Intercept_res = &d_res[(i + j*n_X)*(2 * win + 1)*(2 * win + 1) * 4 + (2 * win + 1)*(2 * win + 1) * 2];\n  float *new_corr_similar_res = &d_res[(i + j*n_X)*(2 * win + 1)*(2 * win + 1) * 4 + (2 * win + 1)*(2 * win + 1) * 3];\n  for (int sj = 0; sj <= win; sj++)\n  {\n    for (int si = -1 * win; si <= win; si++)\n    {\n      if (i + si >= 0 && i + si < n_X&&j + sj + StartY >= 0 && j + sj + StartY < TotalY)\n      {\n        double x_sum = 0;\n        double y_sum = 0;\n        double x_mean = 0;\n        double y_mean = 0;\n        double xy_sum = 0;\n        double x2_sum = 0;\n        double y2_sum = 0;\n        for (int k = 0; k < n_B; k++)\n        {\n          x_sum += reference_data[(i + si) + (j + sj + Buffer_Up)*n_X + k*n_X*n_Y];\n          y_sum += reference_data[i + (j + Buffer_Up)*n_X + k*n_X*n_Y];\n        }\n        x_mean = x_sum / n_B;\n        y_mean = y_sum / n_B;\n        for (int k = 0; k < n_B; k++)\n        {\n          xy_sum += (reference_data[i + si + (j + sj + Buffer_Up)*n_X + k*n_X*n_Y] - x_mean) * (reference_data[i + (j + Buffer_Up)*n_X + k*n_X*n_Y] - y_mean);\n          x2_sum += (reference_data[i + si + (j + sj + Buffer_Up)*n_X + k*n_X*n_Y] - x_mean) * (reference_data[(i + si) + (j + sj + Buffer_Up)*n_X + k*n_X*n_Y] - x_mean);\n          y2_sum += (reference_data[i + (j + Buffer_Up)*n_X + k *n_X*n_Y] - y_mean) * (reference_data[i + (j + Buffer_Up)*n_X + k*n_X*n_Y] - y_mean);\n        }\n        corr_res[si + win + (sj + win)*(2 * win + 1)] = xy_sum / sqrt(x2_sum*y2_sum);\n        d_res[(i + si + (j + sj)*n_X)*(2 * win + 1)*(2 * win + 1) * 4 - si + win + (-1 * sj + win)*(2 * win + 1)] = xy_sum / sqrt(x2_sum*y2_sum);\n\n        int count_tempQA = 0;\n        x_sum = 0;\n        y_sum = 0;\n        x_mean = 0;\n        y_mean = 0;\n        xy_sum = 0;\n        x2_sum = 0;\n        y2_sum = 0;\n        for (int k = 0; k < n_B; k++)\n        {\n          for (int y = 0; y < n_Years; y++)\n          {\n            if ((img_QA[i + (j + Buffer_Up)*n_X + k*n_X*n_Y + y*n_X*n_Y*n_B] == 0.f || \n                 img_QA[i + (j + Buffer_Up)*n_X + k*n_X*n_Y + y*n_X*n_Y*n_B] == 1.f)\n                && (img_QA[i + si + (j + sj + Buffer_Up)*n_X + k*n_X*n_Y + y*n_X*n_Y*n_B] == 0.f || \n                    img_QA[i + si + (j + sj + Buffer_Up)*n_X + k*n_X*n_Y + y*n_X*n_Y*n_B] == 1.f) &&\n                    reference_data[i + (j + Buffer_Up)*n_X + k*n_X*n_Y] != 0.f &&\n                    reference_data[(i + si) + (j + sj + Buffer_Up)*n_X + k*n_X*n_Y] != 0.f)\n            {\n              count_tempQA++;\n              x_sum += img_NDVI[i + si + (j + sj + Buffer_Up)*n_X + k*n_X*n_Y + y*n_X*n_Y*n_B] / reference_data[(i + si) + (j + sj + Buffer_Up)*n_X + k*n_X*n_Y];\n              y_sum += img_NDVI[i + (j + Buffer_Up) *n_X + k*n_X*n_Y + y*n_X*n_Y*n_B] / reference_data[i + (j + Buffer_Up)*n_X + k*n_X*n_Y];\n              xy_sum += (img_NDVI[i + si + (j + sj + Buffer_Up)*n_X + k*n_X*n_Y + y*n_X*n_Y*n_B] / reference_data[(i + si) + (j + sj + Buffer_Up)*n_X + k*n_X*n_Y] - x_mean) * (img_NDVI[i + (j + Buffer_Up) *n_X + k*n_X*n_Y + y*n_X*n_Y*n_B] / reference_data[i + (j + Buffer_Up)*n_X + k*n_X*n_Y] - y_mean);\n              x2_sum += (img_NDVI[i + si + (j + sj + Buffer_Up)*n_X + k*n_X*n_Y + y*n_X*n_Y*n_B] / reference_data[(i + si) + (j + sj + Buffer_Up)*n_X + k*n_X*n_Y] - x_mean) * (img_NDVI[i + si + (j + sj + Buffer_Up)*n_X + k*n_X*n_Y + y*n_X*n_Y*n_B] / reference_data[(i + si) + (j + sj + Buffer_Up)*n_X + k*n_X*n_Y] - x_mean);\n              y2_sum += (img_NDVI[i + (j + Buffer_Up) *n_X + k*n_X*n_Y + y*n_X*n_Y*n_B] / reference_data[i + (j + Buffer_Up)*n_X + k*n_X*n_Y] - y_mean) * (img_NDVI[i + (j + Buffer_Up)*n_X + k*n_X*n_Y + y*n_X*n_Y*n_B] / reference_data[i + (j + Buffer_Up)*n_X + k*n_X*n_Y] - y_mean);\n            }\n          }\n        }\n        if (count_tempQA >= 30)\n        {\n          Slope_res[si + win + (sj + win)*(2 * win + 1)] = (xy_sum*count_tempQA - x_sum*y_sum) / (x2_sum*count_tempQA - x_sum*x_sum);\n          Intercept_res[si + win + (sj + win)*(2 * win + 1)] = (x2_sum*y_sum - x_sum*xy_sum) / (x2_sum*count_tempQA - x_sum*x_sum);\n          d_res[(i + si + (j + sj)*n_X)*(2 * win + 1)*(2 * win + 1) * 4 + (2 * win + 1)*(2 * win + 1) - si + win + (-1 * sj + win)*(2 * win + 1)] = (xy_sum*count_tempQA - y_sum*x_sum) / (y2_sum*count_tempQA - y_sum*y_sum);\n          d_res[(i + si + (j + sj)*n_X)*(2 * win + 1)*(2 * win + 1) * 4 + (2 * win + 1)*(2 * win + 1) * 2 - si + win + (-1 * sj + win)*(2 * win + 1)] = (y2_sum*x_sum - y_sum*xy_sum) / (y2_sum*count_tempQA - y_sum*y_sum);\n\n          x_mean = x_sum / count_tempQA;\n          y_mean = y_sum / count_tempQA;\n          xy_sum = 0;\n          x2_sum = 0;\n          y2_sum = 0;\n          for (int k = 0; k < n_B; k++)\n          {\n            for (int y = 0; y < n_Years; y++)\n            {\n              if ((img_QA[i + (j + Buffer_Up)*n_X + k*n_X*n_Y + y*n_X*n_Y*n_B] == 0.f ||\n                   img_QA[i + (j + Buffer_Up)*n_X + k*n_X*n_Y + y*n_X*n_Y*n_B] == 1.f) &&\n                   (img_QA[i + si + (j + sj + Buffer_Up)*n_X + k*n_X*n_Y + y*n_X*n_Y*n_B] == 0.f ||\n                    img_QA[i + si + (j + sj + Buffer_Up)*n_X + k*n_X*n_Y + y*n_X*n_Y*n_B] == 1.f) &&\n                    reference_data[i + (j + Buffer_Up)*n_X + k*n_X*n_Y] != 0.f &&\n                    reference_data[(i + si) + (j + sj + Buffer_Up)*n_X + k*n_X*n_Y] != 0.f)\n              {\n                xy_sum += (img_NDVI[i + si + (j + sj + Buffer_Up)*n_X + k*n_X*n_Y + y*n_X*n_Y*n_B] / reference_data[(i + si) + (j + sj + Buffer_Up)*n_X + k*n_X*n_Y] - x_mean) * (img_NDVI[i + (j + Buffer_Up)*n_X + k*n_X*n_Y + y*n_X*n_Y*n_B] / reference_data[i + (j + Buffer_Up)*n_X + k*n_X*n_Y] - y_mean);\n                x2_sum += (img_NDVI[i + si + (j + sj + Buffer_Up)*n_X + k*n_X*n_Y + y*n_X*n_Y*n_B] / reference_data[(i + si) + (j + sj + Buffer_Up)*n_X + k*n_X*n_Y] - x_mean) * (img_NDVI[i + si + (j + sj + Buffer_Up)*n_X + k*n_X*n_Y + y*n_X*n_Y*n_B] / reference_data[(i + si) + (j + sj + Buffer_Up)*n_X + k*n_X*n_Y] - x_mean);\n                y2_sum += (img_NDVI[i + (j + Buffer_Up)*n_X + k*n_X*n_Y + y*n_X*n_Y*n_B] / reference_data[i + (j + Buffer_Up)*n_X + k*n_X*n_Y] - y_mean) * (img_NDVI[i + (j + Buffer_Up) *n_X + k*n_X*n_Y + y*n_X*n_Y*n_B] / reference_data[i + (j + Buffer_Up)*n_X + k*n_X*n_Y] - y_mean);\n              }\n            }\n          }\n          new_corr_similar_res[si + win + (sj + win)*(2 * win + 1)] = xy_sum / sqrt(x2_sum*y2_sum);\n          d_res[(i + si + (j + sj)*n_X)*(2 * win + 1)*(2 * win + 1) * 4 + (2 * win + 1)*(2 * win + 1) * 3 - si + win + (-1 * sj + win)*(2 * win + 1)] = xy_sum / sqrt(x2_sum*y2_sum);\n        }\n\n        if (reference_data[(i + si) + (j + sj + Buffer_Up)*n_X + 3 * n_X*n_Y] == 0) //Why 3?\n        {\n          corr_res[si + win + (sj + win)*(2 * win + 1)] = 0;\n          Slope_res[si + win + (sj + win)*(2 * win + 1)] = 0;\n          Intercept_res[si + win + (sj + win)*(2 * win + 1)] = 0;\n          new_corr_similar_res[si + win + (sj + win)*(2 * win + 1)] = 0;\n        }\n        if (reference_data[i + (j + Buffer_Up)*n_X + 3 * n_X*n_Y] == 0)\n        {\n          d_res[(i + si + (j + sj)*n_X)*(2 * win + 1)*(2 * win + 1) * 4 - si + win + (-1 * sj + win)*(2 * win + 1)] = 0;\n          d_res[(i + si + (j + sj)*n_X)*(2 * win + 1)*(2 * win + 1) * 4 + (2 * win + 1)*(2 * win + 1) - si + win + (-1 * sj + win)*(2 * win + 1)] = 0;\n          d_res[(i + si + (j + sj)*n_X)*(2 * win + 1)*(2 * win + 1) * 4 + (2 * win + 1)*(2 * win + 1) * 2 - si + win + (-1 * sj + win)*(2 * win + 1)] = 0;\n          d_res[(i + si + (j + sj)*n_X)*(2 * win + 1)*(2 * win + 1) * 4 + (2 * win + 1)*(2 * win + 1) * 3 - si + win + (-1 * sj + win)*(2 * win + 1)] = 0;\n        }\n      }\n    }\n  }\n}",
            "__global__ void STSG_filter(const float *__restrict__ img_NDVI,\n                            const float *__restrict__ img_QA,\n                            const float *__restrict__ reference_data,\n                            int StartY, int TotalY, int Buffer_Up, int Buffer_Dn,\n                            int n_X, int n_Y, int n_B, int n_Years, int win,\n                            float sampcorr, int snow_address,\n                            float *__restrict__ vector_out,\n                            float *__restrict__ d_vector_in,\n                            float *__restrict__ d_res,\n                            float *__restrict__ d_res_3,\n                            int *__restrict__ d_index)\n{\n  int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i >= n_X)\n    return;\n  int j = blockIdx.y * blockDim.y + threadIdx.y;\n  if (j >= n_Y)\n    return;\n\n  int samp = 0;\n  int aap = 0;\n  int *similar_index = &d_index[(i + j*n_X)*(2 * win + 1)*(2 * win + 1) * 3];\n  float *slope_intercept = &d_res_3[(i + j*n_X)*(2 * win + 1)*(2 * win + 1) * 3];\n  float *corr_similar = &d_res_3[(i + j*n_X)*(2 * win + 1)*(2 * win + 1) * 3 + (2 * win + 1)*(2 * win + 1) * 2];\n  for (int y = 0; y < n_Years; y++)\n  {\n    float *vector_in = &d_vector_in[(i + j*n_X)*n_B];\n    for (int k = 0; k < n_B; k++)\n      vector_in[k] = img_NDVI[i + (j + Buffer_Up) *n_X + k*n_X*(n_Y + Buffer_Up + Buffer_Dn) + y*n_X*(n_Y + Buffer_Up + Buffer_Dn)*n_B];\n\n    float vector_in_max_1 = vector_in[0];\n    float vector_in_max_2 = vector_in[1];\n    float vector_in_max_3 = vector_in[2];\n    if (vector_in_max_2 >= vector_in_max_1)\n    {\n      vector_in_max_2 = vector_in_max_1;\n      vector_in_max_1 = vector_in[1];\n    }\n    if (vector_in_max_3 >= vector_in_max_1)\n    {\n      vector_in_max_3 = vector_in_max_2;\n      vector_in_max_2 = vector_in_max_1;\n      vector_in_max_1 = vector_in[2];\n    }\n    else if (vector_in_max_3 >= vector_in_max_2&&vector_in_max_3 < vector_in_max_1)\n    {\n      vector_in_max_3 = vector_in_max_2;\n      vector_in_max_2 = vector_in[2];\n    }\n    for (int k = 3; k < n_B; k++)\n    {\n      if (vector_in[k] >= vector_in_max_1)\n      {\n        vector_in_max_3 = vector_in_max_2;\n        vector_in_max_2 = vector_in_max_1;\n        vector_in_max_1 = vector_in[k];\n      }\n      else if (vector_in[k] >= vector_in_max_2&&vector_in[k] < vector_in_max_1)\n      {\n        vector_in_max_3 = vector_in_max_2;\n        vector_in_max_2 = vector_in[k];\n      }\n      else if (vector_in[k] >= vector_in_max_3&&vector_in[k] < vector_in_max_2)\n        vector_in_max_3 = vector_in[k];\n    }\n\n    if (((vector_in_max_1 + vector_in_max_2 + vector_in_max_3) / 3) > 0.15f)  //Why top 3?\n    {\n      int indic = 0;\n      //searching similar pixels\n      if (y == 0)\n      {\n        float *corr_res = &d_res[(i + j*n_X)*(2 * win + 1)*(2 * win + 1) * 4];\n        float *Slope_res = &d_res[(i + j*n_X)*(2 * win + 1)*(2 * win + 1) * 4 + (2 * win + 1)*(2 * win + 1)];\n        float *Intercept_res = &d_res[(i + j*n_X)*(2 * win + 1)*(2 * win + 1) * 4 + (2 * win + 1)*(2 * win + 1) * 2];\n        float *new_corr_similar_res = &d_res[(i + j*n_X)*(2 * win + 1)*(2 * win + 1) * 4 + (2 * win + 1)*(2 * win + 1) * 3];\n        int count_corr_Slope = 0;\n        for (int m = 0; m < (2 * win + 1)*(2 * win + 1); m++)\n        {\n          if (corr_res[m] >= sampcorr&&Slope_res[m] != 0.f)\n            count_corr_Slope++;\n        }\n        if (count_corr_Slope >= 2)\n        {\n          samp = count_corr_Slope - 1;\n          int *new_corr = &d_index[(i + j*n_X)*(2 * win + 1)*(2 * win + 1) * 3 + (2 * win + 1)*(2 * win + 1) * 2];\n          for (int m = 0; m < (2 * win + 1)*(2 * win + 1); m++)\n            new_corr[m] = m;\n          for (int m = 0; m < count_corr_Slope; m++)\n          {\n            for (int n = m + 1; n < (2 * win + 1)*(2 * win + 1); n++)\n            {\n              if (corr_res[new_corr[m]] < corr_res[new_corr[n]])\n              {\n                int temp = new_corr[m];\n                new_corr[m] = new_corr[n];\n                new_corr[n] = temp;\n              }\n            }\n          }\n\n          for (int k = 0; k < samp; k++)\n          {\n            similar_index[1 + k * 2] = int(new_corr[k + 1] / (2 * win + 1)) + j - win;\n            similar_index[k * 2] = new_corr[k + 1] - int(new_corr[k + 1] / (2 * win + 1))*(2 * win + 1) + i - win;\n            slope_intercept[1 + k * 2] = Slope_res[int(new_corr[k + 1] / (2 * win + 1)) + (2 * win + 1)*(new_corr[k + 1] - int(new_corr[k + 1] / (2 * win + 1))*(2 * win + 1))];\n            slope_intercept[k * 2] = Intercept_res[int(new_corr[k + 1] / (2 * win + 1)) + (2 * win + 1)*(new_corr[k + 1] - int(new_corr[k + 1] / (2 * win + 1))*(2 * win + 1))];\n            corr_similar[k] = new_corr_similar_res[int(new_corr[k + 1] / (2 * win + 1)) + (2 * win + 1)* (new_corr[k + 1] - int(new_corr[k + 1] / (2 * win + 1))*(2 * win + 1))];\n          }\n          aap = 1;\n        }\n        else\n          aap = 0;\n      }\n\n      //generate the trend curve\n      float *trend_NDVI = &d_res[(i + j*n_X)*(2 * win + 1)*(2 * win + 1) * 4];\n      if (aap == 1)\n      {\n        int count_trend_NDVI = 0;\n        int nocount_trend_NDVI = 0;\n        int *res_trend_NDVI = &d_index[(i + j*n_X)*(2 * win + 1)*(2 * win + 1) * 3 + (2 * win + 1)*(2 * win + 1) * 2];\n        int *nores_trend_NDVI = &res_trend_NDVI[n_B];\n        int count_conres = 0;\n        for (int k = 0; k < n_B; k++)\n        {\n          float *temp_NDVI = &d_res[(i + j*n_X)*(2 * win + 1)*(2 * win + 1) * 4 + (2 * win + 1)*(2 * win + 1)];\n          int count_temp_NDVI = 0;\n          float total_new_corr_similar = 0;\n          float total_new_temp = 0;\n          for (int m = 0; m < samp; m++)\n          {\n            if (similar_index[m * 2] >= 0 && similar_index[m * 2] < n_X && similar_index[m * 2 + 1] + Buffer_Up >= 0 && similar_index[m * 2 + 1] + Buffer_Up < TotalY\n                && (img_QA[similar_index[m * 2] + (similar_index[1 + m * 2] + Buffer_Up) * n_X + k*n_X*(n_Y + Buffer_Up + Buffer_Dn) + y*n_X*(n_Y + Buffer_Up + Buffer_Dn)*n_B] == 0.f || img_QA[similar_index[m * 2] + (similar_index[1 + m * 2] + Buffer_Up) * n_X + k*n_X*(n_Y + Buffer_Up + Buffer_Dn) + y*n_X*(n_Y + Buffer_Up + Buffer_Dn)*n_B] == 1.f)\n                && reference_data[similar_index[m * 2] + (similar_index[1 + m * 2] + Buffer_Up) *n_X + k*n_X * (n_Y + Buffer_Up + Buffer_Dn)] != 0.f)\n            {\n              float new_ratio = img_NDVI[similar_index[m * 2] + (similar_index[1 + m * 2] + Buffer_Up) * n_X + k*n_X*(n_Y + Buffer_Up + Buffer_Dn) + y*n_X*(n_Y + Buffer_Up + Buffer_Dn)*n_B] / reference_data[similar_index[m * 2] + (similar_index[1 + m * 2] + Buffer_Up)* n_X + k*n_X*(n_Y + Buffer_Up + Buffer_Dn)];\n              temp_NDVI[m] = (slope_intercept[m * 2] + new_ratio*slope_intercept[1 + m * 2])*reference_data[i + (j + Buffer_Up)*n_X + k*n_X*(n_Y + Buffer_Up + Buffer_Dn)];\n              if (temp_NDVI[m] >= 1.f || temp_NDVI[m] <= -0.2f)\n                temp_NDVI[m] = 0.f;\n              if (temp_NDVI[m] != 0.f)\n              {\n                count_temp_NDVI++;\n                total_new_corr_similar += corr_similar[m];\n              }\n            }\n            else\n              temp_NDVI[m] = 0.f;\n          }\n          if (count_temp_NDVI != 0)\n          {\n            for (int m = 0; m < samp; m++)\n            {\n              if (temp_NDVI[m] != 0)\n                total_new_temp += corr_similar[m] / total_new_corr_similar * temp_NDVI[m];\n            }\n            trend_NDVI[k] = total_new_temp;\n            if (trend_NDVI[k] != 0)\n            {\n              res_trend_NDVI[count_trend_NDVI++] = k;\n              if (count_trend_NDVI > 1 && k - res_trend_NDVI[count_trend_NDVI - 2] >= 3)\n                count_conres++;\n            }\n            else\n              nores_trend_NDVI[nocount_trend_NDVI++] = k;\n          }\n          else\n          {\n            trend_NDVI[k] = 0;\n            nores_trend_NDVI[nocount_trend_NDVI++] = k;\n          }\n        }\n\n        //generating the trend_NDVI\n        if (count_trend_NDVI >= n_B / 2 && count_conres == 0)\n        {\n          for (int m = 0; m < nocount_trend_NDVI; m++)\n          {\n            int sta = 0;\n            if (nores_trend_NDVI[m] < res_trend_NDVI[0])\n              sta = 0;\n            else if (nores_trend_NDVI[m] > res_trend_NDVI[count_trend_NDVI - 1])\n              sta = count_trend_NDVI - 4;\n            else\n            {\n              for (int n = 0; n < count_trend_NDVI - 1; n++)\n              {\n                if (res_trend_NDVI[n] < nores_trend_NDVI[m] && nores_trend_NDVI[m] < res_trend_NDVI[n + 1])\n                {\n                  if (n - 1 < 0)\n                    sta = 0;\n                  else if (count_trend_NDVI - n < 4)\n                    sta = count_trend_NDVI - 4;\n                  else\n                    sta = n - 1;\n                  break;\n                }\n              }\n            }\n\n            float x[4], y[4];\n            for (int n = 0; n < 4; n++)\n            {\n              x[n] = res_trend_NDVI[sta + n];\n              y[n] = trend_NDVI[res_trend_NDVI[sta + n]];\n            }\n            float sig, p;\n            float y2[4] = { 0 };\n            float u[4] = { 0 };\n            for (int n = 1; n < 3; n++)\n            {\n              sig = (x[n] - x[n - 1]) / (x[n + 1] - x[n - 1]);\n              p = sig*y2[n - 1] + 2.f;\n              y2[n] = (sig - 1) / p;\n              u[n] = (y[n + 1] - y[n]) / (x[n + 1] - x[n]) - (y[n] - y[n - 1]) / (x[n] - x[n - 1]);\n              u[n] = (6.f*u[n] / (x[n + 1] - x[n - 1]) - sig*u[n - 1]) / p;\n            }\n            for (int n = 3; n >= 0; n--)\n              y2[n] = y2[n] * y2[n + 1] + u[n];\n\n            int klo = 0;\n            int khi = 3;\n            while (khi - klo > 1)            \n            {\n              int k = (khi + klo) >> 1;\n              if (x[k] > nores_trend_NDVI[m])\n                khi = k;\n              else klo = k;\n            }\n            float h = x[khi] - x[klo];\n            float a = (x[khi] - nores_trend_NDVI[m]) / h;\n            float b = (nores_trend_NDVI[m] - x[klo]) / h;\n            trend_NDVI[nores_trend_NDVI[m]] = a*y[klo] + b*y[khi] + ((a*a*a - a)*y2[klo] + (b*b*b - b)*y2[khi])*h*h / 6.f;\n          }\n          indic = 1;\n        }\n        else\n          indic = 0;\n      }\n      else\n        indic = 0;\n\n      //begin; STSG\n      if (indic == 1)\n      {\n        if (snow_address == 1)\n        {\n          //processing contaminated NDVI by snow\n          int count_vector_QA = 0;\n          for (int k = 0; k < n_B; k++)\n          {\n            if (img_QA[i + (j + Buffer_Up) *n_X + k*n_X*(n_Y + Buffer_Up + Buffer_Dn) + y*n_X*(n_Y + Buffer_Up + Buffer_Dn)*n_B] == 2.f)\n              count_vector_QA++;\n          }\n          if (count_vector_QA != 0)\n          {\n            int bv_count = 0;\n            float bv_total = 0.;\n            for (int yeari = 0; yeari < n_Years; yeari++)\n            {\n              for (int k = 0; k < 6; k++)\n              {\n                if (img_QA[i + (j + Buffer_Up)*n_X + k*n_X*(n_Y + Buffer_Up + Buffer_Dn) + yeari*n_X*(n_Y + Buffer_Up + Buffer_Dn)*n_B] == 0.f ||\n                    img_QA[i + (j + Buffer_Up)*n_X + k*n_X*(n_Y + Buffer_Up + Buffer_Dn) + yeari*n_X*(n_Y + Buffer_Up + Buffer_Dn)*n_B] == 1.f)\n                {\n                  bv_count++;\n                  bv_total = bv_total + img_NDVI[i + (j + Buffer_Up) *n_X + k*n_X*(n_Y + Buffer_Up + Buffer_Dn) + yeari*n_X*(n_Y + Buffer_Up + Buffer_Dn)*n_B];\n                }\n              }\n            }\n            if (bv_count != 0)\n            {\n              float bv = bv_total / bv_count;\n              for (int k = 0; k < n_B; k++)\n              {\n                if (img_QA[i + (j + Buffer_Up) *n_X + k*n_X*(n_Y + Buffer_Up + Buffer_Dn) + y*n_X*(n_Y + Buffer_Up + Buffer_Dn)*n_B] == 2.f)\n                {\n                  vector_in[k] = bv;\n                  trend_NDVI[k] = bv;\n                }\n              }\n            }\n          }\n        }\n\n        //Calculate the weights for each point\n        float gdis = 0.f;\n        float *fl = &d_res[(i + j*n_X)*(2 * win + 1)*(2 * win + 1) * 4 + (2 * win + 1)*(2 * win + 1) * 2];\n        int count_fl = 0;\n        float mean_fl = 0;\n        for (int k = 0; k < n_B; k++)\n        {\n          if (img_QA[i + (j + Buffer_Up)*n_X + k*n_X*(n_Y + Buffer_Up + Buffer_Dn) + y*n_X*(n_Y + Buffer_Up + Buffer_Dn)*n_B] == 0.f ||\n              img_QA[i + (j + Buffer_Up)*n_X + k*n_X*(n_Y + Buffer_Up + Buffer_Dn) + y*n_X*(n_Y + Buffer_Up + Buffer_Dn)*n_B] == 1.f)\n          {\n            fl[k] = vector_in[k] - trend_NDVI[k];\n            count_fl++;\n            mean_fl += fl[k];\n          }\n          else\n            fl[k] = -1.f;\n        }\n        if (count_fl != 0)\n          mean_fl = mean_fl / count_fl;\n        for (int k = 0; k < n_B; k++)\n        {\n          if (fl[k] == -1.f)\n            fl[k] = mean_fl;\n        }\n\n        for (int k = 0; k < n_B; k++)\n        {\n          float min_fl = 0;\n          float max_fl = 0;\n          for (int k = 0; k < n_B; k++)\n          {\n            if (min_fl > fl[k])\n              min_fl = fl[k];\n            if (max_fl < fl[k])\n              max_fl = fl[k];\n          }\n          fl[k] = (fl[k] - min_fl) / (max_fl - min_fl);\n          if ((vector_in[k] - trend_NDVI[k]) >= 0.f)\n            gdis = gdis + fl[k] * (vector_in[k] - trend_NDVI[k]);\n          else\n            gdis = gdis + fl[k] * (trend_NDVI[k] - vector_in[k]);\n        }\n\n        for (int k = 0; k < n_B; k++)\n        {\n          if (img_QA[i + (j + Buffer_Up)*n_X + k*n_X*(n_Y + Buffer_Up + Buffer_Dn) + y*n_X*(n_Y + Buffer_Up + Buffer_Dn)*n_B] == 0.f)\n            trend_NDVI[k] = vector_in[k];\n          if (img_QA[i + (j + Buffer_Up) *n_X + k*n_X*(n_Y + Buffer_Up + Buffer_Dn) + y*n_X*(n_Y + Buffer_Up + Buffer_Dn)*n_B] != 0.f &&\n              img_QA[i + (j + Buffer_Up) *n_X + k*n_X*(n_Y + Buffer_Up + Buffer_Dn) + y*n_X*(n_Y + Buffer_Up + Buffer_Dn)*n_B] != 1.f)\n            vector_in[k] = trend_NDVI[k];\n        }\n\n        float *vec_fil = &d_res[(i + j*n_X)*(2 * win + 1)*(2 * win + 1) * 4 + (2 * win + 1)*(2 * win + 1) * 3];\n        float ormax = gdis;\n        int loop_times = 1;\n        while (gdis <= ormax && loop_times < 50)\n        {\n          loop_times = loop_times + 1;\n          for (int k = 0; k < n_B; k++)\n            vec_fil[k] = trend_NDVI[k];\n          //The Savitzky - Golay fitting\n          //savgolFilter = SAVGOL(4, 4, 0, 6); set the window width(4, 4) and degree(6) for repetition\n          double savgolFilter[] = { -0.00543880, 0.0435097, -0.152289, 0.304585, 0.619267, 0.304585, -0.152289, 0.0435097, -0.00543880 };\n          int savgolFilterW = sizeof(savgolFilter) / sizeof(savgolFilter[0]);\n          int ra4W = n_B;\n          float *new_ra4 = &d_res[(i + j*n_X)*(2 * win + 1)*(2 * win + 1) * 4 + (2 * win + 1)*(2 * win + 1) * 3 + n_B];\n          for (int ii = 0; ii < savgolFilterW + ra4W - 1; ii++)\n          {\n            if (ii < (savgolFilterW - 1) / 2)\n              new_ra4[ii] = ((vector_in[0] >= trend_NDVI[0]) ? vector_in[0] : trend_NDVI[0]);\n            else if (ii >((savgolFilterW - 1) / 2 + ra4W - 1))\n              new_ra4[ii] = ((vector_in[ra4W - 1] >= trend_NDVI[ra4W - 1]) ? vector_in[ra4W - 1] : trend_NDVI[ra4W - 1]);\n            else\n              new_ra4[ii] = ((vector_in[ii - (savgolFilterW - 1) / 2] >= trend_NDVI[ii - (savgolFilterW - 1) / 2]) ? vector_in[ii - (savgolFilterW - 1) / 2] : trend_NDVI[ii - (savgolFilterW - 1) / 2]);\n          }\n          for (int ii = 0; ii < ra4W; ii++)\n          {\n            float temp = 0;\n            for (int jj = 0; jj < savgolFilterW; jj++)\n              temp += savgolFilter[jj] * new_ra4[ii + jj];\n            trend_NDVI[ii] = temp;\n          }\n          ormax = gdis;\n          //Calculate the fitting - effect index\n          gdis = 0.f;\n          for (int k = 0; k < n_B; k++)\n          {\n            if ((vector_in[k] - trend_NDVI[k]) >= 0)\n              gdis = gdis + fl[k] * (vector_in[k] - trend_NDVI[k]);\n            else\n              gdis = gdis + fl[k] * (trend_NDVI[k] - vector_in[k]);\n          }\n        }\n\n        for (int k = 0; k < n_B; k++)\n          vector_out[i + (j + StartY)*n_X + k*n_X*TotalY + y*n_X*TotalY*n_B] = vec_fil[k];\n        for (int smi = 0; smi < n_B - 4; smi++)\n        {\n          float a1 = vec_fil[smi];\n          float a2 = vec_fil[smi + 1];\n          float a3 = vec_fil[smi + 2];\n          float a4 = vec_fil[smi + 3];\n          float a5 = vec_fil[smi + 4];\n          if ((a1 > a2) && (a2 < a3) && (a3 > a4) && (a4 < a5))\n          {\n            vector_out[i + (j + StartY)*n_X + (smi + 1) * n_X*TotalY + y*n_X*TotalY*n_B] = (a1 + a3) / 2.f;\n            vector_out[i + (j + StartY)*n_X + (smi + 3) * n_X*TotalY + y*n_X*TotalY*n_B] = (a3 + a5) / 2.f;\n          }\n        }\n      }\n\n      // SG filter\n      if (indic == 0)\n      {\n        if (snow_address == 1)\n        {\n          //processing contaminated NDVI by snow\n          int count_vector_QA = 0;\n          for (int k = 0; k < n_B; k++)\n          {\n            if (img_QA[i + (j + Buffer_Up)*n_X + k*n_X*(n_Y + Buffer_Up + Buffer_Dn) + y*n_X*(n_Y + Buffer_Up + Buffer_Dn)*n_B] == 2.f)\n              count_vector_QA++;\n          }\n          if (count_vector_QA != 0)\n          {\n            int bv_count = 0;\n            float bv_total = 0.;\n            for (int yeari = 0; yeari < n_Years; yeari++)\n            {\n              for (int k = 0; k < 6; k++)\n              {\n                if (img_QA[i + (j + Buffer_Up)*n_X + k*n_X*(n_Y + Buffer_Up + Buffer_Dn) + yeari*n_X*(n_Y + Buffer_Up + Buffer_Dn)*n_B] == 0.f ||\n                    img_QA[i + (j + Buffer_Up)*n_X + k*n_X*(n_Y + Buffer_Up + Buffer_Dn) + yeari*n_X*(n_Y + Buffer_Up + Buffer_Dn)*n_B] == 1.f)\n                {\n                  bv_count++;\n                  bv_total = bv_total + img_NDVI[i + (j + Buffer_Up)*n_X + k*n_X*(n_Y + Buffer_Up + Buffer_Dn) + yeari*n_X*(n_Y + Buffer_Up + Buffer_Dn)*n_B];\n                }\n              }\n            }\n            if (bv_count != 0)\n            {\n              float bv = bv_total / bv_count;\n              for (int k = 0; k < n_B; k++)\n              {\n                if (img_QA[i + (j + Buffer_Up)*n_X + k*n_X*(n_Y + Buffer_Up + Buffer_Dn) + y*n_X*(n_Y + Buffer_Up + Buffer_Dn)*n_B] == 2.f)\n                {\n                  vector_in[k] = bv;\n                  trend_NDVI[k] = bv;\n                }\n              }\n            }\n          }\n        }\n\n        int count_vector_QA = 0;\n        int *res_vector_QA = &d_index[(i + j*n_X)*(2 * win + 1)*(2 * win + 1) * 3 + (2 * win + 1)*(2 * win + 1) * 2];\n        for (int k = 0; k < n_B; k++)\n        {\n          if (img_QA[i + (j + Buffer_Up)*n_X + k*n_X*(n_Y + Buffer_Up + Buffer_Dn) + y*n_X*(n_Y + Buffer_Up + Buffer_Dn)*n_B] <= 2.f &&\n              img_QA[i + (j + Buffer_Up)*n_X + k*n_X*(n_Y + Buffer_Up + Buffer_Dn) + y*n_X*(n_Y + Buffer_Up + Buffer_Dn)*n_B] != -1.f)\n            res_vector_QA[count_vector_QA++] = k;\n        }\n        if (count_vector_QA < n_B&&count_vector_QA>1)\n        {\n          double k;\n          int x = 0;\n          int l = 0;\n          if (res_vector_QA[count_vector_QA - 1] != n_B - 1)\n          {\n            k = (vector_in[res_vector_QA[count_vector_QA - 1]] - vector_in[res_vector_QA[count_vector_QA - 2]]) / (res_vector_QA[count_vector_QA - 1] - res_vector_QA[count_vector_QA - 2]);\n            vector_in[n_B - 1] = vector_in[res_vector_QA[count_vector_QA - 1]] + k*(n_B - 1 - res_vector_QA[count_vector_QA - 1]);\n            res_vector_QA[count_vector_QA++] = n_B - 1;\n          }\n          int count_res_vector_QA = count_vector_QA;\n          while (count_vector_QA < n_B&&x < count_res_vector_QA - 1)\n          {\n            l = res_vector_QA[x + 1] - res_vector_QA[x];\n            int n = 1;\n            while (l > 1)\n            {\n              k = (vector_in[res_vector_QA[x + 1]] - vector_in[res_vector_QA[x]]) / (res_vector_QA[x + 1] - res_vector_QA[x]);\n              vector_in[res_vector_QA[x] + n] = vector_in[res_vector_QA[x]] + k*n;\n              count_vector_QA++;\n              n++;\n              l--;\n            }\n            x++;\n          }\n          if (res_vector_QA[0] != 0)\n          {\n            k = (vector_in[res_vector_QA[1]] - vector_in[res_vector_QA[0]]) / (res_vector_QA[1] - res_vector_QA[0]);\n            l = res_vector_QA[0];\n            int n = 0;\n            do\n            {\n              vector_in[n] = vector_in[res_vector_QA[0]] - k*l;\n              n++;\n              l--;\n            } while (l >= 1);\n          }\n        }\n\n        float* rst = &d_res[(i + j*n_X)*(2 * win + 1)*(2 * win + 1) * 4 + (2 * win + 1)*(2 * win + 1) * 3];\n        //savgolFilter = SAVGOL(4, 4, 0, 2); set the window width(4, 4) and degree(2) for computing trend curve\n        double savgolFilter[] = { -0.0909091, 0.0606061, 0.168831, 0.233766, 0.255411, 0.233766, 0.168831, 0.0606061, -0.0909091 };\n        int savgolFilterW = sizeof(savgolFilter) / sizeof(savgolFilter[0]);\n        int vector_inW = n_B;\n        float *new_vector_in = &d_res[(i + j*n_X)*(2 * win + 1)*(2 * win + 1) * 4 + (2 * win + 1)*(2 * win + 1) * 3 + n_B];\n        for (int ii = 0; ii < savgolFilterW + vector_inW - 1; ii++)\n        {\n          if (ii < (savgolFilterW - 1) / 2)\n            new_vector_in[ii] = vector_in[0];\n          else if (ii >((savgolFilterW - 1) / 2 + vector_inW - 1))\n            new_vector_in[ii] = vector_in[vector_inW - 1];\n          else\n            new_vector_in[ii] = vector_in[ii - (savgolFilterW - 1) / 2];\n        }\n        for (int ii = 0; ii < vector_inW; ii++)\n        {\n          float temp = 0;\n          for (int jj = 0; jj < savgolFilterW; jj++)\n            temp += savgolFilter[jj] * new_vector_in[ii + jj];\n          rst[ii] = temp;\n        }\n\n        //Calculate the weights for each point\n        float gdis = 0.0;\n        float *fl = &d_res[(i + j*n_X)*(2 * win + 1)*(2 * win + 1) * 4 + (2 * win + 1)*(2 * win + 1) * 2];\n        float maxdif = 0;\n        for (int k = 0; k < n_B; k++)\n        {\n          if ((vector_in[k] - rst[k]) >= 0)\n            fl[k] = vector_in[k] - rst[k];\n          else\n            fl[k] = rst[k] - vector_in[k];\n          if (k == 0)\n            maxdif = fl[k];\n          else\n          {\n            if (maxdif < fl[k])\n              maxdif = fl[k];\n          }\n        }\n        for (int k = 0; k < n_B; k++)\n        {\n          if (vector_in[k] >= rst[k])\n          {\n            fl[k] = 1.f;\n            gdis = gdis + fl[k] * (vector_in[k] - rst[k]);\n          }\n          else\n          {\n            fl[k] = 1.f - (rst[k] - vector_in[k]) / maxdif;\n            gdis = gdis + fl[k] * (rst[k] - vector_in[k]);\n          }\n        }\n\n        float ormax = gdis;\n        int loop_times = 1;\n        while (gdis <= ormax && loop_times < 15)\n        {\n          loop_times = loop_times + 1;\n          for (int k = 0; k < n_B; k++)\n            vector_out[i + (j + StartY)*n_X + k*n_X*TotalY + y*n_X*TotalY*n_B] = rst[k];\n          //The Savitzky - Golay fitting\n          //savgolFilter = SAVGOL(4, 4, 0, 6); set the window width(4, 4) and degree(6) for repetition\n          double savgolFilter[] = { -0.00543880, 0.0435097, -0.152289, 0.304585, 0.619267, 0.304585, -0.152289, 0.0435097, -0.00543880 };\n          int savgolFilterW = sizeof(savgolFilter) / sizeof(savgolFilter[0]);\n          int ra4W = n_B;\n          float *new_ra4 = &d_res[(i + j*n_X)*(2 * win + 1)*(2 * win + 1) * 4 + (2 * win + 1)*(2 * win + 1) * 3 + n_B];\n          for (int ii = 0; ii < savgolFilterW + ra4W - 1; ii++)\n          {\n            if (ii < (savgolFilterW - 1) / 2)\n              new_ra4[ii] = (vector_in[0] >= rst[0]) ? vector_in[0] : rst[0];\n            else if (ii >((savgolFilterW - 1) / 2 + ra4W - 1))\n              new_ra4[ii] = (vector_in[ra4W - 1] >= rst[ra4W - 1]) ? vector_in[ra4W - 1] : rst[ra4W - 1];\n            else\n              new_ra4[ii] = (vector_in[ii - (savgolFilterW - 1) / 2] >= rst[ii - (savgolFilterW - 1) / 2]) ? vector_in[ii - (savgolFilterW - 1) / 2] : rst[ii - (savgolFilterW - 1) / 2];\n          }\n          for (int ii = 0; ii < ra4W; ii++)\n          {\n            float temp = 0;\n            for (int jj = 0; jj < savgolFilterW; jj++)\n              temp += savgolFilter[jj] * new_ra4[ii + jj];\n            rst[ii] = temp;\n          }\n          ormax = gdis;\n          //Calculate the fitting - effect index\n          gdis = 0.f;\n          for (int k = 0; k < n_B; k++)\n          {\n            if ((vector_in[k] - rst[k]) >= 0)\n              gdis = gdis + fl[k] * (vector_in[k] - rst[k]);\n            else\n              gdis = gdis + fl[k] * (rst[k] - vector_in[k]);\n          }\n        }\n      }\n    }\n  }\n}"
        ]
    },
    "libor-cuda": {
        "/Users/gbolet/hecbench-roofline/src/libor-cuda/main.cu": [
            "inline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\ninline __host__ __device__ float4 sqrtf(float4 v)\n{\n    return make_float4(sqrtf(v.x), sqrtf(v.y), sqrtf(v.z), sqrtf(v.w));\n}\n\n__device__\nvoid path_calc_b1(float *L, \n                  const float *z, \n                  float *L2,\n                  const float *lambda,\n                  const float delta,\n                  const int Nmat,\n                  const int N)\n{\n  int   i, n;\n  float sqez, lam, con1, v, vrat;\n\n  for (i=0; i<N; i++) L2[i] = L[i];\n   \n  for(n=0; n<Nmat; n++) {\n    sqez = sqrtf(delta)*z[n];\n    v = 0.f;\n\n    for (i=n+1; i<N; i++) {\n      lam  = lambda[i-n-1];\n      con1 = delta*lam;\n      v   += __fdividef(con1*L[i],1.f+delta*L[i]);\n      vrat = __expf(con1*v + lam*(sqez-0.5f*con1));\n      L[i] = L[i]*vrat;\n\n      // store these values for reverse path\n      L2[i+(n+1)*N] = L[i];\n    }\n  }\n}\n\n__device__\nvoid path_calc_b2(float *L_b, \n                  const float *z, \n                  const float *L2, \n                  const float *lambda, \n                  const float delta,\n                  const int Nmat,\n                  const int N)\n{\n  int   i, n;\n  float faci, v1;\n\n  for (n=Nmat-1; n>=0; n--) {\n    v1 = 0.f;\n    for (i=N-1; i>n; i--) {\n      v1    += lambda[i-n-1]*L2[i+(n+1)*N]*L_b[i];\n      faci   = __fdividef(delta,1.f+delta*L2[i+n*N]);\n      L_b[i] = L_b[i]*__fdividef(L2[i+(n+1)*N],L2[i+n*N])\n              + v1*lambda[i-n-1]*faci*faci;\n \n    }\n  }\n}\n\n__device__\nfloat portfolio_b(float *L, \n                  float *L_b,\n                  const float *lambda, \n                  const   int *maturities, \n                  const float *swaprates, \n                  const float delta,\n                  const int Nmat,\n                  const int N,\n                  const int Nopt)\n{\n  int   m, n;\n  float b, s, swapval,v;\n  float B[NMAT], S[NMAT], B_b[NMAT], S_b[NMAT];\n\n  b = 1.f;\n  s = 0.f;\n  for (m=0; m<N-Nmat; m++) {\n    n    = m + Nmat;\n    b    = __fdividef(b,1.f+delta*L[n]);\n    s    = s + delta*b;\n    B[m] = b;\n    S[m] = s;\n  }\n\n  v = 0.f;\n\n  for (m=0; m<NMAT; m++) {\n    B_b[m] = 0.f;\n    S_b[m] = 0.f;\n  }\n\n  for (n=0; n<Nopt; n++){\n    m = maturities[n] - 1;\n    swapval = B[m] + swaprates[n]*S[m] - 1.f;\n    if (swapval<0) {\n      v     += -100.f*swapval;\n      S_b[m] += -100.f*swaprates[n];\n      B_b[m] += -100.f;\n    }\n  }\n\n  for (m=N-Nmat-1; m>=0; m--) {\n    n = m + Nmat;\n    B_b[m] += delta*S_b[m];\n    L_b[n]  = -B_b[m]*B[m]*__fdividef(delta,1.f+delta*L[n]);\n    if (m>0) {\n      S_b[m-1] += S_b[m];\n      B_b[m-1] += __fdividef(B_b[m],1.f+delta*L[n]);\n    }\n  }\n\n  // apply discount\n\n  b = 1.f;\n  for (n=0; n<Nmat; n++) b = b/(1.f+delta*L[n]);\n\n  v = b*v;\n\n  for (n=0; n<Nmat; n++){\n    L_b[n] = -v*delta/(1.f+delta*L[n]);\n  }\n\n  for (n=Nmat; n<N; n++){\n    L_b[n] = b*L_b[n];\n  }\n\n  return v;\n}\n\n__global__\nvoid Pathcalc_Portfolio_KernelGPU(\n  float *__restrict__ d_v, \n  float *__restrict__ d_Lb,\n  const float *__restrict__ lambda, \n  const   int *__restrict__ maturities, \n  const float *__restrict__ swaprates, \n  const float delta,\n  const int Nmat,\n  const int N,\n  const int Nopt)\n{\n  const int     tid = blockDim.x * blockIdx.x + threadIdx.x;\n  const int threadN = blockDim.x * gridDim.x;\n\n  int   i,path;\n  float L[NN], L2[L2_SIZE], z[NN];\n  float *L_b = L;\n  \n  // Monte Carlo LIBOR path calculation\n\n  for(path = tid; path < NPATH; path += threadN){\n    // initialise the data for current thread\n    for (i=0; i<N; i++) {\n      // for real application, z should be randomly generated\n      z[i] = 0.3f;\n      L[i] = 0.05f;\n    }\n    path_calc_b1(L, z, L2, lambda, delta, Nmat, N);\n    d_v[path] = portfolio_b(L, L_b, lambda, maturities, swaprates, delta, Nmat, N, Nopt);\n    path_calc_b2(L_b, z, L2, lambda, delta, Nmat, N);\n    d_Lb[path] = L_b[NN-1];\n  }\n}",
            "inline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\ninline __host__ __device__ float4 sqrtf(float4 v)\n{\n    return make_float4(sqrtf(v.x), sqrtf(v.y), sqrtf(v.z), sqrtf(v.w));\n}\n\n__device__\nvoid path_calc(float *L, \n               const float *z, \n               const float *lambda, \n               const float delta,\n               const int Nmat, \n               const int N)\n{\n  int   i, n;\n  float sqez, lam, con1, v, vrat;\n\n  for(n=0; n<Nmat; n++) {\n    sqez = sqrtf(delta)*z[n];\n    v = 0.f;\n\n    for (i=n+1; i<N; i++) {\n      lam  = lambda[i-n-1];\n      con1 = delta*lam;\n      v   += __fdividef(con1*L[i],1.f+delta*L[i]);\n      vrat = __expf(con1*v + lam*(sqez-0.5f*con1));\n      L[i] = L[i]*vrat;\n    }\n  }\n}\n\n__device__\nfloat portfolio(float *L,\n                const float *lambda, \n                const   int *maturities, \n                const float *swaprates, \n                const float delta,\n                const int Nmat,\n                const int N,\n                const int Nopt)\n{\n  int   n, m, i;\n  float v, b, s, swapval, B[40], S[40];\n\t\n  b = 1.f;\n  s = 0.f;\n\n  for(n=Nmat; n<N; n++) {\n    b = b/(1.f+delta*L[n]);\n    s = s + delta*b;\n    B[n-Nmat] = b;\n    S[n-Nmat] = s;\n  }\n\n  v = 0.f;\n\n  for(i=0; i<Nopt; i++){\n    m = maturities[i] - 1;\n    swapval = B[m] + swaprates[i]*S[m] - 1.f;\n    if(swapval<0)\n      v += -100.f*swapval;\n  }\n\n  // apply discount\n\n  b = 1.f;\n  for (n=0; n<Nmat; n++) b = b/(1.f+delta*L[n]);\n\n  v = b*v;\n\n  return v;\n}\n\n__global__\nvoid Pathcalc_Portfolio_KernelGPU2(\n  float *__restrict__ d_v, \n  const float *__restrict__ lambda, \n  const   int *__restrict__ maturities, \n  const float *__restrict__ swaprates, \n  const float delta,\n  const int Nmat,\n  const int N,\n  const int Nopt)\n{\n  const int     tid = blockDim.x * blockIdx.x + threadIdx.x;\n  const int threadN = blockDim.x * gridDim.x;\n\n  int   i, path;\n  float L[NN], z[NN];\n  \n  // Monte Carlo LIBOR path calculation\n\n  for(path = tid; path < NPATH; path += threadN){\n    // initialise the data for current thread\n    for (i=0; i<N; i++) {\n      // for real application, z should be randomly generated\n      z[i] = 0.3f;\n      L[i] = 0.05f;\n    }\t   \n    path_calc(L, z, lambda, delta, Nmat, N);\n    d_v[path] = portfolio(L, lambda, maturities, swaprates, delta, Nmat, N, Nopt);\n  }\n}"
        ]
    },
    "sssp-cuda": {
        "/Users/gbolet/hecbench-roofline/src/sssp-cuda/main.cu": [
            "__device__ __forceinline__\ndouble atomicMax(double *address, double val)\n{\n  unsigned long long ret = __double_as_longlong(*address);\n  while(val > __longlong_as_double(ret))\n  {\n    unsigned long long old = ret;\n    if((ret = atomicCAS((unsigned long long *)address, old, __double_as_longlong(val))) == old)\n      break;\n  }\n  return __longlong_as_double(ret);\n}\n\n__global__ void SSSP_gpu(\n    const Node *__restrict__ graph_nodes_av,\n    const Edge *__restrict__ graph_edges_av,\n    int *__restrict__ cost,\n    int *__restrict__ color,\n    const int *__restrict__ q1,\n          int *__restrict__ q2,\n    const int *__restrict__ n_t,\n    int *__restrict__ head,\n    int *__restrict__ tail,\n    int *__restrict__ overflow,\n    const int *__restrict__ gray_shade,\n    int *__restrict__ iter)\n{\n  __shared__ int l_mem[W_QUEUE_SIZE+2];\n  __shared__ int tail_bin;\n  int* l_q2 = l_mem;\n  int* shift = l_mem + W_QUEUE_SIZE;\n  int* base = l_mem + W_QUEUE_SIZE + 1;\n\n  const int tid     = threadIdx.x;\n  const int gtid    = blockIdx.x * blockDim.x + threadIdx.x;\n  const int WG_SIZE = blockDim.x;\n\n  int n_t_local = *n_t; // atomicAdd(n_t, 0);\n  int gray_shade_local = *gray_shade; // atomicAdd(&gray_shade[0], 0);\n\n  if(tid == 0) {\n    // Reset queue\n    tail_bin = 0;\n  }\n\n  // Fetch frontier elements from the queue\n  if(tid == 0)\n    *base = atomicAdd(&head[0], WG_SIZE);\n  __syncthreads();\n\n  int my_base = *base;\n  while(my_base < n_t_local) {\n\n    // If local queue might overflow\n    if(tail_bin >= W_QUEUE_SIZE / 2) {\n      if(tid == 0) {\n        // Add local tail_bin to tail\n        *shift = atomicAdd(&tail[0], tail_bin);\n      }\n      __syncthreads();\n      int local_shift = tid;\n      while(local_shift < tail_bin) {\n        q2[*shift + local_shift] = l_q2[local_shift];\n        // Multiple threads are copying elements at the same time, so we shift by multiple elements for next iteration\n        local_shift += WG_SIZE;\n      }\n      __syncthreads();\n      if(tid == 0) {\n        // Reset local queue\n        tail_bin = 0;\n      }\n      __syncthreads();\n    }\n\n    if(my_base + tid < n_t_local && *overflow == 0) {\n      // Visit a node from the current frontier\n      int pid = q1[my_base + tid];\n      //////////////// Visit node ///////////////////////////\n      atomicExch(&color[pid], BLACK); // Node visited\n      int  cur_cost = cost[pid]; // atomicAdd(&cost[pid], 0); // Look up shortest-path distance to this node\n      Node cur_node;\n      cur_node.x = graph_nodes_av[pid].x;\n      cur_node.y = graph_nodes_av[pid].y;\n      Edge cur_edge;\n      // For each outgoing edge\n      for(int i = cur_node.x; i < cur_node.y + cur_node.x; i++) {\n        cur_edge.x = graph_edges_av[i].x;\n        cur_edge.y = graph_edges_av[i].y;\n        int id     = cur_edge.x;\n        int cost_local   = cur_edge.y;\n        cost_local += cur_cost;\n        int orig_cost = atomicMax(&cost[id], cost_local);\n        if(orig_cost < cost_local) {\n          int old_color = atomicMax(&color[id], gray_shade_local);\n          if(old_color != gray_shade_local) {\n            // Push to the queue\n            int tail_index = atomicAdd(&tail_bin, 1);\n            if(tail_index >= W_QUEUE_SIZE) {\n              *overflow = 1;\n            } else\n              l_q2[tail_index] = id;\n          }\n        }\n      }\n    }\n\n    if(tid == 0)\n      *base = atomicAdd(&head[0], WG_SIZE); // Fetch more frontier elements from the queue\n    __syncthreads();\n    my_base = *base;\n  }\n  /////////////////////////////////////////////////////////\n  // Compute size of the output and allocate space in the global queue\n  if(tid == 0) {\n    *shift = atomicAdd(&tail[0], tail_bin);\n  }\n  __syncthreads();\n  ///////////////////// CONCATENATE INTO GLOBAL MEMORY /////////////////////\n  int local_shift = tid;\n  while(local_shift < tail_bin) {\n    q2[*shift + local_shift] = l_q2[local_shift];\n    // Multiple threads are copying elements at the same time, so we shift by multiple elements for next iteration\n    local_shift += WG_SIZE;\n  }\n  //////////////////////////////////////////////////////////////////////////\n\n  if(gtid == 0) {\n    atomicAdd(&iter[0], 1);\n  }\n}"
        ]
    },
    "backprop-cuda": {
        "/Users/gbolet/hecbench-roofline/src/backprop-cuda/bpnn_layerforward.h": [
            "__global__ void kernel_layerforward(\n  const float*__restrict__ input,\n        float*__restrict__ input_weights,\n        float*__restrict__ hidden_partial_sum,\n  const int hid) \n{\n  __shared__ float input_node[HEIGHT];\n  __shared__ float weight_matrix[HEIGHT * WIDTH];\n\n  // gridDim.y << gridDim.x\n  int by = blockIdx.y; \n  int tx = threadIdx.x; \n  int ty = threadIdx.y;\n\n  int index = ( hid + 1 ) * HEIGHT * by + ( hid + 1 ) * ty + tx + 1 + ( hid + 1 ) ;  \n\n  int index_in = HEIGHT * by + ty + 1;\n\n  if ( tx == 0 )\n    input_node[ty] = input[index_in] ;\n  __syncthreads();\n\n  weight_matrix[ty * WIDTH + tx] =  input_weights[index];\n  __syncthreads();\n\n  weight_matrix[ty * WIDTH + tx]= weight_matrix[ty * WIDTH + tx] * input_node[ty];\n  __syncthreads();\n\n  for ( int i = 1 ; i <= HEIGHT ; i=i*2){\n    int power_two = i; \n\n    if( ty % power_two == 0 )\n      weight_matrix[ty * WIDTH + tx]= weight_matrix[ty * WIDTH + tx] + weight_matrix[(ty + power_two/2)* WIDTH + tx];\n\n    __syncthreads();\n\n  }\n\n  input_weights[index] =  weight_matrix[ty * WIDTH + tx];\n\n  __syncthreads();\n\n  if ( tx == 0 ) {\n    hidden_partial_sum[by * hid + ty] = weight_matrix[tx* WIDTH + ty];\n  }\n}"
        ],
        "/Users/gbolet/hecbench-roofline/src/backprop-cuda/bpnn_adjust_weights.h": [
            "__global__ void kernel_adjust_weights (\n  const float*__restrict__ ly, \n       float *__restrict__ w, \n  const float*__restrict__ delta, \n        float*__restrict__ oldw, \n  const int hid)\n{\n  int by = blockIdx.y; \n  int tx = threadIdx.x; \n  int ty = threadIdx.y;\n\n  int index =  ( hid + 1 ) * HEIGHT * by + ( hid + 1 ) * ty + tx + 1 + ( hid + 1 ) ;  \n  int index_y = HEIGHT * by + ty + 1;\n  int index_x = tx + 1;\n\n  w[index] += ((ETA * delta[index_x] * ly[index_y]) + (MOMENTUM * oldw[index]));\n  oldw[index] = ((ETA * delta[index_x] * ly[index_y]) + (MOMENTUM * oldw[index]));\n\n  __syncthreads();\n\n  if (ty == 0 && by ==0){\n    w[index_x] += ((ETA * delta[index_x]) + (MOMENTUM * oldw[index_x]));\n    oldw[index_x] = ((ETA * delta[index_x]) + (MOMENTUM * oldw[index_x]));\n  }\n}"
        ]
    },
    "multinomial-cuda": {
        "/Users/gbolet/hecbench-roofline/src/multinomial-cuda/main.cu": [
            "#define GPU_NUM_THREADS 256\n\n\n__device__ __forceinline__\ndouble atomicMax(double *address, double val)\n{\n  unsigned long long ret = __double_as_longlong(*address);\n  while(val > __longlong_as_double(ret))\n  {\n    unsigned long long old = ret;\n    if((ret = atomicCAS((unsigned long long *)address, old, __double_as_longlong(val))) == old)\n      break;\n  }\n  return __longlong_as_double(ret);\n}\n\n__global__ void sampleMultinomialOnce(\n    int* dest,\n    int distributions,\n    int categories,\n    const scalar_t*__restrict__ sampled,\n    const scalar_t*__restrict__ dist,\n    int stride_dist,\n    int stride_categories)\n{\n  __shared__ accscalar_t smem[GPU_NUM_THREADS];\n  __shared__ bool found;\n  __shared__ int foundPos;\n\n  typedef cub::BlockReduce<accscalar_t, GPU_NUM_THREADS> BlockReduce;\n  __shared__ typename BlockReduce::TempStorage temp_storage;\n\n  accscalar_t accZero = static_cast<accscalar_t>(0);\n  scalar_t zero = static_cast<scalar_t>(0);\n\n  for (int curDist = blockIdx.x; curDist < distributions; curDist += gridDim.x) {\n    // Each block handles one distribution\n    // First pass, find the total sum of the distribution\n    accscalar_t sum = accZero;\n    scalar_t val;\n    for (int cat = threadIdx.x; cat < categories; cat += blockDim.x) {\n      val = dist[curDist * stride_dist + cat * stride_categories];\n      sum += static_cast<accscalar_t>(val);\n    }\n\n    // threadIdx.x == 0 has the sum value from this\n    sum = BlockReduce(temp_storage).Sum(sum);\n\n    // Broadcast sum and sample value\n    if (threadIdx.x == 0) {\n      // Make sure the sum of our distribution didn't overflow\n      foundPos = 0;\n      smem[0] = sum;\n      smem[1] = sampled[curDist];\n    }\n    __syncthreads();\n\n    sum = smem[0];\n    scalar_t sample = static_cast<scalar_t>(smem[1]);\n    __syncthreads();\n\n    // zero sum\n    if (sum == accZero) {\n      // Choose the first element\n      if (threadIdx.x == 0) {\n        dest[curDist] = 0;\n      }\n      continue;\n    }\n\n    int chunks = (categories + (int)blockDim.x - 1) / blockDim.x;\n    accscalar_t prevHighProb = accZero;\n    found = false;\n\n    for (int chunk = 0; chunk < chunks && !found; ++chunk) {\n      // All threads in bounds load a value\n      int cat = chunk * blockDim.x + threadIdx.x;\n\n      accscalar_t dist_val = cat < categories ?\n                             static_cast<accscalar_t>(dist[curDist * stride_dist + cat * stride_categories]) / sum :\n                             accZero;\n\n      smem[threadIdx.x] = dist_val;\n      __syncthreads();\n\n      // Perform an inclusive prefix sum of the shared memory contents\n      for (int offset = 1; offset < blockDim.x; offset *= 2) {\n        accscalar_t val = accZero;\n\n        if (threadIdx.x >= offset) {\n          val = smem[threadIdx.x - offset] + smem[threadIdx.x];\n        }\n\n        __syncthreads();\n        if (threadIdx.x >= offset) {\n          smem[threadIdx.x] = val;\n        }\n        __syncthreads();\n      }\n\n      // Each thread will check to see if the sample falls in its bucket\n      scalar_t curBucket =\n          static_cast<scalar_t>(smem[threadIdx.x] + prevHighProb);\n      scalar_t prevBucket = static_cast<scalar_t>(\n          threadIdx.x == 0 ? prevHighProb\n                          : smem[threadIdx.x - 1] + prevHighProb);\n      bool inBucket =\n          (cat < categories) &&\n          (!(sample >= curBucket) &&\n          (sample >= prevBucket) &&\n          (dist_val > zero));\n\n      if (inBucket) {\n        atomicMax(&foundPos, cat);\n        found = true;\n      }\n\n      // Store the previous scan's high value for future use\n      prevHighProb = prevHighProb + smem[blockDim.x - 1];\n\n      __syncthreads();\n    }\n\n    if (threadIdx.x == 0) {\n      if (found) {\n          dest[curDist] = foundPos;\n      } else {\n        // This should address a rare bug where we don't select a valid index. This likely occurs when\n        // due to floating point arithmetic rounding errors, our cumulative sum does not add up to 1, but\n        // and our uniform sample is greater than this value. In this case we likely have unitialized memory\n        // in dest[curDist]. So basically we will loop through the distribution and pick the largest index\n        // where the distribution is non-zero. This is obviously terribly inefficient, but due to the\n        // rarity in which this occurs, this should not be an issue.\n        for (int cat = categories - 1; cat >= 0; --cat) {\n          if (dist[curDist * stride_dist + cat * stride_categories] > zero) {\n            dest[curDist] = cat;\n            break;\n          }\n        }\n      }\n    }\n  }\n}"
        ]
    },
    "score-cuda": {
        "/Users/gbolet/hecbench-roofline/src/score-cuda/main.cu": [
            "#define T ((int)32)\n\n\n__device__\nunsigned char clamp(float f)\n{\n  int i = (int)(f * 255.5f);\n\n  if (i < 0) i = 0;\n  if (i > 255) i = 255;\n\n  return (unsigned char)i;\n}\n\n__launch_bounds__(BLOCK_SIZE)\n__global__\nvoid findTopK(int*__restrict__ indices_, \n              int*__restrict__ count_, \n              const T*__restrict__ scores_,\n              const float threshold,\n              const int classwise_topK,\n              const int num_classes,\n              const int num_priors)\n{\n  /* We need to sort boxes based on their confidence scores. The confidence scores fall in\n   * the range [0.0, 1.0]. We break the range into bins and perform count sort. This is an\n   * approximate algorithm.\n   *\n   * Each block handles a particular class of a particular batch item.\n   */\n  const auto c = blockIdx.x;\n  const auto b = blockIdx.y;\n\n  // indices: [batch_size, num_classes, classwise_topK]\n  // count: [batch_size, num_classes]\n  // scores: [batch_size, num_classes, num_priors]\n\n  auto indices = indices_ + (b * num_classes + c) * classwise_topK;\n  auto count = count_ + b * num_classes + c;\n  auto scores = scores_ + (b * num_classes + c) * num_priors;\n\n  /* We do not require a large number of bins to find the top K confidence scores. We will use\n   * a reasonable number of bins which will fit in the shared memory.\n   *\n   * Note that smaller scores will have a smaller index, i.e. the `bins` are ordered in\n   * ascending order.\n   */\n\n  __shared__ int bins[BINS];\n\n  #pragma unroll\n  for (int unroll = 0; unroll < BINS / BLOCK_SIZE; unroll++)\n    bins[unroll * BLOCK_SIZE + threadIdx.x] = 0;\n\n  __syncthreads();\n\n  for (int i = threadIdx.x; i < num_priors; i = i + BLOCK_SIZE)\n  {\n    const float confidence = scores[i];\n    if (confidence > threshold)\n    {\n      float conf_scaled = __fdividef(confidence - threshold, 1.f - threshold);\n      int bin_index = conf_scaled * BINS;\n\n      /* We store counts of confidence scores in the bins. Our ultimate goal is to store the indices\n       * of the `classwise_topK` confidence values in the `indices` array.\n       *\n       * We use a little trick to parallelize the process of filling up the `indices` array.\n       * We want every thread in the block to participate in the process. To do so, we want the\n       * bins array to be shifted by one place to the left. We will be computing the suffix sum\n       * of the bins array later. Details and reasons for doing so will be explained later.\n       */\n      bin_index = clamp<int>(bin_index, 0, BINS - 1) - 1; // shift left by one\n\n      if (bin_index >= 0)\n        atomicAdd(&bins[bin_index], 1);\n    }\n  }\n\n  __syncthreads();\n\n  constexpr int WARP_SIZE = 32; /* must be equal to warpSize */\n\n  if (threadIdx.x < WARP_SIZE)\n  {\n    /* We can compute suffix sum of an array in groups of N numbers.\n     * Let N be 4 for this example.\n     *\n     * 1) Last 4 numbers\n     *                      1   2   3   4   |   5   6   7   8   |   9   10  11  12\n     * group suffix sum:                                            42  33  23  12\n     *\n     * 2) Middle 4 numbers\n     *                      1   2   3   4   |   5   6   7   8   |   9   10  11  12\n     * group suffix sum:                    |   26  21  15  8   |\n     *\n     * We add `42` (first element in the previous group) to each element to get:\n     *\n     *                      1   2   3   4   |   5   6   7   8   |   9   10  11  12\n     *                                      |   68  63  57  50  |   42  33  23  12\n     * 3) First 4 numbers\n     *\n     *                      1   2   3   4   |   5   6   7   8   |   9   10  11  12\n     * group suffix sum:    10  9   7   4   |\n     *\n     * We add `68` (first element in the previous group) to each element to get:\n     *\n     *                      1   2   3   4   |   5   6   7   8   |   9   10  11  12\n     * group suffix sum:    78  77  75  72  |   68  63  57  50  |   42  33  23  12\n     *\n     * What we are left with now is the suffix sum of the entire array.\n     *\n     * We use the aforementioned logic in the code below but work in groups of `warpSize`.\n     */\n\n    /* We calculate suffix sums WARP_SIZE elements at a time starting from the right end.\n     * Hence, we will need BINS / WARP_SIZE number of iterations.\n     *\n     * Each iteration uses shuffle instructions to exchange data between threads. Shuffle\n     * instructions cannot be used in warp-divergent code. If the bins are a multiple of\n     * the warpSize, all the threads in the warp will participate.\n     */\n    static_assert(BINS % WARP_SIZE == 0, \"number of bins must be a multiple of warp size\");\n\n    const int thread_id = threadIdx.x;\n    const int inverse_lane_id = WARP_SIZE - thread_id - 1;\n\n    int previous_group_first_element = 0;\n    for (int iter = BINS / WARP_SIZE - 1; iter >= 0; iter--)\n    {\n      const int idx = iter * WARP_SIZE + thread_id;\n      auto value = bins[idx];\n\n      for (int i = 1; i < WARP_SIZE; i *= 2)\n      {\n        auto n = __shfl_down_sync(0xFFFFFFFF, value, i);\n        if (inverse_lane_id >= i)\n          value += n;\n      }\n\n      value += previous_group_first_element;\n      bins[idx] = value;\n\n      previous_group_first_element = __shfl_sync(0xFFFFFFFF, value, 0);\n    }\n  }\n\n  if (threadIdx.x == 0) *count = 0;\n\n  __syncthreads();\n\n  for (int i = threadIdx.x; i < num_priors; i = i + BLOCK_SIZE)\n  {\n    const float confidence = scores[i];\n    if (confidence > threshold)\n    {\n      float conf_scaled = __fdividef(confidence - threshold, 1.f - threshold);\n      int bin_index = conf_scaled * BINS;\n      bin_index = clamp<int>(bin_index, 0, BINS - 1);\n\n      /* This bounding box is eligible to be selected unless it does not fall in\n       * the `classwise_topK`. If it did, we would have to compute the location where it needs\n       * to be stored.\n       *\n       * Suppose we had just 4 bins and say the following were the counts:\n       * BIN0 2\n       * BIN1 1\n       * BIN2 3\n       * BIN3 0 (last bin is always zero as we shift left by one while populating the bins)\n       *\n       * We will try our best to store the boxes in a sorted order in the `indices` array.\n       * This requires that the boxes in later bins (higher confidence scores) must be\n       * stored earlier.\n       *\n       * We compute the suffix sum of the array. This gives us:\n       * BIN0 6\n       * BIN1 4\n       * BIN2 3\n       * BIN3 0\n       *\n       * The bins now give us the location in the `indices` array from which the indices of the\n       * scores corresponding to that bin would be stored. We atomically increment the bin count\n       * everytime we store a box corresponding to that bin. Therefore, the value in the bins\n       * gives the index in the `indices` array where the next box corresponding to that bin  must\n       * be put.\n       */\n\n      const int idx = atomicAdd(&bins[bin_index], 1);\n      if (idx < classwise_topK)\n      {\n        indices[idx] = i;\n        atomicAdd(&count[0], 1);\n      }\n    }\n  }\n}"
        ]
    },
    "attentionMultiHead-cuda": {
        "/Users/gbolet/hecbench-roofline/src/attentionMultiHead-cuda/main.cu": [
            "#define T ((int)32)\n\n\ninline __host__ __device__ uint4 make_uint4(int4 a)\n{\n    return make_uint4(uint(a.x), uint(a.y), uint(a.z), uint(a.w));\n}\n\ninline __host__ __device__ uint4 max(uint4 a, uint4 b)\n{\n    return make_uint4(max(a.x,b.x), max(a.y,b.y), max(a.z,b.z), max(a.w,b.w));\n}\n\n__inline__ __device__ T warpReduceMax(T val)\n{\n  #pragma unroll\n  for (int mask = 16; mask > 0; mask >>= 1)\n    val = max(val, __shfl_xor_sync(FINAL_MASK, val, mask, 32));\n  return val;\n}\n\n__inline__ __device__ T warpReduceSum(T val)\n{\n#pragma unroll\n  for (int mask = 16; mask > 0; mask >>= 1)\n    val += __shfl_xor_sync(FINAL_MASK, val, mask, 32);\n  return val;\n}\n\ninline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\ninline __host__ __device__ float4 expf(float4 v)\n{\n    return make_float4(expf(v.x), expf(v.y), expf(v.z), expf(v.w));\n}\n\n__inline__ __device__ T blockReduceMax(T val)\n{\n  static __shared__ T shared[32];\n  int lane = threadIdx.x & 0x1f;  // in-warp idx\n  int wid = threadIdx.x >> 5;     // warp idx\n\n  val = warpReduceMax(val);  // get maxx in each warp\n\n  if (lane == 0)  // record in-warp maxx by warp Idx\n    shared[wid] = val;\n\n  __syncthreads();\n\n  // Modify from blockDim.x << 5 to blockDim.x / 32. to prevent\n  // blockDim.x is not divided by 32\n  val = (threadIdx.x < (blockDim.x / 32.f)) ? shared[lane] : -1e20f;\n  val = warpReduceMax(val);\n\n  return val;\n}\n\n__inline__ __device__ T blockReduceSum(T val)\n{\n  static __shared__ T shared[32];\n  int lane = threadIdx.x & 0x1f;\n  int wid = threadIdx.x >> 5;\n\n  val = warpReduceSum<T>(val);\n\n  if (lane == 0)\n    shared[wid] = val;\n\n  __syncthreads();\n\n  // Modify from blockDim.x << 5 to blockDim.x / 32. to prevent\n  // blockDim.x is not divided by 32\n  val = (threadIdx.x < (blockDim.x / 32.f)) ? shared[lane] : (T)(0.0f);\n  val = warpReduceSum<T>(val);\n\n  return val;\n}\n\n__global__\nvoid mha (\n   const float *__restrict__ q, \n   const float *__restrict__ k, \n   const float *__restrict__ v, \n   const int beam_size, \n   const int n_steps, \n   const int qk_col, \n   const int v_col, \n   const int nhead, \n   const float scale,\n   const int THRESHOLD,\n   float *__restrict__ dst)\n{\n  /* \n     Each block processes one head from one candidate.\n\n     dim_per_head is the size of partition processed by each head.\n\n     candidate_id is the index of candidate processed by this block. \n     We have beam_size candidates in total.\n\n     head_id is the index of head processed by this block.\n\n   */\n  int dim_per_head = qk_col / nhead;\n  int candidate_id = blockIdx.x / nhead;\n  int head_id = blockIdx.x % nhead;\n\n  /*\n     sq is the query vector shared by all threads inside the same block.\n\n     The size of sq should be dim_per_head.\n\n     Each block only load the a part of the query vector that belongs to the corresponding candidate.\n   */\n  extern __shared__ float buffer[];\n  float *sq = buffer;\n  //float *logits = (float*)&(buffer[dim_per_head]);\n  float *logits = buffer + dim_per_head;\n\n\n  // pos is the start position of the corresponding query matrix prococessed by this block.\n  int pos = candidate_id * qk_col + head_id * dim_per_head + threadIdx.x;\n  if(threadIdx.x < dim_per_head) sq[threadIdx.x] = q[pos];\n  __syncthreads();\n\n  // calculate the correlation between the query and key QK^T/sqrt(d_k)\n\n  float summ = 0.f;\n  if(threadIdx.x < n_steps)\n  {   \n    const float *k2 = k + candidate_id * qk_col * n_steps + head_id * dim_per_head + threadIdx.x * qk_col;\n    for (int i = 0; i < dim_per_head; i++)\n      summ += sq[i] * k2[i];\n    summ *= scale;\n  }   \n\n  // calculate the softmax value of the first step softmax(QK^T/sqrt(d_k)) using warp shuffle.\n\n  __shared__ float s_max_val;\n  __shared__ float s_sum;\n\n  float local_i = threadIdx.x < n_steps ? summ : -1e-20f;\n  float local_o;\n\n  float max_val = blockReduceMax(local_i);\n\n  if(threadIdx.x == 0)\n    s_max_val = max_val;\n  __syncthreads();\n\n  local_i -= s_max_val;\n\n  if(local_i < -THRESHOLD) local_i = -THRESHOLD;\n\n  local_o = expf(local_i);\n\n  float val = (threadIdx.x < n_steps) ? local_o : 0.f;\n  val = blockReduceSum(val);\n  if(threadIdx.x == 0) s_sum = val;\n  __syncthreads();\n\n  if(threadIdx.x < n_steps) logits[threadIdx.x] = local_o / s_sum;\n  __syncthreads();\n\n  // calculate the weighted sum on value matrix V softmax(QK^T/sqrt(d_k))V \n  summ = 0.f;\n  if(threadIdx.x < dim_per_head)\n  {\n    int tid = candidate_id * v_col * n_steps + head_id * dim_per_head + threadIdx.x;\n    for(int i = 0; i < n_steps; ++i)\n      summ += logits[i] * v[tid + i * v_col];\n    dst[candidate_id * v_col + head_id * dim_per_head + threadIdx.x] = summ;\n  }\n}"
        ]
    },
    "shmembench-cuda": {
        "/Users/gbolet/hecbench-roofline/src/shmembench-cuda/shmem_kernels.cu": [
            "inline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\n__device__ float4 init_val(int i){\n  return make_float4(i, i+11, i+19, i+23);\n}\n\n__device__ float4 reduce_vector(float4 v1, float4 v2, float4 v3, float4 v4, float4 v5, float4 v6){\n  return make_float4(v1.x + v2.x + v3.x + v4.x + v5.x + v6.x, \n                     v1.y + v2.y + v3.y + v4.y + v5.y + v6.y,\n                     v1.z + v2.z + v3.z + v4.z + v5.z + v6.z,\n                     v1.w + v2.w + v3.w + v4.w + v5.w + v6.w);\n}\n\n__device__ void set_vector(float4 *target, int offset, float4 v){\n  target[offset].x = v.x;\n  target[offset].y = v.y;\n  target[offset].z = v.z;\n  target[offset].w = v.w;\n}\n\n__device__ void shmem_swap(float4 *v1, float4 *v2){\n  float4 tmp;\n  tmp = *v2;\n  *v2 = *v1;\n  *v1 = tmp;\n}\n\n__global__ void benchmark_shmem(float4 *g_data){\n\n  __shared__ float4 shm_buffer[BLOCK_SIZE*6];\n\n  int tid = threadIdx.x; \n  int globaltid = blockIdx.x*blockDim.x + tid;\n  set_vector(shm_buffer, tid+0*blockDim.x, init_val(tid));\n  set_vector(shm_buffer, tid+1*blockDim.x, init_val(tid+1));\n  set_vector(shm_buffer, tid+2*blockDim.x, init_val(tid+3));\n  set_vector(shm_buffer, tid+3*blockDim.x, init_val(tid+7));\n  set_vector(shm_buffer, tid+4*blockDim.x, init_val(tid+13));\n  set_vector(shm_buffer, tid+5*blockDim.x, init_val(tid+17));\n\n  __syncthreads();  // __threadfence_block() is faster though\n\n  #pragma unroll 32\n  for(int j=0; j<TOTAL_ITERATIONS; j++){\n    shmem_swap(shm_buffer+tid+0*blockDim.x, shm_buffer+tid+1*blockDim.x);\n    shmem_swap(shm_buffer+tid+2*blockDim.x, shm_buffer+tid+3*blockDim.x);\n    shmem_swap(shm_buffer+tid+4*blockDim.x, shm_buffer+tid+5*blockDim.x);\n\n    __syncthreads();\n\n    shmem_swap(shm_buffer+tid+1*blockDim.x, shm_buffer+tid+2*blockDim.x);\n    shmem_swap(shm_buffer+tid+3*blockDim.x, shm_buffer+tid+4*blockDim.x);\n\n    __syncthreads();\n  }\n\n  g_data[globaltid] = reduce_vector(shm_buffer[tid+0*blockDim.x], \n                                    shm_buffer[tid+1*blockDim.x],\n                                    shm_buffer[tid+2*blockDim.x],\n                                    shm_buffer[tid+3*blockDim.x],\n                                    shm_buffer[tid+4*blockDim.x],\n                                    shm_buffer[tid+5*blockDim.x]);\n}"
        ]
    },
    "logan-cuda": {
        "/Users/gbolet/hecbench-roofline/src/logan-cuda/src/logan_functions.cu": [
            "#define seed 0.1f\n\n\nTx_\n__device__ __host__ max_logan(const Tx_& _Left, const Ty_& Right_)\n{   // return smaller of _Left and Right_\n  return (Right_ < _Left ? _Left : Right_);\n}\n\n__inline__ __device__ void warpReduce(volatile short *input, int myTId)\n{\n  input[myTId] = (input[myTId] > input[myTId + 32]) ? input[myTId] : input[myTId + 32]; \n  input[myTId] = (input[myTId] > input[myTId + 16]) ? input[myTId] : input[myTId + 16];\n  input[myTId] = (input[myTId] > input[myTId + 8]) ? input[myTId] : input[myTId + 8]; \n  input[myTId] = (input[myTId] > input[myTId + 4]) ? input[myTId] : input[myTId + 4];\n  input[myTId] = (input[myTId] > input[myTId + 2]) ? input[myTId] : input[myTId + 2];\n  input[myTId] = (input[myTId] > input[myTId + 1]) ? input[myTId] : input[myTId + 1];\n}\n\nint\n__device__ getLowerDiagonal(SeedL const &myseed){\n  return myseed.lowerDiagonal;\n}\n\nint\n__device__ getUpperDiagonal(SeedL const &myseed){\n  return myseed.upperDiagonal;\n}\n\nvoid\n__device__ __host__ setLowerDiagonal(SeedL &myseed,int const value){\n  myseed.lowerDiagonal = value;\n}\n\nvoid\n__device__ __host__ setUpperDiagonal(SeedL &myseed,int const value){\n  myseed.upperDiagonal = value;\n}\n\n__inline__ __device__ void calcExtendedLowerDiag(int &lowerDiag,\n    int const &minCol,\n    int const &antiDiagNo)\n{\n  int minRow = antiDiagNo - minCol;\n  if (minCol - minRow < lowerDiag)\n    lowerDiag = minCol - minRow;\n}\n\n__inline__ __device__ void calcExtendedUpperDiag(\n    int &upperDiag,\n    int const &maxCol,\n    int const &antiDiagNo)\n{\n  int maxRow = antiDiagNo + 1 - maxCol;\n  if (maxCol - 1 - maxRow > upperDiag)\n    upperDiag = maxCol - 1 - maxRow;\n}\n\n__inline__ __device__ void computeAntidiag(\n    const short *antiDiag1,\n    const short *antiDiag2,\n          short *antiDiag3,\n    const char* querySeg,\n    const char* databaseSeg,\n    const int best,\n    const int scoreDropOff,\n    const int cols,\n    const int rows,\n    const int minCol,\n    const int maxCol,\n    const int antiDiagNo,\n    const int offset1,\n    const int offset2,\n    const ExtensionDirectionL direction,\n    int n_threads)\n{\n  int tid = threadIdx.x;\n\n  for(int i = 0; i < maxCol; i+=n_threads){\n\n    int col = tid + minCol + i;\n    int queryPos, dbPos;\n\n    queryPos = col - 1;\n    dbPos = col + rows - antiDiagNo - 1;\n\n    if(col < maxCol){\n\n      int tmp = max_logan(antiDiag2[col-offset2],antiDiag2[col-offset2-1]) + GAP_EXT;\n\n      int score = (querySeg[queryPos] == databaseSeg[dbPos]) ? MATCH : MISMATCH;\n\n      tmp = max_logan(antiDiag1[col-offset1-1]+score,tmp);\n\n      antiDiag3[tid+1+i] = (tmp < best - scoreDropOff) ? UNDEF : tmp;\n\n    }\n  }\n}\n\n__inline__ __device__ void initAntiDiag3(\n    short *antiDiag3,\n    int &a3size,\n    int const &offset,\n    int const &maxCol,\n    int const &antiDiagNo,\n    int const &minScore,\n    int const &gapCost,\n    int const &undefined)\n{\n  a3size = maxCol + 1 - offset;\n\n  antiDiag3[0] = undefined;\n  antiDiag3[maxCol - offset] = undefined;\n\n  if (antiDiagNo * gapCost > minScore)\n  {\n    if (offset == 0) // init first column\n      antiDiag3[0] = antiDiagNo * gapCost;\n    if (antiDiagNo - maxCol == 0) // init first row\n      antiDiag3[maxCol - offset] = antiDiagNo * gapCost;\n  }\n}\n\n__inline__ __device__ void initAntiDiags(\n    short *antiDiag1,\n    short *antiDiag2,\n    short *antiDiag3,\n    int &a2size,\n    int &a3size,\n    int const &dropOff,\n    int const &gapCost,\n    int const &undefined)\n{\n  a2size = 1;\n\n  antiDiag2[0] = 0;\n\n  a3size = 2;\n\n  antiDiag3[0] = gapCost;\n  antiDiag3[1] = gapCost;\n}\n\n__inline__ __device__ short reduce_max(short *input, int dim, int n_threads)\n{\n  unsigned int myTId = threadIdx.x;   \n  if(dim>32){\n    for(int i = n_threads/2; i >32; i>>=1){\n      if(myTId < i){\n        input[myTId] = (input[myTId] > input[myTId + i]) ? input[myTId] : input[myTId + i];\n      }\n      __syncthreads();\n    }\n  }\n  if(myTId<32)\n    warpReduce(input, myTId);\n  __syncthreads();\n  return input[0];\n}\n\n__inline__ __device__ void updateExtendedSeedL(\n    SeedL &seed,\n    ExtensionDirectionL direction, //as there are only 4 directions we may consider even smaller data types\n    int cols,\n    int rows,\n    int lowerDiag,\n    int upperDiag)\n{\n  if (direction == EXTEND_LEFTL)\n  {\n    int beginDiag = seed.beginDiagonal;\n    // Set lower and upper diagonals.\n\n    if (getLowerDiagonal(seed) > beginDiag + lowerDiag)\n      setLowerDiagonal(seed, beginDiag + lowerDiag);\n    if (getUpperDiagonal(seed) < beginDiag + upperDiag)\n      setUpperDiagonal(seed, beginDiag + upperDiag);\n\n    // Set new start position of seed.\n    seed.beginPositionH -= rows;\n    seed.beginPositionV -= cols;\n  } else {  // direction == EXTEND_RIGHTL\n    // Set new lower and upper diagonals.\n    int endDiag = seed.endDiagonal;\n    if (getUpperDiagonal(seed) < endDiag - lowerDiag)\n      setUpperDiagonal(seed, (endDiag - lowerDiag));\n    if (getLowerDiagonal(seed) > (endDiag - upperDiag))\n      setLowerDiagonal(seed, endDiag - upperDiag);\n\n    // Set new end position of seed.\n    seed.endPositionH += rows;\n    seed.endPositionV += cols;\n\n  }\n}\n\n__global__ void extendSeedLGappedXDropOneDirectionGlobal(\n    SeedL *__restrict__ seed,\n    const char *__restrict__ querySegArray,\n    const char *__restrict__ databaseSegArray,\n    const ExtensionDirectionL direction,\n    const int scoreDropOff,\n    int *__restrict__ res,\n    const int *__restrict__ offsetQuery,\n    const int *__restrict__ offsetTarget,\n    const int offAntidiag,\n    short *__restrict__ antidiag,\n    const int n_threads)\n{\n  extern __shared__ short temp_alloc[];\n  short *temp= &temp_alloc[0];\n\n  int myId = blockIdx.x;\n  int myTId = threadIdx.x;\n  const char *querySeg;\n  const char *databaseSeg;\n\n  if(myId==0){\n    querySeg = querySegArray;\n    databaseSeg = databaseSegArray;\n  }\n  else{\n    querySeg = querySegArray + offsetQuery[myId-1];\n    databaseSeg = databaseSegArray + offsetTarget[myId-1];\n  }\n\n  short *antiDiag1 = &antidiag[myId*offAntidiag*3]; \n  short* antiDiag2 = &antiDiag1[offAntidiag];\n  short* antiDiag3 = &antiDiag2[offAntidiag];\n\n  SeedL mySeed(seed[myId]);  \n  //dimension of the antidiagonals\n  int a1size = 0, a2size = 0, a3size = 0;\n  int cols, rows;\n\n  if(myId == 0){\n    cols = offsetQuery[myId]+1;\n    rows = offsetTarget[myId]+1;\n  }\n  else{\n    cols = offsetQuery[myId]-offsetQuery[myId-1]+1;\n    rows = offsetTarget[myId]-offsetTarget[myId-1]+1;\n  }\n\n  if (rows == 1 || cols == 1) return;\n\n  int minCol = 1;\n  int maxCol = 2;\n\n  int offset1 = 0; // number of leading columns that need not be calculated in antiDiag1\n  int offset2 = 0; //                                                       in antiDiag2\n  int offset3 = 0; //                                                       in antiDiag3\n\n  initAntiDiags(antiDiag1,antiDiag2, antiDiag3, a2size, a3size, scoreDropOff, GAP_EXT, UNDEF);\n  int antiDiagNo = 1; // the currently calculated anti-diagonal\n\n  int best = 0; // maximal score value in the DP matrix (for drop-off calculation)\n\n  int lowerDiag = 0;\n  int upperDiag = 0;\n\n  while (minCol < maxCol)\n  {  \n    ++antiDiagNo;\n\n    //antidiagswap\n    //antiDiag2 -> antiDiag1\n    //antiDiag3 -> antiDiag2\n    //antiDiag1 -> antiDiag3\n    short *t = antiDiag1;\n    antiDiag1 = antiDiag2;\n    antiDiag2 = antiDiag3;\n    antiDiag3 = t;\n    int t_l = a1size;\n    a1size = a2size;\n    a2size = a3size;\n    a3size = t_l;\n    offset1 = offset2;\n    offset2 = offset3;\n    offset3 = minCol-1;\n\n    initAntiDiag3(antiDiag3, a3size, offset3, maxCol, antiDiagNo, best - scoreDropOff, GAP_EXT, UNDEF);\n\n    computeAntidiag(antiDiag1, antiDiag2, antiDiag3, querySeg, databaseSeg,\n                    best, scoreDropOff, cols, rows, minCol, maxCol, antiDiagNo,\n                    offset1, offset2, direction, n_threads);     \n    //roofline analysis\n    __syncthreads();  \n\n    int tmp, antiDiagBest = UNDEF;  \n    for(int i=0; i<a3size; i+=n_threads){\n      int size = a3size-i;\n\n      if(myTId<n_threads){\n        temp[myTId] = (myTId<size) ? antiDiag3[myTId+i]:UNDEF;        \n      }\n      __syncthreads();\n\n      tmp = reduce_max(temp,size, n_threads);\n      antiDiagBest = (tmp>antiDiagBest) ? tmp:antiDiagBest;\n\n    }\n    best = (best > antiDiagBest) ? best : antiDiagBest;\n\n    while (minCol - offset3 < a3size && antiDiag3[minCol - offset3] == UNDEF &&\n        minCol - offset2 - 1 < a2size && antiDiag2[minCol - offset2 - 1] == UNDEF)\n    {\n      ++minCol;\n    }\n\n    // Calculate new maxCol\n    while (maxCol - offset3 > 0 && (antiDiag3[maxCol - offset3 - 1] == UNDEF) &&\n        (antiDiag2[maxCol - offset2 - 1] == UNDEF))\n    {\n      --maxCol;\n    }\n    ++maxCol;\n\n    // Calculate new lowerDiag and upperDiag of extended seed\n    calcExtendedLowerDiag(lowerDiag, minCol, antiDiagNo);\n    calcExtendedUpperDiag(upperDiag, maxCol - 1, antiDiagNo);\n\n    // end of databaseSeg reached?\n    minCol = (minCol > (antiDiagNo + 2 - rows)) ? minCol : (antiDiagNo + 2 - rows);\n    // end of querySeg reached?\n    maxCol = (maxCol < cols) ? maxCol : cols;\n  }\n\n  int longestExtensionCol = a3size + offset3 - 2;\n  int longestExtensionRow = antiDiagNo - longestExtensionCol;\n  int longestExtensionScore = antiDiag3[longestExtensionCol - offset3];\n  \n  if (longestExtensionScore == UNDEF)\n  {\n    if (antiDiag2[a2size -2] != UNDEF)\n    {\n      // reached end of query segment\n      longestExtensionCol = a2size + offset2 - 2;\n      longestExtensionRow = antiDiagNo - 1 - longestExtensionCol;\n      longestExtensionScore = antiDiag2[longestExtensionCol - offset2];\n    }\n    else if (a2size > 2 && antiDiag2[a2size-3] != UNDEF)\n    {\n      // reached end of database segment\n      longestExtensionCol = a2size + offset2 - 3;\n      longestExtensionRow = antiDiagNo - 1 - longestExtensionCol;\n      longestExtensionScore = antiDiag2[longestExtensionCol - offset2];\n    }\n  }\n  \n  if (longestExtensionScore == UNDEF){\n  \n    // general case\n    for (int i = 0; i < a1size; ++i){\n  \n      if (antiDiag1[i] > longestExtensionScore){\n  \n        longestExtensionScore = antiDiag1[i];\n        longestExtensionCol = i + offset1;\n        longestExtensionRow = antiDiagNo - 2 - longestExtensionCol;\n  \n      }\n    }\n  }\n  \n  if (longestExtensionScore != UNDEF)\n    updateExtendedSeedL(mySeed, direction, longestExtensionCol, longestExtensionRow, lowerDiag, upperDiag);\n\n  seed[myId] = mySeed;\n  res[myId] = longestExtensionScore;\n}"
        ]
    },
    "affine-cuda": {
        "/Users/gbolet/hecbench-roofline/src/affine-cuda/kernel.h": [
            "inline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\ninline __host__ __device__ float4 floorf(float4 v)\n{\n    return make_float4(floorf(v.x), floorf(v.y), floorf(v.z), floorf(v.w));\n}\n\n__global__ void affine (\n  const unsigned short *__restrict__ src,\n        unsigned short *__restrict__ dst) \n{\n  int x = blockIdx.x*blockDim.x+threadIdx.x;\n  int y = blockIdx.y*blockDim.y+threadIdx.y;\n\n  const float lx_rot   = 30.0f;\n  const float ly_rot   = 0.0f; \n  const float lx_expan = 0.5f;\n  const float ly_expan = 0.5f; \n  int   lx_move  = 0;\n  int   ly_move  = 0;\n  float affine[2][2];   // coefficients\n  float i_affine[2][2];\n  float beta[2];\n  float i_beta[2];\n  float det;\n  float x_new, y_new;\n  float x_frac, y_frac;\n  float gray_new;\n  int   m, n;\n  unsigned short output_buffer;\n\n  // forward affine transformation \n  affine[0][0] = lx_expan * cosf(lx_rot*PI/180.0f);\n  affine[0][1] = ly_expan * sinf(ly_rot*PI/180.0f);\n  affine[1][0] = lx_expan * sinf(lx_rot*PI/180.0f);\n  affine[1][1] = ly_expan * cosf(ly_rot*PI/180.0f);\n  beta[0]      = lx_move;\n  beta[1]      = ly_move;\n\n  // determination of inverse affine transformation\n  det = (affine[0][0] * affine[1][1]) - (affine[0][1] * affine[1][0]);\n  if (det == 0.0f)\n  {\n    i_affine[0][0] = 1.0f;\n    i_affine[0][1] = 0.0f;\n    i_affine[1][0] = 0.0f;\n    i_affine[1][1] = 1.0f;\n    i_beta[0]      = -beta[0];\n    i_beta[1]      = -beta[1];\n  } \n  else \n  {\n    i_affine[0][0] =  affine[1][1]/det;\n    i_affine[0][1] = -affine[0][1]/det;\n    i_affine[1][0] = -affine[1][0]/det;\n    i_affine[1][1] =  affine[0][0]/det;\n    i_beta[0]      = -i_affine[0][0]*beta[0]-i_affine[0][1]*beta[1];\n    i_beta[1]      = -i_affine[1][0]*beta[0]-i_affine[1][1]*beta[1];\n  }\n\n  // Output image generation by inverse affine transformation and bilinear transformation\n\n  x_new  = i_beta[0] + i_affine[0][0]*(x-X_SIZE/2.0f) + i_affine[0][1]*(y-Y_SIZE/2.0f) + X_SIZE/2.0f;\n  y_new  = i_beta[1] + i_affine[1][0]*(x-X_SIZE/2.0f) + i_affine[1][1]*(y-Y_SIZE/2.0f) + Y_SIZE/2.0f;\n\n  m      = (int)floorf(x_new);\n  n      = (int)floorf(y_new);\n\n  x_frac = x_new - m;\n  y_frac = y_new - n;\n\n  if ((m >= 0) && (m + 1 < X_SIZE) && (n >= 0) && (n+1 < Y_SIZE))\n  {\n    gray_new = (1.0f - y_frac) * ((1.0f - x_frac) * (src[(n * X_SIZE) + m]) + x_frac * (src[(n * X_SIZE) + m + 1])) + \n      y_frac  * ((1.0f - x_frac) * (src[((n + 1) * X_SIZE) + m]) + x_frac * (src[((n + 1) * X_SIZE) + m + 1]));\n\n    output_buffer = (unsigned short)gray_new;\n  } \n  else if (((m + 1 == X_SIZE) && (n >= 0) && (n < Y_SIZE)) || ((n + 1 == Y_SIZE) && (m >= 0) && (m < X_SIZE))) \n  {\n    output_buffer = src[(n * X_SIZE) + m];\n  } \n  else \n  {\n    output_buffer = WHITE;\n  }\n\n  dst[(y * X_SIZE)+x] = output_buffer;\n}"
        ]
    },
    "stddev-cuda": {
        "/Users/gbolet/hecbench-roofline/src/stddev-cuda/main.cu": [
            "inline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\ninline __host__ __device__ float4 sqrtf(float4 v)\n{\n    return make_float4(sqrtf(v.x), sqrtf(v.y), sqrtf(v.z), sqrtf(v.w));\n}\n\n__global__ void sampleKernel (Type *std, IdxType D, IdxType N) {\n  IdxType i = blockDim.x * blockIdx.x + threadIdx.x;\n  if (i < D) std[i] = sqrtf(std[i] / N);\n}",
            "__global__ void sopKernel(\n        Type *__restrict__ std, \n  const Type *__restrict__ data, \n  IdxType D, \n  IdxType N) \n{\n  const int RowsPerBlkPerIter = TPB / ColsPerBlk;\n  IdxType thisColId = threadIdx.x % ColsPerBlk;\n  IdxType thisRowId = threadIdx.x / ColsPerBlk;\n  IdxType colId = thisColId + ((IdxType)blockIdx.y * ColsPerBlk);\n  IdxType rowId = thisRowId + ((IdxType)blockIdx.x * RowsPerBlkPerIter);\n  Type thread_data = Type(0);\n  const IdxType stride = RowsPerBlkPerIter * gridDim.x;\n  for (IdxType i = rowId; i < N; i += stride) {\n    Type val = (colId < D) ? data[i * D + colId] : Type(0);\n    thread_data += val * val;\n  }\n  __shared__ Type sstd[ColsPerBlk];\n  if (threadIdx.x < ColsPerBlk) sstd[threadIdx.x] = Type(0);\n  __syncthreads();\n\n  atomicAdd(sstd + thisColId, thread_data);\n  __syncthreads();\n\n  if (threadIdx.x < ColsPerBlk) atomicAdd(std + colId, sstd[thisColId]);\n}"
        ]
    },
    "pns-cuda": {
        "/Users/gbolet/hecbench-roofline/src/pns-cuda/petri_kernel.cu": [
            "__device__ __shared__ uint32 mt[MERS_N];\n\n__device__\nvoid RandomInit(uint32 seed) \n{\n  int i;\n  // re-seed generator\n  if(threadIdx.x == 0)\n    {\n      mt[0]= seed & 0xffffffffUL;\n      for (i=1; i < MERS_N; i++) \n\t{\n\t  mt[i] = (1812433253UL * (mt[i-1] ^ (mt[i-1] >> 30)) + i);\n  \t}\n    }\n  __syncthreads();\n}\n\n__device__ \nvoid BRandom() \n{\n  // generate 32 random bits\n  uint32 y;\n  int thdx;\n\n  // block size is 256\n  // step 1: 0-226, MERS_N-MERS_M=227\n  if (threadIdx.x<MERS_N-MERS_M) \n    {\n      y = (mt[threadIdx.x] & UPPER_MASK) | (mt[threadIdx.x+1] & LOWER_MASK);\n      y = mt[threadIdx.x+MERS_M] ^ (y >> 1) ^ ( (y & 1)? MERS_A: 0);\n    }\n  __syncthreads();\n  if (threadIdx.x<MERS_N-MERS_M) \n    {\n      mt[threadIdx.x] = y;\n    }\n  __syncthreads();\n  \n  // step 2: 227-453\n  thdx = threadIdx.x + (MERS_N-MERS_M);\n  if (threadIdx.x<MERS_N-MERS_M) \n    {\n      y = (mt[thdx] & UPPER_MASK) | (mt[thdx+1] & LOWER_MASK);\n      y = mt[threadIdx.x] ^ (y >> 1) ^ ( (y & 1)? MERS_A: 0);\n    }\n  __syncthreads();\n  if (threadIdx.x<MERS_N-MERS_M) \n    {\n      mt[thdx] = y;\n    }\n  __syncthreads();\n  \n  // step 3: 454-622\n  thdx += (MERS_N-MERS_M);\n  if (thdx < MERS_N-1) \n    {\n      y = (mt[thdx] & UPPER_MASK) | (mt[thdx+1] & LOWER_MASK);\n      y = mt[threadIdx.x+(MERS_N-MERS_M)] ^ (y >> 1) ^ ( (y & 1)? MERS_A: 0);\n    }\n  __syncthreads();\n  if (thdx < MERS_N-1) \n    {\n      mt[thdx] = y;\n    }\n  __syncthreads();\n\n  // step 4: 623\n  if (threadIdx.x == 0) \n    {\n      y = (mt[MERS_N-1] & UPPER_MASK) | (mt[0] & LOWER_MASK);\n      mt[MERS_N-1] = mt[MERS_M-1] ^ (y >> 1) ^ ( (y & 1)? MERS_A: 0);\n    }\n  __syncthreads();\n\n  // Tempering (May be omitted):\n  y ^=  y >> MERS_U;\n  y ^= (y << MERS_S) & MERS_B;\n  y ^= (y << MERS_T) & MERS_C;\n  y ^=  y >> MERS_L;\n\n}\n\n__device__ \nvoid fire_transition(char* g_places, int* conflict_array, int tr, \n\t\t     int tc, int step, int N, int thd_thrd) \n{\n  int val1, val2, val3, to_update;\n  int mark1, mark2;\n\t\n  to_update = 0;\n  if (threadIdx.x<thd_thrd) \n    {\n      // check if the transition is enabled and conflict-free\n      val1 = (tr==0)? (N+N)-1: tr-1;\n      val2 = (tr & 0x1)? (tc==N-1? 0: tc+1): tc;\n      val3 = (tr==(N+N)-1)? 0: tr+1;\n      mark1 = g_places[val1*N+val2];\n      mark2 = g_places[tr*N+tc];\n      if ( (mark1>0) && (mark2>0) ) \n\t{\n\t  to_update = 1;\n\t  conflict_array[tr*N+tc] = step;\n\t}\n    }\n  __syncthreads();\n\n  if (to_update) \n    {\n      // If there are conflicts, transitions on even/odd rows are \n      // kept when the step is even/odd\n      to_update = ((step & 0x01) == (tr & 0x01) ) || \n\t( (conflict_array[val1*N+val2]!=step) && \n\t  (conflict_array[val3*N+((val2==0)? N-1: val2-1)]!=step) );\n    }\n\n  // now update state\n  // 6 kernel memory accesses \n  if (to_update) \n    {\n      g_places[val1*N+val2] = mark1-1;  // the place above\n      g_places[tr*N+tc] = mark2-1; // the place on the left\n    }\n  __syncthreads();\n  if (to_update) \n    {\n      g_places[val3*N+val2]++;  // the place below\n      g_places[tr*N+(tc==N-1? 0: tc+1)]++; // the place on the right\n    }\n  __syncthreads();\n}\n\n__device__ \nvoid compute_reward_stat(int *__restrict__ g_places,\n                         float* __restrict__ g_vars,\n                         int* __restrict__ g_maxs, \n\t\t\t int NSQUARE2) \n{\n  float sum = 0;\n  int i;\n  int max = 0;\n  int temp, data; \n  int loop_num = NSQUARE2 >> (BLOCK_SIZE_BITS+2);\n  for (i=0; i<=loop_num-1; i++) \n    {  // a bug. i<loop_num should be changed to i<=loop_num-1\n      data = g_places[threadIdx.x+(i<<BLOCK_SIZE_BITS)];\n\t    \n      temp = data & 0x0FF;\n      sum += temp*temp;\n      max = max<temp? temp: max;\n      temp = (data>>8) & 0x0FF;\n      sum += temp*temp;\n      max = max<temp? temp: max;\n      temp = (data>>16) & 0x0FF;\n      sum += temp*temp;\n      max = max<temp? temp: max;\n      temp = (data>>24) & 0x0FF;\n      sum += temp*temp;\n      max = max<temp? temp: max;\n    }\n\n  i = NSQUARE2>>2;\n  i &= 0x0FF;\n  loop_num *= BLOCK_SIZE; \n  // I do not know why loop_num<<=BLOCK_SIZE_BITS does not work\n  if (threadIdx.x <= i-1) \n    {\n      data = g_places[threadIdx.x+loop_num];\n\t    \n      temp = data & 0x0FF;\n      sum += temp*temp;\n      max = max<temp? temp: max;\n      temp = (data>>8) & 0x0FF;\n      sum += temp*temp;\n      max = max<temp? temp: max;\n      temp = (data>>16) & 0x0FF;\n      sum += temp*temp;\n      max = max<temp? temp: max;\n      temp = (data>>24) & 0x0FF;\n      sum += temp*temp;\n      max = max<temp? temp: max;\n    }\n\t\n  ((float*)mt)[threadIdx.x] = (float)sum;\n  mt[threadIdx.x+BLOCK_SIZE] = (uint32)max;\n  __syncthreads();\n\t\t\n  for (i=(BLOCK_SIZE>>1); i>0; i = (i>>1) ) \n    {\n      if (threadIdx.x<i) \n\t{\n\t  ((float*)mt)[threadIdx.x] += ((float*)mt)[threadIdx.x+i];\n\t  if (mt[threadIdx.x+BLOCK_SIZE]<mt[threadIdx.x+i+BLOCK_SIZE])\n\t    mt[threadIdx.x+BLOCK_SIZE] = mt[threadIdx.x+i+BLOCK_SIZE];\n\t}\n      __syncthreads();\n    }\n\t\t\n  if (threadIdx.x==0) \n    {\n      g_vars[blockIdx.x] = (((float*)mt)[0])/NSQUARE2-1; \n      // D(X)=E(X^2)-E(X)^2, E(X)=1\n      g_maxs[blockIdx.x] = (int)mt[BLOCK_SIZE];\n    }\n}\n\n__device__ \nvoid initialize_grid(int* g_places, int NSQUARE2, int seed) \n{\n  // N is an even number\n  int i;\n  int loop_num = NSQUARE2 >> (BLOCK_SIZE_BITS+2);\n\t\n  for (i=0; i<loop_num; i++) \n    {\n      g_places[threadIdx.x+(i<<BLOCK_SIZE_BITS)] = 0x01010101;\n    }\n    \n  if (threadIdx.x < (NSQUARE2>>2)-(loop_num<<BLOCK_SIZE_BITS)) \n    {\n      g_places[threadIdx.x+(loop_num<<BLOCK_SIZE_BITS)] = 0x01010101;\n    }\n\t\n  RandomInit(blockIdx.x+seed);\n}\n\n__device__ \nvoid run_trajectory(int* g_places, int N, int max_steps) \n{\n  int step, NSQUARE2, val;\n\n  step = 0;\n  NSQUARE2 = (N+N)*N;\n\t\n  while (step<max_steps) \n    {\n      BRandom(); // select the next MERS_N (624) transitions\n\n      // process 256 transitions\n      val = mt[threadIdx.x]%NSQUARE2;\n      fire_transition((char*)g_places, g_places+(NSQUARE2>>2), \n\t\t      val/N, val%N, step+7, N, BLOCK_SIZE);\n      \n      // process 256 transitions\n      val = mt[threadIdx.x+BLOCK_SIZE]%NSQUARE2;\n      fire_transition((char*)g_places, g_places+(NSQUARE2>>2), \n\t\t      val/N, val%N, step+11, N, BLOCK_SIZE);\n\t\t                \n      // process 112 transitions\n      if (  threadIdx.x < MERS_N-(BLOCK_SIZE<<1)  ) \n\t{\n\t  val = mt[threadIdx.x+(BLOCK_SIZE<<1)]%NSQUARE2;\n\t}\n      fire_transition((char*)g_places, g_places+(NSQUARE2>>2), \n\t\t      val/N, val%N, step+13, N, MERS_N-(BLOCK_SIZE<<1));\n\n      step += MERS_N>>1; \n      // experiments show that for N>2000 and max_step<20000, \n      // the step increase is larger than 320\n    }\n}\n\n__global__ \nvoid PetrinetKernel(int* __restrict__ g_s,\n                    float* __restrict__ g_v,\n                    int*__restrict__ g_m,\n                    int n, int s, int seed) \n{\n  // block size must be 256\n  // n is an even number\n  int NSQUARE2 = n*n*2;\n  int* g_places = g_s+blockIdx.x*((NSQUARE2>>2)+NSQUARE2);   \n  // place numbers, conflict_array\n  initialize_grid(g_places, NSQUARE2, seed);\n  \n  run_trajectory(g_places, n, s);\n  compute_reward_stat(g_places, g_v, g_m, NSQUARE2);\n}"
        ]
    },
    "grep-cuda": {
        "/Users/gbolet/hecbench-roofline/src/grep-cuda/pnfa.cu": [
            "__device__ inline void\npaddstate(List *l, State *s, List *addStateList)\n{  \n  addStateList->n = 0;\n  PUSH(addStateList, s);\n  /* follow unlabeled arrows */\n  while(!IS_EMPTY(addStateList)) {  \n\n    s = POP(addStateList);\n\n    // lastlist check is present to ensure that if\n    // multiple states point to this state, then only\n    //one instance of the state is added to the list\n    if(s == NULL);\n    else if (s->c == Split) {\n      PUSH(addStateList, s->out);\n      PUSH(addStateList, s->out1);  \n    }\n    else {\n      l->s[l->n++] = s;\n    }\n  }\n}\n\n__device__ inline int\nispmatch(List *l)\n{\n  int i;\n\n  for(i=0; i<l->n; i++) {\n    if(l->s[i]->c == Match)\n      return 1;\n  }\n  return 0;\n}\n\n__device__ inline void\npstep(List *clist, int c, List *nlist)\n{\n  int i;\n  State *s;\n  nlist->n = 0;\n  for(i=0; i<clist->n; i++){\n    s = clist->s[i];\n\n    if(s->c == c || s->c == Any){\n      List addStartState;\n      paddstate(nlist, s->out, &addStartState);\n    }\n  }\n}\n\n__device__ inline int panypmatch(State *start, char *s, List *dl1, List *dl2) { \n  int c;\n  List *clist, *nlist, *t;\n\n  clist = pstartlist(start, dl1);\n  nlist = dl2;\n  for(; *s; s++){\n    c = *s & 0xFF;\n    pstep(clist, c, nlist);\n    t = clist; clist = nlist; nlist = t;  // swap clist, nlist \n  }\n  return ispmatch(clist);\n}\n\n__global__ void parallelMatch(\n  char *bigLine,\n  const u32 *tableOfLineStarts, \n  int numLines,\n  char *regexLines,\n  const u32 *regexTable, \n  unsigned char *devResult,\n  State *pmatchstate) \n{\n\n  __shared__ char buf[BUFFER_SIZE];\n  __shared__  int pnstate;\n  __shared__ State s[100];\n  __shared__ State *st;\n\n  if (threadIdx.x == 0) {\n    pre2post(regexLines + regexTable[0], buf);\n\n    pnstate = 0;\n    st = ppost2nfa(buf, s, &pnstate, pmatchstate);\n  }\n\n  __syncthreads();\n\n  List d1;\n  List d2;  \n\n  int i;\n  for (i = blockIdx.x * blockDim.x + threadIdx.x; i < numLines; i += gridDim.x * blockDim.x) { \n\n    char * lineSegment = bigLine + tableOfLineStarts[i];\n    if (panypmatch(st, lineSegment, &d1, &d2)) \n      devResult[i] = 1;\n    else\n      devResult[i] = 0;\n\n  }\n}"
        ]
    },
    "egs-cuda": {
        "/Users/gbolet/hecbench-roofline/src/egs-cuda/kernels.cu": [
            "__device__ indices get_indices() {\n  indices idx;\n  // index of the block in the grid\n  idx.b = blockIdx.y * gridDim.x + blockIdx.x;\n\n  // index of the particle on the stack\n  idx.p = idx.b * blockDim.x + threadIdx.x;\n\n  // index of the warp in the block\n  idx.w = threadIdx.x / WARP_SIZE;\n\n  // index of the thread in the warp\n  idx.t = threadIdx.x % WARP_SIZE;\n\n  return idx;\n}\n\n__device__ __noinline__ void MT_generate_array() {\n  indices idx = get_indices();\n\n  volatile uint *status = MT_statuses_shared[idx.w];\n  volatile float *random_array = random_array_shared[idx.w];\n\n  uint M = MT_params_shared[idx.w].M;\n  uint mask = MT_params_shared[idx.w].mask;\n  uint sh1 = MT_params_shared[idx.w].sh1;\n  uint sh2 = MT_params_shared[idx.w].sh2;\n\n  int first_bound = MT_N - M;\n  if (first_bound > WARP_SIZE * MT_NUM_PER_THREAD)\n    first_bound = WARP_SIZE * MT_NUM_PER_THREAD;\n\n  // update first min(N - M, MT_NUM_PER_WARP) elements\n  for (uint i = idx.t; i < first_bound; i += WARP_SIZE) {\n    // recursion\n    uint x = (status[i] & mask) ^ status[i + 1];\n    x ^= x << sh1;\n    x ^= status[i + M] >> sh2;\n    x ^= MT_tables_shared[idx.w].recursion[x & 0x0FU];\n\n    // temper output and fill random array\n    uint t = status[i + M - 1];\n    t ^= t >> 16;\n    t ^= t >> 8;\n    // set the last bit to 1 to get a float in the range (1,2) (excluding endpoints)\n    t = ((x >> 9) ^ MT_tables_shared[idx.w].tempering[t & 0x0FU]) | 0x01U;\n    random_array[i] = *((float*)&t) - 1.0F;\n\n    // update status\n    status[i] = x;\n  }\n\n  // update remaining elements\n  for (int i = first_bound + idx.t; i < MT_N; i += WARP_SIZE) {\n    // recursion\n    uint x = (status[i] & mask) ^ status[i + 1 - (i + 1 >= MT_N ? MT_N : 0)];\n    x ^= x << sh1;\n    x ^= status[i + M - (i + M >= MT_N ? MT_N : 0)] >> sh2;\n    x ^= MT_tables_shared[idx.w].recursion[x & 0x0FU];\n\n    // temper output and fill random array\n    if (i < WARP_SIZE * MT_NUM_PER_THREAD) {\n      uint t = status[i + M - 1 - (i + M - 1 >= MT_N ? MT_N : 0)];\n      t ^= t >> 16;\n      t ^= t >> 8;\n      t = ((x >> 9) ^ MT_tables_shared[idx.w].tempering[t & 0x0FU]) | 0x01U;\n      random_array[i] = *((float*)&t) - 1.0F;\n    }\n\n    // update status\n    status[i] = x;\n  }\n}\n\ninline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\ninline __host__ __device__ float4 expf(float4 v)\n{\n    return make_float4(expf(v.x), expf(v.y), expf(v.z), expf(v.w));\n}\n\ninline __host__ __device__ float4 sqrtf(float4 v)\n{\n    return make_float4(sqrtf(v.x), sqrtf(v.y), sqrtf(v.z), sqrtf(v.w));\n}\n\n__device__ float get_rand(indices idx) {\n  volatile uchar *rand_idx = rand_idx_shared;\n  int i = rand_idx[idx.w] + 1;\n\n  // all random numbers int he current array have been used, update the array\n  if (i >= MT_NUM_PER_THREAD) {\n    MT_generate_array();\n    i = 0;\n  }\n\n  if (idx.t == 0)\n    rand_idx[idx.w] = i;\n\n  // return the random number for this thread\n  return random_array_shared[idx.w][i * WARP_SIZE + idx.t];\n}\n\n__device__ void uphi21(indices idx, float costhe, float sinthe, particle_t &p) {\n  float r1 = get_rand(idx);\n\n  if (p.process) {\n    float phi = 2.0F * 3.14159265F * r1;\n    float cosphi, sinphi;\n    __sincosf(phi, &sinphi, &cosphi);\n\n    float sinps2 = p.u * p.u + p.v * p.v;\n    // small polar angle\n    if (sinps2 < SMALL_POLAR_ANGLE_THRESHOLD) {\n      p.u = sinthe * cosphi;\n      p.v = sinthe * sinphi;\n      p.w = p.w * costhe;\n    }\n    else {\n      float sinpsi = sqrt(sinps2);\n      float us = sinthe * cosphi;\n      float vs = sinthe * sinphi;\n      float sindel = p.v / sinpsi;\n      float cosdel = p.u / sinpsi;\n\n      p.u = p.w * cosdel * us - sindel * vs + p.u * costhe; \n      p.v = p.w * sindel * us + cosdel * vs + p.v * costhe;\n      p.w = -sinpsi * us + p.w * costhe;\n    }\n  }\n}\n\ninline __host__ __device__ float3 make_float3(uint3 a)\n{\n    return make_float3(float(a.x), float(a.y), float(a.z));\n}\n\n__device__ int isWhere(float p, uint nreg, float *bounds) {\n  if ((p < bounds[0]) || (p > bounds[nreg]))\n    return -1;\n  if (nreg == 1)\n    return 0;\n\n  int ml = 0;\n  int mu = nreg;\n  while (mu - ml > 1) {\n    int mav = (ml + mu) / 2;\n    if (p <= bounds[mav])\n      mu = mav; \n    else \n      ml = mav;\n  }\n  return  mu - 1;\n}\n\n__device__ uint howfar(indices idx, particle_t &p, float &t) {\n  if (p.region > 0) {\n    // because of the above mentioned shift, we have to substract 1\n    uint ir = p.region - 1;\n\n    int iz = ir / (phantom.N.x * phantom.N.y); \n    ir -= iz * phantom.N.x * phantom.N.y; \n    int iy = ir / phantom.N.x;\n    int ix = ir - iy * phantom.N.x;\n    uint inew = p.region;\n\n    if (p.u > 0.0F) {\n      float d = (phantom.x_bounds[ix + 1] - p.x) / p.u;\n      if (d <= t) { \n        t = d; \n        if (ix + 1 < phantom.N.x) \n          inew = p.region + 1; \n        else \n          inew = 0;\n      }\n    }\n    else if (p.u < 0.0F) {\n      float d = (phantom.x_bounds[ix] - p.x) / p.u;\n      if (d <= t) { \n        t = d; \n        if (ix - 1 >= 0) \n          inew = p.region - 1; \n        else \n          inew = 0;\n      }\n    }\n\n    if (p.v > 0.0F) {\n      float d = (phantom.y_bounds[iy + 1] - p.y) / p.v;\n      if (d <= t) { \n        t = d; \n        if (iy + 1 < phantom.N.y) \n          inew = p.region + phantom.N.x; \n        else \n          inew = 0;\n      }\n    }\n    else if (p.v < 0.0F) {\n      float d = (phantom.y_bounds[iy] - p.y) / p.v;\n      if (d <= t) { \n        t = d; \n        if (iy - 1 >= 0) \n          inew = p.region - phantom.N.x; \n        else \n          inew = 0;\n      }\n    }\n\n    if (p.w > 0.0F) {\n      float d = (phantom.z_bounds[iz + 1] - p.z) / p.w;\n      if (d <= t) { \n        t = d; \n        if (iz + 1 < phantom.N.z) \n          inew = p.region + phantom.N.x * phantom.N.y; \n        else \n          inew = 0;\n      }\n    }\n    else if (p.w < 0.0F) {\n      float d = (phantom.z_bounds[iz] - p.z) / p.w;\n      if (d <= t) { \n        t = d; \n        if (iz - 1 >= 0) \n          inew = p.region - phantom.N.x * phantom.N.y; \n        else \n          inew = 0;\n      }\n    }\n\n    return inew;\n  }\n  // this part corresponds to the function howfarFromOut of the class EGS_XYZGeometry \n  // in the file egs_nd_geometry.h (v 1.26 2009/07/06) of the EGSnrc C++ Class Library\n  else {\n    int ix, iy, iz;\n    float t1;\n\n    ix = -1;\n    if ((p.x <= phantom.x_bounds[0]) && (p.u > 0.0F)) {\n      t1 = (phantom.x_bounds[0] - p.x) / p.u; \n      ix = 0;\n    }\n    else if ((p.x >= phantom.x_bounds[phantom.N.x]) && (p.u < 0.0F)) {\n      t1 = (phantom.x_bounds[phantom.N.x] - p.x) / p.u; \n      ix = phantom.N.x - 1;\n    }\n\n    if ((ix >= 0) && (t1 <= t)) {\n      float y1 = p.y + p.v * t1;\n      iy = isWhere(y1, phantom.N.y, phantom.y_bounds);\n\n      if (iy >= 0) {\n        float z1 = p.z + p.w * t1;\n        iz = isWhere(z1, phantom.N.z, phantom.z_bounds);\n\n        if (iz >= 0) {\n          t = t1; \n          return ix + iy * phantom.N.x + iz * phantom.N.x * phantom.N.y + 1;             \n        }\n      }\n    }\n\n    iy = -1;\n    if ((p.y <= phantom.y_bounds[0]) && (p.v > 0.0F)) {\n      t1 = (phantom.y_bounds[0] - p.y) / p.v; \n      iy = 0;\n    }\n    else if ((p.y >= phantom.y_bounds[phantom.N.y]) && (p.v < 0.0F)) {\n      t1 = (phantom.y_bounds[phantom.N.y] - p.y) / p.v; \n      iy = phantom.N.y - 1;\n    }\n\n    if ((iy >= 0) && (t1 <= t)) {\n      float x1 = p.x + p.u * t1;\n      ix = isWhere(x1, phantom.N.x, phantom.x_bounds);\n\n      if (ix >= 0) {\n        float z1 = p.z + p.w * t1;\n        iz = isWhere(z1, phantom.N.z, phantom.z_bounds);\n\n        if (iz >= 0) {\n          t = t1; \n          return ix + iy * phantom.N.x + iz * phantom.N.x * phantom.N.y + 1; \n        }\n      }\n    }\n\n    iz = -1;\n    if ((p.z <= phantom.z_bounds[0]) && (p.w > 0.0F)) {\n      t1 = (phantom.z_bounds[0] - p.z) / p.w; \n      iz = 0;\n    }\n    else if ((p.z >= phantom.z_bounds[phantom.N.z]) && (p.w < 0.0F)) {\n      t1 = (phantom.z_bounds[phantom.N.z] - p.z) / p.w; \n      iz = phantom.N.z - 1;\n    }\n\n    if ((iz >= 0) && (t1 <= t)) {\n      float x1 = p.x + p.u * t1;\n      ix = isWhere(x1, phantom.N.x, phantom.x_bounds);\n\n      if (ix >= 0) {\n        float y1 = p.y + p.v * t1;\n        iy = isWhere(y1, phantom.N.y, phantom.y_bounds);\n\n        if (iy >= 0) {\n          t = t1; \n          return ix + iy * phantom.N.x + iz * phantom.N.x * phantom.N.y + 1; \n        }\n      }\n    }\n\n    return 0;\n  }\n}\n\ninline __host__ __device__ int4 make_int4(float4 a)\n{\n    return make_int4(int(a.x), int(a.y), int(a.z), int(a.w));\n}\n\ninline __host__ __device__ int4 abs(int4 v)\n{\n    return make_int4(abs(v.x), abs(v.y), abs(v.z), abs(v.w));\n}\n\ninline __host__ __device__ float4 floorf(float4 v)\n{\n    return make_float4(floorf(v.x), floorf(v.y), floorf(v.z), floorf(v.w));\n}\n\ninline __host__ __device__ float2 make_float2(uint2 a)\n{\n    return make_float2(float(a.x), float(a.y));\n}\n\n__device__ bool almostEqual(float A, float B) {\n  // Make sure maxUlps is non-negative and small enough that the\n  // default NAN won't compare as equal to anything.\n\n  // Make aInt lexicographically ordered as a twos-complement int\n  int aInt = *(int*)&A;\n  if (aInt < 0)\n    aInt = 0x80000000 - aInt;\n\n  // Make bInt lexicographically ordered as a twos-complement int\n  int bInt = *(int*)&B;\n  if (bInt < 0)\n    bInt = 0x80000000 - bInt;\n\n  int intDiff = abs(aInt - bInt);\n  if (intDiff <= MAX_ULPS)\n    return true;\n\n  return false;\n}\n\n__device__ void score_pixel(indices idx, int x, int y, uchar cat, float wt, float e) {\n  // score the particle if the pixel is on the detector\n  if ((x >= 0) && (x < (int)detector.N.x) &&\n      (y >= 0) && (y < (int)detector.N.y)) {\n    atomicAdd(&detector_scores_count[idx.b][cat][y * detector.N.x + x], wt);\n    atomicAdd(&detector_scores_energy[idx.b][cat][y * detector.N.x + x], e);\n  }\n}\n\n__device__ void MT_read_status(indices idx) {\n  uint MT_idx = idx.b * SIMULATION_WARPS_PER_BLOCK + idx.w;\n\n  volatile uchar *rand_idx = rand_idx_shared;\n  if (idx.t == 0) {\n    MT_params_shared[idx.w] = MT_params[MT_idx];\n    rand_idx[idx.w] = 0;\n  }\n\n  for (uint i = idx.t; i < MT_N; i += WARP_SIZE)\n    MT_statuses_shared[idx.w][i] = MT_statuses[MT_idx * MT_NUM_STATUS + i];\n\n  ((uint*)&(MT_tables_shared[idx.w]))[idx.t] = ((uint*)&(MT_tables[MT_idx]))[idx.t];\n\n  MT_generate_array();\n}\n\n__device__ void MT_write_status(indices idx) {\n  uint MT_idx = idx.b * SIMULATION_WARPS_PER_BLOCK + idx.w;\n\n  for (uint i = idx.t; i < MT_N; i += WARP_SIZE)\n    MT_statuses[MT_idx * MT_NUM_STATUS + i] = MT_statuses_shared[idx.w][i];\n}\n\n__device__ void compton(indices idx, particle_t &p) {\n  if (p.process) {\n    p.status = p_photon_step;\n\n    // count scatter event\n    p.latch += 1;\n  }\n\n  float ko = p.e / ELECTRON_REST_MASS_FLOAT;\n  float broi = 1.0F + 2.0F * ko;\n  float bro = 1.0F / broi;\n\n  // sampling loop\n  bool loop_done = !p.process;\n\n  float sinthe = 0.0F;\n  float costhe = 0.0F;\n  float br = 0.0F;\n\n  do {\n    float r1 = get_rand(idx);\n    float r2 = get_rand(idx);\n    float r3 = get_rand(idx);\n\n    if (!loop_done) {\n      if (ko > 2.0F) {\n        float broi2 = broi * broi;\n        float alph1 = logf(broi);\n        float alph2 = ko * (broi + 1.0F) * bro * bro;\n        float alpha = alph1 + alph2;\n\n        if (r1 * alpha < alph1)\n          br = expf(alph1 * r2) * bro;\n        else\n          br = sqrtf(r2 * broi2 + 1.0F - r2) * bro;\n\n        costhe = (1.0F - br) / (ko * br);\n        sinthe = fmax(0.0F, costhe * (2.0F - costhe));\n        float aux = 1.0F + br * br;\n        float rejf3 = aux - br * sinthe;\n\n        if (r3 * aux < rejf3)\n          loop_done = true;\n      }\n      else {\n        float bro1 = 1.0F - bro;\n        float rejmax = broi + bro;\n\n        br = bro + bro1 * r1;\n        costhe = (1.0F - br) / (ko * br);\n        sinthe = fmax(0.0F, costhe * (2.0F - costhe));\n        float rejf3 = 1.0F + br * br - br * sinthe;\n        if (r2 * br * rejmax < rejf3)\n          loop_done = true;\n      }\n    }\n\n  } while (!__all_sync(MASK, loop_done));\n\n  costhe = 1.0F - costhe;\n  sinthe = sqrtf(sinthe);\n\n  if (p.process)\n    p.e *= br;\n\n  uphi21(idx, costhe, sinthe, p);\n}\n\n__device__ void cutoff_discard(indices idx, particle_t &p) {\n  if (p.process)\n    p.status = p_empty;\n}\n\n__device__ void new_particle(indices idx, particle_t &p, volatile float *weight_list) {\n  float r1 = get_rand(idx);\n  float r2 = get_rand(idx);\n\n  // for spectrum\n#ifdef USE_ENERGY_SPECTRUM\n  float r3 = get_rand(idx);\n  float r4 = get_rand(idx); \n#endif\n\n  if (p.process) {\n    // set charge and next step\n    p.charge = 0;\n    p.reserved = 0;\n    p.status = p_photon_step;\n\n    // set energy and region\n    p.region = 0;\n\n#ifdef USE_ENERGY_SPECTRUM\n    float aj = r3 * (float)source.n; \n    int j = (int)aj; \n    aj -= j;\n    if (aj > source.wi[j])\n      j = source.bin[j];\n\n    float x = source.xi[j]; \n    float dx = source.xi[j+1] - x;\n\n    p.e = x + dx * r4;\n#else\n    p.e = source.energy;\n#endif\n\n    // set source point\n    p.x = source.source_point.x;\n    p.y = source.source_point.y;\n    p.z = source.source_point.z;\n\n    // target point is uniformly distributed on rectangle\n    float3 target_point = make_float3(source.rectangle_min.x + source.rectangle_size.x * r1, \n        source.rectangle_min.y + source.rectangle_size.y * r2, \n        source.rectangle_z);\n\n    // calculate direction\n    p.u = target_point.x - p.x;\n    p.v = target_point.y - p.y;\n    p.w = target_point.z - p.z;\n\n    // normalize direction\n    float d2i = 1 / (p.u * p.u + p.v * p.v + p.w * p.w);\n    float di = sqrtf(d2i);\n    p.u *= di;\n    p.v *= di;\n    p.w *= di;\n\n    // calculate weight and set latch\n    p.wt = source.rectangle_area * fabsf(p.w) * d2i;\n    weight_list[idx.t] = p.wt;\n    p.latch = 0;\n  }    \n  else\n    weight_list[idx.t] = 0.0F;\n}\n\n__device__ void pair_production(indices idx, particle_t &p) {\n  if (p.process)\n    p.status = p_empty;\n}\n\n__device__ void photo(indices idx, particle_t &p) {\n  if (p.process)\n    p.status = p_empty;\n}\n\n__device__ void photon_step(indices idx, particle_t &p) {\n  region_data_t reg_dat = region_data[p.region];\n\n  if (p.process) {\n    if (p.e <= reg_dat.pcut)\n      p.status = p_cutoff_discard;\n    else if (p.wt <= 0.0F)\n      p.status = p_user_discard;       \n  }\n\n  float r1 = get_rand(idx);\n  float dpmfp = -logf(r1);\n\n  float gle = logf(p.e);\n  int lgle = 0;\n  float gmfpr0 = 0.0F;\n  float tstep = 0.0F;\n  float gmfp_val = 0.0F;\n  float cohfac = 0.0F;\n  ushort old_medium = reg_dat.med;\n\n  if (p.process && (p.status == p_photon_step)) {\n    if (reg_dat.med == VACUUM)\n      tstep = VACUUM_STEP;\n    else {\n      float2 ge_dat = ge[reg_dat.med];\n      lgle = (int)(ge_dat.x + ge_dat.y * gle);\n      float2 gmfp_dat = gmfp[reg_dat.med * MXGE + lgle];\n      gmfpr0 = gmfp_dat.x + gmfp_dat.y * gle;\n\n      gmfp_val = gmfpr0 / reg_dat.rhof;\n\n      if ((reg_dat.flags & f_rayleigh) > 0) {\n        float2 cohe_dat = cohe[reg_dat.med * MXGE + lgle];\n        cohfac = cohe_dat.x + cohe_dat.y * gle; \n        gmfp_val *= cohfac;\n      }\n      tstep = gmfp_val * dpmfp;\n    }\n\n    // HOWFAR\n    uint new_region = howfar(idx, p, tstep);\n\n    char idisc = 0;\n    if (new_region == 0) {\n      if (reg_dat.med == VACUUM)\n        idisc = 1;\n      else\n        idisc = -1;\n    }\n\n    if (idisc > 0) {\n      p.region = 0;\n      p.status = p_user_discard;\n    }\n    else {\n      p.x += p.u * tstep;\n      p.y += p.v * tstep;\n      p.z += p.w * tstep;\n\n      if (reg_dat.med != VACUUM) \n        dpmfp = fmax(0.0F, dpmfp - tstep / gmfp_val);\n\n      old_medium = reg_dat.med;\n\n      if (new_region != p.region) {\n        p.region = new_region;\n        reg_dat = region_data[new_region];\n      }\n\n      if (p.e <= reg_dat.pcut)\n        p.status = p_cutoff_discard;\n      else if (idisc < 0)\n        p.status = p_user_discard;\n    } \n  }\n\n  // determine next step if not already discarded \n\n  bool process = p.process && (p.status == p_photon_step);\n\n  if (process && ((reg_dat.med != old_medium) || (reg_dat.med == VACUUM) || (dpmfp >= EPSGMFP))) {\n    p.status = p_photon_step;\n    process = false;\n  }\n\n  float r2 = get_rand(idx);\n  if (process && ((reg_dat.flags & f_rayleigh) > 0)) {\n    if (r2 < 1.0F - cohfac) {\n      p.status = p_rayleigh;\n      process = false;\n    }\n  }\n\n  float r3 = get_rand(idx);\n  if (process) {\n    float2 gbr1_dat = gbr1[reg_dat.med * MXGE + lgle];\n    float gbr1_val = gbr1_dat.x + gbr1_dat.y * gle;\n    float2 gbr2_dat = gbr2[reg_dat.med * MXGE + lgle];\n    float gbr2_val = gbr2_dat.x + gbr2_dat.y * gle;\n\n    if ((r3 <= gbr1_val) && (p.e > 2.0F * ELECTRON_REST_MASS_FLOAT))\n      p.status = p_pair;\n    else {\n      if (r3 < gbr2_val)\n        p.status = p_compton; \n      else\n        p.status = p_photo;\n    }\n  }\n}\n\n__device__ void rayleigh(indices idx, particle_t &p) {\n  region_data_t reg_dat;\n\n  if (p.process) {\n    reg_dat = region_data[p.region];\n    p.status = p_photon_step;\n\n    // count scatter event\n    p.latch += (1 << 16);\n  }\n\n  float xmax = 0.0F;\n  float pmax_val = 0.0F;\n\n  if (p.process) {\n    float gle = logf(p.e);\n    float2 ge_dat = ge[reg_dat.med];\n    int lgle = (int)(ge_dat.x + ge_dat.y * gle);\n\n    float2 pmax_dat = pmax[reg_dat.med * MXGE + lgle];\n    pmax_val = pmax_dat.x + pmax_dat.y * gle; \n    xmax = HC_INVERSE * p.e;\n  }\n\n  int dwi = RAYCDFSIZE - 1;\n  int ibin = 0;\n  int ib = 0;\n\n  float xv = 0.0F;\n  float costhe = 0.0F;\n  float costhe2 = 0.0F;\n  float sinthe = 0.0F;\n\n  bool loop_done = !p.process;\n\n  do {\n    bool inner_loop_done = loop_done;\n\n    do {\n      float r1 = get_rand(idx);\n\n      if (!inner_loop_done) {\n        float temp = r1 * pmax_val;\n        // indexing in C starts at 0 and not 1 as in FORTRAN\n        ibin = (int)(temp * (float)dwi);\n        ib = i_array[reg_dat.med * RAYCDFSIZE + ibin] - 1;\n        int next_ib = i_array[reg_dat.med * RAYCDFSIZE + ibin + 1] - 1;\n\n        if (next_ib > ib) {\n          do {\n            rayleigh_data_t ray_dat = rayleigh_data[reg_dat.med * MXRAYFF + ib + 1];\n            if ((temp < ray_dat.fcum) || (ib >= RAYCDFSIZE - 2))\n              break;\n            ib++;\n          } while (true);\n        }\n\n        rayleigh_data_t ray_dat = rayleigh_data[reg_dat.med * MXRAYFF + ib];\n        temp = (temp - ray_dat.fcum) * ray_dat.c_array;\n        xv = ray_dat.xgrid * expf(logf(1.0F + temp) * ray_dat.b_array);\n\n        if (xv < xmax)\n          inner_loop_done = true;\n      }\n\n    } while (!__all_sync(MASK, inner_loop_done));\n\n    float r2 = get_rand(idx);\n\n    if (!loop_done) {\n      xv = xv / p.e;\n      costhe = 1.0F - TWICE_HC2 * xv * xv;\n      costhe2 = costhe * costhe;\n\n      if (2.0F * r2 < 1.0F + costhe2)\n        loop_done = true;\n    }\n\n  } while (!__all_sync(MASK, loop_done));\n\n  sinthe = sqrtf(1.0F - costhe2);\n  uphi21(idx, costhe, sinthe, p);\n}\n\n__device__ void user_discard(indices idx, particle_t &p) {\n  if (!p.process)\n    return;\n\n  p.status = p_empty;\n\n  // do not score if...\n  if ((p.charge != 0) ||   // it is not a photon\n      (p.region != 0) ||   // it is not in region 0 (outside)\n      (p.w == 0.0F)) {     // it is not going parallel to the z direction\n    return;\n  }\n\n  // propagate to image plane\n  float delta = (detector.center.z - p.z) / p.w;\n\n  // photon does not hit the detector\n  if (delta < 0.0F)\n    return;\n\n  float2 pos = make_float2(p.x + delta * p.u,\n      p.y + delta * p.v);\n\n  // find pixel where the photon hits the detector\n  float2 pix = make_float2((pos.x - detector.center.x) / detector.d.x + (float)detector.N.x / 2.0F,\n      (pos.y - detector.center.y) / detector.d.y + (float)detector.N.y / 2.0F);\n\n  float2 lower = make_float2(floorf(pix.x), floorf(pix.y));\n  float2 upper = make_float2(ceilf(pix.x), ceilf(pix.y));\n\n  int split = 0;\n  int2 pixel;\n\n  // split pixels if photon hits detector very close to pixel boundaries\n  if (almostEqual(pix.x, lower.x)) {\n    split += 1;\n    pixel.x = (int)lower.x;\n  } else if (almostEqual(pix.x, upper.x)) {\n    split += 1;\n    pixel.x = (int)upper.x;\n  }\n  else\n    pixel.x = (int)lower.x;\n\n  if (almostEqual(pix.y, lower.y)) {\n    split += 2;\n    pixel.y = (int)lower.y;\n  } else if (almostEqual(pix.y, upper.y)) {\n    split += 2;\n    pixel.y = (int)upper.y;\n  }\n  else\n    pixel.y = (int)lower.y;\n\n  ushort num_compton = p.latch & 0xFFFFU;\n  ushort num_rayleigh = p.latch >> 16;\n  uchar cat = 0;\n\n  if ((num_compton == 0) && (num_rayleigh == 0))\n    cat = 0;\n  else if ((num_compton == 1) && (num_rayleigh == 0))\n    cat = 1;\n  else if ((num_compton == 0) && (num_rayleigh == 1))\n    cat = 2;\n  else\n    cat = 3;\n\n  switch (split) {\n    case 0:\n      score_pixel(idx, pixel.x, pixel.y, cat, p.wt, p.e * p.wt);\n      break;\n\n    case 1:\n      p.wt /= 2.0F;\n      p.e *= p.wt;\n      score_pixel(idx, pixel.x, pixel.y, cat, p.wt, p.e);\n      score_pixel(idx, pixel.x - 1, pixel.y, cat, p.wt, p.e);\n      break;\n\n    case 2:\n      p.wt /= 2.0F;\n      p.e *= p.wt;\n      score_pixel(idx, pixel.x, pixel.y, cat, p.wt, p.e);\n      score_pixel(idx, pixel.x, pixel.y - 1, cat, p.wt, p.e);\n      break;\n\n    case 3:\n      p.wt /= 4.0F;\n      p.e *= p.wt;\n      score_pixel(idx, pixel.x, pixel.y, cat, p.wt, p.e);\n      score_pixel(idx, pixel.x - 1, pixel.y, cat, p.wt, p.e);\n      score_pixel(idx, pixel.x, pixel.y - 1, cat, p.wt, p.e);\n      score_pixel(idx, pixel.x - 1, pixel.y - 1, cat, p.wt, p.e);\n      break;\n  }\n}\n\n__global__ void simulation_step_kernel(bool init, bool limit_reached) {\n  indices idx = get_indices();\n\n  volatile uint *step_counters = step_counters_shared[idx.w];\n  volatile float *weight_list = weight_list_shared[idx.w];\n  volatile double *combined_weight_list = combined_weight_list_shared;\n\n  // list depth counter\n#ifdef DO_LIST_DEPTH_COUNT\n  volatile uint *list_depth = list_depth_shared;\n  volatile uint *num_inner_iterations = num_inner_iterations_shared;\n#endif\n\n  // reset detector pixels\n  for (uint i = 0; i < NUM_DETECTOR_CAT; i++) {\n    for (uint j = threadIdx.x; j < detector.N.x * detector.N.y; j += blockDim.x) {\n      detector_scores_count[idx.b][i][j] = 0.0F;\n      detector_scores_energy[idx.b][i][j] = 0.0F;\n    }\n  }\n\n  __syncthreads();\n\n  // reset step counts\n  if (idx.t < NUM_CAT)\n    step_counters[idx.t] = 0;\n\n  // reset weight counter\n  if (idx.t == 0)\n    combined_weight_list[idx.w] = 0.0F;\n\n  // list depth counter\n#ifdef DO_LIST_DEPTH_COUNT\n  if (idx.t == 0) {\n    list_depth[idx.w] = 0;\n    num_inner_iterations[idx.w] = 0;\n  }\n#endif\n\n  // read MT status\n  MT_read_status(idx);\n\n  particle_t p;\n\n  // read particle from stack\n  uint4 tmp = stack.a[idx.p];\n  p.status = ((uchar*)&tmp.x)[0];\n  p.reserved = ((uchar*)&tmp.x)[1];\n  p.charge = ((uchar*)&tmp.x)[2];\n  p.process = ((uchar*)&tmp.x)[3];\n  p.e = *(float*)&tmp.y;\n  p.wt = *(float*)&tmp.z;\n  p.region = tmp.w;\n\n  tmp = stack.b[idx.p];\n  p.latch = tmp.x;\n  p.x = *(float*)&tmp.y;\n  p.y = *(float*)&tmp.z;\n  p.z = *(float*)&tmp.w;\n\n  tmp = stack.c[idx.p];\n  p.u = *(float*)&tmp.x;\n  p.v = *(float*)&tmp.y;\n  p.w = *(float*)&tmp.z;\n\n  if (init)\n    p.status = p_new_particle;\n\n  bool done;\n\n  for (uint i = 0; i < SIMULATION_ITERATIONS; i++) {\n\n    // list depth counter\n#ifdef DO_LIST_DEPTH_COUNT\n    if (idx.t == 0)\n      num_inner_iterations[idx.w] += 1;\n#endif\n\n    done = false;\n    if ((p.status == p_empty) && (!limit_reached))\n      p.status = p_new_particle;\n\n    // photon step\n    p.process = (p.status == p_photon_step);\n    done |= p.process;\n    if (__any_sync(MASK, p.process)) {\n      photon_step(idx, p);\n      uint count_mask = __ballot_sync(MASK, p.process);\n      if (idx.t == 0) {\n        step_counters[p_photon_step] += __popc(count_mask);\n#ifdef DO_LIST_DEPTH_COUNT\n        list_depth[idx.w] += 1;\n#endif\n      }\n    }\n    if (__all_sync(MASK, done))\n      continue;\n\n    // new particle\n    p.process = (p.status == p_new_particle);\n    done |= p.process;\n    if (__any_sync(MASK, p.process)) {\n      new_particle(idx, p, weight_list);\n\n      // add the weights together\n      if (idx.t == 0) {\n        double combined_weight = 0.0F;\n\n        for (uchar j = 0; j < WARP_SIZE; j++)\n          combined_weight += (double)weight_list[j];\n\n        combined_weight_list[idx.w] += combined_weight;\n      }\n\n      uint count_mask = __ballot_sync(MASK, p.process);\n      if (idx.t == 0) {\n        step_counters[p_new_particle] += __popc(count_mask);\n#ifdef DO_LIST_DEPTH_COUNT\n        list_depth[idx.w] += 1;\n#endif\n      }\n    }\n    if (__all_sync(MASK, done))\n      continue;\n\n    // user discard\n    p.process = (p.status == p_user_discard);\n    done |= p.process;\n    if (__any_sync(MASK, p.process)) {\n      user_discard(idx, p);\n      uint count_mask = __ballot_sync(MASK, p.process);\n      if (idx.t == 0) {\n        step_counters[p_user_discard] += __popc(count_mask);\n#ifdef DO_LIST_DEPTH_COUNT\n        list_depth[idx.w] += 1;\n#endif\n      }\n    }\n    if (__all_sync(MASK, done))\n      continue;\n\n    // compton\n    p.process = (p.status == p_compton);\n    done |= p.process;\n    if (__any_sync(MASK, p.process)) {\n      compton(idx, p);\n      uint count_mask = __ballot_sync(MASK, p.process);\n      if (idx.t == 0) {\n        step_counters[p_compton] += __popc(count_mask);\n#ifdef DO_LIST_DEPTH_COUNT\n        list_depth[idx.w] += 1;\n#endif\n      }\n    }\n    if (__all_sync(MASK, done))\n      continue;\n\n    // photoelectric effect\n    p.process = (p.status == p_photo);\n    done |= p.process;\n    if (__any_sync(MASK, p.process)) {\n      photo(idx, p);\n      uint count_mask = __ballot_sync(MASK, p.process);\n      if (idx.t == 0) {\n        step_counters[p_photo] += __popc(count_mask);\n#ifdef DO_LIST_DEPTH_COUNT\n        list_depth[idx.w] += 1;\n#endif\n      }\n    }\n    if (__all_sync(MASK, done))\n      continue;\n\n    // rayleigh\n    p.process = (p.status == p_rayleigh);\n    done |= p.process;\n    if (__any_sync(MASK, p.process)) {\n      rayleigh(idx, p);\n      uint count_mask = __ballot_sync(MASK, p.process);\n      if (idx.t == 0) {\n        step_counters[p_rayleigh] += __popc(count_mask);\n#ifdef DO_LIST_DEPTH_COUNT\n        list_depth[idx.w] += 1;\n#endif\n      }\n    }\n    if (__all_sync(MASK, done))\n      continue;\n\n    // pair production\n    p.process = (p.status == p_pair);\n    done |= p.process;\n    if (__any_sync(MASK, p.process)) {\n      pair_production(idx, p);\n      uint count_mask = __ballot_sync(MASK, p.process);\n      if (idx.t == 0) {\n        step_counters[p_pair] += __popc(count_mask);\n#ifdef DO_LIST_DEPTH_COUNT\n        list_depth[idx.w] += 1;\n#endif\n      }\n    }\n    if (__all_sync(MASK, done))\n      continue;\n\n    // cutoff discard\n    p.process = (p.status == p_cutoff_discard);\n    done |= p.process;\n    if (__any_sync(MASK, p.process)) {\n      cutoff_discard(idx, p);\n      uint count_mask = __ballot_sync(MASK, p.process);\n      if (idx.t == 0) {\n        step_counters[p_cutoff_discard] += __popc(count_mask);\n#ifdef DO_LIST_DEPTH_COUNT\n        list_depth[idx.w] += 1;\n#endif\n      }\n    }\n\n    if ((__all_sync(MASK, !done)) && (limit_reached))\n      break;\n\n  }\n\n  __syncthreads();\n\n  // combine the counters in shared memory and write them to global memory\n  if (threadIdx.x < NUM_CAT) {\n    uint total_count = 0;\n\n    // step through the warps\n    for (uchar i = 0; i < SIMULATION_WARPS_PER_BLOCK; i++)\n      total_count += step_counters_shared[i][threadIdx.x];\n\n    (*total_step_counts)[idx.b][threadIdx.x] = total_count;\n  }\n\n  // combine the weights in shared memory and write them to global memory\n  if (threadIdx.x == 0) {\n    double total_weight = 0.0F;\n\n    // step through the warps\n    for (uchar i = 0; i < SIMULATION_WARPS_PER_BLOCK; i++)\n      total_weight += combined_weight_list[i];\n\n    if (total_weight > 0.0F)\n      (*total_weights)[idx.b] += total_weight;\n  }\n\n  // list depth counter\n#ifdef DO_LIST_DEPTH_COUNT\n  if (threadIdx.x == 0) {\n    uint tot_list_depth = 0;\n    uint tot_it = 0;\n\n    for (uchar i =0; i < SIMULATION_WARPS_PER_BLOCK; i++) {\n      tot_list_depth += list_depth[i];\n      tot_it += num_inner_iterations[i];\n    }\n\n    (*total_list_depth)[idx.b] = tot_list_depth;\n    (*total_num_inner_iterations)[idx.b] = tot_it;\n  }\n#endif\n\n  // write particle back to stack\n  ((uchar*)&tmp.x)[0] = p.status;\n  ((uchar*)&tmp.x)[1] = p.reserved;\n  ((uchar*)&tmp.x)[2] = p.charge;\n  ((uchar*)&tmp.x)[3] = p.process;\n  tmp.y = *(uint*)&p.e;\n  tmp.z = *(uint*)&p.wt;\n  tmp.w = p.region;\n  stack.a[idx.p] = tmp;\n\n  tmp.x = p.latch;\n  tmp.y = *(uint*)&p.x;\n  tmp.z = *(uint*)&p.y;\n  tmp.w = *(uint*)&p.z;\n  stack.b[idx.p] = tmp;\n\n  tmp.x = *(uint*)&p.u;\n  tmp.y = *(uint*)&p.v;\n  tmp.z = *(uint*)&p.w;\n  stack.c[idx.p] = tmp;\n\n  // write MT status\n  MT_write_status(idx);\n}",
            "__global__ void sum_detector_scores_kernel() {\n  bool do_count = blockIdx.x / NUM_DETECTOR_CAT;\n  uchar cat = blockIdx.x % NUM_DETECTOR_CAT;\n\n  detector_scores_t *scores;\n  double *totals;\n  if (do_count) {\n    scores = &detector_scores_count;\n    totals = detector_totals_count[cat];\n  }\n  else {\n    scores = &detector_scores_energy;\n    totals = detector_totals_energy[cat];\n  }\n\n  for (uint i = threadIdx.x; i < detector.N.x * detector.N.y; i += blockDim.x) {\n    double total = 0.0F;\n\n    for (uint j = 0; j < SIMULATION_NUM_BLOCKS; j++)\n      total += (double)((*scores)[j][cat][i]);\n\n    totals[i] += total;\n  }\n}"
        ]
    },
    "srad-cuda": {
        "/Users/gbolet/hecbench-roofline/src/srad-cuda/srad2_kernel.cu": [
            "#define fp float\n\n\n__global__ void srad2(const  fp d_lambda, \n                    const int d_Nr, \n                    const int d_Nc, \n                    const long d_Ne, \n                    const int *d_iN, \n                    const int *d_iS, \n                    const int *d_jE, \n                    const int *d_jW,\n                    const fp *d_dN, \n                    const fp *d_dS, \n                    const fp *d_dE, \n                    const fp *d_dW, \n                    const fp *d_c, \n                    fp *d_I)\n{\n\n  // indexes\n    int bx = blockIdx.x;                  // get current horizontal block index\n  int tx = threadIdx.x;                   // get current horizontal thread index\n  int ei = bx*NUMBER_THREADS+tx;          // more threads than actual elements !!!\n  int row;                                // column, x position\n  int col;                                // row, y position\n\n  // variables\n  fp d_cN,d_cS,d_cW,d_cE;\n  fp d_D;\n\n  // figure out row/col location in new matrix\n  row = (ei+1) % d_Nr - 1;                // (0-n) row\n  col = (ei+1) / d_Nr + 1 - 1;            // (0-n) column\n  if((ei+1) % d_Nr == 0){\n    row = d_Nr - 1;\n    col = col - 1;\n  }\n\n  if(ei<d_Ne){                            // make sure that only threads matching jobs run\n\n    // diffusion coefficent\n    d_cN = d_c[ei];                       // north diffusion coefficient\n    d_cS = d_c[d_iS[row] + d_Nr*col];     // south diffusion coefficient\n    d_cW = d_c[ei];                       // west diffusion coefficient\n    d_cE = d_c[row + d_Nr * d_jE[col]];   // east diffusion coefficient\n\n    // divergence (equ 58)\n    d_D = d_cN*d_dN[ei] + d_cS*d_dS[ei] + d_cW*d_dW[ei] + d_cE*d_dE[ei];// divergence\n\n    // image update (equ 61) (every element of IMAGE)\n    d_I[ei] = d_I[ei] + (fp)0.25*d_lambda*d_D;// updates image (based on input time step and divergence)\n\n  }\n\n}"
        ],
        "/Users/gbolet/hecbench-roofline/src/srad-cuda/srad_kernel.cu": [
            "#define fp float\n\n\n__global__ void srad(  fp d_lambda, \n                   const int d_Nr, \n                   const int d_Nc, \n                   const long d_Ne, \n                   const int *d_iN, \n                   const int *d_iS, \n                   const int *d_jE, \n                   const int *d_jW, \n                   fp *d_dN, \n                   fp *d_dS, \n                   fp *d_dE, \n                   fp *d_dW, \n                   const fp d_q0sqr, \n                   fp *d_c, \n                   const fp *d_I)\n{\n\n  // indexes\n  int bx = blockIdx.x;                    // get current horizontal block index\n  int tx = threadIdx.x;                   // get current horizontal thread index\n  int ei = bx*NUMBER_THREADS+tx;          // more threads than actual elements !!!\n  int row;                                // column, x position\n  int col;                                // row, y position\n\n  // variables\n  fp d_Jc;\n  fp d_dN_loc, d_dS_loc, d_dW_loc, d_dE_loc;\n  fp d_c_loc;\n  fp d_G2,d_L,d_num,d_den,d_qsqr;\n  \n  // figure out row/col location in new matrix\n  row = (ei+1) % d_Nr - 1;                // (0-n) row\n  col = (ei+1) / d_Nr + 1 - 1;            // (0-n) column\n  if((ei+1) % d_Nr == 0){\n    row = d_Nr - 1;\n    col = col - 1;\n  }\n  \n  if(ei<d_Ne){                            // make sure that only threads matching jobs run\n    \n    // directional derivatives, ICOV, diffusion coefficent\n    d_Jc = d_I[ei];                       // get value of the current element\n    \n    // directional derivates (every element of IMAGE)(try to copy to shared memory or temp files)\n    d_dN_loc = d_I[d_iN[row] + d_Nr*col] - d_Jc;            // north direction derivative\n    d_dS_loc = d_I[d_iS[row] + d_Nr*col] - d_Jc;            // south direction derivative\n    d_dW_loc = d_I[row + d_Nr*d_jW[col]] - d_Jc;            // west direction derivative\n    d_dE_loc = d_I[row + d_Nr*d_jE[col]] - d_Jc;            // east direction derivative\n           \n    // normalized discrete gradient mag squared (equ 52,53)\n    d_G2 = (d_dN_loc*d_dN_loc + d_dS_loc*d_dS_loc + d_dW_loc*d_dW_loc + d_dE_loc*d_dE_loc) / (d_Jc*d_Jc);  // gradient (based on derivatives)\n    \n    // normalized discrete laplacian (equ 54)\n    d_L = (d_dN_loc + d_dS_loc + d_dW_loc + d_dE_loc) / d_Jc;      // laplacian (based on derivatives)\n\n    // ICOV (equ 31/35)\n    d_num  = ((fp)0.5*d_G2) - (((fp)1.0/(fp)16.0)*(d_L*d_L)) ;            // num (based on gradient and laplacian)\n    d_den  = (fp)1 + ((fp)0.25*d_L);                        // den (based on laplacian)\n    d_qsqr = d_num/(d_den*d_den);                    // qsqr (based on num and den)\n   \n    // diffusion coefficent (equ 33) (every element of IMAGE)\n    d_den = (d_qsqr-d_q0sqr) / (d_q0sqr * ((fp)1.0+d_q0sqr)) ;        // den (based on qsqr and q0sqr)\n    d_c_loc = (fp)1.0 / ((fp)1.0+d_den) ;                    // diffusion coefficient (based on den)\n      \n    // saturate diffusion coefficent to 0-1 range\n    if (d_c_loc < 0){                          // if diffusion coefficient < 0\n      d_c_loc = 0;                          // ... set to 0\n    }\n    else if (d_c_loc > 1){                        // if diffusion coefficient > 1\n      d_c_loc = 1;                          // ... set to 1\n    }\n\n    // save data to global memory\n    d_dN[ei] = d_dN_loc; \n    d_dS[ei] = d_dS_loc; \n    d_dW[ei] = d_dW_loc; \n    d_dE[ei] = d_dE_loc;\n    d_c[ei] = d_c_loc;\n      \n  }\n  \n}"
        ],
        "/Users/gbolet/hecbench-roofline/src/srad-cuda/extract_kernel.cu": [
            "#define fp float\n\n\n__global__ void extract(const  long d_Ne, fp *d_I)\n{ \n\n  // indexes\n  int bx = blockIdx.x;                      // get current horizontal block index\n  int tx = threadIdx.x;                     // get current horizontal thread index\n  int ei = (bx*NUMBER_THREADS)+tx;          // unique thread id, more threads than actual elements !!!\n\n  // copy input to output & log uncompress\n  if(ei<d_Ne){                              // do only for the number of elements, omit extra threads\n\n    d_I[ei] = exp(d_I[ei]/(fp)255);             // exponentiate input IMAGE and copy to output image\n\n  }\n\n}"
        ],
        "/Users/gbolet/hecbench-roofline/src/srad-cuda/prepare_kernel.cu": [
            "#define fp float\n\n\n__global__ void prepare(const  long d_Ne,\n                      const fp *d_I,       // pointer to output image (DEVICE GLOBAL MEMORY)\n                      fp *d_sums,          // pointer to input image (DEVICE GLOBAL MEMORY)\n                      fp *d_sums2)\n{\n\n  // indexes\n  int bx = blockIdx.x;                     // get current horizontal block index\n  int tx = threadIdx.x;                    // get current horizontal thread index\n  int ei = (bx*NUMBER_THREADS)+tx;         // unique thread id, more threads than actual elements !!!\n\n  // copy input to output & log uncompress\n  if(ei<d_Ne){                             // do only for the number of elements, omit extra threads\n\n    d_sums[ei] = d_I[ei];\n    d_sums2[ei] = d_I[ei]*d_I[ei];\n\n  }\n\n}"
        ]
    },
    "present-cuda": {
        "/Users/gbolet/hecbench-roofline/src/present-cuda/main.cu": [
            "__global__ void present(\n    const int num,\n    const int rounds,\n    const uint8_t *__restrict__ plains, \n    const uint8_t *__restrict__ keys, \n          uint8_t *__restrict__ ciphers, \n    const uint8_t *__restrict__ sbox, \n    const uint8_t *__restrict__ sbox_pmt_0, \n    const uint8_t *__restrict__ sbox_pmt_1, \n    const uint8_t *__restrict__ sbox_pmt_2, \n    const uint8_t *__restrict__ sbox_pmt_3) \n{ \n  int gid = blockIdx.x * blockDim.x + threadIdx.x;\n  if (gid >= num) return;\n  const uint8_t *plain = plains + gid * 8;\n  const uint8_t *key = keys + gid * 10;\n  uint8_t *cipher = ciphers + gid * 8;\n  uint8_t rounh_counter = 1;\n\n  uint8_t state[8];\n  uint8_t rounh_key[10];\n\n  // add key\n  state[0] = plain[0] ^ key[0];\n  state[1] = plain[1] ^ key[1];\n  state[2] = plain[2] ^ key[2];\n  state[3] = plain[3] ^ key[3];\n  state[4] = plain[4] ^ key[4];\n  state[5] = plain[5] ^ key[5];\n  state[6] = plain[6] ^ key[6];\n  state[7] = plain[7] ^ key[7];\n\n  // update key\n  rounh_key[9] = key[6] << 5 | key[7] >> 3;\n  rounh_key[8] = key[5] << 5 | key[6] >> 3;\n  rounh_key[7] = key[4] << 5 | key[5] >> 3;\n  rounh_key[6] = key[3] << 5 | key[4] >> 3;\n  rounh_key[5] = key[2] << 5 | key[3] >> 3;\n  rounh_key[4] = key[1] << 5 | key[2] >> 3;\n  rounh_key[3] = key[0] << 5 | key[1] >> 3;\n  rounh_key[2] = key[9] << 5 | key[0] >> 3;\n  rounh_key[1] = key[8] << 5 | key[9] >> 3;\n  rounh_key[0] = key[7] << 5 | key[8] >> 3;\n\n  rounh_key[0] = (rounh_key[0] & 0x0F) | sbox[rounh_key[0] >> 4];\n\n  rounh_key[7] ^= rounh_counter >> 1;\n  rounh_key[8] ^= rounh_counter << 7;\n\n  // substitution and permutation\n  cipher[0] = \n    (sbox_pmt_3[state[0]] & 0xC0) | \n    (sbox_pmt_2[state[1]] & 0x30) |\n    (sbox_pmt_1[state[2]] & 0x0C) |\n    (sbox_pmt_0[state[3]] & 0x03);\n  cipher[1] = \n    (sbox_pmt_3[state[4]] & 0xC0) | \n    (sbox_pmt_2[state[5]] & 0x30) |\n    (sbox_pmt_1[state[6]] & 0x0C) | \n    (sbox_pmt_0[state[7]] & 0x03);\n\n  cipher[2] = \n    (sbox_pmt_0[state[0]] & 0xC0) | \n    (sbox_pmt_3[state[1]] & 0x30) |\n    (sbox_pmt_2[state[2]] & 0x0C) |\n    (sbox_pmt_1[state[3]] & 0x03);\n  cipher[3] = \n    (sbox_pmt_0[state[4]] & 0xC0) | \n    (sbox_pmt_3[state[5]] & 0x30) |\n    (sbox_pmt_2[state[6]] & 0x0C) |\n    (sbox_pmt_1[state[7]] & 0x03);\n\n  cipher[4] = \n    (sbox_pmt_1[state[0]] & 0xC0) | \n    (sbox_pmt_0[state[1]] & 0x30) |\n    (sbox_pmt_3[state[2]] & 0x0C) |\n    (sbox_pmt_2[state[3]] & 0x03);\n  cipher[5] = \n    (sbox_pmt_1[state[4]] & 0xC0) | \n    (sbox_pmt_0[state[5]] & 0x30) |\n    (sbox_pmt_3[state[6]] & 0x0C) |\n    (sbox_pmt_2[state[7]] & 0x03);\n\n  cipher[6] = \n    (sbox_pmt_2[state[0]] & 0xC0) | \n    (sbox_pmt_1[state[1]] & 0x30) |\n    (sbox_pmt_0[state[2]] & 0x0C) |\n    (sbox_pmt_3[state[3]] & 0x03);\n  cipher[7] = \n    (sbox_pmt_2[state[4]] & 0xC0) | \n    (sbox_pmt_1[state[5]] & 0x30) |\n    (sbox_pmt_0[state[6]] & 0x0C) |\n    (sbox_pmt_3[state[7]] & 0x03);\n\n  for (rounh_counter = 2; rounh_counter <= rounds; rounh_counter++) {\n    state[0] = cipher[0] ^ rounh_key[0];\n    state[1] = cipher[1] ^ rounh_key[1];\n    state[2] = cipher[2] ^ rounh_key[2];\n    state[3] = cipher[3] ^ rounh_key[3];\n    state[4] = cipher[4] ^ rounh_key[4];\n    state[5] = cipher[5] ^ rounh_key[5];\n    state[6] = cipher[6] ^ rounh_key[6];\n    state[7] = cipher[7] ^ rounh_key[7];\n\n    cipher[0] = \n      (sbox_pmt_3[state[0]] & 0xC0) | \n      (sbox_pmt_2[state[1]] & 0x30) |\n      (sbox_pmt_1[state[2]] & 0x0C) |\n      (sbox_pmt_0[state[3]] & 0x03);\n    cipher[1] = \n      (sbox_pmt_3[state[4]] & 0xC0) | \n      (sbox_pmt_2[state[5]] & 0x30) |\n      (sbox_pmt_1[state[6]] & 0x0C) | \n      (sbox_pmt_0[state[7]] & 0x03);\n\n    cipher[2] = \n      (sbox_pmt_0[state[0]] & 0xC0) | \n      (sbox_pmt_3[state[1]] & 0x30) |\n      (sbox_pmt_2[state[2]] & 0x0C) |\n      (sbox_pmt_1[state[3]] & 0x03);\n    cipher[3] = \n      (sbox_pmt_0[state[4]] & 0xC0) | \n      (sbox_pmt_3[state[5]] & 0x30) |\n      (sbox_pmt_2[state[6]] & 0x0C) |\n      (sbox_pmt_1[state[7]] & 0x03);\n\n    cipher[4] = \n      (sbox_pmt_1[state[0]] & 0xC0) | \n      (sbox_pmt_0[state[1]] & 0x30) |\n      (sbox_pmt_3[state[2]] & 0x0C) |\n      (sbox_pmt_2[state[3]] & 0x03);\n    cipher[5] = \n      (sbox_pmt_1[state[4]] & 0xC0) | \n      (sbox_pmt_0[state[5]] & 0x30) |\n      (sbox_pmt_3[state[6]] & 0x0C) |\n      (sbox_pmt_2[state[7]] & 0x03);\n\n    cipher[6] = \n      (sbox_pmt_2[state[0]] & 0xC0) | \n      (sbox_pmt_1[state[1]] & 0x30) |\n      (sbox_pmt_0[state[2]] & 0x0C) |\n      (sbox_pmt_3[state[3]] & 0x03);\n    cipher[7] = \n      (sbox_pmt_2[state[4]] & 0xC0) | \n      (sbox_pmt_1[state[5]] & 0x30) |\n      (sbox_pmt_0[state[6]] & 0x0C) |\n      (sbox_pmt_3[state[7]] & 0x03);\n\n    rounh_key[5] ^= rounh_counter << 2; // do this first, which may be faster\n\n    // use state[] for temporary storage\n    state[2] = rounh_key[9];\n    state[1] = rounh_key[8];\n    state[0] = rounh_key[7];\n\n    rounh_key[9] = rounh_key[6] << 5 | rounh_key[7] >> 3;\n    rounh_key[8] = rounh_key[5] << 5 | rounh_key[6] >> 3;\n    rounh_key[7] = rounh_key[4] << 5 | rounh_key[5] >> 3;\n    rounh_key[6] = rounh_key[3] << 5 | rounh_key[4] >> 3;\n    rounh_key[5] = rounh_key[2] << 5 | rounh_key[3] >> 3;\n    rounh_key[4] = rounh_key[1] << 5 | rounh_key[2] >> 3;\n    rounh_key[3] = rounh_key[0] << 5 | rounh_key[1] >> 3;\n    rounh_key[2] = state[2] << 5 | rounh_key[0] >> 3;\n    rounh_key[1] = state[1] << 5 | state[2] >> 3;\n    rounh_key[0] = state[0] << 5 | state[1] >> 3;\n\n    rounh_key[0] = (rounh_key[0] & 0x0F) | sbox[rounh_key[0] >> 4];\n  }\n\n  // if round is not equal to 31, then do not perform the last adding key operation\n  // this can be used in constructing PRESENT based algorithm, such as MAC\n  if (31 == rounds) {\n    cipher[0] ^= rounh_key[0];\n    cipher[1] ^= rounh_key[1];\n    cipher[2] ^= rounh_key[2];\n    cipher[3] ^= rounh_key[3];\n    cipher[4] ^= rounh_key[4];\n    cipher[5] ^= rounh_key[5];\n    cipher[6] ^= rounh_key[6];\n    cipher[7] ^= rounh_key[7];\n  }\n}"
        ]
    },
    "murmurhash3-cuda": {
        "/Users/gbolet/hecbench-roofline/src/murmurhash3-cuda/murmurhash3.cu": [
            "#define FORCE_INLINE inline __attribute__((always_inline))\n\n\n__host__ __device__\nFORCE_INLINE uint64_t getblock64 ( const uint8_t * p, uint32_t i )\n{\n  uint64_t s = 0;\n  for (uint32_t n = 0; n < 8; n++) {\n    s |= ((uint64_t)p[8*i+n] << (n*8));\n  }\n  return s;\n}\n\n__host__ __device__\ninline uint64_t rotl64 ( uint64_t x, int8_t r )\n{\n  return (x << r) | (x >> (64 - r));\n}\n\n__host__ __device__ \nvoid MurmurHash3_x64_128 (const void * key, const uint32_t len,\n                          const uint32_t seed, void * out)\n{\n  const uint8_t * data = (const uint8_t*)key;\n  const uint32_t nblocks = len / 16;\n\n  uint64_t h1 = seed;\n  uint64_t h2 = seed;\n\n  const uint64_t c1 = BIG_CONSTANT(0x87c37b91114253d5);\n  const uint64_t c2 = BIG_CONSTANT(0x4cf5ad432745937f);\n\n  for(uint32_t i = 0; i < nblocks; i++)\n  {\n    uint64_t k1 = getblock64(data,i*2+0);\n    uint64_t k2 = getblock64(data,i*2+1);\n\n    k1 *= c1; k1  = rotl64(k1,31); k1 *= c2; h1 ^= k1;\n\n    h1 = rotl64(h1,27); h1 += h2; h1 = h1*5+0x52dce729;\n\n    k2 *= c2; k2  = rotl64(k2,33); k2 *= c1; h2 ^= k2;\n\n    h2 = rotl64(h2,31); h2 += h1; h2 = h2*5+0x38495ab5;\n  }\n\n  const uint8_t * tail = (const uint8_t*)(data + nblocks*16);\n\n  uint64_t k1 = 0;\n  uint64_t k2 = 0;\n\n  switch(len & 15)\n  {\n    case 15: k2 ^= ((uint64_t)tail[14]) << 48;\n    case 14: k2 ^= ((uint64_t)tail[13]) << 40;\n    case 13: k2 ^= ((uint64_t)tail[12]) << 32;\n    case 12: k2 ^= ((uint64_t)tail[11]) << 24;\n    case 11: k2 ^= ((uint64_t)tail[10]) << 16;\n    case 10: k2 ^= ((uint64_t)tail[ 9]) << 8;\n    case  9: k2 ^= ((uint64_t)tail[ 8]) << 0;\n       k2 *= c2; k2  = rotl64(k2,33); k2 *= c1; h2 ^= k2;\n\n    case  8: k1 ^= ((uint64_t)tail[ 7]) << 56;\n    case  7: k1 ^= ((uint64_t)tail[ 6]) << 48;\n    case  6: k1 ^= ((uint64_t)tail[ 5]) << 40;\n    case  5: k1 ^= ((uint64_t)tail[ 4]) << 32;\n    case  4: k1 ^= ((uint64_t)tail[ 3]) << 24;\n    case  3: k1 ^= ((uint64_t)tail[ 2]) << 16;\n    case  2: k1 ^= ((uint64_t)tail[ 1]) << 8;\n    case  1: k1 ^= ((uint64_t)tail[ 0]) << 0;\n       k1 *= c1; k1  = rotl64(k1,31); k1 *= c2; h1 ^= k1;\n  };\n\n  h1 ^= len; h2 ^= len;\n\n  h1 += h2;\n  h2 += h1;\n\n  h1 = fmix64(h1);\n  h2 = fmix64(h2);\n\n  h1 += h2;\n  h2 += h1;\n\n  ((uint64_t*)out)[0] = h1;\n  ((uint64_t*)out)[1] = h2;\n}\n\n__global__\nvoid MurmurHash3_x64_128_kernel (\n   const uint8_t *__restrict__ d_keys,\n   const uint32_t *__restrict__ d_length,\n   const uint32_t *__restrict__ length, \n         uint64_t *__restrict__ d_out,\n   const uint32_t numKeys )\n{\n  uint32_t i = blockDim.x * blockIdx.x + threadIdx.x;\n  if (i < numKeys) \n    MurmurHash3_x64_128 (d_keys+d_length[i], length[i], i, d_out+i*2);\n}"
        ]
    },
    "keogh-cuda": {
        "/Users/gbolet/hecbench-roofline/src/keogh-cuda/main.cu": [
            "__global__\nvoid lb_keogh(const float *__restrict__ subject,\n              const float *__restrict__ avgs,\n              const float *__restrict__ stds, \n                    float *__restrict__ lb_keogh,\n              const float *__restrict__ lower_bound,\n              const float *__restrict__ upper_bound,\n              const int M,\n              const int N) \n{\n  // shared memory\n  extern __shared__ float cache[];\n\n  int lid = threadIdx.x;\n  int blockSize = blockDim.x * blockIdx.x;\n  int idx = blockSize + lid;\n\n  for (int k = lid; k < blockDim.x + M; k += blockDim.x)\n    if (blockSize + k < N) cache[k] = subject[blockSize + k];\n\n  __syncthreads();\n\n  if (idx < N-M+1) {\n\n    // obtain statistics\n    float residues = 0;\n    float avg = avgs[idx];\n    float std = stds[idx];\n\n    for (int i = 0; i < M; ++i) {\n      // differences to envelopes\n      float value = (cache[lid+i] - avg) / std;\n      float lower = value - lower_bound[i];\n      float upper = value - upper_bound[i];\n\n      // Euclidean or Manhattan distance?\n      residues += upper*upper*(upper > 0) + lower*lower*(lower < 0);\n    }\n\n    lb_keogh[idx] = residues;\n  }\n}"
        ]
    },
    "langford-cuda": {
        "/Users/gbolet/hecbench-roofline/src/langford-cuda/main.cu": [
            "__device__ int ffsll(int64_t x) {\n  for (int i = 0; i < 64; i++)\n    if ((x >> i) & 1) return (i+1);\n  return 0;\n}\n\n__device__\nvoid dfs(int64_t* p_result,\n    Availability<n> &availability,\n    Open<n> &open,\n    Stack<n> &stack,\n    PositionsGPUAligned<n>& pgpualigned,\n    const int32_t logical_thread_index)\n{\n  constexpr int two_n = 2 * n;\n  constexpr int64_t msb = lsb << (int64_t)(n - 1);\n  constexpr int64_t nn1 = lsb << (2 * n - 1);\n  PositionsGPU<n> &pos = *((PositionsGPU<n>*)(&pgpualigned[0]));\n  // initially none of the numbers 1, 2, ..., n have been placed;\n  // this is represented by setting bits 0..n-1 to 1 in avail\n  availability[0] = msb | (msb - 1);\n  open[0] = 0;\n  open[1] = 0;\n  int top = 0;\n  int8_t k, m, d, num_open;\n  // The following \"push\" and \"pop\" should be lambdas, but unfortunately Cuda C++ does not\n  // yet support reference capture in lambdas that can run on both CPU and GPU.\n  // Hoping for a compiler fix soon.\n#define push(k, m, d, num_open) do { \\\n  stack[top++] = k; \\\n  stack[top++] = m; \\\n  stack[top++] = d; \\\n  stack[top++] = num_open; \\\n} while (0)\n#define pop(k, m, d, num_open) do { \\\n  num_open = stack[--top]; \\\n  d = stack[--top]; \\\n  m = stack[--top]; \\\n  k = stack[--top]; \\\n} while (0)\n  // every solution starts out by opening a below-pair at position 0\n  push(0, -1, 0, 0);\n  while (top) {\n    pop(k, m, d, num_open);\n    int64_t* openings = open + 2 * k + 2;\n    openings[0] = openings[-2];\n    openings[1] = openings[-1];\n    int32_t avail = availability[k];\n    // On CPU, this macro trick improves perf over 10% by letting the compiler\n    // take advantage of the fact that d can only be 0 or 1.\n    // Makes no difference on GPU.\n#define place_macro(d) do { \\\n  if (m>=0) { \\\n    pos[m] = k; \\\n    avail ^= (lsb32 << m); \\\n    openings[d] &= (openings[d] - 1); \\\n  } else { \\\n    openings[d] |= (nn1 >> k); \\\n    ++num_open; \\\n  } \\\n} while (0)\n    if (d) {\n      place_macro(1);\n    } else {\n      place_macro(0);\n    }\n++k;\navailability[k] = avail;\nif (k == two_n) {\n  // this is equivalent to results.push_back(pos);\n  // p_results[0] is a counter;  after it follow the data\n  // atomic increment of counter in device memory (i.e., RAM DIMMs on the GPU board)\n  int64_t cnt = atomicAdd((unsigned long long*)p_result, (unsigned long long)1);\n  if (cnt < kLimit) {\n    constexpr int kAlignedCnt = (n + 7) / 8;\n    int64_t* dst = p_result + 1 + (kAlignedCnt * cnt);\n#pragma unroll\n    for (int i=0; i<kAlignedCnt; ++i) {\n      dst[i] = pgpualigned[i];\n    }\n  }\n  // if cnt reaches or exceeds kLimit, that will be detected and the program will fail\n} else {\n  // A super-naive way to divide the work across threads.  A hash of the current state at k_limit\n  // determines whether the current thread should be pursuing a completion from this state or not.\n  // The depth k_limit is chosen empirically to be both shallow enough so it's quick to reach and\n  // deep enough to allow plenty of concurrency. This seems to work remarkably well in practice.\n  constexpr int8_t k_limit = (n > 19 ? (8 + (n / 3)) : (n - 5));\n  if (kNumLogicalThreads > 1 &&\n      k == k_limit &&\n      // multiply by a nice Mersenne prime to divide the work evenly across the threads... it works well...\n      uint64_t(131071 * (openings[1] - openings[0]) + avail) % kNumLogicalThreads != logical_thread_index) {\n    // some other thread will work on this\n    continue;\n  }\n  // Now push on the stack the the children of the current node in the search tree.\n  int8_t offset = k - two_n - 2;\n  for (d=0; d<2; ++d) {\n    if (openings[d]) { // if there is an opening, try closing it\n      //m = offset + __ffsll(openings[d]);\n      m = offset + ffsll(openings[d]);\n      // m could be -1, for example if the decision at pos k-1 was to open;\n      // only m from 0 .. n - 1 are useful\n      if (((unsigned)m < n) && ((avail >> m) & 1)) {\n        if (m || k <= n) { // this dedups L <==> R reversal twins\n          push(k, m, d, num_open);\n        }\n      }\n    }\n  }\n  if (num_open < n) {\n    push(k, -1, 1, num_open);\n    push(k, -1, 0, num_open);\n  }\n}\n}\n}\n\n__global__\nvoid dfs_gpu(int64_t* p_result) {\n  __shared__ Availability<n> availability[kThreadsPerBlock];\n  // PositionsGPU<n> pos;\n  __shared__ Open<n> open[kThreadsPerBlock];\n  // there are 2*n positions with 3 decisions per position and 4 bytes per decision on the stack\n  __shared__ Stack<n> stack[kThreadsPerBlock];\n  __shared__ PositionsGPUAligned<n> pgpualigned[kThreadsPerBlock];\n  // the size of the arrays above add up to ~2KB for n=32\n  // this bodes well for fitting tousands of threads inside on-chip memory\n  // assume 1D grid of 1D blocks of threads\n  const int32_t result_index = blockIdx.x * kThreadsPerBlock + threadIdx.x;\n  dfs<n>(p_result,\n      availability[threadIdx.x],\n      open[threadIdx.x],\n      stack[threadIdx.x],\n      pgpualigned[threadIdx.x],\n      result_index);\n}"
        ]
    },
    "projectile-cuda": {
        "/Users/gbolet/hecbench-roofline/src/projectile-cuda/Projectile.cu": [
            "__global__ void CalculateRange(const Projectile *obj, Projectile *pObj) {  \n  \n  int i = blockDim.x*blockIdx.x + threadIdx.x;\n  if (i >= num_elements) return;\n  float proj_angle = obj[i].getangle();\n  float proj_vel = obj[i].getvelocity();\n  float sin_value = sinf(proj_angle * kPIValue / 180.0f);\n  float cos_value = cosf(proj_angle * kPIValue / 180.0f);\n  float total_time = fabsf((2 * proj_vel * sin_value)) / kGValue;\n  float max_range =  fabsf(proj_vel * total_time * cos_value);\n  float max_height = (proj_vel * proj_vel * sin_value * sin_value) / 2.0f *\n                     kGValue;  // h = v^2 * sin^2theta/2g\n\n  pObj[i].setRangeandTime(max_range, total_time, proj_angle, proj_vel, max_height);\n}"
        ]
    },
    "binomial-cuda": {
        "/Users/gbolet/hecbench-roofline/src/binomial-cuda/kernel.cu": [
            "inline __host__ __device__ uint4 make_uint4(int4 a)\n{\n    return make_uint4(uint(a.x), uint(a.y), uint(a.z), uint(a.w));\n}\n\ninline __host__ __device__ uint4 max(uint4 a, uint4 b)\n{\n    return make_uint4(max(a.x,b.x), max(a.y,b.y), max(a.z,b.z), max(a.w,b.w));\n}\n\n__device__ inline double expiryCallValue(double S, double X, double vDt, int i)\n{\n  double d = S * exp(vDt * (2.0 * i - NUM_STEPS)) - X;\n  return (d > 0.0) ? d : 0.0;\n}\n\n__global__ void binomialOptionsKernel(const __TOptionData *__restrict d_OptionData,\n                                      real *__restrict d_CallValue)\n{\n  __shared__ real call_exchange[THREADBLOCK_SIZE + 1];\n\n  const int     tid = threadIdx.x;\n  const real      S = d_OptionData[blockIdx.x].S;\n  const real      X = d_OptionData[blockIdx.x].X;\n  const real    vDt = d_OptionData[blockIdx.x].vDt;\n  const real puByDf = d_OptionData[blockIdx.x].puByDf;\n  const real pdByDf = d_OptionData[blockIdx.x].pdByDf;\n\n  real call[ELEMS_PER_THREAD + 1];\n#pragma unroll\n  for(int i = 0; i < ELEMS_PER_THREAD; ++i)\n    call[i] = expiryCallValue(S, X, vDt, tid * ELEMS_PER_THREAD + i);\n\n  if (tid == 0)\n    call_exchange[THREADBLOCK_SIZE] = expiryCallValue(S, X, vDt, NUM_STEPS);\n\n  int final_it = max(0, tid * ELEMS_PER_THREAD - 1);\n\n#pragma unroll 16\n  for(int i = NUM_STEPS; i > 0; --i)\n  {\n    call_exchange[tid] = call[0];\n    __syncthreads();\n    call[ELEMS_PER_THREAD] = call_exchange[tid + 1];\n    __syncthreads();\n\n    if (i > final_it)\n    {\n#pragma unroll\n      for(int j = 0; j < ELEMS_PER_THREAD; ++j)\n        call[j] = puByDf * call[j + 1] + pdByDf * call[j];\n    }\n  }\n\n  if (tid == 0)\n  {\n    d_CallValue[blockIdx.x] = call[0];\n  }\n}"
        ]
    },
    "convolution3D-cuda": {
        "/Users/gbolet/hecbench-roofline/src/convolution3D-cuda/main.cu": [
            "#define T ((int)32)\n\n\n__global__\nvoid conv3d_s1(const T * __restrict__ X,\n               const T * __restrict__ W,\n                     T * __restrict__ Y,\n               const int C,\n               const int M,\n               const int K,\n               const int Hin,\n               const int Win,\n               const int Hout,\n               const int Wout,\n               const int W_grid)\n{\n  int n = blockIdx.x;\n  int m = blockIdx.y;\n  int h = blockIdx.z / W_grid * TILE_WIDTH + threadIdx.y;\n  int w = blockIdx.z % W_grid * TILE_WIDTH + threadIdx.x;\n  if (h < Hout && w < Wout) {\n    T s = 0;\n    for (int c = 0; c < C; c++) {\n      for (int p = 0; p < K; p++) {\n        for (int q = 0; q < K; q++) {\n          s += X[II(n, c, h+p, w+q)] * W[WI(m, c, p, q)];\n        }\n      }\n    }\n    Y[OI(n, m, h, w)] = s;\n  }\n}",
            "#define T ((int)32)\n\n\n__global__\nvoid conv3d_s2(const T * __restrict__ X,\n               const T * __restrict__ W,\n                     T * __restrict__ Y,\n               const int C,\n               const int M,\n               const int K,\n               const int Hin,\n               const int Win,\n               const int Hout,\n               const int Wout,\n               const int W_grid)\n{\n  int m = blockIdx.x;\n  int h = blockIdx.y / W_grid * TILE_WIDTH + threadIdx.y;\n  int w = blockIdx.y % W_grid * TILE_WIDTH + threadIdx.x;\n  int n = blockIdx.z;\n  if (h < Hout && w < Wout) {\n    T s = 0;\n    for (int c = 0; c < C; c++) {\n      for (int p = 0; p < K; p++) {\n        for (int q = 0; q < K; q++) {\n          s += X[II(n, c, h+p, w+q)] * W[WI(m, c, p, q)];\n        }\n      }\n    }\n    Y[OI(n, m, h, w)] = s;\n  }\n}",
            "#define T ((int)32)\n\n\n__global__\nvoid conv3d_s3(const T * __restrict__ X,\n               const T * __restrict__ W,\n                     T * __restrict__ Y,\n               const int C,\n               const int M,\n               const int K,\n               const int Hin,\n               const int Win,\n               const int Hout,\n               const int Wout,\n               const int W_grid)\n{\n  int h = blockIdx.x / W_grid * TILE_WIDTH + threadIdx.y;\n  int w = blockIdx.x % W_grid * TILE_WIDTH + threadIdx.x;\n  int n = blockIdx.y;\n  int m = blockIdx.z;\n  if (h < Hout && w < Wout) {\n    T s = 0;\n    for (int c = 0; c < C; c++) {\n      for (int p = 0; p < K; p++) {\n        for (int q = 0; q < K; q++) {\n          s += X[II(n, c, h+p, w+q)] * W[WI(m, c, p, q)];\n        }\n      }\n    }\n    Y[OI(n, m, h, w)] = s;\n  }\n}"
        ]
    },
    "collision-cuda": {
        "/Users/gbolet/hecbench-roofline/src/collision-cuda/main.cu": [
            "__device__ int hasDuplicate[32];\n\n#define T ((int)32)\n\n\n__device__ __forceinline__ int getBit(int val, int pos) {\n  return (val >> pos) & 0x1;\n}\n\n__device__ __forceinline__ int getLaneId() {\n  int laneId = threadIdx.x % WARP_SIZE;\n  return laneId;\n}\n\n__device__ T warpBitonicSort(T val) {\n  const int laneId = getLaneId();\n  // 2\n  val = detail::shflSwap<T, Comparator>(\n    val, 0x01, getBit(laneId, 1) ^ getBit(laneId, 0));\n\n  // 4\n  val = detail::shflSwap<T, Comparator>(\n    val, 0x02, getBit(laneId, 2) ^ getBit(laneId, 1));\n  val = detail::shflSwap<T, Comparator>(\n    val, 0x01, getBit(laneId, 2) ^ getBit(laneId, 0));\n\n  // 8\n  val = detail::shflSwap<T, Comparator>(\n    val, 0x04, getBit(laneId, 3) ^ getBit(laneId, 2));\n  val = detail::shflSwap<T, Comparator>(\n    val, 0x02, getBit(laneId, 3) ^ getBit(laneId, 1));\n  val = detail::shflSwap<T, Comparator>(\n    val, 0x01, getBit(laneId, 3) ^ getBit(laneId, 0));\n\n  // 16\n  val = detail::shflSwap<T, Comparator>(\n    val, 0x08, getBit(laneId, 4) ^ getBit(laneId, 3));\n  val = detail::shflSwap<T, Comparator>(\n    val, 0x04, getBit(laneId, 4) ^ getBit(laneId, 2));\n  val = detail::shflSwap<T, Comparator>(\n    val, 0x02, getBit(laneId, 4) ^ getBit(laneId, 1));\n  val = detail::shflSwap<T, Comparator>(\n    val, 0x01, getBit(laneId, 4) ^ getBit(laneId, 0));\n\n  // 32\n  val = detail::shflSwap<T, Comparator>(\n    val, 0x10, getBit(laneId, 4));\n  val = detail::shflSwap<T, Comparator>(\n    val, 0x08, getBit(laneId, 3));\n  val = detail::shflSwap<T, Comparator>(\n    val, 0x04, getBit(laneId, 2));\n  val = detail::shflSwap<T, Comparator>(\n    val, 0x02, getBit(laneId, 1));\n  val = detail::shflSwap<T, Comparator>(\n    val, 0x01, getBit(laneId, 0));\n\n  return val;\n}\n\n__device__ __forceinline__ bool warpHasCollision(T val) {\n  // -sort all values\n  // -compare our lower neighbor's value against ourselves (excepting\n  //  the first lane)\n  // -if any lane as a difference of 0, there is a duplicate\n  //  (excepting the first lane)\n  val = warpBitonicSort<T, LessThan<T>>(val);\n  const T lower = __shfl_up_sync(MASK, val, 1);\n\n  // Shuffle for lane 0 will present its same value, so only\n  // subsequent lanes will detect duplicates\n  const bool dup = (lower == val) && (getLaneId() != 0);\n  return (__any_sync(MASK, dup) != 0);\n}\n\n__global__ void checkDuplicates(int num, const int* v) {\n  hasDuplicate[threadIdx.x] = (int) warpHasCollision(v[threadIdx.x]);\n}",
            "#define T ((int)32)\n\n\n__device__ __forceinline__ int getBit(int val, int pos) {\n  return (val >> pos) & 0x1;\n}\n\n__device__ __forceinline__ int getLaneId() {\n  int laneId = threadIdx.x % WARP_SIZE;\n  return laneId;\n}\n\n__device__ T warpBitonicSort(T val) {\n  const int laneId = getLaneId();\n  // 2\n  val = detail::shflSwap<T, Comparator>(\n    val, 0x01, getBit(laneId, 1) ^ getBit(laneId, 0));\n\n  // 4\n  val = detail::shflSwap<T, Comparator>(\n    val, 0x02, getBit(laneId, 2) ^ getBit(laneId, 1));\n  val = detail::shflSwap<T, Comparator>(\n    val, 0x01, getBit(laneId, 2) ^ getBit(laneId, 0));\n\n  // 8\n  val = detail::shflSwap<T, Comparator>(\n    val, 0x04, getBit(laneId, 3) ^ getBit(laneId, 2));\n  val = detail::shflSwap<T, Comparator>(\n    val, 0x02, getBit(laneId, 3) ^ getBit(laneId, 1));\n  val = detail::shflSwap<T, Comparator>(\n    val, 0x01, getBit(laneId, 3) ^ getBit(laneId, 0));\n\n  // 16\n  val = detail::shflSwap<T, Comparator>(\n    val, 0x08, getBit(laneId, 4) ^ getBit(laneId, 3));\n  val = detail::shflSwap<T, Comparator>(\n    val, 0x04, getBit(laneId, 4) ^ getBit(laneId, 2));\n  val = detail::shflSwap<T, Comparator>(\n    val, 0x02, getBit(laneId, 4) ^ getBit(laneId, 1));\n  val = detail::shflSwap<T, Comparator>(\n    val, 0x01, getBit(laneId, 4) ^ getBit(laneId, 0));\n\n  // 32\n  val = detail::shflSwap<T, Comparator>(\n    val, 0x10, getBit(laneId, 4));\n  val = detail::shflSwap<T, Comparator>(\n    val, 0x08, getBit(laneId, 3));\n  val = detail::shflSwap<T, Comparator>(\n    val, 0x04, getBit(laneId, 2));\n  val = detail::shflSwap<T, Comparator>(\n    val, 0x02, getBit(laneId, 1));\n  val = detail::shflSwap<T, Comparator>(\n    val, 0x01, getBit(laneId, 0));\n\n  return val;\n}\n\n__device__ __forceinline__ unsigned int warpCollisionMask(T val) {\n  // -sort all (lane, value) pairs on value\n  // -compare our lower neighbor's value against ourselves (excepting\n  //  the first lane)\n  // -if any lane as a difference of 0, there is a duplicate\n  //  (excepting the first lane)\n  // -shuffle sort (originating lane, dup) pairs back to the original\n  //  lane and report\n  Pair<T, int> pVal(val, getLaneId());\n\n  pVal = warpBitonicSort<Pair<T, int>, LessThan<Pair<T, int> > >(pVal);\n\n  // If our neighbor is the same as us, we know our thread's value is\n  // duplicated. All except for lane 0, since shfl will present its\n  // own value (and if lane 0's value is duplicated, lane 1 will pick\n  // that up)\n  const unsigned long lower = __shfl_up_sync(MASK, pVal.k, 1);\n  Pair<int, bool> dup(pVal.v, (lower == pVal.k) && (getLaneId() != 0));\n\n  // Sort back based on lane ID so each thread originally knows\n  // whether or not it duplicated\n  dup = warpBitonicSort<Pair<int, bool>,\n                        LessThan<Pair<int, bool> > >(dup);\n  return __ballot_sync(MASK, dup.v);\n}\n\n__global__ void checkDuplicateMask(int num, const int* v) {\n  unsigned int mask = warpCollisionMask(v[threadIdx.x]);\n  if (threadIdx.x == 0) {\n    duplicateMask = mask;\n  }\n}"
        ]
    },
    "maxpool3d-cuda": {
        "/Users/gbolet/hecbench-roofline/src/maxpool3d-cuda/main.cu": [
            "#define DTYPE float\n\n\ninline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\ninline __host__ __device__ float4 fmaxf(float4 a, float4 b)\n{\n    return make_float4(fmaxf(a.x,b.x), fmaxf(a.y,b.y), fmaxf(a.z,b.z), fmaxf(a.w,b.w));\n}\n\n__global__ void\nmaxpool3d(\n  const DTYPE* i_img,\n        DTYPE* o_img,\n  const int Hstride,\n  const int Vstride,\n  const int pool_width,\n  const int pool_height,\n  const int i_img_width,\n  const int i_img_height,\n  const int o_img_width,\n  const int o_img_height )\n{\n  const int x = blockDim.x * blockIdx.x + threadIdx.x;\n  const int y = blockDim.y * blockIdx.y + threadIdx.y;\n  const int z = blockDim.z * blockIdx.z + threadIdx.z;\n  const int xidx = Hstride * x;\n  const int yidx = Vstride * y;\n  DTYPE maxval = (DTYPE)0;\n\n  for (int r = 0; r < pool_height; r++) \n  { \n    const int idxIntmp = ((z * i_img_height + yidx + r) * i_img_width) + xidx;\n    for(int c = 0; c < pool_width; c++)\n    {\n      const int idxIn = idxIntmp + c;\n      maxval = fmaxf(maxval, i_img[idxIn]);\n    }\n  }\n  o_img[(((z * o_img_height) + y) * o_img_width) + x] = maxval;\n}"
        ]
    },
    "wyllie-cuda": {
        "/Users/gbolet/hecbench-roofline/src/wyllie-cuda/main.cu": [
            "__global__\nvoid wyllie ( long *list , const int size )\n{\n  int index = blockIdx.x * blockDim.x + threadIdx.x;\n  if(index < size )\n  {\n    long node, next;\n    while ( ((node = list[index]) >> 32) != NIL && \n            ((next = list[node >> 32]) >> 32) != NIL )\n    {\n      long temp = (node & MASK) ;\n      temp += (next & MASK) ;\n      temp += (next >> 32) << 32;\n      __syncthreads();\n      list [ index ] = temp ;\n    } \n  }\n}"
        ]
    },
    "sobel-cuda": {
        "/Users/gbolet/hecbench-roofline/src/sobel-cuda/kernels.cu": [
            "inline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\ninline __host__ __device__ float4 sqrtf(float4 v)\n{\n    return make_float4(sqrtf(v.x), sqrtf(v.y), sqrtf(v.z), sqrtf(v.w));\n}\n\ninline __device__ float4 convert_float4(uchar4 data) \n{\n   float4 r = make_float4(data.x, data.y, data.z, data.w);\n   return r;\n}\n\ninline __device__ uchar4 convert_uchar4(float4 v) {\n  uchar4 res;\n  res.x = (uchar) ((v.x > 255.f) ? 255.f : (v.x < 0.f ? 0.f : v.x));\n  res.y = (uchar) ((v.y > 255.f) ? 255.f : (v.y < 0.f ? 0.f : v.y));\n  res.z = (uchar) ((v.z > 255.f) ? 255.f : (v.z < 0.f ? 0.f : v.z));\n  res.w = (uchar) ((v.w > 255.f) ? 255.f : (v.w < 0.f ? 0.f : v.w));\n  return res;\n}\n\n__global__\nvoid sobel_filter(const uchar4*__restrict__ inputImage, \n                        uchar4*__restrict__ outputImage, \n                  const uint width,\n                  const uint height)\n{\n  uint x = blockDim.x * blockIdx.x + threadIdx.x;\n  uint y = blockDim.y * blockIdx.y + threadIdx.y;\n\n  /* Read each texel component and calculate the filtered value using neighbouring texel components */\n  if( x >= 1 && x < (width-1) && y >= 1 && y < height - 1)\n  {\n    int c = x + y * width;\n    float4 i00 = convert_float4(inputImage[c - 1 - width]);\n    float4 i01 = convert_float4(inputImage[c - width]);\n    float4 i02 = convert_float4(inputImage[c + 1 - width]);\n\n    float4 i10 = convert_float4(inputImage[c - 1]);\n    float4 i12 = convert_float4(inputImage[c + 1]);\n\n    float4 i20 = convert_float4(inputImage[c - 1 + width]);\n    float4 i21 = convert_float4(inputImage[c + width]);\n    float4 i22 = convert_float4(inputImage[c + 1 + width]);\n\n    const float4 two = make_float4(2.f, 2.f, 2.f, 2.f);\n\n    float4 Gx = i00 + two * i10 + i20 - i02 - two * i12 - i22;\n\n    float4 Gy = i00 - i20  + two * i01 - two * i21 + i02 - i22;\n\n    /* taking root of sums of squares of Gx and Gy */\n    outputImage[c] = convert_uchar4(make_float4(sqrtf(Gx.x*Gx.x + Gy.x*Gy.x)/2.f,\n                                                sqrtf(Gx.y*Gx.y + Gy.y*Gy.y)/2.f,\n                                                sqrtf(Gx.z*Gx.z + Gy.z*Gy.z)/2.f,\n                                                sqrtf(Gx.w*Gx.w + Gy.w*Gy.w)/2.f));\n  }\n}"
        ]
    },
    "gpp-cuda": {
        "/Users/gbolet/hecbench-roofline/src/gpp-cuda/kernel.h": [
            "#define dataType double\n\n\n__global__ void solver(\n    int number_bands, int ngpown, int ncouls,\n    const int *__restrict__ inv_igp_index,\n    const int *__restrict__ indinv,\n    const dataType *__restrict__ wx_array,\n    const CustomComplex<dataType> *__restrict__ wtilde_array,\n    const CustomComplex<dataType> *__restrict__ aqsmtemp,\n    const CustomComplex<dataType> *__restrict__ aqsntemp,\n    const CustomComplex<dataType> *__restrict__ I_eps_array,\n    const dataType *__restrict__ vcoul,\n    dataType *__restrict__ achtemp_re,\n    dataType *__restrict__ achtemp_im) \n{\n  dataType achtemp_re_loc[nend - nstart], achtemp_im_loc[nend - nstart];\n  for (int iw = nstart; iw < nend; ++iw) {\n    achtemp_re_loc[iw] = 0.00;\n    achtemp_im_loc[iw] = 0.00;\n  }\n\n  for (int n1 = blockIdx.x; n1 < number_bands; n1 += gridDim.x) // 512 iterations\n  {\n    for (int my_igp = blockIdx.y; my_igp < ngpown; my_igp += gridDim.y) // 1634 iterations\n    {\n      int indigp = inv_igp_index[my_igp];\n      int igp = indinv[indigp];\n      CustomComplex<dataType> sch_store1 =\n          CustomComplex_conj(aqsmtemp(n1, igp)) * aqsntemp(n1, igp) * 0.5 *\n          vcoul[igp];\n\n      for (int ig = threadIdx.x; ig < ncouls; ig += blockDim.x) {\n        #pragma unroll\n        for (int iw = nstart; iw < nend; ++iw) // 3 iterations\n        {\n          CustomComplex<dataType> wdiff =\n              wx_array[iw] - wtilde_array(my_igp, ig);\n          CustomComplex<dataType> delw =\n              wtilde_array(my_igp, ig) * CustomComplex_conj(wdiff) *\n              (1 / CustomComplex_real((wdiff * CustomComplex_conj(wdiff))));\n          CustomComplex<dataType> sch_array =\n              delw * I_eps_array(my_igp, ig) * sch_store1;\n\n          achtemp_re_loc[iw] += CustomComplex_real(sch_array);\n          achtemp_im_loc[iw] += CustomComplex_imag(sch_array);\n        }\n      }\n    } // ngpown\n  }   // number_bands\n\n  // Add the final results here\n  for (int iw = nstart; iw < nend; ++iw) {\n    atomicAdd(&achtemp_re[iw], achtemp_re_loc[iw]);\n    atomicAdd(&achtemp_im[iw], achtemp_im_loc[iw]);\n  }\n}"
        ]
    },
    "interleave-cuda": {
        "/Users/gbolet/hecbench-roofline/src/interleave-cuda/main.cu": [
            "__global__ void add_kernel_interleaved(\n    INTERLEAVED_T * const dest_ptr,\n    const INTERLEAVED_T * const src_ptr,\n    const unsigned int num_elements)\n{\n  const unsigned int tid = blockIdx.x * blockDim.x + threadIdx.x;\n  if (tid < num_elements)\n  {\n    for (unsigned int i=0; i<COUNT; i++)\n    {\n      dest_ptr[tid].s0 += src_ptr[tid].s0;\n      dest_ptr[tid].s1 += src_ptr[tid].s1;\n      dest_ptr[tid].s2 += src_ptr[tid].s2;\n      dest_ptr[tid].s3 += src_ptr[tid].s3;\n      dest_ptr[tid].s4 += src_ptr[tid].s4;\n      dest_ptr[tid].s5 += src_ptr[tid].s5;\n      dest_ptr[tid].s6 += src_ptr[tid].s6;\n      dest_ptr[tid].s7 += src_ptr[tid].s7;\n      dest_ptr[tid].s8 += src_ptr[tid].s8;\n      dest_ptr[tid].s9 += src_ptr[tid].s9;\n      dest_ptr[tid].sa += src_ptr[tid].sa;\n      dest_ptr[tid].sb += src_ptr[tid].sb;\n      dest_ptr[tid].sc += src_ptr[tid].sc;\n      dest_ptr[tid].sd += src_ptr[tid].sd;\n      dest_ptr[tid].se += src_ptr[tid].se;\n      dest_ptr[tid].sf += src_ptr[tid].sf;\n    }\n  }\n}",
            "__global__ void add_kernel_non_interleaved(\n    NON_INTERLEAVED_T * const dest_ptr,\n    const NON_INTERLEAVED_T * const src_ptr,\n    const unsigned int num_elements)\n{\n  const unsigned int tid = blockIdx.x * blockDim.x + threadIdx.x;\n  if (tid < num_elements)\n  {\n    for (unsigned int i=0; i<COUNT; i++)\n    {\n      dest_ptr->s0[tid] += src_ptr->s0[tid];\n      dest_ptr->s1[tid] += src_ptr->s1[tid];\n      dest_ptr->s2[tid] += src_ptr->s2[tid];\n      dest_ptr->s3[tid] += src_ptr->s3[tid];\n      dest_ptr->s4[tid] += src_ptr->s4[tid];\n      dest_ptr->s5[tid] += src_ptr->s5[tid];\n      dest_ptr->s6[tid] += src_ptr->s6[tid];\n      dest_ptr->s7[tid] += src_ptr->s7[tid];\n      dest_ptr->s8[tid] += src_ptr->s8[tid];\n      dest_ptr->s9[tid] += src_ptr->s9[tid];\n      dest_ptr->sa[tid] += src_ptr->sa[tid];\n      dest_ptr->sb[tid] += src_ptr->sb[tid];\n      dest_ptr->sc[tid] += src_ptr->sc[tid];\n      dest_ptr->sd[tid] += src_ptr->sd[tid];\n      dest_ptr->se[tid] += src_ptr->se[tid];\n      dest_ptr->sf[tid] += src_ptr->sf[tid];\n    }\n  }\n}"
        ]
    },
    "bezier-surface-cuda": {
        "/Users/gbolet/hecbench-roofline/src/bezier-surface-cuda/main.cu": [
            "#define FLOAT \t\t1\n\n\n__host__ __device__\ninline FLOAT BezierBlend(int k, FLOAT mu, int n) {\n  int nn, kn, nkn;\n  FLOAT   blend = 1;\n  nn        = n;\n  kn        = k;\n  nkn       = n - k;\n  while(nn >= 1) {\n    blend *= nn;\n    nn--;\n    if(kn > 1) {\n      blend /= (FLOAT)kn;\n      kn--;\n    }\n    if(nkn > 1) {\n      blend /= (FLOAT)nkn;\n      nkn--;\n    }\n  }\n  if(k > 0)\n    blend *= pow(mu, (FLOAT)k);\n  if(n - k > 0)\n    blend *= pow(1 - mu, (FLOAT)(n - k));\n  return (blend);\n}\n\n__global__\nvoid BezierGPU(const XYZ *inp, XYZ *outp, const int NI, const int NJ, const int RESOLUTIONI, const int RESOLUTIONJ) {\n  int i, j, ki, kj;\n  FLOAT   mui, muj, bi, bj;\n\n  i = blockDim.x * blockIdx.x + threadIdx.x;\n  if (i > RESOLUTIONI) return;\n\n  mui = i / (FLOAT)(RESOLUTIONI - 1);\n  for(j = 0; j < RESOLUTIONJ; j++) {\n    muj     = j / (FLOAT)(RESOLUTIONJ - 1);\n    XYZ out = {0, 0, 0};\n    //#pragma unroll\n    for(ki = 0; ki <= NI; ki++) {\n      bi = BezierBlend(ki, mui, NI);\n      //#pragma unroll\n      for(kj = 0; kj <= NJ; kj++) {\n        bj = BezierBlend(kj, muj, NJ);\n        out.x += (inp[ki * (NJ + 1) + kj].x * bi * bj);\n        out.y += (inp[ki * (NJ + 1) + kj].y * bi * bj);\n        out.z += (inp[ki * (NJ + 1) + kj].z * bi * bj);\n      }\n    }\n    outp[i * RESOLUTIONJ + j] = out;\n  }\n\n}"
        ]
    },
    "hogbom-cuda": {
        "/Users/gbolet/hecbench-roofline/src/hogbom-cuda/kernels.cu": [
            "__global__\nvoid k_findPeak(\n  const float *__restrict__ image, \n  size_t size,\n  Peak *__restrict__ absPeak)\n{\n\n  __shared__ float maxVal[findPeakWidth];\n  __shared__ size_t maxPos[findPeakWidth];\n  const int column = threadIdx.x + (blockIdx.x * blockDim.x);\n  maxVal[threadIdx.x] = 0.f;\n  maxPos[threadIdx.x] = 0;\n\n  for (int idx = column; idx < size; idx += findPeakWidth*findPeakNBlocks) {\n    if (fabsf(image[idx]) > fabsf(maxVal[threadIdx.x])) {\n      maxVal[threadIdx.x] = image[idx];\n      maxPos[threadIdx.x] = idx;\n    }\n  }\n\n  __syncthreads();\n\n  if (threadIdx.x == 0) {\n    absPeak[blockIdx.x].val = 0.f;\n    absPeak[blockIdx.x].pos = 0;\n    for (int i = 0; i < findPeakWidth; ++i) {\n      if (fabsf(maxVal[i]) > fabsf(absPeak[blockIdx.x].val)) {\n        absPeak[blockIdx.x].val = maxVal[i];\n        absPeak[blockIdx.x].pos = maxPos[i];\n      }\n    }\n  }\n}",
            "__host__ __device__\n    Position(int _x, int _y) : x(_x), y(_y) { }\n\n__device__\nstatic size_t posToIdx(const int width, const Position& pos)\n{\n  return (pos.y * width) + pos.x;\n}\n\n__global__\nvoid k_subtractPSF(\n    const float *__restrict__ d_psf,\n    const int psfWidth,\n          float *__restrict__ d_residual,\n    const int residualWidth,\n    const int startx, const int starty,\n    const int stopx, const int stopy,\n    const int diffx, const int diffy,\n    const float absPeakVal, const float gain)\n{   \n  const int x = startx + threadIdx.x + blockIdx.x * blockDim.x;\n  const int y = starty + threadIdx.y + blockIdx.y * blockDim.y;\n\n  // thread blocks are of size 16, but the workload is not always a multiple of 16\n  if (x <= stopx && y <= stopy) {\n    d_residual[posToIdx(residualWidth, Position(x, y))] -= gain * absPeakVal\n      * d_psf[posToIdx(psfWidth, Position(x - diffx, y - diffy))];\n  }\n}"
        ]
    },
    "mpc-cuda": {
        "/Users/gbolet/hecbench-roofline/src/mpc-cuda/main.cu": [
            "static inline __device__\nvoid prefixsum(int &val, int sbuf[TPB])\n{\n  const int warp = threadIdx.x >> 5;\n  const int lane = threadIdx.x & 31;\n\n  for (int d = 1; d < 32; d *= 2) {\n    int tmp = __shfl_up(val, d);\n    if (lane >= d) val += tmp;\n  }\n  if (lane == 31) sbuf[warp] = val;\n\n  __syncthreads();\n  if (warp == 0) {\n    int v = sbuf[lane];\n    for (int d = 1; d < 32; d *= 2) {\n      int tmp = __shfl_up(v, d);\n      if (lane >= d) v += tmp;\n    }\n    sbuf[lane] = v;\n  }\n\n  __syncthreads();\n  if (warp > 0) {\n    val += sbuf[warp - 1];\n  }\n}\n\nstatic __global__ __launch_bounds__(1024, 2)\nvoid MPCcompress(\n  const int n, \n  long* __restrict__ const original,\n  long* __restrict__ const compressed,\n  volatile int* __restrict__ const goffset,\n  unsigned char dim)\n{\n  const int tid = threadIdx.x;\n  const int tidm1 = tid - 1;\n  const int tidmdim = tid - dim;\n  const int lanex = tid & 63;\n  const int warpx = tid & 0x3c0;\n  const int bid = blockIdx.x;\n  const int gdim = gridDim.x;\n  const int bid1 = ((bid + 1) == gdim) ? 0 : (bid + 1);\n  const int init = 1 + (n + 63) / 64;\n  const int chunksm1 = ((n + (TPB - 1)) / TPB) - 1;\n\n  __shared__ int start, top;\n  __shared__ long sbuf1[TPB], sbuf2[TPB];\n\n  for (int chunk = bid; chunk <= chunksm1; chunk += gdim) {\n    const int idx = tid + chunk * TPB;\n\n    long v1 = 0;\n    if (idx < n) {\n      v1 = original[idx];\n      sbuf1[tid] = v1;\n    }\n\n    __syncthreads();\n    if (tid >= dim) {\n      if (idx < n) {\n        v1 -= sbuf1[tidmdim];\n      }\n    }\n    sbuf2[tid] = v1;\n\n    __syncthreads();\n    long v2 = 0;\n\n    for (int i = 63; i >= 0; i--) {\n      v2 = (v2 << 1) + ((sbuf2[warpx + i] >> lanex) & 1);\n    }\n    sbuf1[tid] = v2;\n\n    __syncthreads();\n    if (tid > 0) {\n      v2 -= sbuf1[tidm1];\n    }\n\n    int loc = 0;\n    if (v2 != 0) loc = 1;\n\n#if (CUDART_VERSION < 9000)\n    unsigned int bitmap = __ballot(loc);\n#else\n    unsigned int bitmap = __ballot_sync(0xffffffff, loc);\n#endif\n\n    if (lanex == 32) {\n      sbuf2[tid] = bitmap;\n    }\n\n    __syncthreads();\n    if (lanex == 0) {\n      if (idx < n) compressed[1 + idx / 64] = (sbuf2[tid + 32] << 32) + bitmap;\n    }\n\n    prefixsum(loc, (int*)sbuf1);\n\n    if (v2 != 0) {\n      sbuf2[loc - 1] = v2;\n    }\n\n    if (tid == (TPB - 1)) {\n      int st = init;\n      if (chunk > 0) {\n        do {\n          st = goffset[bid];\n        } while (st < 0);  // busy waiting\n      }\n      goffset[bid1] = st + loc;\n      goffset[bid] = -1;\n      if (chunk == chunksm1) {\n        compressed[0] = (((long)(st + loc)) << 32) + (0x43504d00 - 1) + dim;\n      }\n      top = loc;\n      start = st;\n    }\n\n    __syncthreads();\n    if (tid < top) {\n      compressed[start + tid] = sbuf2[tid];\n    }\n  }\n}",
            "static inline __device__\nvoid prefixsum(int &val, int sbuf[TPB])\n{\n  const int warp = threadIdx.x >> 5;\n  const int lane = threadIdx.x & 31;\n\n  for (int d = 1; d < 32; d *= 2) {\n    int tmp = __shfl_up(val, d);\n    if (lane >= d) val += tmp;\n  }\n  if (lane == 31) sbuf[warp] = val;\n\n  __syncthreads();\n  if (warp == 0) {\n    int v = sbuf[lane];\n    for (int d = 1; d < 32; d *= 2) {\n      int tmp = __shfl_up(v, d);\n      if (lane >= d) v += tmp;\n    }\n    sbuf[lane] = v;\n  }\n\n  __syncthreads();\n  if (warp > 0) {\n    val += sbuf[warp - 1];\n  }\n}\n\nstatic inline __device__\nvoid prefixsumdimlong(long &val, long sbuf[TPB], const unsigned char dim)\n{\n  const int tid = threadIdx.x;\n  const int warp = tid >> 5;\n  const int lane = tid & 31;\n  const int tix = (warp * dim) + (tid % dim);\n\n  for (int d = dim; d < 32; d *= 2) {\n    unsigned int tmpl = __shfl_up((int)val, d);\n    long tmph = __shfl_up((int)(val >> 32), d);\n    if (lane >= d) val += (tmph << 32) + tmpl;\n  }\n  if ((lane + dim) > 31) sbuf[tix] = val;\n\n  __syncthreads();\n  if (warp < dim) {\n    const int idx = (lane * dim) + warp;\n    long v = sbuf[idx];\n    for (int d = 1; d < 32; d *= 2) {\n      unsigned int tmpl = __shfl_up((int)v, d);\n      long tmph = __shfl_up((int)(v >> 32), d);\n      if (lane >= d) v += (tmph << 32) + tmpl;\n    }\n    sbuf[idx] = v;\n  }\n\n  __syncthreads();\n  if (warp > 0) {\n    val += sbuf[tix - dim];\n  }\n}\n\nstatic inline __device__\nvoid prefixsumlong(long &val, long sbuf[TPB])\n{\n  const int warp = threadIdx.x >> 5;\n  const int lane = threadIdx.x & 31;\n\n  for (int d = 1; d < 32; d *= 2) {\n    unsigned int tmpl = __shfl_up((int)val, d);\n    long tmph = __shfl_up((int)(val >> 32), d);\n    if (lane >= d) val += (tmph << 32) + tmpl;\n  }\n  if (lane == 31) sbuf[warp] = val;\n\n  __syncthreads();\n  if (warp == 0) {\n    long v = sbuf[lane];\n    for (int d = 1; d < 32; d *= 2) {\n      unsigned int tmpl = __shfl_up((int)v, d);\n      long tmph = __shfl_up((int)(v >> 32), d);\n      if (lane >= d) v += (tmph << 32) + tmpl;\n    }\n    sbuf[lane] = v;\n  }\n\n  __syncthreads();\n  if (warp > 0) {\n    val += sbuf[warp - 1];\n  }\n}\n\nstatic __global__ __launch_bounds__(1024, 2)\nvoid MPCdecompress(\n  long* __restrict__ const compressed, \n  long* __restrict__ const decompressed,\n  volatile int* __restrict__ const goffset)\n{\n  const int dim = (compressed[0] & 31) + 1;\n  const int n = compressed[0] >> 32;\n  const int tid = threadIdx.x;\n  const int lanex = tid & 63;\n  const int warpx = tid & 0x3c0;\n  const int bid = blockIdx.x;\n  const int gdim = gridDim.x;\n  const int bid1 = ((bid + 1) == gdim) ? 0 : (bid + 1);\n  const int init = 1 + (n + 63) / 64;\n  const int nru = (n - 1) | 63;\n  const int chunksm1 = ((n + (TPB - 1)) / TPB) - 1;\n\n  __shared__ int start, top;\n  __shared__ long sbuf1[TPB], sbuf2[TPB];\n\n  for (int chunk = bid; chunk <= chunksm1; chunk += gdim) {\n    const int idx = tid + chunk * TPB;\n\n    int flag = 0;\n    if (idx <= nru) {\n      flag = (compressed[1 + idx / 64] >> lanex) & 1;\n    }\n    int loc = flag;\n\n    __syncthreads();\n    prefixsum(loc, (int*)sbuf1);\n\n    if (tid == (TPB - 1)) {\n      int st = init;\n      if (chunk > 0) {\n        do {\n          st = goffset[bid];\n        } while (st < 0);  // busy waiting\n      }\n      goffset[bid1] = st + loc;\n      goffset[bid] = -1;\n      top = loc;\n      start = st;\n    }\n\n    __syncthreads();\n    if (tid < top) {\n      sbuf2[tid] = compressed[start + tid];\n    }\n\n    __syncthreads();\n    long v2 = 0;\n    if (flag != 0) {\n      v2 = sbuf2[loc - 1];\n    }\n\n    prefixsumlong(v2, sbuf1);\n\n    sbuf2[tid] = v2;\n\n    __syncthreads();\n    long v1 = 0;\n    for (int i = 63; i >= 0; i--) {\n      v1 = (v1 << 1) + ((sbuf2[warpx + i] >> lanex) & 1);\n    }\n\n    prefixsumdimlong(v1, sbuf1, dim);\n\n    if (idx < n) {\n      decompressed[idx] = v1;\n    }\n  }\n}"
        ]
    },
    "quant-cuda": {
        "/Users/gbolet/hecbench-roofline/src/quant-cuda/main.cu": [
            "#define NUM 1024\n\n\n__host__ __device__ uint8_t\ndQuantize(float* smem_code, const float rand, float x)\n{\n    int pivot = 127;\n    int upper_pivot = 255;\n    int lower_pivot = 0;\n\n    float lower = -1.0f;\n    float upper = 1.0f;\n\n    float val = smem_code[pivot];\n    for(int i = 64; i > 0; i>>=1)\n    {\n        if(x > val)\n        {\n            lower_pivot = pivot;\n            lower = val;\n            pivot+=i;\n        }\n        else\n        {\n            upper_pivot = pivot;\n            upper = val;\n            pivot-=i;\n        }\n        val = smem_code[pivot];\n    }\n\n    if(upper_pivot == 255)\n        upper = smem_code[upper_pivot];\n    if(lower_pivot == 0)\n        lower = smem_code[lower_pivot];\n\n    if(!STOCHASTIC)\n    {\n      if(x > val)\n      {\n        float midpoint = (upper+val)*0.5f;\n        if(x > midpoint)\n        {\n          return upper_pivot;\n        }\n        else\n          return pivot;\n      }\n      else\n      {\n        float midpoint = (lower+val)*0.5f;\n        if(x < midpoint)\n          return lower_pivot;\n        else\n          return pivot;\n      }\n    }\n    else\n    {\n      if(x > val)\n      {\n        float dist_to_upper = fabsf(upper-x);\n        float dist_full = upper-val;\n        if(rand >= dist_to_upper/dist_full) return upper_pivot;\n        else return pivot;\n      }\n      else\n      {\n        float dist_to_lower = fabsf(lower-x);\n        float dist_full = val-lower;\n        if(rand >= dist_to_lower/dist_full) return lower_pivot;\n        else return pivot;\n      }\n    }\n}\n\n__global__ void kQuantize(const float *__restrict__ code,\n                          const float * __restrict__ A,\n                          uint8_t *out, const int n)\n{\n  const int bid = blockIdx.x;\n  const int tid = threadIdx.x;\n  const int dim = gridDim.x;\n  const int n_full = dim * NUM_BLOCK;\n  const int base_idx = bid * NUM_BLOCK;\n\n  float vals[NUM];\n  uint8_t qvals[NUM];\n\n  typedef BlockLoad<float, TH, NUM> LoadFloat;\n  typedef BlockStore<uint8_t, TH, NUM> StoreChar;\n\n  __shared__ typename LoadFloat::TempStorage loadf;\n  __shared__ typename StoreChar::TempStorage storec;\n  __shared__ float smem_code[256];\n\n  for (int i = tid; i < 256; i += blockDim.x)\n  {\n    smem_code[i] = code[i];\n  }\n\n  for (unsigned int i = base_idx; i < n_full; i += dim*NUM_BLOCK)\n  {\n      int valid_items = n - i > NUM_BLOCK ? NUM_BLOCK : n - i;\n\n      LoadFloat(loadf).Load(&(A[i]), vals, valid_items);\n\n      __syncthreads();\n\n      #pragma unroll\n      for(int j = 0; j < NUM; j++)\n          qvals[j] = dQuantize<0>(smem_code, 0.0f, vals[j]);\n\n      StoreChar(storec).Store(&(out[i]), qvals, valid_items);\n  }\n}"
        ]
    },
    "warpsort-cuda": {
        "/Users/gbolet/hecbench-roofline/src/warpsort-cuda/warpsort.cu": [
            "#define K 1.38065e-23        // J/K, Boltzmann constant\n\n\n#define N\t\t\t\t\t  6\n\n\n__device__ void\nwarpSortRegisters(const DeviceTensor<K, 1>& key,\n                  const DeviceTensor<V, 1>& value,\n                  DeviceTensor<K, 1>& sortedKey,\n                  DeviceTensor<V, 1>& sortedValue) {\n\n  // Load the elements we have available\n  Pair<K, V> pairs[N];\n  WarpRegisterPairLoaderUtils<K, V, N>::load(\n    pairs, key, value,\n    NumericLimits<K>::minPossible(),\n    NumericLimits<V>::minPossible());\n\n  // Recursively split, shuffle sort and merge sort back\n  Pair<K, V> sortedPairs[N];\n  warpSortRegisters<Pair<K, V>, Comparator, N>(pairs, sortedPairs);\n\n  // Write the warp's registers back out\n  WarpRegisterPairLoaderUtils<K, V, N>::save(\n    sortedKey, sortedValue, sortedPairs, key.getSize(0));\n}\n\n__device__ bool warpSort(const DeviceTensor<K, 1>& key,\n                         const DeviceTensor<V, 1>& value,\n                         DeviceTensor<K, 1>& sortedKey,\n                         DeviceTensor<V, 1>& sortedValue) {\n  assert(key.getSize(0) <= sortedKey.getSize(0) &&\n         value.getSize(0) <= sortedValue.getSize(0) &&\n         key.getSize(0) == value.getSize(0));\n\n  if (key.getSize(0) <= WARP_SIZE) {\n    warpSortRegisters<K, V, Comparator, 1>(\n      key, value, sortedKey, sortedValue);\n    return true;\n  } else if (key.getSize(0) <= 2 * WARP_SIZE) {\n    warpSortRegisters<K, V, Comparator, 2>(\n      key, value, sortedKey, sortedValue);\n    return true;\n  } else if (key.getSize(0) <= 3 * WARP_SIZE) {\n    warpSortRegisters<K, V, Comparator, 3>(\n      key, value, sortedKey, sortedValue);\n    return true;\n  } else if (key.getSize(0) <= 4 * WARP_SIZE) {\n    warpSortRegisters<K, V, Comparator, 4>(\n      key, value, sortedKey, sortedValue);\n    return true;\n  }\n\n  // size too large\n  return false;\n}\n\n__global__ void\nsortDevice(DeviceTensor<float, 1> data,\n           DeviceTensor<float, 1> out,\n           DeviceTensor<int, 1> indices) {\n  warpSort<float, int, GreaterThan<Pair<float, int> > >(data, out, indices);\n}"
        ]
    },
    "aidw-cuda": {
        "/Users/gbolet/hecbench-roofline/src/aidw-cuda/main.cu": [
            "inline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\ninline __host__ __device__ float4 sqrtf(float4 v)\n{\n    return make_float4(sqrtf(v.x), sqrtf(v.y), sqrtf(v.z), sqrtf(v.w));\n}\n\n__global__\nvoid AIDW_Kernel(\n    const float *__restrict dx, \n    const float *__restrict dy,\n    const float *__restrict dz,\n    const int dnum,\n    const float *__restrict ix,\n    const float *__restrict iy,\n          float *__restrict iz,\n    const int inum,\n    const float area,\n    const float *__restrict avg_dist) \n\n{\n  int tid = blockIdx.x * blockDim.x + threadIdx.x;\n  if(tid < inum) {\n    float sum = 0.f, dist = 0.f, t = 0.f, z = 0.f, alpha = 1.f;\n\n    float r_obs = avg_dist[tid];                // The observed average nearest neighbor distance\n    float r_exp = 0.5f / sqrtf(dnum / area);    // The expected nearest neighbor distance for a random pattern\n    float R_S0 = r_obs / r_exp;                 // The nearest neighbor statistic\n\n    // Normalize the R(S0) measure such that it is bounded by 0 and 1 by a fuzzy membership function \n    float u_R = 0.f;\n    if(R_S0 >= R_min) u_R = 0.5f-0.5f * cosf(3.1415926f / R_max * (R_S0 - R_min));\n    if(R_S0 >= R_max) u_R = 1.f;\n\n    // Determine the appropriate distance-decay parameter alpha by a triangular membership function\n    // Adaptive power parameter: a (alpha)\n    if(u_R>= 0.f && u_R<=0.1f)  alpha = a1; \n    if(u_R>0.1f && u_R<=0.3f)  alpha = a1*(1.f-5.f*(u_R-0.1f)) + a2*5.f*(u_R-0.1f);\n    if(u_R>0.3f && u_R<=0.5f)  alpha = a3*5.f*(u_R-0.3f) + a1*(1.f-5.f*(u_R-0.3f));\n    if(u_R>0.5f && u_R<=0.7f)  alpha = a3*(1.f-5.f*(u_R-0.5f)) + a4*5.f*(u_R-0.5f);\n    if(u_R>0.7f && u_R<=0.9f)  alpha = a5*5.f*(u_R-0.7f) + a4*(1.f-5.f*(u_R-0.7f));\n    if(u_R>0.9f && u_R<=1.f)  alpha = a5;\n    alpha *= 0.5f; // Half of the power\n\n    // Weighted average\n    for(int j = 0; j < dnum; j++) {\n      dist = (ix[tid] - dx[j]) * (ix[tid] - dx[j]) + (iy[tid] - dy[j]) * (iy[tid] - dy[j]) ;\n      t = 1.f / powf(dist, alpha);  sum += t;  z += dz[j] * t;\n    }\n    iz[tid] = z / sum;\n  }\n}",
            "inline __host__ __device__ uint4 make_uint4(int4 a)\n{\n    return make_uint4(uint(a.x), uint(a.y), uint(a.z), uint(a.w));\n}\n\ninline __host__ __device__ uint4 min(uint4 a, uint4 b)\n{\n    return make_uint4(min(a.x,b.x), min(a.y,b.y), min(a.z,b.z), min(a.w,b.w));\n}\n\ninline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\ninline __host__ __device__ float4 sqrtf(float4 v)\n{\n    return make_float4(sqrtf(v.x), sqrtf(v.y), sqrtf(v.z), sqrtf(v.w));\n}\n\n__global__\nvoid AIDW_Kernel_Tiled(\n    const float *__restrict dx, \n    const float *__restrict dy,\n    const float *__restrict dz,\n    const int dnum,\n    const float *__restrict ix,\n    const float *__restrict iy,\n          float *__restrict iz,\n    const int inum,\n    const float area,\n    const float *__restrict avg_dist)\n{\n  // Shared Memory\n  __shared__ float sdx[BLOCK_SIZE];\n  __shared__ float sdy[BLOCK_SIZE];\n  __shared__ float sdz[BLOCK_SIZE];\n\n  int tid = threadIdx.x + blockIdx.x * blockDim.x; \n  if (tid >= inum) return;\n\n  float dist = 0.f, t = 0.f, alpha = 0.f;\n\n  int part = (dnum - 1) / BLOCK_SIZE;\n  int m, e;\n\n  float sum_up = 0.f;\n  float sum_dn = 0.f;   \n  float six_s, siy_s;\n\n  float r_obs = avg_dist[tid];               //The observed average nearest neighbor distance\n  float r_exp = 0.5f / sqrtf(dnum / area); // The expected nearest neighbor distance for a random pattern\n  float R_S0 = r_obs / r_exp;                //The nearest neighbor statistic\n\n  float u_R = 0.f;\n  if(R_S0 >= R_min) u_R = 0.5f-0.5f * cosf(3.1415926f / R_max * (R_S0 - R_min));\n  if(R_S0 >= R_max) u_R = 1.f;\n\n  // Determine the appropriate distance-decay parameter alpha by a triangular membership function\n  // Adaptive power parameter: a (alpha)\n  if(u_R>= 0.f && u_R<=0.1f)  alpha = a1; \n  if(u_R>0.1f && u_R<=0.3f)  alpha = a1*(1.f-5.f*(u_R-0.1f)) + a2*5.f*(u_R-0.1f);\n  if(u_R>0.3f && u_R<=0.5f)  alpha = a3*5.f*(u_R-0.3f) + a1*(1.f-5.f*(u_R-0.3f));\n  if(u_R>0.5f && u_R<=0.7f)  alpha = a3*(1.f-5.f*(u_R-0.5f)) + a4*5.f*(u_R-0.5f);\n  if(u_R>0.7f && u_R<=0.9f)  alpha = a5*5.f*(u_R-0.7f) + a4*(1.f-5.f*(u_R-0.7f));\n  if(u_R>0.9f && u_R<=1.f)  alpha = a5;\n  alpha *= 0.5f; // Half of the power\n\n  float six_t = ix[tid];\n  float siy_t = iy[tid];\n  int lid = threadIdx.x;\n  for(m = 0; m <= part; m++) {  // Weighted Sum  \n    int num_threads = min(BLOCK_SIZE, dnum - BLOCK_SIZE *m);\n    if (lid < num_threads) {\n      sdx[lid] = dx[lid + BLOCK_SIZE * m];\n      sdy[lid] = dy[lid + BLOCK_SIZE * m];\n      sdz[lid] = dz[lid + BLOCK_SIZE * m];\n    }\n    __syncthreads();\n\n    for(e = 0; e < BLOCK_SIZE; e++) {\n      six_s = six_t - sdx[e];\n      siy_s = siy_t - sdy[e];\n      dist = (six_s * six_s + siy_s * siy_s);\n      t = 1.f / powf(dist, alpha);  sum_dn += t;  sum_up += t * sdz[e];\n    }\n    __syncthreads();\n  }\n  iz[tid] = sum_up / sum_dn;\n}"
        ]
    },
    "background-subtract-cuda": {
        "/Users/gbolet/hecbench-roofline/src/background-subtract-cuda/main.cu": [
            "inline __host__ __device__ int4 make_int4(float4 a)\n{\n    return make_int4(int(a.x), int(a.y), int(a.z), int(a.w));\n}\n\ninline __host__ __device__ int4 abs(int4 v)\n{\n    return make_int4(abs(v.x), abs(v.y), abs(v.z), abs(v.w));\n}\n\n__global__ void findMovingPixels(\n  const size_t imgSize,\n  const unsigned char *__restrict__ Img,\n  const unsigned char *__restrict__ Img1,\n  const unsigned char *__restrict__ Img2,\n  const unsigned char *__restrict__ Tn,\n        unsigned char *__restrict__ Mp) // moving pixel map\n{\n  size_t i = blockDim.x * blockIdx.x + threadIdx.x;\n  if (i >= imgSize) return;\n  if ( abs(Img[i] - Img1[i]) > Tn[i] || abs(Img[i] - Img2[i]) > Tn[i] )\n    Mp[i] = 255;\n  else {\n    Mp[i] = 0;\n  }\n}",
            "__global__ void updateBackground(\n  const size_t imgSize,\n  const unsigned char *__restrict__ Img,\n  const unsigned char *__restrict__ Mp,\n        unsigned char *__restrict__ Bn)\n{\n  size_t i = blockDim.x * blockIdx.x + threadIdx.x;\n  if (i >= imgSize) return;\n  if ( Mp[i] == 0 ) Bn[i] = 0.92f * Bn[i] + 0.08f * Img[i];\n}",
            "inline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\ninline __host__ __device__ float4 fmaxf(float4 a, float4 b)\n{\n    return make_float4(fmaxf(a.x,b.x), fmaxf(a.y,b.y), fmaxf(a.z,b.z), fmaxf(a.w,b.w));\n}\n\n__global__ void updateThreshold(\n  const size_t imgSize,\n  const unsigned char *__restrict__ Img,\n  const unsigned char *__restrict__ Mp,\n  const unsigned char *__restrict__ Bn,\n        unsigned char *__restrict__ Tn)\n{\n  size_t i = blockDim.x * blockIdx.x + threadIdx.x;\n  if (i >= imgSize) return;\n  if (Mp[i] == 0) {\n    float th = 0.92f * Tn[i] + 0.24f * (Img[i] - Bn[i]);\n    Tn[i] = fmaxf(th, 20.f);\n  }\n}"
        ]
    },
    "lid-driven-cavity-cuda": {
        "/Users/gbolet/hecbench-roofline/src/lid-driven-cavity-cuda/main.cu": [
            "#define Real float\n\n\n#define T ((int)32)\n\n\n__device__ void push(T const &v) {\n    buf[free_index] = v;\n    free_index += THREADS;\n  }\n\n__global__\nvoid set_BCs (Real*__restrict__ u, Real*__restrict__ v) \n{\n  int ind = (blockIdx.x * blockDim.x) + threadIdx.x + 1;\n\n  // left boundary\n  u(0, ind) = ZERO;\n  v(0, ind) = -v(1, ind);\n\n  // right boundary\n  u(NUM, ind) = ZERO;\n  v(NUM + 1, ind) = -v(NUM, ind);\n\n  // bottom boundary\n  u(ind, 0) = -u(ind, 1);\n  v(ind, 0) = ZERO;\n\n  // top boundary\n  u(ind, NUM + 1) = TWO - u(ind, NUM);\n  v(ind, NUM) = ZERO;\n\n  if (ind == NUM) {\n    // left boundary\n    u(0, 0) = ZERO;\n    v(0, 0) = -v(1, 0);\n    u(0, NUM + 1) = ZERO;\n    v(0, NUM + 1) = -v(1, NUM + 1);\n\n    // right boundary\n    u(NUM, 0) = ZERO;\n    v(NUM + 1, 0) = -v(NUM, 0);\n    u(NUM, NUM + 1) = ZERO;\n    v(NUM + 1, NUM + 1) = -v(NUM, NUM + 1);\n\n    // bottom boundary\n    u(0, 0) = -u(0, 1);\n    v(0, 0) = ZERO;\n    u(NUM + 1, 0) = -u(NUM + 1, 1);\n    v(NUM + 1, 0) = ZERO;\n\n    // top boundary\n    u(0, NUM + 1) = TWO - u(0, NUM);\n    v(0, NUM) = ZERO;\n    u(NUM + 1, NUM + 1) = TWO - u(NUM + 1, NUM);\n    v(ind, NUM + 1) = ZERO;\n  } // end if\n\n}",
            "#define Real float\n\n\n#define T ((int)32)\n\n\ninline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\ninline __host__ __device__ float4 fabs(float4 v)\n{\n    return make_float4(fabs(v.x), fabs(v.y), fabs(v.z), fabs(v.w));\n}\n\n__device__ void push(T const &v) {\n    buf[free_index] = v;\n    free_index += THREADS;\n  }\n\n__global__ \nvoid calculate_F (const Real dt,\n                  const Real*__restrict__ u,\n                  const Real*__restrict__ v,\n                        Real*__restrict__ F) \n{  \n  int row = (blockIdx.x * blockDim.x) + threadIdx.x + 1;\n  int col = (blockIdx.y * blockDim.y) + threadIdx.y + 1;\n\n  if (col == NUM) {\n    // right boundary, F_ij = u_ij\n    // also do left boundary\n    F(0, row) = u(0, row);\n    F(NUM, row) = u(NUM, row);\n  } else {\n\n    // u velocities\n    Real u_ij = u(col, row);\n    Real u_ip1j = u(col + 1, row);\n    Real u_ijp1 = u(col, row + 1);\n    Real u_im1j = u(col - 1, row);\n    Real u_ijm1 = u(col, row - 1);\n\n    // v velocities\n    Real v_ij = v(col, row);\n    Real v_ip1j = v(col + 1, row);\n    Real v_ijm1 = v(col, row - 1);\n    Real v_ip1jm1 = v(col + 1, row - 1);\n\n    // finite differences\n    Real du2dx, duvdy, d2udx2, d2udy2;\n\n    du2dx = (((u_ij + u_ip1j) * (u_ij + u_ip1j) - (u_im1j + u_ij) * (u_im1j + u_ij))\n        + mix_param * (fabs(u_ij + u_ip1j) * (u_ij - u_ip1j)\n          - fabs(u_im1j + u_ij) * (u_im1j - u_ij))) / (FOUR * dx);\n    duvdy = ((v_ij + v_ip1j) * (u_ij + u_ijp1) - (v_ijm1 + v_ip1jm1) * (u_ijm1 + u_ij)\n        + mix_param * (fabs(v_ij + v_ip1j) * (u_ij - u_ijp1)\n          - fabs(v_ijm1 + v_ip1jm1) * (u_ijm1 - u_ij))) / (FOUR * dy);\n    d2udx2 = (u_ip1j - (TWO * u_ij) + u_im1j) / (dx * dx);\n    d2udy2 = (u_ijp1 - (TWO * u_ij) + u_ijm1) / (dy * dy);\n\n    F(col, row) = u_ij + dt * (((d2udx2 + d2udy2) / Re_num) - du2dx - duvdy + gx);\n\n  } // end if\n\n}",
            "#define Real float\n\n\n#define T ((int)32)\n\n\ninline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\ninline __host__ __device__ float4 fabs(float4 v)\n{\n    return make_float4(fabs(v.x), fabs(v.y), fabs(v.z), fabs(v.w));\n}\n\n__device__ void push(T const &v) {\n    buf[free_index] = v;\n    free_index += THREADS;\n  }\n\n__global__ \nvoid calculate_G (const Real dt,\n                  const Real*__restrict__ u,\n                  const Real*__restrict__ v,\n                        Real*__restrict__ G) \n{\n  int row = (blockIdx.x * blockDim.x) + threadIdx.x + 1;\n  int col = (blockIdx.y * blockDim.y) + threadIdx.y + 1;\n\n  if (row == NUM) {\n    // top and bottom boundaries\n    G(col, 0) = v(col, 0);\n    G(col, NUM) = v(col, NUM);\n\n  } else {\n\n    // u velocities\n    Real u_ij = u(col, row);\n    Real u_ijp1 = u(col, row + 1);\n    Real u_im1j = u(col - 1, row);\n    Real u_im1jp1 = u(col - 1, row + 1);\n\n    // v velocities\n    Real v_ij = v(col, row);\n    Real v_ijp1 = v(col, row + 1);\n    Real v_ip1j = v(col + 1, row);\n    Real v_ijm1 = v(col, row - 1);\n    Real v_im1j = v(col - 1, row);\n\n    // finite differences\n    Real dv2dy, duvdx, d2vdx2, d2vdy2;\n\n    dv2dy = ((v_ij + v_ijp1) * (v_ij + v_ijp1) - (v_ijm1 + v_ij) * (v_ijm1 + v_ij)\n        + mix_param * (fabs(v_ij + v_ijp1) * (v_ij - v_ijp1)\n          - fabs(v_ijm1 + v_ij) * (v_ijm1 - v_ij))) / (FOUR * dy);\n    duvdx = ((u_ij + u_ijp1) * (v_ij + v_ip1j) - (u_im1j + u_im1jp1) * (v_im1j + v_ij)\n        + mix_param * (fabs(u_ij + u_ijp1) * (v_ij - v_ip1j) \n          - fabs(u_im1j + u_im1jp1) * (v_im1j - v_ij))) / (FOUR * dx);\n    d2vdx2 = (v_ip1j - (TWO * v_ij) + v_im1j) / (dx * dx);\n    d2vdy2 = (v_ijp1 - (TWO * v_ij) + v_ijm1) / (dy * dy);\n\n    G(col, row) = v_ij + dt * (((d2vdx2 + d2vdy2) / Re_num) - dv2dy - duvdx + gy);\n\n  } // end if\n\n}",
            "#define Real float\n\n\n__global__ \nvoid sum_pressure (const Real*__restrict__ pres_red,\n                   const Real*__restrict__ pres_black, \n                         Real*__restrict__ pres_sum) \n{\n  int row = (blockIdx.x * blockDim.x) + threadIdx.x + 1;\n  int col = (blockIdx.y * blockDim.y) + threadIdx.y + 1;\n\n  // shared memory for block's sum\n  __shared__ Real sum_cache[BLOCK_SIZE];\n\n  int NUM_2 = NUM >> 1;\n\n  Real pres_r = pres_red(col, row);\n  Real pres_b = pres_black(col, row);\n\n  // add squared pressure\n  sum_cache[threadIdx.x] = (pres_r * pres_r) + (pres_b * pres_b);\n\n  // synchronize threads in block to ensure all thread values stored\n  __syncthreads();\n\n  // add up values for block\n  int i = BLOCK_SIZE >> 1;\n  while (i != 0) {\n    if (threadIdx.x < i) {\n      sum_cache[threadIdx.x] += sum_cache[threadIdx.x + i];\n    }\n    __syncthreads();\n    i >>= 1;\n  }\n\n  // store block's summed values\n  if (threadIdx.x == 0) {\n    pres_sum[blockIdx.y + (gridDim.y * blockIdx.x)] = sum_cache[0];\n  }\n\n}",
            "#define Real float\n\n\n__global__ \nvoid set_horz_pres_BCs (Real*__restrict__ pres_red, Real*__restrict__ pres_black) \n{\n  int col = (blockIdx.x * blockDim.x) + threadIdx.x + 1;\n  col = (col * 2) - 1;\n\n  int NUM_2 = NUM >> 1;\n\n  // p_i,0 = p_i,1\n  pres_black(col, 0) = pres_red(col, 1);\n  pres_red(col + 1, 0) = pres_black(col + 1, 1);\n\n  // p_i,jmax+1 = p_i,jmax\n  pres_red(col, NUM_2 + 1) = pres_black(col, NUM_2);\n  pres_black(col + 1, NUM_2 + 1) = pres_red(col + 1, NUM_2);\n\n}",
            "#define Real float\n\n\n__global__\nvoid set_vert_pres_BCs (Real*__restrict__ pres_red, Real*__restrict__ pres_black) \n{\n  int row = (blockIdx.x * blockDim.x) + threadIdx.x + 1;\n\n  int NUM_2 = NUM >> 1;\n\n  // p_0,j = p_1,j\n  pres_black(0, row) = pres_red(1, row);\n  pres_red(0, row) = pres_black(1, row);\n\n  // p_imax+1,j = p_imax,j\n  pres_black(NUM + 1, row) = pres_red(NUM, row);\n  pres_red(NUM + 1, row) = pres_black(NUM, row);\n\n}",
            "#define Real float\n\n\n__global__\nvoid calc_residual (const Real dt,\n                    const Real*__restrict__ F,\n                    const Real*__restrict__ G, \n                    const Real*__restrict__ pres_red,\n                    const Real*__restrict__ pres_black,\n                          Real*__restrict__ res_array)\n{\n  int row = (blockIdx.x * blockDim.x) + threadIdx.x + 1;\n  int col = (blockIdx.y * blockDim.y) + threadIdx.y + 1;\n\n  int NUM_2 = NUM >> 1;\n\n  Real p_ij, p_im1j, p_ip1j, p_ijm1, p_ijp1, rhs, res, res2;\n\n  // red point\n  p_ij = pres_red(col, row);\n\n  p_im1j = pres_black(col - 1, row);\n  p_ip1j = pres_black(col + 1, row);\n  p_ijm1 = pres_black(col, row - (col & 1));\n  p_ijp1 = pres_black(col, row + ((col + 1) & 1));\n\n  rhs = (((F(col, (2 * row) - (col & 1)) - F(col - 1, (2 * row) - (col & 1))) / dx)\n      +  ((G(col, (2 * row) - (col & 1)) - G(col, (2 * row) - (col & 1) - 1)) / dy)) / dt;\n\n  // calculate residual\n  res = ((p_ip1j - (TWO * p_ij) + p_im1j) / (dx * dx))\n    + ((p_ijp1 - (TWO * p_ij) + p_ijm1) / (dy * dy)) - rhs;\n\n  // black point\n  p_ij = pres_black(col, row);\n\n  p_im1j = pres_red(col - 1, row);\n  p_ip1j = pres_red(col + 1, row);\n  p_ijm1 = pres_red(col, row - ((col + 1) & 1));\n  p_ijp1 = pres_red(col, row + (col & 1));\n\n  // right-hand side\n  rhs = (((F(col, (2 * row) - ((col + 1) & 1)) - F(col - 1, (2 * row) - ((col + 1) & 1))) / dx)\n      +  ((G(col, (2 * row) - ((col + 1) & 1)) - G(col, (2 * row) - ((col + 1) & 1) - 1)) / dy)) / dt;\n\n  // calculate residual\n  res2 = ((p_ip1j - (TWO * p_ij) + p_im1j) / (dx * dx))\n    + ((p_ijp1 - (TWO * p_ij) + p_ijm1) / (dy * dy)) - rhs;\n\n  // shared memory for block's sum\n  __shared__ Real sum_cache[BLOCK_SIZE];\n\n  sum_cache[threadIdx.x] = (res * res) + (res2 * res2);\n\n  // synchronize threads in block to ensure all residuals stored\n  __syncthreads();\n\n  // add up squared residuals for block\n  int i = BLOCK_SIZE >> 1;\n  while (i != 0) {\n    if (threadIdx.x < i) {\n      sum_cache[threadIdx.x] += sum_cache[threadIdx.x + i];\n    }\n    __syncthreads();\n    i >>= 1;\n  }\n\n  // store block's summed residuals\n  if (threadIdx.x == 0) {\n    res_array[blockIdx.y + (gridDim.y * blockIdx.x)] = sum_cache[0];\n  }\n}",
            "#define Real float\n\n\ninline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\ninline __host__ __device__ float4 fabs(float4 v)\n{\n    return make_float4(fabs(v.x), fabs(v.y), fabs(v.z), fabs(v.w));\n}\n\n__global__ \nvoid calculate_u (const Real dt,\n                  const Real*__restrict__ F, \n                  const Real*__restrict__ pres_red,\n                  const Real*__restrict__ pres_black, \n                        Real*__restrict__ u,\n                        Real*__restrict__ max_u)\n{\n  int row = (blockIdx.x * blockDim.x) + threadIdx.x + 1;\n  int col = (blockIdx.y * blockDim.y) + threadIdx.y + 1;\n\n  // allocate shared memory to store max velocities\n  __shared__ Real max_cache[BLOCK_SIZE];\n  max_cache[threadIdx.x] = ZERO;\n\n  int NUM_2 = NUM >> 1;\n  Real new_u = ZERO;\n\n  if (col != NUM) {\n\n    Real p_ij, p_ip1j, new_u2;\n\n    // red point\n    p_ij = pres_red(col, row);\n    p_ip1j = pres_black(col + 1, row);\n\n    new_u = F(col, (2 * row) - (col & 1)) - (dt * (p_ip1j - p_ij) / dx);\n    u(col, (2 * row) - (col & 1)) = new_u;\n\n    // black point\n    p_ij = pres_black(col, row);\n    p_ip1j = pres_red(col + 1, row);\n\n    new_u2 = F(col, (2 * row) - ((col + 1) & 1)) - (dt * (p_ip1j - p_ij) / dx);\n    u(col, (2 * row) - ((col + 1) & 1)) = new_u2;\n\n    // check for max of these two\n    new_u = fmax(fabs(new_u), fabs(new_u2));\n\n    if ((2 * row) == NUM) {\n      // also test for max velocity at vertical boundary\n      new_u = fmax(new_u, fabs( u(col, NUM + 1) ));\n    }\n  } else {\n    // check for maximum velocity in boundary cells also\n    new_u = fmax(fabs( u(NUM, (2 * row)) ), fabs( u(0, (2 * row)) ));\n    new_u = fmax(fabs( u(NUM, (2 * row) - 1) ), new_u);\n    new_u = fmax(fabs( u(0, (2 * row) - 1) ), new_u);\n\n    new_u = fmax(fabs( u(NUM + 1, (2 * row)) ), new_u);\n    new_u = fmax(fabs( u(NUM + 1, (2 * row) - 1) ), new_u);\n\n  } // end if\n\n  // store maximum u for block from each thread\n  max_cache[threadIdx.x] = new_u;\n\n  // synchronize threads in block to ensure all velocities stored\n  __syncthreads();\n\n  // calculate maximum for block\n  int i = BLOCK_SIZE >> 1;\n  while (i != 0) {\n    if (threadIdx.x < i) {\n      max_cache[threadIdx.x] = fmax(max_cache[threadIdx.x], max_cache[threadIdx.x + i]);\n    }\n    __syncthreads();\n    i >>= 1;\n  }\n\n  // store block's maximum\n  if (threadIdx.x == 0) {\n    max_u[blockIdx.y + (gridDim.y * blockIdx.x)] = max_cache[0];\n  }\n}",
            "#define Real float\n\n\n#define T ((int)32)\n\n\ninline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\ninline __host__ __device__ float4 fabs(float4 v)\n{\n    return make_float4(fabs(v.x), fabs(v.y), fabs(v.z), fabs(v.w));\n}\n\n__device__ void push(T const &v) {\n    buf[free_index] = v;\n    free_index += THREADS;\n  }\n\n__global__ \nvoid calculate_v (const Real dt,\n                  const Real*__restrict__ G, \n                  const Real*__restrict__ pres_red,\n                  const Real*__restrict__ pres_black, \n                        Real*__restrict__ v,\n                        Real*__restrict__ max_v)\n{\n  int row = (blockIdx.x * blockDim.x) + threadIdx.x + 1;\n  int col = (blockIdx.y * blockDim.y) + threadIdx.y + 1;\n\n  // allocate shared memory to store maximum velocities\n  __shared__ Real max_cache[BLOCK_SIZE];\n  max_cache[threadIdx.x] = ZERO;\n\n  int NUM_2 = NUM >> 1;\n  Real new_v = ZERO;\n\n  if (row != NUM_2) {\n    Real p_ij, p_ijp1, new_v2;\n\n    // red pressure point\n    p_ij = pres_red(col, row);\n    p_ijp1 = pres_black(col, row + ((col + 1) & 1));\n\n    new_v = G(col, (2 * row) - (col & 1)) - (dt * (p_ijp1 - p_ij) / dy);\n    v(col, (2 * row) - (col & 1)) = new_v;\n\n    // black pressure point\n    p_ij = pres_black(col, row);\n    p_ijp1 = pres_red(col, row + (col & 1));\n\n    new_v2 = G(col, (2 * row) - ((col + 1) & 1)) - (dt * (p_ijp1 - p_ij) / dy);\n    v(col, (2 * row) - ((col + 1) & 1)) = new_v2;\n\n    // check for max of these two\n    new_v = fmax(fabs(new_v), fabs(new_v2));\n\n    if (col == NUM) {\n      // also test for max velocity at vertical boundary\n      new_v = fmax(new_v, fabs( v(NUM + 1, (2 * row)) ));\n    }\n\n  } else {\n\n    if ((col & 1) == 1) {\n      // black point is on boundary, only calculate red point below it\n      Real p_ij = pres_red(col, row);\n      Real p_ijp1 = pres_black(col, row + ((col + 1) & 1));\n\n      new_v = G(col, (2 * row) - (col & 1)) - (dt * (p_ijp1 - p_ij) / dy);\n      v(col, (2 * row) - (col & 1)) = new_v;\n    } else {\n      // red point is on boundary, only calculate black point below it\n      Real p_ij = pres_black(col, row);\n      Real p_ijp1 = pres_red(col, row + (col & 1));\n\n      new_v = G(col, (2 * row) - ((col + 1) & 1)) - (dt * (p_ijp1 - p_ij) / dy);\n      v(col, (2 * row) - ((col + 1) & 1)) = new_v;\n    }\n\n    // get maximum v velocity\n    new_v = fabs(new_v);\n\n    // check for maximum velocity in boundary cells also\n    new_v = fmax(fabs( v(col, NUM) ), new_v);\n    new_v = fmax(fabs( v(col, 0) ), new_v);\n\n    new_v = fmax(fabs( v(col, NUM + 1) ), new_v);\n  } // end if\n\n  // store absolute value of velocity\n  max_cache[threadIdx.x] = new_v;\n\n  // synchronize threads in block to ensure all velocities stored\n  __syncthreads();\n\n  // calculate maximum for block\n  int i = BLOCK_SIZE >> 1;\n  while (i != 0) {\n    if (threadIdx.x < i) {\n      max_cache[threadIdx.x] = fmax(max_cache[threadIdx.x], max_cache[threadIdx.x + i]);\n    }\n    __syncthreads();\n    i >>= 1;\n  }\n\n  // store block's summed residuals\n  if (threadIdx.x == 0) {\n    max_v[blockIdx.y + (gridDim.y * blockIdx.x)] = max_cache[0];\n  }\n}"
        ]
    },
    "laplace-cuda": {
        "/Users/gbolet/hecbench-roofline/src/laplace-cuda/kernels.h": [
            "#define Real float\n\n\n__global__\nvoid red_kernel (const Real *__restrict__ aP,\n                 const Real *__restrict__ aW,\n                 const Real *__restrict__ aE,\n                 const Real *__restrict__ aS,\n                 const Real *__restrict__ aN,\n                 const Real *__restrict__ b,\n                 const Real *__restrict__ temp_black,\n                       Real *__restrict__ temp_red,\n                       Real *__restrict__ norm_L2)\n{\n  int row = 1 + (blockIdx.x * blockDim.x) + threadIdx.x;\n  int col = 1 + (blockIdx.y * blockDim.y) + threadIdx.y;\n\n  int ind_red = col * ((NUM >> 1) + 2) + row; // local (red) index\n  int ind = 2 * row - (col & 1) - 1 + NUM * (col - 1); // global index\n\n  Real temp_old = temp_red[ind_red];\n\n  Real res = b[ind]\n        + aW[ind] * temp_black[row + (col - 1) * ((NUM >> 1) + 2)]\n        + aE[ind] * temp_black[row + (col + 1) * ((NUM >> 1) + 2)]\n        + aS[ind] * temp_black[row - (col & 1) + col * ((NUM >> 1) + 2)]\n        + aN[ind] * temp_black[row + ((col + 1) & 1) + col * ((NUM >> 1) + 2)];\n\n  Real temp_new = temp_old * (ONE - omega) + omega * (res / aP[ind]);\n\n  temp_red[ind_red] = temp_new;\n  res = temp_new - temp_old;\n\n  norm_L2[ind_red] = res * res;\n\n}",
            "#define Real float\n\n\n__global__\nvoid black_kernel (const Real *__restrict__ aP,\n                   const Real *__restrict__ aW,\n                   const Real *__restrict__ aE,\n                   const Real *__restrict__ aS,\n                   const Real *__restrict__ aN,\n                   const Real *__restrict__ b,\n                   const Real *__restrict__ temp_red,\n                         Real *__restrict__ temp_black,\n                         Real *__restrict__ norm_L2)\n{\n  int row = 1 + (blockIdx.x * blockDim.x) + threadIdx.x;\n  int col = 1 + (blockIdx.y * blockDim.y) + threadIdx.y;\n\n  int ind_black = col * ((NUM >> 1) + 2) + row; // local (black) index\n  int ind = 2 * row - ((col + 1) & 1) - 1 + NUM * (col - 1); // global index\n\n  Real temp_old = temp_black[ind_black];\n\n  Real res = b[ind]\n        + aW[ind] * temp_red[row + (col - 1) * ((NUM >> 1) + 2)]\n        + aE[ind] * temp_red[row + (col + 1) * ((NUM >> 1) + 2)]\n        + aS[ind] * temp_red[row - ((col + 1) & 1) + col * ((NUM >> 1) + 2)]\n        + aN[ind] * temp_red[row + (col & 1) + col * ((NUM >> 1) + 2)];\n\n  Real temp_new = temp_old * (ONE - omega) + omega * (res / aP[ind]);\n\n  temp_black[ind_black] = temp_new;\n  res = temp_new - temp_old;\n\n  norm_L2[ind_black] = res * res;\n}"
        ]
    },
    "tensorAccessor-cuda": {
        "/Users/gbolet/hecbench-roofline/src/tensorAccessor-cuda/main.cu": [
            "__global__\nvoid tensor_packed_accessor_kernel (\n    PackedTensorAccessor64<float, 1, RestrictPtrTraits> r,\n    PackedTensorAccessor64<float, 2, RestrictPtrTraits> m,\n    PackedTensorAccessor64<float, 1, RestrictPtrTraits> v)\n{\n  int64_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < r.size(0)) {\n    float val = 0.0f;\n    for (int64_t j = 0; j < m.size(1); j++) {\n      val += m[i][j] * v[j];\n    }\n    r[i] = val;\n  }\n}",
            "__global__\nvoid raw_accessor_kernel (\n    const int64_t nrow,\n    const int64_t ncol,\n          float *__restrict__ r,\n    const float *__restrict__ m,\n    const float *__restrict__ v)\n{\n  int64_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < nrow) {\n    float val = 0.0f;\n    for (int64_t j = 0; j < ncol; j++) {\n      val += m[i * ncol + j] * v[j];\n    }\n    r[i] = val;\n  }\n}"
        ]
    },
    "hmm-cuda": {
        "/Users/gbolet/hecbench-roofline/src/hmm-cuda/ViterbiGPU.cu": [
            "__global__ void\nviterbi (const float*__restrict__ maxProbOld, \n         const float*__restrict__ mtState, \n         const float*__restrict__ mtEmit, \n                 int*__restrict__ obs, \n               float*__restrict__ maxProbNew, \n                 int*__restrict__ path, \n         const int nState,\n         const int t)\n{\n  // find the most probable previous state leading to iState\n  int iState = blockDim.x * blockIdx.x + threadIdx.x;\n  if (iState < nState) {\n    float maxProb = 0.0;\n    int maxState = -1;\n    for (int preState = 0; preState < nState; preState++) \n    {\n      float p = maxProbOld[preState] + mtState[iState*nState + preState];\n      if (p > maxProb) \n      {\n        maxProb = p;\n        maxState = preState;\n      }\n    }\n    maxProbNew[iState] = maxProb + mtEmit[obs[t]*nState+iState];\n    path[(t-1)*nState+iState] = maxState;\n  }\n}"
        ]
    },
    "blockAccess-cuda": {
        "/Users/gbolet/hecbench-roofline/src/blockAccess-cuda/main.cu": [
            "__global__ void reference (const float * __restrict__ A,\n                           unsigned char *out, const unsigned int n)\n{\n  for (unsigned int idx = blockIdx.x * blockDim.x + threadIdx.x;\n       idx < n/4; idx += gridDim.x * blockDim.x) {\n    const float4 v = reinterpret_cast<const float4*>(A)[idx];\n    uchar4 o;\n    o.x = (int)v.x;\n    o.y = (int)v.y;\n    o.z = (int)v.z;\n    o.w = (int)v.w;\n    reinterpret_cast<uchar4*>(out)[idx] = o;\n  }\n}"
        ]
    },
    "cbsfil-cuda": {
        "/Users/gbolet/hecbench-roofline/src/cbsfil-cuda/kernels.h": [
            "__device__ float InitialAntiCausalCoefficient(\n    float* c,         // last coefficient\n    uint DataLength,  // number of samples or coefficients\n    int step)         // element interleave in bytes\n{\n  // this initialization corresponds to clamping boundaries\n  return((POLE / (POLE - 1.0f)) * *c);\n}\n\n__device__ float InitialCausalCoefficient(\n    float* c,         // coefficients\n    uint DataLength,  // number of coefficients\n    int step)         // element interleave in bytes\n{\n  const uint Horizon = 12 < DataLength ? 12 : DataLength;\n\n  // this initialization corresponds to clamping boundaries\n  // accelerated loop\n  float zn = POLE;\n  float Sum = *c;\n  for (uint n = 0; n < Horizon; n++) {\n    Sum += zn * *c;\n    zn *= POLE;\n    c = (float*)((uchar*)c + step);\n  }\n  return(Sum);\n}\n\n__device__ void ConvertToInterpolationCoefficients(\n    float* coeffs,    // input samples --> output coefficients\n    uint DataLength,  // number of samples or coefficients\n    int step)         // element interleave in bytes\n{\n  // compute the overall gain\n  const float Lambda = (1.0f - POLE) * (1.0f - 1.0f / POLE);\n\n  // causal initialization\n  float* c = coeffs;\n  float previous_c;  //cache the previously calculated c rather than look it up again (faster!)\n  *c = previous_c = Lambda * InitialCausalCoefficient(c, DataLength, step);\n  // causal recursion\n  for (uint n = 1; n < DataLength; n++) {\n    c = (float*)((uchar*)c + step);\n    *c = previous_c = Lambda * *c + POLE * previous_c;\n  }\n  // anticausal initialization\n  *c = previous_c = InitialAntiCausalCoefficient(c, DataLength, step);\n  // anticausal recursion\n  for (int n = DataLength - 2; 0 <= n; n--) {\n    c = (float*)((uchar*)c - step);\n    *c = previous_c = POLE * (previous_c - *c);\n  }\n}\n\n__global__ void toCoef2DX(\n    float* image,\n    uint pitch,\n    uint width,\n    uint height)\n{\n  // process lines horizontally\n  const uint y = blockIdx.x * blockDim.x + threadIdx.x;\n  if (y < height) {\n    float* line = (float*)((uchar*)image + y * pitch);  //direct access\n    ConvertToInterpolationCoefficients(line, width, sizeof(float));\n  }\n}",
            "__device__ float InitialAntiCausalCoefficient(\n    float* c,         // last coefficient\n    uint DataLength,  // number of samples or coefficients\n    int step)         // element interleave in bytes\n{\n  // this initialization corresponds to clamping boundaries\n  return((POLE / (POLE - 1.0f)) * *c);\n}\n\n__device__ float InitialCausalCoefficient(\n    float* c,         // coefficients\n    uint DataLength,  // number of coefficients\n    int step)         // element interleave in bytes\n{\n  const uint Horizon = 12 < DataLength ? 12 : DataLength;\n\n  // this initialization corresponds to clamping boundaries\n  // accelerated loop\n  float zn = POLE;\n  float Sum = *c;\n  for (uint n = 0; n < Horizon; n++) {\n    Sum += zn * *c;\n    zn *= POLE;\n    c = (float*)((uchar*)c + step);\n  }\n  return(Sum);\n}\n\n__device__ void ConvertToInterpolationCoefficients(\n    float* coeffs,    // input samples --> output coefficients\n    uint DataLength,  // number of samples or coefficients\n    int step)         // element interleave in bytes\n{\n  // compute the overall gain\n  const float Lambda = (1.0f - POLE) * (1.0f - 1.0f / POLE);\n\n  // causal initialization\n  float* c = coeffs;\n  float previous_c;  //cache the previously calculated c rather than look it up again (faster!)\n  *c = previous_c = Lambda * InitialCausalCoefficient(c, DataLength, step);\n  // causal recursion\n  for (uint n = 1; n < DataLength; n++) {\n    c = (float*)((uchar*)c + step);\n    *c = previous_c = Lambda * *c + POLE * previous_c;\n  }\n  // anticausal initialization\n  *c = previous_c = InitialAntiCausalCoefficient(c, DataLength, step);\n  // anticausal recursion\n  for (int n = DataLength - 2; 0 <= n; n--) {\n    c = (float*)((uchar*)c - step);\n    *c = previous_c = POLE * (previous_c - *c);\n  }\n}\n\n__global__ void toCoef2DY(\n    float* image,\n    uint pitch,\n    uint width,\n    uint height)\n{\n  // process lines vertically\n  const uint x = blockIdx.x * blockDim.x + threadIdx.x;\n  if (x < width) {\n    float* line = image + x;  //direct access\n    ConvertToInterpolationCoefficients(line, height, pitch);\n  }\n}"
        ]
    },
    "gabor-cuda": {
        "/Users/gbolet/hecbench-roofline/src/gabor-cuda/main.cu": [
            "__global__\nvoid gabor (\n  double *gabor_spatial,\n  const unsigned int height,\n  const unsigned int width,\n  const double center_y,\n  const double center_x,\n  const double ctheta,\n  const double stheta,\n  const double scale,\n  const double sx_2,\n  const double sy_2,\n  const double fx)\n{\n  int x = blockIdx.x * blockDim.x + threadIdx.x;\n  int y = blockIdx.y * blockDim.y + threadIdx.y;\n\n  double centered_x, centered_y, u, v;\n\n  if (x < width && y < height) {\n    centered_y = (double)y - center_y;\n    centered_x = (double)x - center_x;\n    u = ctheta * centered_x - stheta * centered_y;\n    v = ctheta * centered_y + stheta * centered_x;\n    gabor_spatial[y*width + x] = scale * exp(-0.5*(u*u/sx_2 + v*v/sy_2)) * cos(2.0*M_PI*fx*u);\n  }\n}"
        ]
    },
    "hwt1d-cuda": {
        "/Users/gbolet/hecbench-roofline/src/hwt1d-cuda/kernel.cu": [
            "__global__\nvoid dwtHaar1D( const float *__restrict__ inSignal,\n                      float *__restrict__ coefsSignal,\n                      float *__restrict__ AverageSignal,\n                      const unsigned int tLevels,\n                      const unsigned int signalLength,\n                      const unsigned int levelsDone,\n\t\t      const unsigned int mLevels)\n              \n{\n    size_t localId = threadIdx.x;\n    size_t groupId = blockIdx.x;\n    size_t localSize = blockDim.x;\n\n    extern __shared__ float sharedArray[];\n    \n    /**\n     * Read input signal data from global memory\n     * to shared memory\n     */\n    float t0 = inSignal[groupId * localSize * 2 + localId];\n    float t1 = inSignal[groupId * localSize * 2 + localSize + localId];\n    // Divide with signal length for normalized decomposition\n    if(0 == levelsDone)\n    {\n       float r = rsqrtf((float)signalLength);\n       t0 *= r;\n       t1 *= r;\n    }\n    sharedArray[localId] = t0;\n    sharedArray[localSize + localId] = t1;\n     \n    __syncthreads();\n    \n    unsigned int levels = tLevels > mLevels ? mLevels: tLevels;\n    unsigned int activeThreads = (1 << levels) / 2;\n    unsigned int midOutPos = signalLength / 2;\n    \n    const float rsqrt_two = 0.7071f;\n    for(unsigned int i = 0; i < levels; ++i)\n    {\n\n        float data0, data1;\n        if(localId < activeThreads)\n        {\n            data0 = sharedArray[2 * localId];\n            data1 = sharedArray[2 * localId + 1];\n        }\n\n        /* make sure all work items have read from sharedArray before modifying it */\n        __syncthreads();\n\n        if(localId < activeThreads)\n        {\n            sharedArray[localId] = (data0 + data1) * rsqrt_two;\n            unsigned int globalPos = midOutPos + groupId * activeThreads + localId;\n            coefsSignal[globalPos] = (data0 - data1) * rsqrt_two;\n       \n            midOutPos >>= 1;\n        }\n        activeThreads >>= 1;\n        __syncthreads();\n    }\n    \n    /**\n     * Write 0th element for the next decomposition\n     * steps which are performed on host \n     */\n    \n     if(0 == localId)\n        AverageSignal[groupId] = sharedArray[0];\n}"
        ]
    },
    "atomicCAS-cuda": {
        "/Users/gbolet/hecbench-roofline/src/atomicCAS-cuda/kernels.h": [
            "#define T ((int)32)\n\n\n__device__ __forceinline__\nfloat atomic_min(float *addr, float value)\n{\n  unsigned ret = __float_as_uint(*addr);\n  while(value < __uint_as_float(ret))\n  {\n    unsigned old = ret;\n    if((ret = atomicCAS((unsigned *)addr, old, __float_as_uint(value))) == old)\n      break;\n  }\n  return __uint_as_float(ret);\n}\n\n__global__ \nvoid atomicMinDerived (T *res)\n{\n  int i = threadIdx.x + blockIdx.x * blockDim.x + 1;\n  atomic_min(res, (T)i);\n}",
            "#define T ((int)32)\n\n\n__device__ __forceinline__\nvoid atomic_max(float *address, float val)\n{\n  unsigned int ret = __float_as_uint(*address);\n  while(val > __uint_as_float(ret))\n  {\n    unsigned int old = ret;\n    if((ret = atomicCAS((unsigned int *)address, old, __float_as_uint(val))) == old)\n      break;\n  }\n}\n\n__global__ \nvoid atomicMaxDerived (T *res)\n{\n  int i = threadIdx.x + blockIdx.x * blockDim.x + 1;\n  atomic_max(res, (T)i);\n}",
            "#define T ((int)32)\n\n\n__device__ __forceinline__\ndouble atomic_add(double *address, double val)\n{\n  // Doing it all as longlongs cuts one __longlong_as_double from the inner loop\n  unsigned long long *ptr = (unsigned long long *)address;\n  unsigned long long old, newdbl, ret = *ptr;\n  do {\n    old = ret;\n    newdbl = __double_as_longlong(__longlong_as_double(old)+val);\n  } while((ret = atomicCAS(ptr, old, newdbl)) != old);\n  return __longlong_as_double(ret);\n}\n\n__global__ \nvoid atomicAddDerived (T *res)\n{\n  int i = threadIdx.x + blockIdx.x * blockDim.x + 1;\n  atomic_add(res, (T)i);\n}"
        ]
    },
    "kernelLaunch-cuda": {
        "/Users/gbolet/hecbench-roofline/src/kernelLaunch-cuda/main.cu": [
            "__global__ void KernelWithSmallArgs(SmallKernelArgs args, char* out) { DO_NOT_OPTIMIZE_AWAY; }",
            "__global__ void KernelWithMediumArgs(MediumKernelArgs args, char* out) { DO_NOT_OPTIMIZE_AWAY; }",
            "__global__ void KernelWithLargeArgs(LargeKernelArgs args, char* out) { DO_NOT_OPTIMIZE_AWAY; }"
        ]
    },
    "minimod-cuda": {
        "/Users/gbolet/hecbench-roofline/src/minimod-cuda/minimig.cu": [
            "__global__ void target_inner_3d_kernel(\n    llint nx, llint ny, llint nz, int ldimx, int ldimy, int ldimz,\n    llint x3, llint x4, llint y3, llint y4, llint z3, llint z4,\n    llint lx, llint ly, llint lz,\n    float hdx_2, float hdy_2, float hdz_2,\n    float coef0,\n    float coefx_1, float coefx_2, float coefx_3, float coefx_4,\n    float coefy_1, float coefy_2, float coefy_3, float coefy_4,\n    float coefz_1, float coefz_2, float coefz_3, float coefz_4,\n    const float *__restrict__ u, float *__restrict__ v, const float *__restrict__ vp,\n    const float *__restrict__ phi, const float *__restrict__ eta)\n{\n  __shared__ float s_u[NDIM+2*R][NDIM+2*R][NDIM+2*R];\n\n  const llint i0 = x3 + blockIdx.z * blockDim.z;\n  const llint j0 = y3 + blockIdx.y * blockDim.y;\n  const llint k0 = z3 + blockIdx.x * blockDim.x;\n  \n  const int ti = threadIdx.z;\n  const int tj = threadIdx.y;\n  const int tk = threadIdx.x;\n\n  const llint i = i0 + ti;\n  const llint j = j0 + tj;\n  const llint k = k0 + tk;\n\n  s_u[ti][tj][tk] = 0.f;\n\n  if (ti < 2*R && tj < 2*R && tk< 2*R)\n    s_u[NDIM+ti][NDIM+tj][NDIM+tk] = 0.f;\n\n  __syncthreads();\n\n  const llint sui = ti + R;\n  const llint suj = tj + R;\n  const llint suk = tk + R;\n\n  const int z_side = ti / R;\n  s_u[ti+z_side*NDIM][suj][suk] = u[IDX3(i+(z_side*2-1)*R,j,k)];\n  const int y_side = tj / R;\n  s_u[sui][tj+y_side*NDIM][suk] = u[IDX3(i,j+(y_side*2-1)*R,k)];\n  s_u[sui][suj][tk] = u[IDX3(i,j,k-R)];\n  s_u[sui][suj][tk+NDIM] = u[IDX3(i,j,k+R)];\n\n  __syncthreads();\n\n  if (i > x4-1 || j > y4-1 || k > z4-1) { return; }\n\n  float lap = coef0 * s_u[sui][suj][suk] + \n            coefx_1 * (s_u[sui+1][suj][suk] + s_u[sui-1][suj][suk]) +\n            coefy_1 * (s_u[sui][suj+1][suk] + s_u[sui][suj-1][suk]) +\n            coefz_1 * (s_u[sui][suj][suk+1] + s_u[sui][suj][suk-1]) +\n            coefx_2 * (s_u[sui+2][suj][suk] + s_u[sui-2][suj][suk]) +\n            coefy_2 * (s_u[sui][suj+2][suk] + s_u[sui][suj-2][suk]) +\n            coefz_2 * (s_u[sui][suj][suk+2] + s_u[sui][suj][suk-2]) +\n            coefx_3 * (s_u[sui+3][suj][suk] + s_u[sui-3][suj][suk]) +\n            coefy_3 * (s_u[sui][suj+3][suk] + s_u[sui][suj-3][suk]) +\n            coefz_3 * (s_u[sui][suj][suk+3] + s_u[sui][suj][suk-3]) +\n            coefx_4 * (s_u[sui+4][suj][suk] + s_u[sui-4][suj][suk]) +\n            coefy_4 * (s_u[sui][suj+4][suk] + s_u[sui][suj-4][suk]) +\n            coefz_4 * (s_u[sui][suj][suk+4] + s_u[sui][suj][suk-4]);\n  v[IDX3(i,j,k)] = 2.f * s_u[sui][suj][suk] + vp[IDX3(i,j,k)] * lap - v[IDX3(i,j,k)];\n}",
            "__global__ void target_pml_3d_kernel(\n    llint nx, llint ny, llint nz, int ldimx, int ldimy, int ldimz,\n    llint x3, llint x4, llint y3, llint y4, llint z3, llint z4,\n    llint lx, llint ly, llint lz,\n    float hdx_2, float hdy_2, float hdz_2,\n    float coef0,\n    float coefx_1, float coefx_2, float coefx_3, float coefx_4,\n    float coefy_1, float coefy_2, float coefy_3, float coefy_4,\n    float coefz_1, float coefz_2, float coefz_3, float coefz_4,\n    const float *__restrict__ u, float *__restrict__ v, const float *__restrict__ vp,\n    float *__restrict__ phi, const float *__restrict__ eta)\n{\n  __shared__ float s_u[NDIM+2*R][NDIM+2*R][NDIM+2*R];\n\n  const llint i0 = x3 + blockIdx.z * blockDim.z;\n  const llint j0 = y3 + blockIdx.y * blockDim.y;\n  const llint k0 = z3 + blockIdx.x * blockDim.x;\n\n  const int ti = threadIdx.z;\n  const int tj = threadIdx.y;\n  const int tk = threadIdx.x;\n\n  const llint i = i0 + ti;\n  const llint j = j0 + tj;\n  const llint k = k0 + tk;\n\n  s_u[ti][tj][tk] = 0.f;\n\n  if (ti < 2*R && tj < 2*R && tk< 2*R)\n    s_u[NDIM+ti][NDIM+tj][NDIM+tk] = 0.f;\n\n  __syncthreads();\n\n  const llint sui = ti + R;\n  const llint suj = tj + R;\n  const llint suk = tk + R;\n\n  const int z_side = ti / R;\n  s_u[ti+z_side*NDIM][suj][suk] = u[IDX3(i+(z_side*2-1)*R,j,k)];\n  const int y_side = tj / R;\n  s_u[sui][tj+y_side*NDIM][suk] = u[IDX3(i,j+(y_side*2-1)*R,k)];\n  s_u[sui][suj][tk] = u[IDX3(i,j,k-R)];\n  s_u[sui][suj][tk+NDIM] = u[IDX3(i,j,k+R)];\n\n  __syncthreads();\n\n  if (i > x4-1 || j > y4-1 || k > z4-1) { return; }\n\n  float lap = coef0 * s_u[sui][suj][suk] + \n            coefx_1 * (s_u[sui+1][suj][suk] + s_u[sui-1][suj][suk]) +\n            coefy_1 * (s_u[sui][suj+1][suk] + s_u[sui][suj-1][suk]) +\n            coefz_1 * (s_u[sui][suj][suk+1] + s_u[sui][suj][suk-1]) +\n            coefx_2 * (s_u[sui+2][suj][suk] + s_u[sui-2][suj][suk]) +\n            coefy_2 * (s_u[sui][suj+2][suk] + s_u[sui][suj-2][suk]) +\n            coefz_2 * (s_u[sui][suj][suk+2] + s_u[sui][suj][suk-2]) +\n            coefx_3 * (s_u[sui+3][suj][suk] + s_u[sui-3][suj][suk]) +\n            coefy_3 * (s_u[sui][suj+3][suk] + s_u[sui][suj-3][suk]) +\n            coefz_3 * (s_u[sui][suj][suk+3] + s_u[sui][suj][suk-3]) +\n            coefx_4 * (s_u[sui+4][suj][suk] + s_u[sui-4][suj][suk]) +\n            coefy_4 * (s_u[sui][suj+4][suk] + s_u[sui][suj-4][suk]) +\n            coefz_4 * (s_u[sui][suj][suk+4] + s_u[sui][suj][suk-4]);\n\n  const float s_eta_c = eta[IDX3(i,j,k)];\n\n  v[IDX3(i,j,k)] = ((2.f*s_eta_c + 2.f - s_eta_c*s_eta_c)*s_u[sui][suj][suk] + \n      \t    (vp[IDX3(i,j,k)] * (lap + phi[IDX3(i,j,k)]) - v[IDX3(i,j,k)])) / \n          (2.f*s_eta_c+1.f);\n\n  phi[IDX3(i,j,k)] = \n   (phi[IDX3(i,j,k)] - \n   ((eta[IDX3(i+1,j,k)]-eta[IDX3(i-1,j,k)]) * \n   (s_u[sui+1][suj][suk]-s_u[sui-1][suj][suk]) * hdx_2 + \n   (eta[IDX3(i,j+1,k)]-eta[IDX3(i,j-1,k)]) *\n   (s_u[sui][suj+1][suk]-s_u[sui][suj-1][suk]) * hdy_2 +\n   (eta[IDX3(i,j,k+1)]-eta[IDX3(i,j,k-1)]) *\n   (s_u[sui][suj][suk+1]-s_u[sui][suj][suk-1]) * hdz_2)) / (1.f + s_eta_c);\n}",
            "__global__ void kernel_add_source_kernel(float *g_u, llint idx, float source) {\n    g_u[idx] += source;\n}"
        ]
    },
    "gemv-cuda": {
        "/Users/gbolet/hecbench-roofline/src/gemv-cuda/kernels.h": [
            "#define T ((int)32)\n\n\n__inline__ __device__ T warpReduceSum(T val)\n{\n#pragma unroll\n  for (int mask = 16; mask > 0; mask >>= 1)\n    val += __shfl_xor_sync(FINAL_MASK, val, mask, 32);\n  return val;\n}\n\n__global__ void gemv_fp16(__half* mat, __half* vec, __half* res, unsigned int n,\n                          unsigned int num_per_thread) {\n  float sum = 0;\n  // each thread load num_per_thread elements from global\n  unsigned int tid = threadIdx.x;\n  unsigned int row = blockIdx.y * blockDim.y + threadIdx.y;\n  unsigned int start_idx = threadIdx.x;\n  float4* mat4 = reinterpret_cast<float4*>(mat);\n  float4* vec4 = reinterpret_cast<float4*>(vec);\n\n#pragma unroll\n  for (int iter = 0; iter < num_per_thread >> 3; iter++) {\n    unsigned int j = start_idx + iter * blockDim.x;\n    if (j < n >> 3) {\n      float4 vec_val = vec4[j];\n      float4 mat_val = mat4[row * (n >> 3) + j];\n      const __half2* vec_h1 = (__half2*)&vec_val.x;\n      const __half2* vec_h2 = (__half2*)&vec_val.y;\n      const __half2* vec_h3 = (__half2*)&vec_val.z;\n      const __half2* vec_h4 = (__half2*)&vec_val.w;\n      const __half2* mat_h1 = (__half2*)&mat_val.x;\n      const __half2* mat_h2 = (__half2*)&mat_val.y;\n      const __half2* mat_h3 = (__half2*)&mat_val.z;\n      const __half2* mat_h4 = (__half2*)&mat_val.w;\n      sum += static_cast<float>(vec_h1->x) * static_cast<float>(mat_h1->x);\n      sum += static_cast<float>(vec_h1->y) * static_cast<float>(mat_h1->y);\n      sum += static_cast<float>(vec_h2->x) * static_cast<float>(mat_h2->x);\n      sum += static_cast<float>(vec_h2->y) * static_cast<float>(mat_h2->y);\n      sum += static_cast<float>(vec_h3->x) * static_cast<float>(mat_h3->x);\n      sum += static_cast<float>(vec_h3->y) * static_cast<float>(mat_h3->y);\n      sum += static_cast<float>(vec_h4->x) * static_cast<float>(mat_h4->x);\n      sum += static_cast<float>(vec_h4->y) * static_cast<float>(mat_h4->y);\n    }\n  }\n\n  sum = warpReduceSum(sum, blockDim.x);\n\n  if (blockDim.x <= WARP_SIZE) {\n    if (tid == 0) {\n      res[row] = __float2half(sum);\n    }\n    return;\n  }\n\n  // Shared mem for partial sums (one per warp in the block)\n  static __shared__ float warpLevelSums[SHARED_MEM_MAX_ROWS][WARP_SIZE];\n  const int laneId = threadIdx.x % WARP_SIZE;\n  const int warpId = threadIdx.x / WARP_SIZE;\n  if (laneId == 0) warpLevelSums[threadIdx.y][warpId] = sum;\n  __syncthreads();\n  // read from shared memory only if that warp existed\n  sum = (threadIdx.x < blockDim.x / WARP_SIZE)\n            ? warpLevelSums[threadIdx.y][laneId]\n            : 0.0;\n  // Final reduce using first warp\n  if (warpId == 0) sum = warpReduceSum(sum, blockDim.x / WARP_SIZE);\n  if (tid == 0) {\n    res[row] = __float2half(sum);\n  }\n}",
            "#define T ((int)32)\n\n\n__inline__ __device__ T warpReduceSum(T val)\n{\n#pragma unroll\n  for (int mask = 16; mask > 0; mask >>= 1)\n    val += __shfl_xor_sync(FINAL_MASK, val, mask, 32);\n  return val;\n}\n\n__global__ void gemv_quantized_int8(int8_t* mat, __half* vec, __half* res,\n                                    unsigned int n, __half scale, __half zero_point,\n                                    unsigned int num_per_thread) {\n  float sum = 0;\n  // each thread load num_per_thread elements from global\n  unsigned int tid = threadIdx.x;\n  unsigned int row = blockIdx.y * blockDim.y + threadIdx.y;\n  unsigned int start_idx = threadIdx.x;\n  __half4* mat4 = reinterpret_cast<__half4*>(mat);\n  float4* vec4 = reinterpret_cast<float4*>(vec);\n\n  float zero_point_f = static_cast<float>(zero_point);\n  float scale_f = static_cast<float>(scale);\n\n#pragma unroll\n  for (int iter = 0; iter < num_per_thread >> 3; iter++) {\n    unsigned int j = start_idx + iter * blockDim.x;\n    if (j < n >> 3) {\n      float4 vec_val = vec4[j];\n      __half4 mat_val = mat4[row * (n >> 3) + j];\n      const __half2* vec_h1 = (__half2*)&vec_val.x;\n      const __half2* vec_h2 = (__half2*)&vec_val.y;\n      const __half2* vec_h3 = (__half2*)&vec_val.z;\n      const __half2* vec_h4 = (__half2*)&vec_val.w;\n      const int8_2* mat_h1 = (int8_2*)&mat_val.x;\n      const int8_2* mat_h2 = (int8_2*)&mat_val.y;\n      const int8_2* mat_h3 = (int8_2*)&mat_val.z;\n      const int8_2* mat_h4 = (int8_2*)&mat_val.w;\n      sum += static_cast<float>(vec_h1->x) *\n             (static_cast<float>(mat_h1->x) - zero_point_f);\n      sum += static_cast<float>(vec_h1->y) *\n             (static_cast<float>(mat_h1->y) - zero_point_f);\n      sum += static_cast<float>(vec_h2->x) *\n             (static_cast<float>(mat_h2->x) - zero_point_f);\n      sum += static_cast<float>(vec_h2->y) *\n             (static_cast<float>(mat_h2->y) - zero_point_f);\n      sum += static_cast<float>(vec_h3->x) *\n             (static_cast<float>(mat_h3->x) - zero_point_f);\n      sum += static_cast<float>(vec_h3->y) *\n             (static_cast<float>(mat_h3->y) - zero_point_f);\n      sum += static_cast<float>(vec_h4->x) *\n             (static_cast<float>(mat_h4->x) - zero_point_f);\n      sum += static_cast<float>(vec_h4->y) *\n             (static_cast<float>(mat_h4->y) - zero_point_f);\n    }\n  }\n\n  sum *= scale_f;\n\n  sum = warpReduceSum(sum, blockDim.x);\n\n  if (blockDim.x <= WARP_SIZE) {\n    if (tid == 0) {\n      res[row] = __float2half(sum);\n    }\n    return;\n  }\n\n  // Shared mem for partial sums (one per warp in the block)\n  static __shared__ float warpLevelSums[SHARED_MEM_MAX_ROWS][WARP_SIZE];\n  const int laneId = threadIdx.x % WARP_SIZE;\n  const int warpId = threadIdx.x / WARP_SIZE;\n  if (laneId == 0) warpLevelSums[threadIdx.y][warpId] = sum;\n  __syncthreads();\n  // read from shared memory only if that warp existed\n  sum = (threadIdx.x < blockDim.x / WARP_SIZE)\n            ? warpLevelSums[threadIdx.y][laneId]\n            : 0.0;\n  // Final reduce using first warp\n  if (warpId == 0) sum = warpReduceSum(sum, blockDim.x / WARP_SIZE);\n  if (tid == 0) {\n    res[row] = __float2half(sum);\n  }\n}",
            "#define T ((int)32)\n\n\n__inline__ __device__ T warpReduceSum(T val)\n{\n#pragma unroll\n  for (int mask = 16; mask > 0; mask >>= 1)\n    val += __shfl_xor_sync(FINAL_MASK, val, mask, 32);\n  return val;\n}\n\n__global__ void gemv_quantized_int4(uint4_2* mat, __half* vec, __half* res,\n                                    unsigned int n, __half scale, __half zero_point,\n                                    unsigned int num_per_thread) {\n  float sum = 0;\n  // each thread load num_per_thread elements from global\n  unsigned int tid = threadIdx.x;\n  unsigned int row = blockIdx.y * blockDim.y + threadIdx.y;\n  unsigned int start_idx = threadIdx.x;\n  uint4_2_4* mat4 = reinterpret_cast<uint4_2_4*>(mat);\n  float4* vec4 = reinterpret_cast<float4*>(vec);\n\n  float zero_point_f = static_cast<float>(zero_point);\n  float scale_f = static_cast<float>(scale);\n\n#pragma unroll\n  for (int iter = 0; iter < num_per_thread >> 4; iter++) {\n    unsigned int j = 2 * (start_idx + iter * blockDim.x);\n    if (j < n >> 3) {\n      float4 vec_val_1 = vec4[j];  // 8 __half\n      float4 vec_val_2 = vec4[j + 1];\n      const __half2* vec_h1 = (__half2*)&vec_val_1.x;\n      const __half2* vec_h2 = (__half2*)&vec_val_1.y;\n      const __half2* vec_h3 = (__half2*)&vec_val_1.z;\n      const __half2* vec_h4 = (__half2*)&vec_val_1.w;\n      const __half2* vec_h5 = (__half2*)&vec_val_2.x;\n      const __half2* vec_h6 = (__half2*)&vec_val_2.y;\n      const __half2* vec_h7 = (__half2*)&vec_val_2.z;\n      const __half2* vec_h8 = (__half2*)&vec_val_2.w;\n\n      uint4_2_4 mat_val_1 = mat4[row * (n >> 3) + j];\n      uint4_2_4 mat_val_2 = mat4[row * (n >> 3) + j + 1];\n      const uint4_2* mat_h1 = (uint4_2*)&mat_val_1.x;\n      const uint4_2* mat_h2 = (uint4_2*)&mat_val_1.y;\n      const uint4_2* mat_h3 = (uint4_2*)&mat_val_1.z;\n      const uint4_2* mat_h4 = (uint4_2*)&mat_val_1.w;\n      const uint4_2* mat_h5 = (uint4_2*)&mat_val_2.x;\n      const uint4_2* mat_h6 = (uint4_2*)&mat_val_2.y;\n      const uint4_2* mat_h7 = (uint4_2*)&mat_val_2.z;\n      const uint4_2* mat_h8 = (uint4_2*)&mat_val_2.w;\n\n      sum += static_cast<float>(vec_h1->x) *\n             (static_cast<float>(mat_h1->getX()) - zero_point_f);\n      sum += static_cast<float>(vec_h1->y) *\n             (static_cast<float>(mat_h1->getY()) - zero_point_f);\n      sum += static_cast<float>(vec_h2->x) *\n             (static_cast<float>(mat_h2->getX()) - zero_point_f);\n      sum += static_cast<float>(vec_h2->y) *\n             (static_cast<float>(mat_h2->getY()) - zero_point_f);\n      sum += static_cast<float>(vec_h3->x) *\n             (static_cast<float>(mat_h3->getX()) - zero_point_f);\n      sum += static_cast<float>(vec_h3->y) *\n             (static_cast<float>(mat_h3->getY()) - zero_point_f);\n      sum += static_cast<float>(vec_h4->x) *\n             (static_cast<float>(mat_h4->getX()) - zero_point_f);\n      sum += static_cast<float>(vec_h4->y) *\n             (static_cast<float>(mat_h4->getY()) - zero_point_f);\n      sum += static_cast<float>(vec_h5->x) *\n             (static_cast<float>(mat_h5->getX()) - zero_point_f);\n      sum += static_cast<float>(vec_h5->y) *\n             (static_cast<float>(mat_h5->getY()) - zero_point_f);\n      sum += static_cast<float>(vec_h6->x) *\n             (static_cast<float>(mat_h6->getX()) - zero_point_f);\n      sum += static_cast<float>(vec_h6->y) *\n             (static_cast<float>(mat_h6->getY()) - zero_point_f);\n      sum += static_cast<float>(vec_h7->x) *\n             (static_cast<float>(mat_h7->getX()) - zero_point_f);\n      sum += static_cast<float>(vec_h7->y) *\n             (static_cast<float>(mat_h7->getY()) - zero_point_f);\n      sum += static_cast<float>(vec_h8->x) *\n             (static_cast<float>(mat_h8->getX()) - zero_point_f);\n      sum += static_cast<float>(vec_h8->y) *\n             (static_cast<float>(mat_h8->getY()) - zero_point_f);\n    }\n  }\n\n  sum *= scale_f;\n\n  sum = warpReduceSum(sum, blockDim.x);\n\n  if (blockDim.x <= WARP_SIZE) {\n    if (tid == 0) {\n      res[row] = __float2half(sum);\n    }\n    return;\n  }\n\n  // Shared mem for partial sums (one per warp in the block)\n  static __shared__ float warpLevelSums[SHARED_MEM_MAX_ROWS][WARP_SIZE];\n  const int laneId = threadIdx.x % WARP_SIZE;\n  const int warpId = threadIdx.x / WARP_SIZE;\n  if (laneId == 0) warpLevelSums[threadIdx.y][warpId] = sum;\n  __syncthreads();\n  // read from shared memory only if that warp existed\n  sum = (threadIdx.x < blockDim.x / WARP_SIZE)\n            ? warpLevelSums[threadIdx.y][laneId]\n            : 0.0;\n  // Final reduce using first warp\n  if (warpId == 0) sum = warpReduceSum(sum, blockDim.x / WARP_SIZE);\n  if (tid == 0) {\n    res[row] = __float2half(sum);\n  }\n}"
        ],
        "/Users/gbolet/hecbench-roofline/src/gemv-cuda/gemv.cu": [
            "__global__ void check_correctness(__half* mat, __half* vec, __half* res, int n) {\n  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  if (idx < n) {\n    float result = 0;\n    for (int j = 0; j < n; ++j) {\n      result += __half2float(mat[idx * n + j]) * __half2float(vec[j]);\n    }\n    float diff = result - __half2float(res[idx]);\n    float delta = 0.125 * n / 512;\n    if (diff > delta || diff < -delta) {\n      printf(\"!!![idx=%d] %f != %f, diff=%f\\n\", idx, __half2float(res[idx]),\n             result, diff);\n    }\n  }\n}",
            "__global__ void check_int8_quantized_correctness(int8_t* mat, __half* vec,\n                                                 __half* res, __half scale,\n                                                 __half zero_point, int n) {\n  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  if (idx < n) {\n    float result = 0;\n    for (int j = 0; j < n; ++j) {\n      float dequantized_val = (static_cast<float>(mat[idx * n + j]) -\n                               static_cast<float>(zero_point)) *\n                              static_cast<float>(scale);\n      result += dequantized_val * __half2float(vec[j]);\n    }\n    float diff = result - __half2float(res[idx]);\n    float delta = 0.125 * n / 512;\n    if (diff > delta || diff < -delta) {\n      printf(\"!!![idx=%d] %f != %f, diff=%f\\n\", idx, __half2float(res[idx]),\n             result, diff);\n    }\n  }\n}",
            "__global__ void check_int4_quantized_correctness(uint4_2* mat, __half* vec,\n                                                 __half* res, __half scale,\n                                                 __half zero_point,\n                                                 int mat_size) {\n  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  if (idx < mat_size * 2) {\n    float result = 0;\n    for (int j = 0; j < mat_size; ++j) {\n      uint8_t x = mat[idx * mat_size + j].getX();\n      uint8_t y = mat[idx * mat_size + j].getY();\n      float dequantized_x =\n          (static_cast<float>(x) - static_cast<float>(zero_point)) *\n          static_cast<float>(scale);\n      float dequantized_y =\n          (static_cast<float>(y) - static_cast<float>(zero_point)) *\n          static_cast<float>(scale);\n      result += dequantized_x * __half2float(vec[j * 2]);\n      result += dequantized_y * __half2float(vec[j * 2 + 1]);\n    }\n    float diff = result - __half2float(res[idx]);\n    float delta = 0.125 * mat_size / 256;\n    if (diff > delta || diff < -delta) {\n      printf(\"!!![idx=%d] %f != %f, diff=%f\\n\", idx, __half2float(res[idx]),\n             result, diff);\n    }\n  }\n}"
        ]
    },
    "winograd-cuda": {
        "/Users/gbolet/hecbench-roofline/src/winograd-cuda/main.cu": [
            "#define DATA_TYPE float\n\n\n__global__ void winograd_conv2d(\n    const DATA_TYPE *__restrict__ input,\n    const DATA_TYPE *__restrict__ transformed_filter ,\n    DATA_TYPE *__restrict__ output,\n    const int offset_i,\n    const int offset_j)\n{\n  int tile_i = blockIdx.x * blockDim.x + threadIdx.x + offset_i;\n  int tile_j = blockIdx.y * blockDim.y + threadIdx.y + offset_j;\n\n  // input transformation\n\n  DATA_TYPE input_tile[4][4], tmp_tile[4][4], transformed_tile[4][4];\n  for (int i = 0; i < 4; i ++) {\n    for (int j = 0; j < 4; j ++) { \n      int x = 2 * tile_i + i;\n      int y = 2 * tile_j + j;\n      if (x >= MAP_SIZE || y >= MAP_SIZE) {\n        input_tile[i][j] = 0;\n        continue;\n      }\n      input_tile[i][j] = input[x * MAP_SIZE + y];\n    }\n  } \n\n  // Bt * d\n  for (int j = 0; j < 4; j ++) {\n    tmp_tile[0][j] = input_tile[0][j] - input_tile[2][j];\n    tmp_tile[1][j] = input_tile[1][j] + input_tile[2][j];\n    tmp_tile[2][j] = -input_tile[1][j] + input_tile[2][j];\n    tmp_tile[3][j] = input_tile[1][j] - input_tile[3][j];\n  }\n  // d * B\n  for (int i = 0; i < 4; i ++) {\n    transformed_tile[i][0] = tmp_tile[i][0] - tmp_tile[i][2];\n    transformed_tile[i][1] = tmp_tile[i][1] + tmp_tile[i][2];\n    transformed_tile[i][2] = -tmp_tile[i][1] + tmp_tile[i][2];\n    transformed_tile[i][3] = tmp_tile[i][1] - tmp_tile[i][3];\n  }\n\n  // element-wise multiplication\n\n  DATA_TYPE multiplied_tile[4][4];\n  for (int i = 0; i < 4; i ++) {\n    for (int j = 0; j < 4; j ++) {\n      multiplied_tile[i][j] = transformed_tile[i][j] * transformed_filter[i * 4 + j];\n    }\n  }\n\n  // output transformation\n\n  DATA_TYPE tmp_tile_1[2][4], final_tile[2][2];\n\n  // At * I\n  for (int j = 0; j < 4; j ++) {\n    tmp_tile_1[0][j] = multiplied_tile[0][j] + multiplied_tile[1][j] + multiplied_tile[2][j];\n    tmp_tile_1[1][j] = multiplied_tile[1][j] - multiplied_tile[2][j] - multiplied_tile[3][j];\n  }\n  // I * A\n  for (int i = 0; i < 2; i ++) {\n    final_tile[i][0] = tmp_tile_1[i][0] + tmp_tile_1[i][1] + tmp_tile_1[i][2];\n    final_tile[i][1] = tmp_tile_1[i][1] - tmp_tile_1[i][2] - tmp_tile_1[i][3];\n  }\n\n  for (int i = 0; i < 2; i ++) {\n    for (int j = 0; j < 2; j ++) {\n      int x = 2 * tile_i + i;\n      int y = 2 * tile_j + j;\n      if (x >= MAP_SIZE - 2 || y >= MAP_SIZE - 2) {\n        continue;\n      }\n      output[x * (MAP_SIZE - 2) + y] = final_tile[i][j];\n    }\n  }\n}"
        ]
    },
    "particle-diffusion-cuda": {
        "/Users/gbolet/hecbench-roofline/src/particle-diffusion-cuda/motionsim.cu": [
            "#define T ((int)32)\n\n\n__host__ __device__ __forceinline__ constexpr T floor(T a, T b) {\n  return (a - b + 1) / b;\n}\n\n__global__\nvoid Simulation(float*__restrict__ a_particleX,\n                float*__restrict__ a_particleY,\n\t\tconst float*__restrict__ a_randomX,\n                const float*__restrict__ a_randomY, \n\t\tsize_t *__restrict__ a_map,\n                const size_t n_particles,\n                unsigned int nIterations,\n                int grid_size,\n                float radius)\n{\n  size_t ii = blockDim.x * blockIdx.x + threadIdx.x;\n  if (ii >= n_particles) return;\n  // Start iterations\n  // Each iteration:\n  //  1. Updates the position of all water molecules\n  //  2. Checks if water molecule is inside a cell or not.\n  //  3. Updates counter in cells array\n  size_t iter = 0;\n  float pX = a_particleX[ii];\n  float pY = a_particleY[ii];\n  size_t map_base = ii * grid_size * grid_size;\n  while (iter < nIterations) {\n    // Computes random displacement for each molecule\n    // This example shows random distances between\n    // -0.05 units and 0.05 units in both X and Y directions\n    // Moves each water molecule by a random vector\n\n    float randnumX = a_randomX[iter * n_particles + ii];\n    float randnumY = a_randomY[iter * n_particles + ii];\n\n    // Transform the scaled random numbers into small displacements\n    float displacementX = randnumX / 1000.0f - 0.0495f;\n    float displacementY = randnumY / 1000.0f - 0.0495f;\n\n    // Move particles using random displacements\n    pX += displacementX;\n    pY += displacementY;\n\n    // Compute distances from particle position to grid point\n    float dX = pX - trunc(pX);\n    float dY = pY - trunc(pY);\n\n    // Compute grid point indices\n    int iX = floor(pX);\n    int iY = floor(pY);\n\n    // Check if particle is still in computation grid\n    if ((pX < grid_size) && (pY < grid_size) && (pX >= 0) && (pY >= 0)) {\n      // Check if particle is (or remained) inside cell.\n      // Increment cell counter in map array if so\n      if ((dX * dX + dY * dY <= radius * radius))\n        // The map array is organized as (particle, y, x)\n        a_map[map_base + iY * grid_size + iX]++;\n    }\n\n    iter++;\n\n  }  // Next iteration\n\n  a_particleX[ii] = pX;\n  a_particleY[ii] = pY;\n}"
        ]
    },
    "hotspot-cuda": {
        "/Users/gbolet/hecbench-roofline/src/hotspot-cuda/kernel.h": [
            "__global__ void calc_temp(\n    int iteration,  //number of iteration\n    const float *__restrict__ power,   //power input\n    const float *__restrict__ temp_src,//temperature input/output\n          float *__restrict__ temp_dst,//temperature input/output\n    int grid_cols,  //Col of grid\n    int grid_rows,  //Row of grid\n    int border_cols,// border offset \n    int border_rows,// border offset\n    float Cap,      //Capacitance\n    float Rx, \n    float Ry, \n    float Rz, \n    float step)\n{\n\n  __shared__ float temp_on_device[BLOCK_SIZE][BLOCK_SIZE];\n  __shared__ float power_on_device[BLOCK_SIZE][BLOCK_SIZE];\n  __shared__ float temp_t[BLOCK_SIZE][BLOCK_SIZE]; // temparary temperature result\n\n  float amb_temp = 80.0f;\n  float step_div_Cap;\n  float Rx_1,Ry_1,Rz_1;\n\n  int bx = blockIdx.x;\n  int by = blockIdx.y;\n\n  int tx = threadIdx.x;\n  int ty = threadIdx.y;\n\n  step_div_Cap = step/Cap;\n\n  Rx_1 = 1.f/Rx;\n  Ry_1 = 1.f/Ry;\n  Rz_1 = 1.f/Rz;\n\n  // each block finally computes result for a small block\n  // after N iterations. \n  // it is the non-overlapping small blocks that cover \n  // all the input data\n\n  // calculate the small block size\n  int small_block_rows = BLOCK_SIZE-iteration*2;//EXPAND_RATE\n  int small_block_cols = BLOCK_SIZE-iteration*2;//EXPAND_RATE\n\n  // calculate the boundary for the block according to \n  // the boundary of its small block\n  int blkY = small_block_rows*by-border_rows;\n  int blkX = small_block_cols*bx-border_cols;\n  int blkYmax = blkY+BLOCK_SIZE-1;\n  int blkXmax = blkX+BLOCK_SIZE-1;\n\n  // calculate the global thread coordination\n  int yidx = blkY+ty;\n  int xidx = blkX+tx;\n\n  // load data if it is within the valid input range\n  int loadYidx=yidx, loadXidx=xidx;\n  int index = grid_cols*loadYidx+loadXidx;\n\n  if(IN_RANGE(loadYidx, 0, grid_rows-1) && IN_RANGE(loadXidx, 0, grid_cols-1)){\n    temp_on_device[ty][tx] = temp_src[index];  // Load the temperature data from global memory to shared memory\n    power_on_device[ty][tx] = power[index];// Load the power data from global memory to shared memory\n  }\n  __syncthreads();\n\n  // effective range within this block that falls within \n  // the valid range of the input data\n  // used to rule out computation outside the boundary.\n  int validYmin = (blkY < 0) ? -blkY : 0;\n  int validYmax = (blkYmax > grid_rows-1) ? BLOCK_SIZE-1-(blkYmax-grid_rows+1) : BLOCK_SIZE-1;\n  int validXmin = (blkX < 0) ? -blkX : 0;\n  int validXmax = (blkXmax > grid_cols-1) ? BLOCK_SIZE-1-(blkXmax-grid_cols+1) : BLOCK_SIZE-1;\n\n  int N = ty-1;\n  int S = ty+1;\n  int W = tx-1;\n  int E = tx+1;\n\n  N = (N < validYmin) ? validYmin : N;\n  S = (S > validYmax) ? validYmax : S;\n  W = (W < validXmin) ? validXmin : W;\n  E = (E > validXmax) ? validXmax : E;\n\n  bool computed;\n  for (int i=0; i<iteration ; i++){ \n    computed = false;\n    if( IN_RANGE(tx, i+1, BLOCK_SIZE-i-2) &&  \\\n        IN_RANGE(ty, i+1, BLOCK_SIZE-i-2) &&  \\\n        IN_RANGE(tx, validXmin, validXmax) && \\\n        IN_RANGE(ty, validYmin, validYmax) ) {\n      computed = true;\n      temp_t[ty][tx] =   temp_on_device[ty][tx] + step_div_Cap * (power_on_device[ty][tx] + \n          (temp_on_device[S][tx] + temp_on_device[N][tx] - 2.f*temp_on_device[ty][tx]) * Ry_1 + \n          (temp_on_device[ty][E] + temp_on_device[ty][W] - 2.f*temp_on_device[ty][tx]) * Rx_1 + \n          (amb_temp - temp_on_device[ty][tx]) * Rz_1);\n\n    }\n    __syncthreads();\n    if(i==iteration-1)\n      break;\n    if(computed)   //Assign the computation range\n      temp_on_device[ty][tx]= temp_t[ty][tx];\n    __syncthreads();\n  }\n\n  // update the global memory\n  // after the last iteration, only threads coordinated within the \n  // small block perform the calculation and switch on ``computed''\n  if (computed){\n    temp_dst[index]= temp_t[ty][tx];    \n  }\n}"
        ]
    },
    "iso2dfd-cuda": {
        "/Users/gbolet/hecbench-roofline/src/iso2dfd-cuda/iso2dfd.cu": [
            "__global__ void iso_2dfd_kernel(\n        float*__restrict__ next,\n  const float*__restrict__ prev,\n  const float*__restrict__ vel, \n  const float dtDIVdxy, const size_t nRows, const size_t nCols)\n{\n  // Compute global id\n  // We can use the get.global.id() function of the item variable\n  //   to compute global id. The 2D array is laid out in memory in row major\n  //   order.\n  size_t gidCol = blockDim.x * blockIdx.x + threadIdx.x;\n  size_t gidRow = blockDim.y * blockIdx.y + threadIdx.y;\n\n  if (gidRow < nRows && gidCol < nCols) {\n\n    size_t gid = (gidRow)*nCols + gidCol;\n\n    // Computation to solve wave equation in 2D\n    // First check if gid is inside the effective grid (not in halo)\n    if ((gidCol >= HALF_LENGTH && gidCol < nCols - HALF_LENGTH) &&\n        (gidRow >= HALF_LENGTH && gidRow < nRows - HALF_LENGTH)) {\n      // Stencil code to update grid point at position given by global id (gid)\n      // New time step for grid point is computed based on the values of the\n      //    the immediate neighbors in both the horizontal and vertical\n      //    directions, as well as the value of grid point at a previous time step\n      float value = 0.f;\n      value += prev[gid + 1] - 2.f * prev[gid] + prev[gid - 1];\n      value += prev[gid + nCols] - 2.f * prev[gid] + prev[gid - nCols];\n      value *= dtDIVdxy * vel[gid];\n      next[gid] = 2.f * prev[gid] - next[gid] + value;\n    }\n  }\n}"
        ]
    },
    "perplexity-cuda": {
        "/Users/gbolet/hecbench-roofline/src/perplexity-cuda/main.cu": [
            "__global__\nvoid sigmas_kernel(const value_t* __restrict__ distances,\n                         value_t* __restrict__ P,\n                   const float perplexity,\n                   const float desired_entropy,\n                   const int epochs,\n                   const float tol,\n                   const value_idx n,\n                   const int k)\n{\n  // For every item in row\n  const int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i >= n) return;\n\n  value_t beta_min = -INFINITY, beta_max = INFINITY;\n  value_t beta = 1;\n  const int ik = i * k;\n  int step;\n\n  for (step = 0; step < epochs; step++) {\n    value_t sum_Pi = FLT_EPSILON;\n\n    // Exponentiate to get Gaussian\n    for (int j = 0; j < k; j++) {\n      P[ik + j] = __expf(-distances[ik + j] * beta);\n      sum_Pi += P[ik + j];\n    }\n\n    // Normalize\n    value_t sum_disti_Pi = 0;\n    const value_t div    = __fdividef(1.0f, sum_Pi);\n    for (int j = 0; j < k; j++) {\n      P[ik + j] *= div;\n      sum_disti_Pi += distances[ik + j] * P[ik + j];\n    }\n\n    const value_t entropy      = __logf(sum_Pi) + beta * sum_disti_Pi;\n    const value_t entropy_diff = entropy - desired_entropy;\n    if (fabsf(entropy_diff) <= tol) {\n      break;\n    }\n\n    // Bisection search\n    if (entropy_diff > 0) {\n      beta_min = beta;\n      if (isinf(beta_max))\n        beta *= 2.0f;\n      else\n        beta = (beta + beta_max) * 0.5f;\n    } else {\n      beta_max = beta;\n      if (isinf(beta_min))\n        beta *= 0.5f;\n      else\n        beta = (beta + beta_min) * 0.5f;\n    }\n  }\n}"
        ]
    },
    "b+tree-cuda": {
        "/Users/gbolet/hecbench-roofline/src/b+tree-cuda/kernel/kernel.cu": [
            "__global__\nvoid findK(const long height,\n           const knode *__restrict__ knodesD,\n           const long knodes_elem,\n           const record *__restrict__ recordsD,\n           long *__restrict__ currKnodeD,\n           long *__restrict__ offsetD,\n           const int *__restrict__ keysD, \n           record *ansD)\n{\n\n  // private thread IDs\n  int thid = threadIdx.x;\n  int bid = blockIdx.x;\n\n  // processtree levels\n  int i;\n  for(i = 0; i < height; i++){\n\n    // if value is between the two keys\n    if((knodesD[currKnodeD[bid]].keys[thid]) <= keysD[bid] && (knodesD[currKnodeD[bid]].keys[thid+1] > keysD[bid])){\n      // this conditional statement is inserted to avoid crush due to but in original code\n      // \"offset[bid]\" calculated below that addresses knodes[] in the next iteration goes outside of its bounds cause segmentation fault\n      // more specifically, values saved into knodes->indices in the main function are out of bounds of knodes that they address\n      if(knodesD[offsetD[bid]].indices[thid] < knodes_elem){\n        offsetD[bid] = knodesD[offsetD[bid]].indices[thid];\n      }\n    }\n    __syncthreads();\n\n    // set for next tree level\n    if(thid==0){\n      currKnodeD[bid] = offsetD[bid];\n    }\n    __syncthreads();\n\n  }\n\n  //At this point, we have a candidate leaf node which may contain\n  //the target record.  Check each key to hopefully find the record\n  if(knodesD[currKnodeD[bid]].keys[thid] == keysD[bid]){\n    ansD[bid].value = recordsD[knodesD[currKnodeD[bid]].indices[thid]].value;\n  }\n\n}"
        ],
        "/Users/gbolet/hecbench-roofline/src/b+tree-cuda/kernel/kernel2.cu": [
            "__global__ void \nfindRangeK(const long height,\n           const knode *__restrict__ knodesD,\n           const long knodes_elem,\n           long *__restrict__ currKnodeD,\n           long *__restrict__ offsetD,\n           long *__restrict__ lastKnodeD,\n           long *__restrict__ offset_2D,\n           const int *__restrict__ startD,\n           const int *__restrict__ endD,\n           int *__restrict__ RecstartD, \n           int *__restrict__ ReclenD)\n{\n\n  // private thread IDs\n  int thid = threadIdx.x;\n  int bid = blockIdx.x;\n\n  for(int i = 0; i < height; i++){\n\n    if((knodesD[currKnodeD[bid]].keys[thid] <= startD[bid]) && (knodesD[currKnodeD[bid]].keys[thid+1] > startD[bid])){\n      // this conditional statement is inserted to avoid crush due to but in original code\n      // \"offset[bid]\" calculated below that later addresses part of knodes goes outside of its bounds cause segmentation fault\n      // more specifically, values saved into knodes->indices in the main function are out of bounds of knodes that they address\n      if(knodesD[currKnodeD[bid]].indices[thid] < knodes_elem){\n        offsetD[bid] = knodesD[currKnodeD[bid]].indices[thid];\n      }\n    }\n    if((knodesD[lastKnodeD[bid]].keys[thid] <= endD[bid]) && (knodesD[lastKnodeD[bid]].keys[thid+1] > endD[bid])){\n      // this conditional statement is inserted to avoid crush due to but in original code\n      // \"offset_2[bid]\" calculated below that later addresses part of knodes goes outside of its bounds cause segmentation fault\n      // more specifically, values saved into knodes->indices in the main function are out of bounds of knodes that they address\n      if(knodesD[lastKnodeD[bid]].indices[thid] < knodes_elem){\n        offset_2D[bid] = knodesD[lastKnodeD[bid]].indices[thid];\n      }\n    }\n    __syncthreads();\n\n    // set for next tree level\n    if(thid==0){\n      currKnodeD[bid] = offsetD[bid];\n      lastKnodeD[bid] = offset_2D[bid];\n    }\n    __syncthreads();\n  }\n\n  // Find the index of the starting record\n  if(knodesD[currKnodeD[bid]].keys[thid] == startD[bid]){\n    RecstartD[bid] = knodesD[currKnodeD[bid]].indices[thid];\n  }\n  __syncthreads();\n\n  // Find the index of the ending record\n  if(knodesD[lastKnodeD[bid]].keys[thid] == endD[bid]){\n    ReclenD[bid] = knodesD[lastKnodeD[bid]].indices[thid] - RecstartD[bid]+1;\n  }\n}"
        ]
    },
    "asmooth-cuda": {
        "/Users/gbolet/hecbench-roofline/src/asmooth-cuda/main.cu": [
            "__global__ void smoothingFilter(\n    int Lx, int Ly, \n    int Threshold, int MaxRad, \n    const float*__restrict__ Img,\n            int*__restrict__ Box,\n          float*__restrict__ Norm)\n{\n  int tid = threadIdx.x;\n  int tjd = threadIdx.y;\n  int i = blockIdx.x * blockDim.x + tid;\n  int j = blockIdx.y * blockDim.y + tjd;\n  int stid = tjd * blockDim.x + tid;\n  int gtid = j * Lx + i;  \n\n  // part of shared memory may be unused\n  __shared__ float s_Img[1024];\n\n  if ( i < Lx && j < Ly )\n    s_Img[stid] = Img[gtid];\n\n  __syncthreads();\n\n  if ( i < Lx && j < Ly )\n  {\n    // Smoothing parameters\n    float sum = 0.f;\n    int q = 1;\n    int s = q;\n    int ksum = 0;\n\n    // Continue until parameters are met\n    while (sum < Threshold && q < MaxRad)\n    {\n      s = q;\n      sum = 0.f;\n      ksum = 0;\n\n      // Normal adaptive smoothing\n      for (int ii = -s; ii < s+1; ii++)\n        for (int jj = -s; jj < s+1; jj++)\n          if ( (i-s >= 0) && (i+s < Ly) && (j-s >= 0) && (j+s < Lx) )\n          {\n            ksum++;\n            // Compute within bounds of block dimensions\n            if( tid-s >= 0 && tid+s < blockDim.x && tjd-s >= 0 && tjd+s < blockDim.y )\n              sum += s_Img[stid + ii*blockDim.x + jj];\n            // Compute block borders with global memory\n            else\n              sum += Img[gtid + ii*Lx + jj];\n          }\n      q++;\n    }\n    Box[gtid] = s;\n\n    // Normalization for each box\n    for (int ii = -s; ii < s+1; ii++)\n      for (int jj = -s; jj < s+1; jj++)\n        if (ksum != 0) \n          atomicAdd(&Norm[gtid + ii*Lx + jj], __fdividef(1.f, (float)ksum));\n  }\n}",
            "__global__ void normalizeFilter(\n    int Lx, int Ly, \n          float*__restrict__ Img,\n    const float*__restrict__ Norm)\n{\n  int i = blockIdx.x * blockDim.x + threadIdx.x;\n  int j = blockIdx.y * blockDim.y + threadIdx.y;\n  if ( i < Lx && j < Ly ) {\n    int gtid = j * Lx + i;  \n    const float norm = Norm[gtid];\n    if (norm != 0) Img[gtid] = __fdividef(Img[gtid], norm);\n  }\n}",
            "__global__ void outFilter( \n    int Lx, int Ly,\n    const float*__restrict__ Img,\n    const   int*__restrict__ Box,\n          float*__restrict__ Out )\n{\n  int tid = threadIdx.x;\n  int tjd = threadIdx.y;\n  int i = blockIdx.x * blockDim.x + tid;\n  int j = blockIdx.y * blockDim.y + tjd;\n  int stid = tjd * blockDim.x + tid;\n  int gtid = j * Lx + i;  \n\n  // part of shared memory may be unused\n  __shared__ float s_Img[1024];\n\n  if ( i < Lx && j < Ly )\n    s_Img[stid] = Img[gtid];\n\n  __syncthreads();\n\n  if ( i < Lx && j < Ly )\n  {\n    const int s = Box[gtid];\n    float sum = 0.f;\n    int ksum  = 0;\n\n    for (int ii = -s; ii < s+1; ii++)\n      for (int jj = -s; jj < s+1; jj++)\n        if ( (i-s >= 0) && (i+s < Lx) && (j-s >= 0) && (j+s < Ly) )\n        {\n          ksum++;\n          if( tid-s >= 0 && tid+s < blockDim.x && tjd-s >= 0 && tjd+s < blockDim.y )\n            sum += s_Img[stid + ii*blockDim.y + jj];\n          else\n            sum += Img[gtid + ii*Ly + jj];\n        }\n    if ( ksum != 0 ) Out[gtid] = __fdividef(sum , (float)ksum);\n  }\n}"
        ]
    },
    "adam-cuda": {
        "/Users/gbolet/hecbench-roofline/src/adam-cuda/main.cu": [
            "#define T ((int)32)\n\n\ninline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\ninline __host__ __device__ float4 sqrtf(float4 v)\n{\n    return make_float4(sqrtf(v.x), sqrtf(v.y), sqrtf(v.z), sqrtf(v.w));\n}\n\n__global__\nvoid adam (\n        T* __restrict__ p,\n        T* __restrict__ m,\n        T* __restrict__ v,\n  const G* __restrict__ g,\n  const float b1,\n  const float b2,\n  const float eps,\n  const float grad_scale,\n  const float step_size,\n  const int time_step,\n  const size_t vector_size,\n  adamMode_t mode,\n  const float decay)\n{\n  const size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  const size_t totThreads = gridDim.x*blockDim.x;\n\n  for (size_t j = i; j < vector_size; j += totThreads) {\n    for (int t = 1; t <= time_step; t++) {\n      T scaled_grad = g[j]/grad_scale;\n      m[j] = b1*m[j] + (1.f-b1)*scaled_grad;\n      v[j] = b2*v[j] + (1.f-b2)*scaled_grad*scaled_grad;\n      float m_corrected = m[j] / (1.f-powf(b1, t));\n      float v_corrected = v[j] / (1.f-powf(b2, t));\n      float denom;\n      if (mode == ADAM_MODE_0)\n        denom = sqrtf(v_corrected + eps);\n      else // Mode 1\n        denom = sqrtf(v_corrected) + eps;\n      float update = (m_corrected/denom) + (decay*p[j]);\n      p[j] -= (step_size*update);\n    }\n  }\n}"
        ]
    },
    "simplemoc-cuda": {
        "/Users/gbolet/hecbench-roofline/src/simplemoc-cuda/main.cu": [
            "__global__\nvoid att (\n  const int*__restrict__ QSR_id_acc,\n  const int*__restrict__ FAI_id_acc,\n  float*__restrict__ fine_flux_acc,\n  float*__restrict__ fine_source_acc,\n  float*__restrict__ sigT_acc,\n  float*__restrict__ state_flux_acc,\n  float*__restrict__ v_acc,\n  const int fine_axial_intervals,\n  const int egroups,\n  const int segments )\n{\n  int gid = blockIdx.x*blockDim.x+threadIdx.x;\n  if (gid >= segments) return; \n\n  const float dz = 0.1f;\n  const float zin = 0.3f; \n  const float weight = 0.5f;\n  const float mu = 0.9f;\n  const float mu2 = 0.3f;\n  const float ds = 0.7f;\n\n  int QSR_id = QSR_id_acc[gid];\n  int FAI_id = FAI_id_acc[gid];\n\n  // load fine source region flux vector\n  int offset = QSR_id * fine_axial_intervals * egroups;\n\n  float *FSR_flux = fine_flux_acc + offset + FAI_id * egroups;\n\n  float* q0 = v_acc;\n  float* q1 = v_acc + egroups;\n  float* q2 = v_acc + egroups * 2;\n  float* sigT = v_acc + egroups * 3;\n  float* tau = v_acc + egroups * 4;\n  float* sigT2 = v_acc + egroups * 5;\n  float* expVal = v_acc + egroups * 6;\n  float* reuse = v_acc + egroups * 7;\n  float* flux_integral = v_acc + egroups * 8;\n  float* tally = v_acc + egroups * 9;\n  float* t1 = v_acc + egroups * 10;\n  float* t2 = v_acc + egroups * 11;\n  float* t3 = v_acc + egroups * 12;\n  float* t4 = v_acc + egroups * 13;\n\n  if( FAI_id == 0 )\n  {\n    float * f2 = fine_source_acc + offset + FAI_id*egroups;\n    float * f3 = fine_source_acc + offset + (FAI_id+1)*egroups; \n    // cycle over energy groups\n    for( int g = 0; g < egroups; g++)\n    {\n      // load neighboring sources\n      const float y2 = f2[g];\n      const float y3 = f3[g];\n\n      // do linear \"fitting\"\n      const float c0 = y2;\n      const float c1 = (y3 - y2) / dz;\n\n      // calculate q0, q1, q2\n      q0[g] = c0 + c1*zin;\n      q1[g] = c1;\n      q2[g] = 0;\n    }\n  }\n  else if ( FAI_id == fine_axial_intervals - 1 )\n  {\n    float * f1 = fine_source_acc + offset + (FAI_id-1)*egroups; \n    float * f2 = fine_source_acc + offset + FAI_id*egroups; \n\n    for( int g = 0; g < egroups; g++)\n    {\n      // load neighboring sources\n      const float y1 = f1[g];\n      const float y2 = f2[g];\n\n      // do linear \"fitting\"\n      const float c0 = y2;\n      const float c1 = (y2 - y1) / dz;\n\n      // calculate q0, q1, q2\n      q0[g] = c0 + c1*zin;\n      q1[g] = c1;\n      q2[g] = 0;\n    }\n  }\n  else\n  {\n    float * f1 = fine_source_acc + offset + (FAI_id-1)*egroups; \n    float * f2 = fine_source_acc + offset + FAI_id*egroups; \n    float * f3 = fine_source_acc + offset + (FAI_id+1)*egroups; \n    // cycle over energy groups\n    for( int g = 0; g < egroups; g++)\n    {\n      // load neighboring sources\n      const float y1 = f1[g]; \n      const float y2 = f2[g];\n      const float y3 = f3[g];\n\n      // do quadratic \"fitting\"\n      const float c0 = y2;\n      const float c1 = (y1 - y3) / (2.f*dz);\n      const float c2 = (y1 - 2.f*y2 + y3) / (2.f*dz*dz);\n\n      // calculate q0, q1, q2\n      q0[g] = c0 + c1*zin + c2*zin*zin;\n      q1[g] = c1 + 2.f*c2*zin;\n      q2[g] = c2;\n    }\n  }\n\n\n  // cycle over energy groups\n  offset = QSR_id * egroups;\n  for( int g = 0; g < egroups; g++)\n  {\n    // load total cross section\n    sigT[g] = sigT_acc[offset + g];\n\n    // calculate common values for efficiency\n    tau[g] = sigT[g] * ds;\n    sigT2[g] = sigT[g] * sigT[g];\n\n    expVal[g] = 1.f - exp( -tau[g] ); // exp is faster on many architectures\n    reuse[g] = tau[g] * (tau[g] - 2.f) + 2.f * expVal[g] / (sigT[g] * sigT2[g]); \n\n    // add contribution to new source flux\n    flux_integral[g] = (q0[g] * tau[g] + (sigT[g] * state_flux_acc[g] - q0[g])\n        * expVal[g]) / sigT2[g] + q1[g] * mu * reuse[g] + q2[g] * mu2 \n      * (tau[g] * (tau[g] * (tau[g] - 3.f) + 6.f) - 6.f * expVal[g]) \n      / (3.f * sigT2[g] * sigT2[g]);\n\n    tally[g] = weight * flux_integral[g];\n    FSR_flux[g] += tally[g];\n    t1[g] = q0[g] * expVal[g] / sigT[g];  \n    t2[g] = q1[g] * mu * (tau[g] - expVal[g]) / sigT2[g]; \n    t3[g] = q2[g] * mu2 * reuse[g];\n    t4[g] = state_flux_acc[g] * (1.f - expVal[g]);\n    state_flux_acc[g] = t1[g]+t2[g]+t3[g]+t4[g];\n  }\n}"
        ]
    },
    "blas-gemm-cuda": {
        "/Users/gbolet/hecbench-roofline/src/blas-gemm-cuda/main.cu": [
            "#define T ((int)32)\n\n\n__global__ void matrix_mul(T *a, T *b, T *c, int M, int K, int N, T alpha, T beta) {\n  int row = blockIdx.y * TILE_Y + threadIdx.y;\n  int col = blockIdx.x * TILE_X + threadIdx.x;\n  if (row < M && col < N) {\n    T s = 0;\n    for (int k = 0; k < K; k++)\n      s += a[row * K + k] * b[k * N + col];\n    c[row * N + col] = alpha * s + beta * c[row * N + col];\n  }\n}"
        ]
    },
    "addBiasQKV-cuda": {
        "/Users/gbolet/hecbench-roofline/src/addBiasQKV-cuda/main.cu": [
            "inline __device__ __nv_bfloat162 float_to_bfloat2(float val) {\n  return __float2bfloat162_rn(val);\n}\n\ninline __device__ void fp8x4_e4m3_to_bfloat2(__nv_bfloat162* out1, __nv_bfloat162* out2, const __nv_fp8x4_e4m3* in)\n{\n  const char4 tmp_val = reinterpret_cast<const char4*>(in)[0];\n  *out1 = __nv_bfloat162((float)reinterpret_cast<const __nv_fp8_e4m3*>(&tmp_val.x)[0],\n                         (float)reinterpret_cast<const __nv_fp8_e4m3*>(&tmp_val.y)[0]);\n  *out2 = __nv_bfloat162((float)reinterpret_cast<const __nv_fp8_e4m3*>(&tmp_val.z)[0],\n                         (float)reinterpret_cast<const __nv_fp8_e4m3*>(&tmp_val.w)[0]);\n}\n\ninline __device__ __nv_bfloat162 hadd2(__nv_bfloat162 x, __nv_bfloat162 y) {\n  return __hadd2(x, y);\n}\n\ninline __device__ __nv_bfloat162 hmul2(__nv_bfloat162 x, __nv_bfloat162 y) {\n  return __hmul2(x, y);\n}\n\n__global__ void FP8TrtAddQKVBiasKernel(FP8TrtAddQKVBiasParam<__nv_fp8_e4m3, __nv_bfloat16> param)\n{\n    // Add bias ([3, head, size]), and then transpose from\n    // [valid_word_num, 3, head, size] -> [valid_word_num, head, 3, size]\n\n    using T1_4 = __nv_fp8x4_e4m3;\n    using T2_2 = __nv_bfloat162;\n\n    const T1_4* qkv_src_ptr = (T1_4*)(param.qkv_src + blockIdx.x * 3 * param.hidden_unit);\n    const T2_2* bias_ptr    = (T2_2*)param.qkv_bias;\n    T1_4*       qkv_tgt_ptr = (T1_4*)(param.qkv_tgt + blockIdx.x * 3 * param.hidden_unit);\n\n    const int size_div_4   = param.size_per_head / 4;\n    const int hidden_div_4 = param.hidden_unit / 4;\n    const int src_id       = threadIdx.z * hidden_div_4 + threadIdx.y * size_div_4 + threadIdx.x;\n\n    T2_2 val1, val2;\n    fp8x4_e4m3_to_bfloat2(&val1, &val2, &qkv_src_ptr[src_id]);\n    T2_2      input_scale_2  = float_to_bfloat2(__ldg(param.input_scale)); \n    T2_2      output_scale_2 = float_to_bfloat2(__ldg(param.output_scale));\n    const int bias_id_0      = src_id * 2;\n    val1                     = hmul2(hadd2(hmul2(val1, input_scale_2), bias_ptr[bias_id_0]), output_scale_2);\n    val2                     = hmul2(hadd2(hmul2(val2, input_scale_2), bias_ptr[bias_id_0 + 1]), output_scale_2);\n\n    // https://docs.nvidia.com/cuda/cuda-math-api/cuda_math_api/struct____nv__fp8x4__e4m3.html\n    qkv_tgt_ptr[(threadIdx.y * 3 * size_div_4 + threadIdx.z * size_div_4) + threadIdx.x] = __nv_fp8x4_e4m3(val1, val2);\n}"
        ]
    },
    "lci-cuda": {
        "/Users/gbolet/hecbench-roofline/src/lci-cuda/kernels.h": [
            "__device__ double alpha(int l)\n{\n  if(l<0) return 0.0;\n  if(l==0) return 1.0;\n  else \n    return double_fact_table[2*l-1] / fact_table[l];\n}\n\ninline __host__ __device__ int4 make_int4(float4 a)\n{\n    return make_int4(int(a.x), int(a.y), int(a.z), int(a.w));\n}\n\ninline __host__ __device__ int4 abs(int4 v)\n{\n    return make_int4(abs(v.x), abs(v.y), abs(v.z), abs(v.w));\n}\n\n__device__ double Omega(int l, int m, int n)\n{\n  return alpha(m-n+l) * alpha(m+n-l) * alpha(n-m+l)/alpha(m+n+l) * (4*l+1) / (2.0*(n+m+l)+1) ;\n}\n\n__device__ double B(int l)\n{\n  return 2.0*(14.0*l*l + 7.0*l - 2.0)/(4.0*l-1.0)/(4.0*l+3.0);\n}\n\n__device__ double C(int l)\n{\n  return (2.0*l-1)*2.0*l*(2.0*l+2)/(4.0*l-3.0)/(4.0*l-1.0);\n}\n\n__device__ double Sum_NL(int l, const double c[])\n{\n  double sum = 0.0;\n  for(int n=1; n<L_max; n++)\n    sum += pow(c[n],2.0)/(4.0*n+1.0);\n  return sum * (2*l-1)*(l+1)*c[l]/3.0;\n}\n\n__device__ double Sum_Omega(int l, const double c[])\n{\n  double sum = 0.0;\n  for(int m=1; m<L_max; m++)\n    for(int n=1; n<L_max; n++)\n      if(abs(m-n)<l+1) sum += Omega(l,m,n) * c[m]*c[n];\n  return sum;\n}\n\n__device__ double U(int l)\n{\n  return -(2.0*l-1)*(2.0*l+1)*(2.0*l+2)/(4.0*l+3)/(4.0*l+5);\n}\n\n__global__ void RHS_f (double t, const double *c, double *RHS)\n{\n  int l = threadIdx.x;\n\n  double T = c[0];\n\n  if (l == 0) {\n    RHS[0] = - T/3.0/t *(1.0+0.1*c[1]);\n  } else if (l < L_max) {\n    double B_bar = B(l) - 4.0/3.0;\n    double LHS_119;\n    if (l > 1) \n      LHS_119 = 1.0/t * ( U(l) * c[l+1] + (B_bar - 2.0/15.0 * c[1]) + C(l) * c[l-1]);\n    else \n      LHS_119 = 1.0/t * ( U(1) * c[2] + (B_bar - 2.0/15.0 * c[1]) + C(1));\n\n    double Sum1 = Sum_Omega(l, c);\n    double Sum2 = Sum_NL(l, c);\n\n    double RHS_119 = - T*one_over_theta0 *(\n        (kappa + M_PI*M_PI*l*(2*l+1)/3.0)*c[l] + \n         kappa*Sum1 + kappa*Sum2);\n\n    RHS[l] = -LHS_119 + RHS_119;\n  }\n}"
        ]
    },
    "convolution1D-cuda": {
        "/Users/gbolet/hecbench-roofline/src/convolution1D-cuda/main.cu": [
            "#define T ((int)32)\n\n\n__global__\nvoid conv1d(const T * __restrict__ in,\n                  T * __restrict__ out,\n            const int input_width,\n            const int mask_width)\n{\n  int i = threadIdx.x + blockIdx.x * blockDim.x;\n  T s = 0;\n  int start = i - mask_width / 2;\n  for (int j = 0; j < mask_width; j++) {\n    if (start + j >= 0 && start + j < input_width) {\n      s += in[start + j] * mask<T>[j];\n    }\n  }\n  out[i] = s;\n}",
            "#define T ((int)32)\n\n\n__global__\nvoid conv1d_tiled(const T *__restrict__ in,\n                        T *__restrict__ out,\n                  const int input_width,\n                  const int mask_width)\n{\n  __shared__ T tile[TILE_SIZE + MAX_MASK_WIDTH - 1];\n  int i = threadIdx.x + blockIdx.x * blockDim.x;\n\n  int n = mask_width / 2;  // last n cells of the previous tile\n\n  // load left cells \n  int halo_left = (blockIdx.x - 1) * blockDim.x + threadIdx.x;\n  if (threadIdx.x >= blockDim.x - n)\n     tile[threadIdx.x - (blockDim.x - n)] = halo_left < 0 ? 0 : in[halo_left];\n\n  // load center cells\n  tile[n + threadIdx.x] = in[blockIdx.x * blockDim.x + threadIdx.x];\n\n  // load right cells\n  int halo_right = (blockIdx.x + 1) * blockDim.x + threadIdx.x;\n  if (threadIdx.x < n)\n     tile[threadIdx.x + blockDim.x + n] = halo_right >= input_width ? 0 : in[halo_right];\n\n  __syncthreads();\n\n  T s = 0;\n  for (int j = 0; j < mask_width; j++)\n    s += tile[threadIdx.x + j] * mask<T>[j];\n\n  out[i] = s;\n}",
            "#define T ((int)32)\n\n\n__global__\nvoid conv1d_tiled_caching(const T *__restrict__ in,\n                                T *__restrict__ out,\n                          const int input_width,\n                          const int mask_width)\n{\n  __shared__ T tile[TILE_SIZE];\n\n  int i = threadIdx.x + blockIdx.x * blockDim.x;\n  tile[threadIdx.x] = in[i];\n  __syncthreads();\n\n  int this_tile_start = blockIdx.x * blockDim.x;\n  int next_tile_start = (blockIdx.x + 1) * blockDim.x;\n  int start = i - (mask_width / 2);\n  T s = 0;\n  for (int j = 0; j < mask_width; j++) {\n    int in_index = start + j;\n    if (in_index >= 0 && in_index < input_width) {\n      if (in_index >= this_tile_start && in_index < next_tile_start) {\n        // in_index = (start + j) = (i - mask_width/2 +j) >= 0,\n        // then map in_index to tile_index\n        s += tile[threadIdx.x + j - (mask_width / 2)] * mask<T>[j];\n      } else {\n        s += in[in_index] * mask<T>[j];\n      }\n    }\n  }\n  out[i] = s;\n}"
        ]
    },
    "mrc-cuda": {
        "/Users/gbolet/hecbench-roofline/src/mrc-cuda/main.cu": [
            "__global__\nvoid MRCGradient (\n    const int N, const int* Y, const float* X1, const float* X2, const float* dOutput,\n    const float margin, float*__restrict__ dX1, float*__restrict__ dX2)\n{\n  int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    float dist = -Y[i] * (X1[i] - X2[i]) + margin;\n    if (dist < 0.f) {\n      dX1[i] = dX2[i] = 0.f;\n    } else {\n      dX1[i] = -Y[i] * dOutput[i];\n      dX2[i] = Y[i] * dOutput[i];\n    }\n  }\n}",
            "__global__\nvoid MRCGradient2(\n    const int N, const int* Y, const float* X1, const float* X2, const float* dOutput,\n    const float margin, float*__restrict__ dX1, float*__restrict__ dX2)\n{\n  int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    float y = Y[i];\n    float o = dOutput[i];\n    float dist = -y * (X1[i] - X2[i]) + margin;\n    dX1[i] = dist < 0.f ? 0.f : -y * o;\n    dX2[i] = dist < 0.f ? 0.f : y * o;\n  }\n}"
        ]
    },
    "axhelm-cuda": {
        "/Users/gbolet/hecbench-roofline/src/axhelm-cuda/axhelmKernel.cpp": [
            "__global__ void axhelm(const int Nelements,\n    const int offset,\n    const dfloat * __restrict__ ggeo,\n    const dfloat * __restrict__ D,\n    const dfloat * __restrict__ lambda,\n    const dfloat * __restrict__ q,\n    dfloat * __restrict__ Aq) \n{\n  __shared__ dfloat s_D[64];\n  __shared__ dfloat s_q[64];\n  __shared__ dfloat s_Gqr[64];\n  __shared__ dfloat s_Gqs[64];\n\n  dfloat r_qt, r_Gqt, r_Auk;\n  dfloat r_q[8];\n  dfloat r_Aq[8];\n  dfloat r_G00, r_G01, r_G02, r_G11, r_G12, r_G22, r_GwJ;\n  dfloat r_lam0, r_lam1;\n\n  int e = blockIdx.x;\n  int j = threadIdx.y;\n  int i = threadIdx.x;\n  s_D[j*8+i] = D[j*8+i];\n  const int base = i + j * 8 + e * 512;\n  for (int k = 0; k < 8; ++k) {\n    r_q[k] = q[base + k * 8 * 8];\n    r_Aq[k] = 0;\n  }\n#pragma unroll 8\n  for (int k = 0; k < 8; ++k) {\n    const int id = e * 512 + k * 8 * 8 + j * 8 + i;\n    const int gbase = e * p_Nggeo * 512 + k * 8 * 8 + j * 8 + i;\n    r_G00 = ggeo[gbase + p_G00ID * 512];\n    r_G01 = ggeo[gbase + p_G01ID * 512];\n    r_G02 = ggeo[gbase + p_G02ID * 512];\n    r_G11 = ggeo[gbase + p_G11ID * 512];\n    r_G12 = ggeo[gbase + p_G12ID * 512];\n    r_G22 = ggeo[gbase + p_G22ID * 512];\n    r_GwJ = ggeo[gbase + p_GWJID * 512];\n    r_lam0 = lambda[id + 0 * offset];\n    r_lam1 = lambda[id + 1 * offset];\n    __syncthreads();\n    s_q[j*8+i] = r_q[k];\n    r_qt = 0;\n#pragma unroll 8\n    for (int m = 0; m < 8; ++m) {\n      r_qt += s_D[k*8+m] * r_q[m];\n    }\n    __syncthreads();\n    dfloat qr = 0;\n    dfloat qs = 0;\n#pragma unroll 8\n    for (int m = 0; m < 8; ++m) {\n      qr += s_D[i*8+m] * s_q[j*8+m];\n      qs += s_D[j*8+m] * s_q[m*8+i];\n    }\n    s_Gqs[j*8+i] = r_lam0 * (r_G01 * qr + r_G11 * qs + r_G12 * r_qt);\n    s_Gqr[j*8+i] = r_lam0 * (r_G00 * qr + r_G01 * qs + r_G02 * r_qt);\n    r_Gqt = r_lam0 * (r_G02 * qr + r_G12 * qs + r_G22 * r_qt);\n    r_Auk = r_GwJ * r_lam1 * r_q[k];\n    __syncthreads();\n#pragma unroll 8\n    for (int m = 0; m < 8; ++m) {\n      r_Auk += s_D[m*8+j] * s_Gqs[m*8+i];\n      r_Aq[m] += s_D[k*8+m] * r_Gqt;\n      r_Auk += s_D[m*8+i] * s_Gqr[j*8+m];\n    }\n    r_Aq[k] += r_Auk;\n    __syncthreads();\n  }\n#pragma unroll 8\n  for (int k = 0; k < 8; ++k) {\n    const int id = e * 512 + k * 8 * 8 + j * 8 + i;\n    Aq[id] = r_Aq[k];\n  }\n}",
            "__global__ void axhelm_n3(const int Nelements,\n    const int offset,\n    const dfloat * __restrict__ ggeo,\n    const dfloat * __restrict__ D,\n    const dfloat * __restrict__ lambda,\n    const dfloat * __restrict__ q,\n    dfloat * __restrict__ Aq) \n{\n  __shared__ dfloat s_D[64];\n  __shared__ dfloat s_U[64];\n  __shared__ dfloat s_V[64];\n  __shared__ dfloat s_W[64];\n  __shared__ dfloat s_GUr[64];\n  __shared__ dfloat s_GUs[64];\n  __shared__ dfloat s_GVr[64];\n  __shared__ dfloat s_GVs[64];\n  __shared__ dfloat s_GWr[64];\n  __shared__ dfloat s_GWs[64];\n  dfloat r_Ut, r_Vt, r_Wt;\n  dfloat r_U[8], r_V[8], r_W[8];\n  dfloat r_AU[8], r_AV[8], r_AW[8];\n  dfloat r_G00, r_G01, r_G02, r_G11, r_G12, r_G22, r_GwJ;\n  dfloat r_lam0, r_lam1;\n\n  int e = blockIdx.x;\n  int j = threadIdx.y;\n  int i = threadIdx.x;\n  s_D[j*8+i] = D[j*8+i];\n  const int base = i + j * 8 + e * 512;\n  for (int k = 0; k < 8; k++) {\n    r_U[k] = q[base + k * 8 * 8 + 0 * offset];\n    r_V[k] = q[base + k * 8 * 8 + 1 * offset];\n    r_W[k] = q[base + k * 8 * 8 + 2 * offset];\n    r_AU[k] = 0;\n    r_AV[k] = 0;\n    r_AW[k] = 0;\n  }\n#pragma unroll 8\n  for (int k = 0; k < 8; ++k) {\n    const int id = e * 512 + k * 8 * 8 + j * 8 + i;\n    const int gbase = e * p_Nggeo * 512 + k * 8 * 8 + j * 8 + i;\n    r_G00 = ggeo[gbase + p_G00ID * 512];\n    r_G01 = ggeo[gbase + p_G01ID * 512];\n    r_G02 = ggeo[gbase + p_G02ID * 512];\n    r_G11 = ggeo[gbase + p_G11ID * 512];\n    r_G12 = ggeo[gbase + p_G12ID * 512];\n    r_G22 = ggeo[gbase + p_G22ID * 512];\n    r_GwJ = ggeo[gbase + p_GWJID * 512];\n    r_lam0 = lambda[id + 0 * offset];\n    r_lam1 = lambda[id + 1 * offset];\n    __syncthreads();\n    s_U[j*8+i] = r_U[k];\n    s_V[j*8+i] = r_V[k];\n    s_W[j*8+i] = r_W[k];\n    r_Ut = 0;\n    r_Vt = 0;\n    r_Wt = 0;\n#pragma unroll 8\n    for (int m = 0; m < 8; m++) {\n      dfloat Dkm = s_D[k*8+m];\n      r_Ut += Dkm * r_U[m];\n      r_Vt += Dkm * r_V[m];\n      r_Wt += Dkm * r_W[m];\n    }\n    __syncthreads();\n    dfloat Ur = 0, Us = 0;\n    dfloat Vr = 0, Vs = 0;\n    dfloat Wr = 0, Ws = 0;\n#pragma unroll 8\n    for (int m = 0; m < 8; m++) {\n      dfloat Dim = s_D[i*8+m];\n      dfloat Djm = s_D[j*8+m];\n      Ur += Dim * s_U[j*8+m];\n      Us += Djm * s_U[m*8+i];\n      Vr += Dim * s_V[j*8+m];\n      Vs += Djm * s_V[m*8+i];\n      Wr += Dim * s_W[j*8+m];\n      Ws += Djm * s_W[m*8+i];\n    }\n    s_GUr[j*8+i] = r_lam0 * (r_G00 * Ur + r_G01 * Us + r_G02 * r_Ut);\n    s_GVr[j*8+i] = r_lam0 * (r_G00 * Vr + r_G01 * Vs + r_G02 * r_Vt);\n    s_GWr[j*8+i] = r_lam0 * (r_G00 * Wr + r_G01 * Ws + r_G02 * r_Wt);\n    s_GUs[j*8+i] = r_lam0 * (r_G01 * Ur + r_G11 * Us + r_G12 * r_Ut);\n    s_GVs[j*8+i] = r_lam0 * (r_G01 * Vr + r_G11 * Vs + r_G12 * r_Vt);\n    s_GWs[j*8+i] = r_lam0 * (r_G01 * Wr + r_G11 * Ws + r_G12 * r_Wt);\n    r_Ut = r_lam0 * (r_G02 * Ur + r_G12 * Us + r_G22 * r_Ut);\n    r_Vt = r_lam0 * (r_G02 * Vr + r_G12 * Vs + r_G22 * r_Vt);\n    r_Wt = r_lam0 * (r_G02 * Wr + r_G12 * Ws + r_G22 * r_Wt);\n    r_AU[k] += r_GwJ * r_lam1 * r_U[k];\n    r_AV[k] += r_GwJ * r_lam1 * r_V[k];\n    r_AW[k] += r_GwJ * r_lam1 * r_W[k];\n    __syncthreads();\n    dfloat AUtmp = 0, AVtmp = 0, AWtmp = 0;\n#pragma unroll 8\n    for (int m = 0; m < 8; m++) {\n      dfloat Dmi = s_D[m*8+i];\n      dfloat Dmj = s_D[m*8+j];\n      dfloat Dkm = s_D[k*8+m];\n      AUtmp += Dmi * s_GUr[j*8+m];\n      AUtmp += Dmj * s_GUs[m*8+i];\n      AVtmp += Dmi * s_GVr[j*8+m];\n      AVtmp += Dmj * s_GVs[m*8+i];\n      AWtmp += Dmi * s_GWr[j*8+m];\n      AWtmp += Dmj * s_GWs[m*8+i];\n      r_AU[m] += Dkm * r_Ut;\n      r_AV[m] += Dkm * r_Vt;\n      r_AW[m] += Dkm * r_Wt;\n    }\n    r_AU[k] += AUtmp;\n    r_AV[k] += AVtmp;\n    r_AW[k] += AWtmp;\n  }\n#pragma unroll 8\n  for (int k = 0; k < 8; k++) {\n    const int id = e * 512 + k * 8 * 8 + j * 8 + i;\n    Aq[id + 0 * offset] = r_AU[k];\n    Aq[id + 1 * offset] = r_AV[k];\n    Aq[id + 2 * offset] = r_AW[k];\n  }\n}"
        ]
    },
    "logic-rewrite-cuda": {
        "/Users/gbolet/hecbench-roofline/src/logic-rewrite-cuda/aig_manager.cu": [
            "__global__ void processRwmanFanins(int * pFanin0, int * pFanin1, int * pNumFanouts, \n                                   int nPIs, int nNodes) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < nNodes) {\n        int nodeId = idx + nPIs + 1;\n        \n        // invert const 0/1\n        if (pFanin0[nodeId] < 2)\n            pFanin0[nodeId] = 1 - pFanin0[nodeId];\n        if (pFanin1[nodeId] < 2)\n            pFanin1[nodeId] = 1 - pFanin1[nodeId];\n        \n        atomicAdd(&pNumFanouts[dUtils::AigNodeID(pFanin0[nodeId])], 1);\n        atomicAdd(&pNumFanouts[dUtils::AigNodeID(pFanin1[nodeId])], 1);\n    }\n}",
            "__global__ void processRwmanOuts(int * pOuts, int * pNumFanouts, int nPOs) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < nPOs) {\n        if (pOuts[idx] < 2)\n            pOuts[idx] = 1 - pOuts[idx];\n        \n        atomicAdd(&pNumFanouts[dUtils::AigNodeID(pOuts[idx])], 1);\n    }\n}",
            "__global__ void showDeviceKernel(int * d_pnObjs, int * d_pnPIs, int * d_pnPOs, int * d_pnNodes, \n                                 int * d_pFanin0, int * d_pFanin1, int * d_pOuts, \n                                 int * d_pNumFanouts, int * d_pLevel) {\n    printf(\"-------Original AIG Device-------\\n\");\n    printf(\"id\\tfanin0\\tfanin1\\tnumFanouts\\n\");\n    for (int i = 0; i < *d_pnObjs; i++) {\n        printf(\"%d\\t\", i);\n        if (d_pFanin0[i] != -1)\n            printf(\"%s%d\\t\", dUtils::AigNodeIsComplement(d_pFanin0[i]) ? \"!\" : \"\", dUtils::AigNodeID(d_pFanin0[i]));\n        else\n            printf(\"\\t\");\n        if (d_pFanin1[i] != -1)\n            printf(\"%s%d\\t\", dUtils::AigNodeIsComplement(d_pFanin1[i]) ? \"!\" : \"\", dUtils::AigNodeID(d_pFanin1[i]));\n        else\n            printf(\"\\t\");\n        printf(\"%d\", d_pNumFanouts[i]);\n        printf(\"\\n\");\n    }\n    for (int i = 0; i < *d_pnPOs; i++) {\n        printf(\"%d\\t\", i + *d_pnObjs);\n        printf(\"%s%d\\n\", dUtils::AigNodeIsComplement(d_pOuts[i]) ? \"!\" : \"\", dUtils::AigNodeID(d_pOuts[i]));\n    }\n    printf(\"nObjs: %d, nPIs: %d, nPOs:%d, nNodes: %d\\n\", *d_pnObjs, *d_pnPIs, *d_pnPOs, *d_pnNodes);\n}",
            "__global__ void printStatsKernel(const int * pnPIs, const int * pnPOs, const int * pnNodes) {\n    printf(\"AIG stats: i/o = %d/%d and = %d\", *pnPIs, *pnPOs, *pnNodes);\n}",
            "__global__ void updateDeviceStats(const int nEntries, const int nPIs, int * pnNodes, int * pnObjs) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx == 0) {\n        *pnNodes = nEntries;\n        *pnObjs = nEntries + nPIs + 1;\n    }\n}"
        ],
        "/Users/gbolet/hecbench-roofline/src/logic-rewrite-cuda/refactor_core.cu": [
            "#define uint64 uint64_hack_\n\n\ninline __host__ __device__ uint4 make_uint4(int4 a)\n{\n    return make_uint4(uint(a.x), uint(a.y), uint(a.z), uint(a.w));\n}\n\ninline __host__ __device__ uint4 max(uint4 a, uint4 b)\n{\n    return make_uint4(max(a.x,b.x), max(a.y,b.y), max(a.z,b.z), max(a.w,b.w));\n}\n\n__device__ __forceinline__ uint32 hash(int x) {\n    return hash((uint32)x);\n}\n\n__device__ __forceinline__ uint64 formAndNodeKey(const int lit1, const int lit2) {\n    // make sure lit1 is smaller than lit2\n    uint32 uLit1 = (uint32)lit1;\n    uint32 uLit2 = (uint32)lit2;\n    uint64 key = ((uint64)uLit1) << 32 | uLit2;\n    return key;\n}\n\n__device__ __forceinline__ ValueT retrieve_single(const KeyT * ht_keys, const ValueT * ht_values, \n                                  const KeyT key, const int capacity) {\n    KeyT loc = hash(key);\n    loc = loc % capacity;\n\n    while (true) {\n        if (ht_keys[loc] == key) {\n            return ht_values[loc];\n        } else if (ht_keys[loc] == HASHTABLE_EMPTY_KEY<KeyT, ValueT>) {\n            return HASHTABLE_EMPTY_VALUE<KeyT, ValueT>;\n        }\n        loc = (loc + 1) % capacity;\n    }\n}\n\n__device__ __forceinline__ unsigned sopCommonCube(Sop * cSop) {\n    unsigned uMask;\n    int i;\n    uMask = ~(unsigned)0;\n    // Kit_SopForEachCube( cSop, uCube, i )\n    for (i = 0; i < cSop->nCubes; i++)\n        uMask &= cSop->pCubes[i];\n    return uMask;\n}\n\n__device__ __forceinline__\nvoid sopDivideByCube(Sop * cSop, Sop * cDiv, Sop * vQuo, Sop * vRem, \n                     VecsMem<unsigned, ISOP_FACTOR_MEM_CAP> * vecsMem) {\n    unsigned uCube, uDiv;\n    int i;\n    // get the only cube\n    assert(cDiv->nCubes == 1);\n    uDiv = cDiv->pCubes[0];\n    // allocate covers\n    vQuo->nCubes = 0;\n    vQuo->pCubes = vecsMem->fetch(cSop->nCubes);\n    vRem->nCubes = 0;\n    vRem->pCubes = vecsMem->fetch(cSop->nCubes);\n    // Kit_SopForEachCube( cSop, uCube, i )\n    for (i = 0; i < cSop->nCubes; i++) {\n        uCube = cSop->pCubes[i];\n        if (subgUtil::cubeContains(uCube, uDiv))\n            // Kit_SopPushCube( vQuo, Kit_CubeSharp(uCube, uDiv) );\n            vQuo->pCubes[vQuo->nCubes++] = subgUtil::cubeSharp(uCube, uDiv);\n        else\n            // Kit_SopPushCube( vRem, uCube );\n            vRem->pCubes[vRem->nCubes++] = uCube;\n    }\n}\n\n__device__ __forceinline__ void sopDivideByLiteralQuo(Sop * cSop, int iLit) {\n    int i, k = 0;\n    // Kit_SopForEachCube( cSop, uCube, i )\n    for (i = 0; i < cSop->nCubes; i++)\n        if (subgUtil::cubeHasLit(cSop->pCubes[i], iLit))\n            cSop->pCubes[k++] = subgUtil::cubeRemLit(cSop->pCubes[i], iLit);\n    cSop->nCubes = k;\n}\n\n__device__ __forceinline__ void sopDivisorZeroKernelRec(Sop * cSop, int nLits) {\n    int iLit;\n    // find any literal that occurs at least two times\n    iLit = sopWorstLiteral(cSop, nLits);\n    if ( iLit == -1 )\n        return;\n    // derive the cube-free quotient\n    sopDivideByLiteralQuo(cSop, iLit); // the same cover\n    sopMakeCubeFree(cSop);             // the same cover\n    // call recursively\n    sopDivisorZeroKernelRec(cSop, nLits);    // the same cover\n}\n\n__device__ __forceinline__ void sopMakeCubeFree(Sop * cSop) {\n    unsigned uMask;\n    int i;\n    uMask = sopCommonCube(cSop);\n    if ( uMask == 0 )\n        return;\n    // remove the common cube\n    // Kit_SopForEachCube( cSop, uCube, i )\n    for (i = 0; i < cSop->nCubes; i++)\n        cSop->pCubes[i] = subgUtil::cubeSharp(cSop->pCubes[i], uMask);\n        // Kit_SopWriteCube( cSop, Kit_CubeSharp(uCube, uMask), i );\n}\n\n__device__ __forceinline__ int sopWorstLiteral(Sop * cSop, int nLits) {\n    int i, k, iMin, nLitsMin, nLitsCur;\n    int fUseFirst = 1;\n\n    // go through each literal\n    iMin = -1;\n    nLitsMin = 1000000;\n    for (i = 0; i < nLits; i++) {\n        // go through all the cubes\n        nLitsCur = 0;\n        // Kit_SopForEachCube( cSop, uCube, k )\n        for (k = 0; k < cSop->nCubes; k++)\n            if (subgUtil::cubeHasLit(cSop->pCubes[k], i))\n                nLitsCur++;\n        // skip the literal that does not occur or occurs once\n        if (nLitsCur < 2)\n            continue;\n        // check if this is the best literal\n        if (fUseFirst) {\n            if (nLitsMin > nLitsCur) {\n                nLitsMin = nLitsCur;\n                iMin = i;\n            }\n        }\n        else {\n            if (nLitsMin >= nLitsCur) {\n                nLitsMin = nLitsCur;\n                iMin = i;\n            }\n        }\n    }\n    if (nLitsMin < 1000000)\n        return iMin;\n    return -1;\n}\n\n__device__ __forceinline__ int sopAnyLiteral(Sop * cSop, int nLits) {\n    int i, k, nLitsCur;\n    // go through each literal\n    for (i = 0; i < nLits; i++) {\n        // go through all the cubes\n        nLitsCur = 0;\n        for (k = 0; k < cSop->nCubes; k++)\n            if (subgUtil::cubeHasLit(cSop->pCubes[k], i))\n                nLitsCur++;\n        if (nLitsCur > 1)\n            return i;\n    }\n    return -1;\n}\n\n__device__ __forceinline__ \nvoid sopDup(Sop * cResult, Sop * cSop, VecsMem<unsigned, ISOP_FACTOR_MEM_CAP> * vecsMem) {\n    int i;\n    // start the cover\n    cResult->nCubes = 0;\n    cResult->pCubes = vecsMem->fetch(cSop->nCubes);\n    // add the cubes\n    // Kit_SopForEachCube( cSop, uCube, i )\n    for (i = 0; i < cSop->nCubes; i++)\n        // Kit_SopPushCube( cResult, uCube );\n        cResult->pCubes[cResult->nCubes++] = cSop->pCubes[i];\n}\n\n__device__ __forceinline__ int sopBestLiteral(Sop * cSop, int nLits, unsigned uMask) {\n    int i, k, iMax, nLitsMax, nLitsCur;\n    int fUseFirst = 1;\n\n    // go through each literal\n    iMax = -1;\n    nLitsMax = -1;\n    for (i = 0; i < nLits; i++) {\n        if (!subgUtil::cubeHasLit(uMask, i))\n            continue;\n        // go through all the cubes\n        nLitsCur = 0;\n        // Kit_SopForEachCube( cSop, uCube, k )\n        for (k = 0; k < cSop->nCubes; k++)\n            if (subgUtil::cubeHasLit(cSop->pCubes[k], i))\n                nLitsCur++;\n        // skip the literal that does not occur or occurs once\n        if (nLitsCur < 2)\n            continue;\n        // check if this is the best literal\n        if (fUseFirst) {\n            if (nLitsMax < nLitsCur) {\n                nLitsMax = nLitsCur;\n                iMax = i;\n            }\n        } else {\n            if (nLitsMax <= nLitsCur) {\n                nLitsMax = nLitsCur;\n                iMax = i;\n            }\n        }\n    }\n    if (nLitsMax >= 0)\n        return iMax;\n    return -1;\n}\n\n__device__ __forceinline__\nint sopFactorTrivialCubeRec(unsigned uCube, int nStart, int nFinish, subgUtil::Subg<SUBG_CAP> * subg) {\n    // printf(\"enter trivial cube, start=%d, end=%d\\n\", nStart, nFinish);\n    int eNode1, eNode2;\n    int i, iLit = -1, nLits, nLits1;\n    assert(uCube);\n    // count the number of literals in this interval\n    nLits = 0;\n    for (i = nStart; i < nFinish; i++)\n        if (subgUtil::cubeHasLit(uCube, i)) {\n            iLit = i;\n            nLits++;\n        }\n    assert(iLit != -1);\n    // quit if there is only one literal        \n    if (nLits == 1) {\n        // printf(\"return iLit = %d\\n\", iLit);\n        return iLit;\n    }\n        // return Kit_EdgeCreate( iLit/2, iLit%2 ); // CST\n    // split the literals into two parts\n    nLits1 = nLits/2;\n    // nLits2 = nLits - nLits1;\n    // find the splitting point\n    nLits = 0;\n    for (i = nStart; i < nFinish; i++)\n        if (subgUtil::cubeHasLit(uCube, i)) {\n            if (nLits == nLits1)\n                break;\n            nLits++;\n        }\n    // recursively construct the tree for the parts\n    eNode1 = sopFactorTrivialCubeRec(uCube, nStart, i, subg);\n    eNode2 = sopFactorTrivialCubeRec(uCube, i, nFinish, subg);\n    return subg->addNodeAnd(eNode1, eNode2);\n}\n\n__device__ __forceinline__ \nvoid sopBestLiteralCover(Sop * cResult, Sop * cSop, unsigned uCube, int nLits, \n                         VecsMem<unsigned, ISOP_FACTOR_MEM_CAP> * vecsMem) {\n    int iLitBest;\n    // get the best literal\n    iLitBest = sopBestLiteral(cSop, nLits, uCube);\n    // start the cover\n    cResult->nCubes = 0;\n    cResult->pCubes = vecsMem->fetch(1);\n    // set the cube\n    // Kit_SopPushCube( cResult, Kit_CubeSetLit(0, iLitBest) );\n    cResult->pCubes[cResult->nCubes++] = subgUtil::cubeSetLit(0, iLitBest);\n}\n\n__device__ __forceinline__\nint sopFactorRec(Sop * cSop, int nLits, VecsMem<unsigned, ISOP_FACTOR_MEM_CAP> * vecsMem, \n                 subgUtil::Subg<SUBG_CAP> * subg) {\n    Sop Div, Quo, Rem, Com;\n    Sop * cDiv = &Div, * cQuo = &Quo, * cRem = &Rem, * cCom = &Com;\n    int eNodeDiv, eNodeQuo, eNodeRem, eNodeAnd;\n\n    assert(cSop->nCubes > 0);\n\n    // get the divisor\n    if (!sopDivisor(cDiv, cSop, nLits, vecsMem))\n        return sopFactorTrivialRec(cSop->pCubes, cSop->nCubes, nLits, subg);\n    \n    // divide the cover by the divisor\n    sopDivideInternal(cSop, cDiv, cQuo, cRem, vecsMem);\n\n    // check the trivial case\n    assert(cQuo->nCubes > 0);\n    if (cQuo->nCubes == 1)\n        return sopFactorLFRec(cSop, cQuo, nLits, vecsMem, subg);\n    \n    // make the quotient cube ABC_FREE\n    sopMakeCubeFree(cQuo);\n\n    // divide the cover by the quotient\n    sopDivideInternal(cSop, cQuo, cDiv, cRem, vecsMem);\n\n    // check the trivial case\n    // if ( Kit_SopIsCubeFree( cDiv ) )\n    if (sopCommonCube(cDiv) == 0) {\n        eNodeDiv = sopFactorRec(cDiv, nLits, vecsMem, subg);\n        eNodeQuo = sopFactorRec(cQuo, nLits, vecsMem, subg);\n        eNodeAnd = subg->addNodeAnd(eNodeDiv, eNodeQuo);\n        if (cRem->nCubes == 0)\n            return eNodeAnd;\n        eNodeRem = sopFactorRec(cRem, nLits, vecsMem, subg);\n        return subg->addNodeOr(eNodeAnd, eNodeRem);\n    }\n\n    // get the common cube\n    sopCommonCubeCover(cCom, cDiv, vecsMem);\n\n    // solve the simple problem\n    return sopFactorLFRec(cSop, cCom, nLits, vecsMem, subg);\n}\n\n__device__ __forceinline__\nint sopFactorTrivialRec(unsigned * pCubes, int nCubes, int nLits, subgUtil::Subg<SUBG_CAP> * subg) {\n    // printf(\"enter trivial\\n\");\n    int eNode1, eNode2;\n    int nCubes1, nCubes2;\n    if (nCubes == 1)\n        return sopFactorTrivialCubeRec(pCubes[0], 0, nLits, subg);\n    // split the cubes into two parts\n    nCubes1 = nCubes/2;\n    nCubes2 = nCubes - nCubes1;\n\n    // recursively construct the tree for the parts\n    eNode1 = sopFactorTrivialRec(pCubes,           nCubes1, nLits, subg);\n    eNode2 = sopFactorTrivialRec(pCubes + nCubes1, nCubes2, nLits, subg);\n    return subg->addNodeOr(eNode1, eNode2);\n}\n\n__device__ __forceinline__ \nvoid sopCommonCubeCover(Sop * cResult, Sop * cSop, VecsMem<unsigned, ISOP_FACTOR_MEM_CAP> * vecsMem) {\n    assert(cSop->nCubes > 0);\n    cResult->nCubes = 0;\n    cResult->pCubes = vecsMem->fetch(1);\n    // Kit_SopPushCube( cResult, Kit_SopCommonCube(cSop) );\n    cResult->pCubes[cResult->nCubes++] = sopCommonCube(cSop);\n}\n\n__device__ __forceinline__\nvoid sopDivideInternal(Sop * cSop, Sop * cDiv, Sop * vQuo, Sop * vRem, \n                       VecsMem<unsigned, ISOP_FACTOR_MEM_CAP> * vecsMem) {\n    unsigned uCube, uDiv;\n    unsigned uCube2 = 0; // Suppress \"might be used uninitialized\"\n    unsigned uDiv2, uQuo;\n    int i, i2, k, k2, nCubesRem;\n    assert(cSop->nCubes >= cDiv->nCubes);\n    // consider special case\n    if (cDiv->nCubes == 1) {\n        sopDivideByCube(cSop, cDiv, vQuo, vRem, vecsMem);\n        return;\n    }\n    // allocate quotient\n    vQuo->nCubes = 0;\n    vQuo->pCubes = vecsMem->fetch(cSop->nCubes / cDiv->nCubes);\n    // for each cube of the cover\n    // it either belongs to the quotient or to the remainder\n    // Kit_SopForEachCube( cSop, uCube, i )\n    for (i = 0; i < cSop->nCubes; i++) {\n        uCube = cSop->pCubes[i];\n        // skip taken cubes\n        if (subgUtil::cubeIsMarked(uCube))\n            continue;\n        // find a matching cube in the divisor\n        uDiv = ~0;\n        // Kit_SopForEachCube( cDiv, uDiv, k )\n        for (k = 0; k < cDiv->nCubes; k++) {\n            uDiv = cDiv->pCubes[k];\n            if (subgUtil::cubeContains(uCube, uDiv))\n                break;\n        }\n        // the cube is not found \n        if (k == cDiv->nCubes)\n            continue;\n        // the quotient cube exists\n        uQuo = subgUtil::cubeSharp(uCube, uDiv);\n        // find corresponding cubes for other cubes of the divisor\n        uDiv2 = ~0;\n        // Kit_SopForEachCube( cDiv, uDiv2, k2 )\n        for (k2 = 0; k2 < cDiv->nCubes; k2++) {\n            uDiv2 = cDiv->pCubes[k2];\n            if (k2 == k) continue;\n            // find a matching cube\n            // Kit_SopForEachCube( cSop, uCube2, i2 )\n            for (i2 = 0; i2 < cSop->nCubes; i2++) {\n                uCube2 = cSop->pCubes[i2];\n                // skip taken cubes\n                if (subgUtil::cubeIsMarked(uCube2))\n                    continue;\n                // check if the cube can be used\n                if (subgUtil::cubeContains(uCube2, uDiv2) && uQuo == subgUtil::cubeSharp(uCube2, uDiv2))\n                    break;\n            }\n            // the case when the cube is not found\n            if (i2 == cSop->nCubes)\n                break;\n        }\n        // we did not find some cubes - continue looking at other cubes\n        if (k2 != cDiv->nCubes)\n            continue;\n        // we found all cubes - add the quotient cube\n        // Kit_SopPushCube( vQuo, uQuo );\n        vQuo->pCubes[vQuo->nCubes++] = uQuo;\n\n        // mark the first cube\n        // Kit_SopWriteCube( cSop, Kit_CubeMark(uCube), i );\n        cSop->pCubes[i] = subgUtil::cubeMark(uCube);\n        // mark other cubes that have this quotient\n        // Kit_SopForEachCube( cDiv, uDiv2, k2 )\n        for (k2 = 0; k2 < cDiv->nCubes; k2++) {\n            uDiv2 = cDiv->pCubes[k2];\n            if (k2 == k) continue;\n            // find a matching cube\n            // Kit_SopForEachCube( cSop, uCube2, i2 )\n            for (i2 = 0; i2 < cSop->nCubes; i2++) {\n                uCube2 = cSop->pCubes[i2];\n                // skip taken cubes\n                if (subgUtil::cubeIsMarked(uCube2))\n                    continue;\n                // check if the cube can be used\n                if (subgUtil::cubeContains(uCube2, uDiv2) && uQuo == subgUtil::cubeSharp(uCube2, uDiv2))\n                    break;\n            }\n            assert(i2 < cSop->nCubes);\n            // the cube is found, mark it \n            // (later we will add all unmarked cubes to the remainder)\n            // Kit_SopWriteCube( cSop, Kit_CubeMark(uCube2), i2 );\n            cSop->pCubes[i2] = subgUtil::cubeMark(uCube2);\n        }\n    }\n    // determine the number of cubes in the remainder\n    nCubesRem = cSop->nCubes - vQuo->nCubes * cDiv->nCubes;\n    // allocate remainder\n    vRem->nCubes = 0;\n    vRem->pCubes = vecsMem->fetch(nCubesRem);\n    // finally add the remaining unmarked cubes to the remainder \n    // and clean the marked cubes in the cover\n    // Kit_SopForEachCube( cSop, uCube, i )\n    for (i = 0; i < cSop->nCubes; i++) {\n        uCube = cSop->pCubes[i];\n        if (!subgUtil::cubeIsMarked(uCube)) {\n            // Kit_SopPushCube( vRem, uCube );\n            vRem->pCubes[vRem->nCubes++] = uCube;\n            continue;\n        }\n        // Kit_SopWriteCube( cSop, Kit_CubeUnmark(uCube), i );\n        cSop->pCubes[i] = subgUtil::cubeUnmark(uCube);\n    }\n    assert(nCubesRem == vRem->nCubes);\n}\n\n__device__ __forceinline__\nint sopDivisor(Sop * cResult, Sop * cSop, int nLits, VecsMem<unsigned, ISOP_FACTOR_MEM_CAP> * vecsMem) {\n    if (cSop->nCubes <= 1)\n        return 0;\n    if (sopAnyLiteral(cSop, nLits) == -1)\n        return 0;\n    // duplicate the cover\n    sopDup(cResult, cSop, vecsMem);\n    // perform the kerneling\n    sopDivisorZeroKernelRec(cResult, nLits);\n    assert(cResult->nCubes > 0);\n    return 1;\n}\n\n__device__ __forceinline__\nint sopFactorLFRec(Sop * cSop, Sop * cSimple, int nLits, VecsMem<unsigned, ISOP_FACTOR_MEM_CAP> * vecsMem, \n                   subgUtil::Subg<SUBG_CAP> * subg) {\n    Sop Div, Quo, Rem;\n    Sop * cDiv = &Div, * cQuo = &Quo, * cRem = &Rem;\n    int eNodeDiv, eNodeQuo, eNodeRem, eNodeAnd;\n    assert(cSimple->nCubes == 1);\n    // get the most often occurring literal\n    sopBestLiteralCover(cDiv, cSop, cSimple->pCubes[0], nLits, vecsMem);\n    // divide the cover by the literal\n    sopDivideByCube(cSop, cDiv, cQuo, cRem, vecsMem);\n    // get the node pointer for the literal\n    eNodeDiv = sopFactorTrivialCubeRec(cDiv->pCubes[0], 0, nLits, subg);\n    // factor the quotient and remainder\n    eNodeQuo = sopFactorRec(cQuo, nLits, vecsMem, subg);\n    eNodeAnd = subg->addNodeAnd(eNodeDiv, eNodeQuo);\n    if (cRem->nCubes == 0)\n        return eNodeAnd;\n    eNodeRem = sopFactorRec(cRem, nLits, vecsMem, subg);\n    return subg->addNodeOr(eNodeAnd, eNodeRem);\n}\n\n__device__ __forceinline__ \nvoid sopCreateInverse(Sop * cResult, unsigned * vInput, int nInputCubes, \n                      VecsMem<unsigned, ISOP_FACTOR_MEM_CAP> * vecsMem) {\n    unsigned uCube, uMask = 0;\n    // start the cover\n    cResult->nCubes = 0;\n    cResult->pCubes = vecsMem->fetch(nInputCubes);\n\n    for (int i = 0; i < nInputCubes; i++){\n        uCube = vInput[i];\n        uMask = ((uCube | (uCube >> 1)) & 0x55555555);\n        uMask |= (uMask << 1);\n        cResult->pCubes[cResult->nCubes++] = uCube ^ uMask;\n    }\n}\n\n__device__ int evaluateSubg(int rootId, int * pNewRootLevel, const int * vCuts, int nVars, int nNodeMax, int nLevelMax, const int * pLevels, \n                            const uint64 * htKeys, const uint32 * htValues, int htCapacity, \n                            const subgUtil::Subg<SUBG_CAP> * subg) {\n    int i, counter, temp;\n    int lit0, lit1, id0, id1, func0, func1, currId, fCompRoot;\n    uint64 key;\n    uint32 retrId;\n    int vFuncs[SUBG_CAP], vLevels[SUBG_CAP];\n\n    // check the case of the resyned cut is a const or a single var of cut nodes\n    if (subg->nSize == nVars + 1) {\n        subgUtil::unbindAndNodeKeyFlag(subg->pArray[nVars], &lit0, &lit1, &fCompRoot);\n        if (lit0 == lit1)\n            return 0;\n    }\n    // initialize funcs (ids) and levels for the leaves\n    for (i = 0; i < nVars; i++) {\n        vFuncs[i] = vCuts[i];\n        vLevels[i] = pLevels[vCuts[i]];\n    }\n    \n    counter = 0;\n    for (i = nVars; i < subg->nSize; i++) {\n        subgUtil::unbindAndNodeKeyFlag(subg->pArray[i], &lit0, &lit1, &fCompRoot);\n        assert(lit0 < lit1);\n        id0 = dUtils::AigNodeID(lit0), id1 = dUtils::AigNodeID(lit1);\n        assert(id0 < i && id1 < i);\n\n        func0 = vFuncs[id0], func1 = vFuncs[id1]; // ids of its children in the original graph\n        if (func0 != -1 && func1 != -1) {\n            // if they are both present, find the resulting node in hashtable\n            func0 = dUtils::AigNodeLitCond(func0, dUtils::AigNodeIsComplement(lit0));\n            func1 = dUtils::AigNodeLitCond(func1, dUtils::AigNodeIsComplement(lit1));\n            if (func0 > func1)\n                temp = func0, func0 = func1, func1 = temp;\n            key = formAndNodeKey(func0, func1);\n            retrId = retrieve_single<uint64, uint32>(htKeys, htValues, key, htCapacity);\n            if (retrId == rootId)\n                return -1;\n            \n            if (retrId == (HASHTABLE_EMPTY_VALUE<uint64, uint32>))\n                currId = -1;\n            else\n                currId = (int)retrId;\n        } else\n            currId = -1;\n        \n        // count the number of added nodes\n        if (currId == -1) {\n            if (++counter > nNodeMax)\n                return -1;\n        }\n        // count the number of new levels\n        vLevels[i] = 1 + max(vLevels[id0], vLevels[id1]);\n        if (vLevels[i] > nLevelMax)\n            return -1;\n        // save func\n        vFuncs[i] = currId;\n    }\n    *pNewRootLevel = vLevels[subg->nSize - 1];\n    return counter;\n}\n\n__host__ __device__ inline\nvoid minatoIsop(const unsigned * puTruth, int nVars, \n                VecsMem<unsigned, ISOP_FACTOR_MEM_CAP> * vecsMem) {\n    Sop cRes, * pcRes = &cRes;\n    unsigned * pResult, * pTemp;\n\n    vecsMem->shrink(0); // clear the memory\n\n    pResult = minatoIsopRec(puTruth, puTruth, nVars, pcRes, vecsMem);\n    assert(truthUtil::truthEqual(puTruth, pResult, nVars));\n\n    if (pcRes->nCubes == 0 || (pcRes->nCubes == 1 && pcRes->pCubes[0] == 0)) {\n        vecsMem->pArray[0] = 0;\n        vecsMem->shrink(pcRes->nCubes);\n        return;\n    }\n\n    // move the cover representation to the beginning of the memory buffer\n    pTemp = vecsMem->fetch(pcRes->nCubes);\n    assert(pTemp != NULL);\n    for (int i = 0; i < pcRes->nCubes; i++)\n        pTemp[i] = pcRes->pCubes[i];\n    for (int i = 0; i < pcRes->nCubes; i++)\n        vecsMem->pArray[i] = pTemp[i];\n    vecsMem->shrink(pcRes->nCubes);\n}\n\n__device__ __forceinline__\nvoid sopFactor(unsigned * vCover, int nCoverSize, int fCompl, const int * vCuts, int nVars, \n               VecsMem<unsigned, ISOP_FACTOR_MEM_CAP> * vecsMem, \n               subgUtil::Subg<SUBG_CAP> * subg) {\n    // printf(\"** Start SOP factor ... nVars = %d, cubes = \", nVars);\n    // for (int i = 0; i < nCoverSize; i++)\n    //     printf(\"0x%08x \", vCover[i]);\n    // printf(\"\\n\");\n\n    Sop sop, * cSop = &sop;\n    int eRoot;\n    assert(nVars < 16);\n\n    // clear subgraph and assign leaves\n    subg->nSize = nVars;\n\n    // check for trivial functions\n    if (nCoverSize == 0) {\n        if (fCompl)\n            subg->createConst1();\n        else\n            subg->createConst0();\n        return;\n    }\n    if (nCoverSize == 1 && vCover[0] == 0) {\n        if (fCompl)\n            subg->createConst0();\n        else\n            subg->createConst1();\n        return;\n    }\n    // perform CST\n    sopCreateInverse(cSop, vCover, nCoverSize, vecsMem);\n    // factor the cover\n    eRoot = sopFactorRec(cSop, 2 * nVars, vecsMem, subg);\n\n    // int lit0, lit1, fCompRoot;\n    // printf(\"    subg: \");\n    // for (int i = nVars; i < subg->nSize; i++) {\n    //     subgUtil::unbindAndNodeKeyFlag(subg->pArray[i], &lit0, &lit1, &fCompRoot);\n    //     printf(\"%s%d,%s%d \", dUtils::AigNodeIsComplement(lit0) ? \"!\" : \"\", lit0 >> 1, dUtils::AigNodeIsComplement(lit1) ? \"!\" : \"\", lit1 >> 1);\n    // }\n    // printf(\"; final node is %scomplemented \", fCompRoot ? \"\" : \"NOT \");\n    // printf(\"; eRoot is %scomplemented\\n\", dUtils::AigNodeIsComplement(eRoot) ? \"\" : \"NOT \");\n\n    // if eRoot is a leaf, then this is the case of the resyned cut is a const or a single var of cut nodes\n    if (dUtils::AigNodeID(eRoot) < nVars) {\n        // add one node with lit0 = lit1 = original lit of eRoot to the subgraph\n        // note that eRoot will not be const 0/1 due to algebraic factoring\n        assert(subg->nSize == nVars);\n        subg->createSingleExistingVar(\n            dUtils::AigNodeLitCond(vCuts[dUtils::AigNodeID(eRoot)], \n                                   dUtils::AigNodeIsComplement(eRoot) != fCompl)\n        );\n        return;\n    }\n\n    // the complementation info of eRoot is already in the root node\n    // if fCompl, do complemnt\n    if (fCompl) {\n        uint64 rootNode = subg->pArray[subg->nSize - 1];\n        int lit0, lit1, fCompRoot;\n        subgUtil::unbindAndNodeKeyFlag(rootNode, &lit0, &lit1, &fCompRoot);\n        subg->pArray[subg->nSize - 1] = subgUtil::formAndNodeKeyFlag(lit0, lit1, 1 - fCompRoot);\n    }\n}\n\n__global__ void resynCut(const int * vResynInd, const int * vCutTable, const int * vCutSizes, const int * vNumSaved, \n                         const uint64 * htKeys, const uint32 * htValues, int htCapacity, const int * pLevels, \n                         uint64 * vSubgTable, int * vSubgLinks, int * vSubgLens, int * pSubgTableNext,\n                         unsigned * vTruth, const int * vTruthRanges, const unsigned * vTruthElem, int nMaxCutSize, int nResyn) {\n    int nThreads = gridDim.x * blockDim.x;\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    int rootId;\n    int nVars, nWords, nSaved, nAdded0, nAdded1;\n    int nNewLevel0, nNewLevel1;\n    int startIdx, endIdx;\n    VecsMem<unsigned, ISOP_FACTOR_MEM_CAP> vecsMem;\n    subgUtil::Subg<SUBG_CAP> subg0, subg1;\n    subgUtil::Subg<SUBG_CAP> * pSubg;\n    int fSelectedSubg;\n\n    for (; idx < nResyn; idx += nThreads) {\n        rootId = vResynInd[idx];\n        nVars = vCutSizes[rootId];\n        nSaved = vNumSaved[rootId];\n        nWords = dUtils::TruthWordNum(nVars);\n        fSelectedSubg = -1;\n\n        startIdx = (idx == 0 ? 0 : vTruthRanges[idx - 1]);\n        endIdx = vTruthRanges[idx];\n        assert(endIdx - startIdx == nWords);\n\n        // isop + factor\n        // printf(\"Root id: %d; cut nodes: \", rootId);\n        // for (int i = 0; i < nVars; i++)\n        //     printf(\"%d \", vCutTable[rootId * CUT_TABLE_SIZE + i]);\n        // printf(\"\\n\");\n\n        // if (truthUtil::isConst0(vTruth + startIdx, nVars))\n        //     printf(\" ** encountered const 0 truth table!\\n\");\n        // if (truthUtil::isConst1(vTruth + startIdx, nVars))\n        //     printf(\" ** encountered const 1 truth table!\\n\");\n\n        minatoIsop(vTruth + startIdx, nVars, &vecsMem);\n        sopFactor(vecsMem.pArray, vecsMem.nSize, 0, &vCutTable[rootId * CUT_TABLE_SIZE], nVars, &vecsMem, &subg0);\n        nAdded0 = evaluateSubg(rootId, &nNewLevel0, &vCutTable[rootId * CUT_TABLE_SIZE], \n                              nVars, nSaved, 1000000000, pLevels, htKeys, htValues, htCapacity, &subg0);\n        if (nAdded0 > -1) {\n            fSelectedSubg = 0;\n        }\n\n        // check isop + factor in the complemented case\n        truthUtil::truthNot(vTruth + startIdx, vTruth + startIdx, nVars);\n        minatoIsop(vTruth + startIdx, nVars, &vecsMem);\n        sopFactor(vecsMem.pArray, vecsMem.nSize, 1, &vCutTable[rootId * CUT_TABLE_SIZE], nVars, &vecsMem, &subg1);\n        nAdded1 = evaluateSubg(rootId, &nNewLevel1, &vCutTable[rootId * CUT_TABLE_SIZE], \n                              nVars, nSaved, 1000000000, pLevels, htKeys, htValues, htCapacity, &subg1);\n        if (nAdded1 > -1) {\n            if (nAdded0 == -1)\n                fSelectedSubg = 1;\n            else {\n                if (nAdded1 < nAdded0 || (nAdded1 == nAdded0 && (subg1.isConst() || nNewLevel1 < nNewLevel0)))\n                    fSelectedSubg = 1;\n            }\n        }\n        \n        // printf(\"  fSelectedSubg = %d, size0 = %d, size1 = %d\\n\", fSelectedSubg, subg0.nSize - nVars, subg1.nSize - nVars);\n        \n        // copy the selected subgraph into the global table\n        // NOTE remember to only copy subg[nVars:] as the final subgraph result\n        if (fSelectedSubg == -1) {\n            // printf(\" REJECT subgraph since the original structure is better\\n\");\n            vSubgLens[idx] = 0;\n            continue;\n        }\n        pSubg = (fSelectedSubg == 0 ? &subg0 : &subg1);\n        vSubgLens[idx] = pSubg->nSize - nVars;\n        assert(vSubgLens[idx] > 0);\n        assert(vSubgLinks[idx] == -1);\n        // printf(\"  NEW subgraph with %d nodes, adding %d, removing original structure saves %d nodes\\n\", \n        //        vSubgLens[idx], fSelectedSubg == 0 ? nAdded0 : nAdded1, nSaved);\n\n        int currRowIdx, lastRowIdx, columnPtr;\n        currRowIdx = idx, columnPtr = 0;\n        vSubgLinks[currRowIdx] = 0;\n        for (int i = nVars; i < pSubg->nSize; i++) {\n            if (columnPtr == SUBG_TABLE_SIZE) {\n                // expand a new row\n                lastRowIdx = currRowIdx;\n                currRowIdx = atomicAdd(pSubgTableNext, 1);\n                assert(currRowIdx < 2 * nResyn - 1);\n                assert(vSubgLinks[currRowIdx] == -1);\n                \n                vSubgLinks[lastRowIdx] = currRowIdx;\n                vSubgLinks[currRowIdx] = 0;\n                columnPtr = 0;\n            }\n            vSubgTable[currRowIdx * SUBG_TABLE_SIZE + (columnPtr++)] = pSubg->pArray[i];\n        }\n\n        // debug\n        // unsigned * vTruthTemp = (unsigned *) malloc(nWords * sizeof(unsigned));\n        // getSubgTruth(pSubg, &vCutTable[rootId * CUT_TABLE_SIZE], nVars, vTruthTemp, vTruthElem, nMaxCutSize);\n        // truthUtil::truthNot(vTruth + startIdx, vTruth + startIdx, nVars);\n        // assert(truthUtil::truthEqual(vTruth + startIdx, vTruthTemp, nVars));\n        // free(vTruthTemp);\n    }\n}",
            "#define uint64 uint64_hack_\n\n\n__device__ __forceinline__ unsigned sopCommonCube(Sop * cSop) {\n    unsigned uMask;\n    int i;\n    uMask = ~(unsigned)0;\n    // Kit_SopForEachCube( cSop, uCube, i )\n    for (i = 0; i < cSop->nCubes; i++)\n        uMask &= cSop->pCubes[i];\n    return uMask;\n}\n\n__device__ __forceinline__\nvoid sopDivideByCube(Sop * cSop, Sop * cDiv, Sop * vQuo, Sop * vRem, \n                     VecsMem<unsigned, ISOP_FACTOR_MEM_CAP> * vecsMem) {\n    unsigned uCube, uDiv;\n    int i;\n    // get the only cube\n    assert(cDiv->nCubes == 1);\n    uDiv = cDiv->pCubes[0];\n    // allocate covers\n    vQuo->nCubes = 0;\n    vQuo->pCubes = vecsMem->fetch(cSop->nCubes);\n    vRem->nCubes = 0;\n    vRem->pCubes = vecsMem->fetch(cSop->nCubes);\n    // Kit_SopForEachCube( cSop, uCube, i )\n    for (i = 0; i < cSop->nCubes; i++) {\n        uCube = cSop->pCubes[i];\n        if (subgUtil::cubeContains(uCube, uDiv))\n            // Kit_SopPushCube( vQuo, Kit_CubeSharp(uCube, uDiv) );\n            vQuo->pCubes[vQuo->nCubes++] = subgUtil::cubeSharp(uCube, uDiv);\n        else\n            // Kit_SopPushCube( vRem, uCube );\n            vRem->pCubes[vRem->nCubes++] = uCube;\n    }\n}\n\n__device__ __forceinline__ void sopDivideByLiteralQuo(Sop * cSop, int iLit) {\n    int i, k = 0;\n    // Kit_SopForEachCube( cSop, uCube, i )\n    for (i = 0; i < cSop->nCubes; i++)\n        if (subgUtil::cubeHasLit(cSop->pCubes[i], iLit))\n            cSop->pCubes[k++] = subgUtil::cubeRemLit(cSop->pCubes[i], iLit);\n    cSop->nCubes = k;\n}\n\n__device__ __forceinline__ void sopDivisorZeroKernelRec(Sop * cSop, int nLits) {\n    int iLit;\n    // find any literal that occurs at least two times\n    iLit = sopWorstLiteral(cSop, nLits);\n    if ( iLit == -1 )\n        return;\n    // derive the cube-free quotient\n    sopDivideByLiteralQuo(cSop, iLit); // the same cover\n    sopMakeCubeFree(cSop);             // the same cover\n    // call recursively\n    sopDivisorZeroKernelRec(cSop, nLits);    // the same cover\n}\n\n__device__ __forceinline__ void sopMakeCubeFree(Sop * cSop) {\n    unsigned uMask;\n    int i;\n    uMask = sopCommonCube(cSop);\n    if ( uMask == 0 )\n        return;\n    // remove the common cube\n    // Kit_SopForEachCube( cSop, uCube, i )\n    for (i = 0; i < cSop->nCubes; i++)\n        cSop->pCubes[i] = subgUtil::cubeSharp(cSop->pCubes[i], uMask);\n        // Kit_SopWriteCube( cSop, Kit_CubeSharp(uCube, uMask), i );\n}\n\n__device__ __forceinline__ int sopWorstLiteral(Sop * cSop, int nLits) {\n    int i, k, iMin, nLitsMin, nLitsCur;\n    int fUseFirst = 1;\n\n    // go through each literal\n    iMin = -1;\n    nLitsMin = 1000000;\n    for (i = 0; i < nLits; i++) {\n        // go through all the cubes\n        nLitsCur = 0;\n        // Kit_SopForEachCube( cSop, uCube, k )\n        for (k = 0; k < cSop->nCubes; k++)\n            if (subgUtil::cubeHasLit(cSop->pCubes[k], i))\n                nLitsCur++;\n        // skip the literal that does not occur or occurs once\n        if (nLitsCur < 2)\n            continue;\n        // check if this is the best literal\n        if (fUseFirst) {\n            if (nLitsMin > nLitsCur) {\n                nLitsMin = nLitsCur;\n                iMin = i;\n            }\n        }\n        else {\n            if (nLitsMin >= nLitsCur) {\n                nLitsMin = nLitsCur;\n                iMin = i;\n            }\n        }\n    }\n    if (nLitsMin < 1000000)\n        return iMin;\n    return -1;\n}\n\n__device__ __forceinline__ int sopAnyLiteral(Sop * cSop, int nLits) {\n    int i, k, nLitsCur;\n    // go through each literal\n    for (i = 0; i < nLits; i++) {\n        // go through all the cubes\n        nLitsCur = 0;\n        for (k = 0; k < cSop->nCubes; k++)\n            if (subgUtil::cubeHasLit(cSop->pCubes[k], i))\n                nLitsCur++;\n        if (nLitsCur > 1)\n            return i;\n    }\n    return -1;\n}\n\n__device__ __forceinline__ \nvoid sopDup(Sop * cResult, Sop * cSop, VecsMem<unsigned, ISOP_FACTOR_MEM_CAP> * vecsMem) {\n    int i;\n    // start the cover\n    cResult->nCubes = 0;\n    cResult->pCubes = vecsMem->fetch(cSop->nCubes);\n    // add the cubes\n    // Kit_SopForEachCube( cSop, uCube, i )\n    for (i = 0; i < cSop->nCubes; i++)\n        // Kit_SopPushCube( cResult, uCube );\n        cResult->pCubes[cResult->nCubes++] = cSop->pCubes[i];\n}\n\n__device__ __forceinline__ int sopBestLiteral(Sop * cSop, int nLits, unsigned uMask) {\n    int i, k, iMax, nLitsMax, nLitsCur;\n    int fUseFirst = 1;\n\n    // go through each literal\n    iMax = -1;\n    nLitsMax = -1;\n    for (i = 0; i < nLits; i++) {\n        if (!subgUtil::cubeHasLit(uMask, i))\n            continue;\n        // go through all the cubes\n        nLitsCur = 0;\n        // Kit_SopForEachCube( cSop, uCube, k )\n        for (k = 0; k < cSop->nCubes; k++)\n            if (subgUtil::cubeHasLit(cSop->pCubes[k], i))\n                nLitsCur++;\n        // skip the literal that does not occur or occurs once\n        if (nLitsCur < 2)\n            continue;\n        // check if this is the best literal\n        if (fUseFirst) {\n            if (nLitsMax < nLitsCur) {\n                nLitsMax = nLitsCur;\n                iMax = i;\n            }\n        } else {\n            if (nLitsMax <= nLitsCur) {\n                nLitsMax = nLitsCur;\n                iMax = i;\n            }\n        }\n    }\n    if (nLitsMax >= 0)\n        return iMax;\n    return -1;\n}\n\n__device__ __forceinline__\nint sopFactorTrivialCubeRec(unsigned uCube, int nStart, int nFinish, subgUtil::Subg<SUBG_CAP> * subg) {\n    // printf(\"enter trivial cube, start=%d, end=%d\\n\", nStart, nFinish);\n    int eNode1, eNode2;\n    int i, iLit = -1, nLits, nLits1;\n    assert(uCube);\n    // count the number of literals in this interval\n    nLits = 0;\n    for (i = nStart; i < nFinish; i++)\n        if (subgUtil::cubeHasLit(uCube, i)) {\n            iLit = i;\n            nLits++;\n        }\n    assert(iLit != -1);\n    // quit if there is only one literal        \n    if (nLits == 1) {\n        // printf(\"return iLit = %d\\n\", iLit);\n        return iLit;\n    }\n        // return Kit_EdgeCreate( iLit/2, iLit%2 ); // CST\n    // split the literals into two parts\n    nLits1 = nLits/2;\n    // nLits2 = nLits - nLits1;\n    // find the splitting point\n    nLits = 0;\n    for (i = nStart; i < nFinish; i++)\n        if (subgUtil::cubeHasLit(uCube, i)) {\n            if (nLits == nLits1)\n                break;\n            nLits++;\n        }\n    // recursively construct the tree for the parts\n    eNode1 = sopFactorTrivialCubeRec(uCube, nStart, i, subg);\n    eNode2 = sopFactorTrivialCubeRec(uCube, i, nFinish, subg);\n    return subg->addNodeAnd(eNode1, eNode2);\n}\n\n__device__ __forceinline__ \nvoid sopBestLiteralCover(Sop * cResult, Sop * cSop, unsigned uCube, int nLits, \n                         VecsMem<unsigned, ISOP_FACTOR_MEM_CAP> * vecsMem) {\n    int iLitBest;\n    // get the best literal\n    iLitBest = sopBestLiteral(cSop, nLits, uCube);\n    // start the cover\n    cResult->nCubes = 0;\n    cResult->pCubes = vecsMem->fetch(1);\n    // set the cube\n    // Kit_SopPushCube( cResult, Kit_CubeSetLit(0, iLitBest) );\n    cResult->pCubes[cResult->nCubes++] = subgUtil::cubeSetLit(0, iLitBest);\n}\n\n__device__ __forceinline__\nint sopFactorRec(Sop * cSop, int nLits, VecsMem<unsigned, ISOP_FACTOR_MEM_CAP> * vecsMem, \n                 subgUtil::Subg<SUBG_CAP> * subg) {\n    Sop Div, Quo, Rem, Com;\n    Sop * cDiv = &Div, * cQuo = &Quo, * cRem = &Rem, * cCom = &Com;\n    int eNodeDiv, eNodeQuo, eNodeRem, eNodeAnd;\n\n    assert(cSop->nCubes > 0);\n\n    // get the divisor\n    if (!sopDivisor(cDiv, cSop, nLits, vecsMem))\n        return sopFactorTrivialRec(cSop->pCubes, cSop->nCubes, nLits, subg);\n    \n    // divide the cover by the divisor\n    sopDivideInternal(cSop, cDiv, cQuo, cRem, vecsMem);\n\n    // check the trivial case\n    assert(cQuo->nCubes > 0);\n    if (cQuo->nCubes == 1)\n        return sopFactorLFRec(cSop, cQuo, nLits, vecsMem, subg);\n    \n    // make the quotient cube ABC_FREE\n    sopMakeCubeFree(cQuo);\n\n    // divide the cover by the quotient\n    sopDivideInternal(cSop, cQuo, cDiv, cRem, vecsMem);\n\n    // check the trivial case\n    // if ( Kit_SopIsCubeFree( cDiv ) )\n    if (sopCommonCube(cDiv) == 0) {\n        eNodeDiv = sopFactorRec(cDiv, nLits, vecsMem, subg);\n        eNodeQuo = sopFactorRec(cQuo, nLits, vecsMem, subg);\n        eNodeAnd = subg->addNodeAnd(eNodeDiv, eNodeQuo);\n        if (cRem->nCubes == 0)\n            return eNodeAnd;\n        eNodeRem = sopFactorRec(cRem, nLits, vecsMem, subg);\n        return subg->addNodeOr(eNodeAnd, eNodeRem);\n    }\n\n    // get the common cube\n    sopCommonCubeCover(cCom, cDiv, vecsMem);\n\n    // solve the simple problem\n    return sopFactorLFRec(cSop, cCom, nLits, vecsMem, subg);\n}\n\n__device__ __forceinline__\nint sopFactorTrivialRec(unsigned * pCubes, int nCubes, int nLits, subgUtil::Subg<SUBG_CAP> * subg) {\n    // printf(\"enter trivial\\n\");\n    int eNode1, eNode2;\n    int nCubes1, nCubes2;\n    if (nCubes == 1)\n        return sopFactorTrivialCubeRec(pCubes[0], 0, nLits, subg);\n    // split the cubes into two parts\n    nCubes1 = nCubes/2;\n    nCubes2 = nCubes - nCubes1;\n\n    // recursively construct the tree for the parts\n    eNode1 = sopFactorTrivialRec(pCubes,           nCubes1, nLits, subg);\n    eNode2 = sopFactorTrivialRec(pCubes + nCubes1, nCubes2, nLits, subg);\n    return subg->addNodeOr(eNode1, eNode2);\n}\n\n__device__ __forceinline__ \nvoid sopCommonCubeCover(Sop * cResult, Sop * cSop, VecsMem<unsigned, ISOP_FACTOR_MEM_CAP> * vecsMem) {\n    assert(cSop->nCubes > 0);\n    cResult->nCubes = 0;\n    cResult->pCubes = vecsMem->fetch(1);\n    // Kit_SopPushCube( cResult, Kit_SopCommonCube(cSop) );\n    cResult->pCubes[cResult->nCubes++] = sopCommonCube(cSop);\n}\n\n__device__ __forceinline__\nvoid sopDivideInternal(Sop * cSop, Sop * cDiv, Sop * vQuo, Sop * vRem, \n                       VecsMem<unsigned, ISOP_FACTOR_MEM_CAP> * vecsMem) {\n    unsigned uCube, uDiv;\n    unsigned uCube2 = 0; // Suppress \"might be used uninitialized\"\n    unsigned uDiv2, uQuo;\n    int i, i2, k, k2, nCubesRem;\n    assert(cSop->nCubes >= cDiv->nCubes);\n    // consider special case\n    if (cDiv->nCubes == 1) {\n        sopDivideByCube(cSop, cDiv, vQuo, vRem, vecsMem);\n        return;\n    }\n    // allocate quotient\n    vQuo->nCubes = 0;\n    vQuo->pCubes = vecsMem->fetch(cSop->nCubes / cDiv->nCubes);\n    // for each cube of the cover\n    // it either belongs to the quotient or to the remainder\n    // Kit_SopForEachCube( cSop, uCube, i )\n    for (i = 0; i < cSop->nCubes; i++) {\n        uCube = cSop->pCubes[i];\n        // skip taken cubes\n        if (subgUtil::cubeIsMarked(uCube))\n            continue;\n        // find a matching cube in the divisor\n        uDiv = ~0;\n        // Kit_SopForEachCube( cDiv, uDiv, k )\n        for (k = 0; k < cDiv->nCubes; k++) {\n            uDiv = cDiv->pCubes[k];\n            if (subgUtil::cubeContains(uCube, uDiv))\n                break;\n        }\n        // the cube is not found \n        if (k == cDiv->nCubes)\n            continue;\n        // the quotient cube exists\n        uQuo = subgUtil::cubeSharp(uCube, uDiv);\n        // find corresponding cubes for other cubes of the divisor\n        uDiv2 = ~0;\n        // Kit_SopForEachCube( cDiv, uDiv2, k2 )\n        for (k2 = 0; k2 < cDiv->nCubes; k2++) {\n            uDiv2 = cDiv->pCubes[k2];\n            if (k2 == k) continue;\n            // find a matching cube\n            // Kit_SopForEachCube( cSop, uCube2, i2 )\n            for (i2 = 0; i2 < cSop->nCubes; i2++) {\n                uCube2 = cSop->pCubes[i2];\n                // skip taken cubes\n                if (subgUtil::cubeIsMarked(uCube2))\n                    continue;\n                // check if the cube can be used\n                if (subgUtil::cubeContains(uCube2, uDiv2) && uQuo == subgUtil::cubeSharp(uCube2, uDiv2))\n                    break;\n            }\n            // the case when the cube is not found\n            if (i2 == cSop->nCubes)\n                break;\n        }\n        // we did not find some cubes - continue looking at other cubes\n        if (k2 != cDiv->nCubes)\n            continue;\n        // we found all cubes - add the quotient cube\n        // Kit_SopPushCube( vQuo, uQuo );\n        vQuo->pCubes[vQuo->nCubes++] = uQuo;\n\n        // mark the first cube\n        // Kit_SopWriteCube( cSop, Kit_CubeMark(uCube), i );\n        cSop->pCubes[i] = subgUtil::cubeMark(uCube);\n        // mark other cubes that have this quotient\n        // Kit_SopForEachCube( cDiv, uDiv2, k2 )\n        for (k2 = 0; k2 < cDiv->nCubes; k2++) {\n            uDiv2 = cDiv->pCubes[k2];\n            if (k2 == k) continue;\n            // find a matching cube\n            // Kit_SopForEachCube( cSop, uCube2, i2 )\n            for (i2 = 0; i2 < cSop->nCubes; i2++) {\n                uCube2 = cSop->pCubes[i2];\n                // skip taken cubes\n                if (subgUtil::cubeIsMarked(uCube2))\n                    continue;\n                // check if the cube can be used\n                if (subgUtil::cubeContains(uCube2, uDiv2) && uQuo == subgUtil::cubeSharp(uCube2, uDiv2))\n                    break;\n            }\n            assert(i2 < cSop->nCubes);\n            // the cube is found, mark it \n            // (later we will add all unmarked cubes to the remainder)\n            // Kit_SopWriteCube( cSop, Kit_CubeMark(uCube2), i2 );\n            cSop->pCubes[i2] = subgUtil::cubeMark(uCube2);\n        }\n    }\n    // determine the number of cubes in the remainder\n    nCubesRem = cSop->nCubes - vQuo->nCubes * cDiv->nCubes;\n    // allocate remainder\n    vRem->nCubes = 0;\n    vRem->pCubes = vecsMem->fetch(nCubesRem);\n    // finally add the remaining unmarked cubes to the remainder \n    // and clean the marked cubes in the cover\n    // Kit_SopForEachCube( cSop, uCube, i )\n    for (i = 0; i < cSop->nCubes; i++) {\n        uCube = cSop->pCubes[i];\n        if (!subgUtil::cubeIsMarked(uCube)) {\n            // Kit_SopPushCube( vRem, uCube );\n            vRem->pCubes[vRem->nCubes++] = uCube;\n            continue;\n        }\n        // Kit_SopWriteCube( cSop, Kit_CubeUnmark(uCube), i );\n        cSop->pCubes[i] = subgUtil::cubeUnmark(uCube);\n    }\n    assert(nCubesRem == vRem->nCubes);\n}\n\n__device__ __forceinline__\nint sopDivisor(Sop * cResult, Sop * cSop, int nLits, VecsMem<unsigned, ISOP_FACTOR_MEM_CAP> * vecsMem) {\n    if (cSop->nCubes <= 1)\n        return 0;\n    if (sopAnyLiteral(cSop, nLits) == -1)\n        return 0;\n    // duplicate the cover\n    sopDup(cResult, cSop, vecsMem);\n    // perform the kerneling\n    sopDivisorZeroKernelRec(cResult, nLits);\n    assert(cResult->nCubes > 0);\n    return 1;\n}\n\n__device__ __forceinline__\nint sopFactorLFRec(Sop * cSop, Sop * cSimple, int nLits, VecsMem<unsigned, ISOP_FACTOR_MEM_CAP> * vecsMem, \n                   subgUtil::Subg<SUBG_CAP> * subg) {\n    Sop Div, Quo, Rem;\n    Sop * cDiv = &Div, * cQuo = &Quo, * cRem = &Rem;\n    int eNodeDiv, eNodeQuo, eNodeRem, eNodeAnd;\n    assert(cSimple->nCubes == 1);\n    // get the most often occurring literal\n    sopBestLiteralCover(cDiv, cSop, cSimple->pCubes[0], nLits, vecsMem);\n    // divide the cover by the literal\n    sopDivideByCube(cSop, cDiv, cQuo, cRem, vecsMem);\n    // get the node pointer for the literal\n    eNodeDiv = sopFactorTrivialCubeRec(cDiv->pCubes[0], 0, nLits, subg);\n    // factor the quotient and remainder\n    eNodeQuo = sopFactorRec(cQuo, nLits, vecsMem, subg);\n    eNodeAnd = subg->addNodeAnd(eNodeDiv, eNodeQuo);\n    if (cRem->nCubes == 0)\n        return eNodeAnd;\n    eNodeRem = sopFactorRec(cRem, nLits, vecsMem, subg);\n    return subg->addNodeOr(eNodeAnd, eNodeRem);\n}\n\n__device__ __forceinline__ \nvoid sopCreateInverse(Sop * cResult, unsigned * vInput, int nInputCubes, \n                      VecsMem<unsigned, ISOP_FACTOR_MEM_CAP> * vecsMem) {\n    unsigned uCube, uMask = 0;\n    // start the cover\n    cResult->nCubes = 0;\n    cResult->pCubes = vecsMem->fetch(nInputCubes);\n\n    for (int i = 0; i < nInputCubes; i++){\n        uCube = vInput[i];\n        uMask = ((uCube | (uCube >> 1)) & 0x55555555);\n        uMask |= (uMask << 1);\n        cResult->pCubes[cResult->nCubes++] = uCube ^ uMask;\n    }\n}\n\n__host__ __device__ inline\nvoid minatoIsop(const unsigned * puTruth, int nVars, \n                VecsMem<unsigned, ISOP_FACTOR_MEM_CAP> * vecsMem) {\n    Sop cRes, * pcRes = &cRes;\n    unsigned * pResult, * pTemp;\n\n    vecsMem->shrink(0); // clear the memory\n\n    pResult = minatoIsopRec(puTruth, puTruth, nVars, pcRes, vecsMem);\n    assert(truthUtil::truthEqual(puTruth, pResult, nVars));\n\n    if (pcRes->nCubes == 0 || (pcRes->nCubes == 1 && pcRes->pCubes[0] == 0)) {\n        vecsMem->pArray[0] = 0;\n        vecsMem->shrink(pcRes->nCubes);\n        return;\n    }\n\n    // move the cover representation to the beginning of the memory buffer\n    pTemp = vecsMem->fetch(pcRes->nCubes);\n    assert(pTemp != NULL);\n    for (int i = 0; i < pcRes->nCubes; i++)\n        pTemp[i] = pcRes->pCubes[i];\n    for (int i = 0; i < pcRes->nCubes; i++)\n        vecsMem->pArray[i] = pTemp[i];\n    vecsMem->shrink(pcRes->nCubes);\n}\n\n__device__ __forceinline__\nvoid sopFactor(unsigned * vCover, int nCoverSize, int fCompl, const int * vCuts, int nVars, \n               VecsMem<unsigned, ISOP_FACTOR_MEM_CAP> * vecsMem, \n               subgUtil::Subg<SUBG_CAP> * subg) {\n    // printf(\"** Start SOP factor ... nVars = %d, cubes = \", nVars);\n    // for (int i = 0; i < nCoverSize; i++)\n    //     printf(\"0x%08x \", vCover[i]);\n    // printf(\"\\n\");\n\n    Sop sop, * cSop = &sop;\n    int eRoot;\n    assert(nVars < 16);\n\n    // clear subgraph and assign leaves\n    subg->nSize = nVars;\n\n    // check for trivial functions\n    if (nCoverSize == 0) {\n        if (fCompl)\n            subg->createConst1();\n        else\n            subg->createConst0();\n        return;\n    }\n    if (nCoverSize == 1 && vCover[0] == 0) {\n        if (fCompl)\n            subg->createConst0();\n        else\n            subg->createConst1();\n        return;\n    }\n    // perform CST\n    sopCreateInverse(cSop, vCover, nCoverSize, vecsMem);\n    // factor the cover\n    eRoot = sopFactorRec(cSop, 2 * nVars, vecsMem, subg);\n\n    // int lit0, lit1, fCompRoot;\n    // printf(\"    subg: \");\n    // for (int i = nVars; i < subg->nSize; i++) {\n    //     subgUtil::unbindAndNodeKeyFlag(subg->pArray[i], &lit0, &lit1, &fCompRoot);\n    //     printf(\"%s%d,%s%d \", dUtils::AigNodeIsComplement(lit0) ? \"!\" : \"\", lit0 >> 1, dUtils::AigNodeIsComplement(lit1) ? \"!\" : \"\", lit1 >> 1);\n    // }\n    // printf(\"; final node is %scomplemented \", fCompRoot ? \"\" : \"NOT \");\n    // printf(\"; eRoot is %scomplemented\\n\", dUtils::AigNodeIsComplement(eRoot) ? \"\" : \"NOT \");\n\n    // if eRoot is a leaf, then this is the case of the resyned cut is a const or a single var of cut nodes\n    if (dUtils::AigNodeID(eRoot) < nVars) {\n        // add one node with lit0 = lit1 = original lit of eRoot to the subgraph\n        // note that eRoot will not be const 0/1 due to algebraic factoring\n        assert(subg->nSize == nVars);\n        subg->createSingleExistingVar(\n            dUtils::AigNodeLitCond(vCuts[dUtils::AigNodeID(eRoot)], \n                                   dUtils::AigNodeIsComplement(eRoot) != fCompl)\n        );\n        return;\n    }\n\n    // the complementation info of eRoot is already in the root node\n    // if fCompl, do complemnt\n    if (fCompl) {\n        uint64 rootNode = subg->pArray[subg->nSize - 1];\n        int lit0, lit1, fCompRoot;\n        subgUtil::unbindAndNodeKeyFlag(rootNode, &lit0, &lit1, &fCompRoot);\n        subg->pArray[subg->nSize - 1] = subgUtil::formAndNodeKeyFlag(lit0, lit1, 1 - fCompRoot);\n    }\n}\n\n__global__ void factorFromTruth(const int * vCuts, const int * vCutRanges, \n                                uint64 * vSubgTable, int * vSubgLinks, int * vSubgLens, int * pSubgTableNext,\n                                const unsigned * vTruth, const unsigned * vTruthNeg, const int * vTruthRanges, \n                                const unsigned * vTruthElem, int nResyn) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    int nVars;\n    int cutStartIdx, cutEndIdx, truthStartIdx, truthEndIdx;\n    int fNeg = 0;\n    VecsMem<unsigned, ISOP_FACTOR_MEM_CAP> vecsMem;\n    subgUtil::Subg<SUBG_CAP> subg;\n\n    // the number of threads launched should be 2 * nResyn\n    if (idx < 2 * nResyn) {\n        if (idx >= nResyn) {\n            idx -= nResyn;\n            fNeg = 1;\n        }\n\n        cutStartIdx = (idx == 0 ? 0 : vCutRanges[idx - 1]);\n        cutEndIdx = vCutRanges[idx];\n        nVars = cutEndIdx - cutStartIdx;\n\n        truthStartIdx = (idx == 0 ? 0 : vTruthRanges[idx - 1]);\n        truthEndIdx = vTruthRanges[idx];\n        assert(truthEndIdx - truthStartIdx == dUtils::TruthWordNum(nVars));\n\n        const unsigned * pTruth = (fNeg ? vTruthNeg + truthStartIdx : vTruth + truthStartIdx);\n\n        // isop + factor\n        minatoIsop(pTruth, nVars, &vecsMem);\n        __syncthreads();\n        sopFactor(vecsMem.pArray, vecsMem.nSize, fNeg, vCuts + cutStartIdx, nVars, &vecsMem, &subg);\n        __syncthreads();\n\n        // save synthesized graph into global table\n        int currRowIdx, lastRowIdx, columnPtr;\n        currRowIdx = 2 * idx + fNeg; // corresponding pairs are consecutively stored\n        columnPtr = 0;\n\n        vSubgLens[currRowIdx] = subg.nSize - nVars;\n        assert(vSubgLens[currRowIdx] > 0);\n        assert(vSubgLinks[currRowIdx] == -1);\n        vSubgLinks[currRowIdx] = 0;\n        for (int i = nVars; i < subg.nSize; i++) {\n            if (columnPtr == SUBG_TABLE_SIZE) {\n                // expand a new row\n                lastRowIdx = currRowIdx;\n                currRowIdx = atomicAdd(pSubgTableNext, 1);\n                assert(currRowIdx < 4 * nResyn - 1);\n                assert(vSubgLinks[currRowIdx] == -1);\n                \n                vSubgLinks[lastRowIdx] = currRowIdx;\n                vSubgLinks[currRowIdx] = 0;\n                columnPtr = 0;\n            }\n            vSubgTable[currRowIdx * SUBG_TABLE_SIZE + (columnPtr++)] = subg.pArray[i];\n        }\n        __syncthreads();\n\n        // unsigned * vTruthTemp = (unsigned *) malloc(dUtils::TruthWordNum(nVars) * sizeof(unsigned));\n        // getSubgTruth(&subg, vCuts + cutStartIdx, nVars, vTruthTemp, vTruthElem, 12);\n        // if (fNeg)\n        //     truthUtil::truthNot(vTruthTemp, vTruthTemp, nVars);\n        // assert(truthUtil::truthEqual(pTruth, vTruthTemp, nVars));\n        // free(vTruthTemp);\n    }\n}"
        ],
        "/Users/gbolet/hecbench-roofline/src/logic-rewrite-cuda/refactor.cu": [
            "inline __host__ __device__ uint4 make_uint4(int4 a)\n{\n    return make_uint4(uint(a.x), uint(a.y), uint(a.z), uint(a.w));\n}\n\ninline __host__ __device__ uint4 max(uint4 a, uint4 b)\n{\n    return make_uint4(max(a.x,b.x), max(a.y,b.y), max(a.z,b.z), max(a.w,b.w));\n}\n\n__device__ int decrementRef(int rootId, int nodeId, int nPIs, int nMaxCutSize, int nMinLevel,\n                            const int * pNumFanouts, const int * pLevels, \n                            int * vCutTable, int * vCutSizes, \n                            int * pTravSize, int * travIds, int * travRefs) {\n    if (dUtils::AigIsPIConst(nodeId, nPIs) || pLevels[nodeId] <= nMinLevel) {\n        // stop expansion, also do not add into the trav list\n        int oldCutSize = vCutSizes[rootId];\n        if (oldCutSize < nMaxCutSize) {\n            // add into cut list if it is not inside\n            for (int i = 0; i < oldCutSize; i++)\n                if (vCutTable[rootId * CUT_TABLE_SIZE + i] == nodeId)\n                    return 1;\n            vCutTable[rootId * CUT_TABLE_SIZE + oldCutSize] = nodeId;\n            vCutSizes[rootId]++;\n            return 1;\n        } else {\n            // the cut has reached max size\n            vCutSizes[rootId] = -1;\n            return -100;\n        }\n    }\n\n    // check whether nodeId is already in the trav list\n    for (int i = 0; i < *pTravSize; i++)\n        if (travIds[i] == nodeId)\n            return --travRefs[i];\n    assert(*pTravSize < STACK_SIZE);\n\n    // nodeId is not in the trav list; insert it\n    travIds[*pTravSize] = nodeId;\n    travRefs[*pTravSize] = pNumFanouts[nodeId] - 1;\n    (*pTravSize)++;\n    return pNumFanouts[nodeId] - 1;\n}\n\n__global__ void getMffcCut(const int * pFanin0, const int * pFanin1, \n                           const int * pNumFanouts, const int * pLevels, \n                           int * vCutTable, int * vCutSizes, int * vConeSizes,\n                           int nNodes, int nPIs, int nMaxCutSize) {\n    // TODO for hyp, there are some nodes whose MFFC is not the same as ABC\n    int nThreads = gridDim.x * blockDim.x;\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    int stack[STACK_SIZE], travIds[STACK_SIZE], travRefs[STACK_SIZE];\n    int stackTop, travSize, coneSize;\n    int nodeId, rootId, faninId, nMinLevel;\n    int fDecRet;\n\n    for (; idx < nNodes; idx += nThreads) {\n        stackTop = -1, travSize = 0, coneSize = 0;\n\n        rootId = idx + nPIs + 1;\n        stack[++stackTop] = rootId; // do not launch threads for PIs\n        nMinLevel = max(0, pLevels[rootId] - 10);\n\n        // printf(\"rootId = %d\\n\", rootId);\n        // printf(\"  minLv: %d\\n\", nMinLevel);\n\n        while (stackTop != -1) {\n            nodeId = stack[stackTop--];\n            coneSize++;\n            // printf(\"  %d \\n\", nodeId);\n\n            // check its two fanins\n            faninId = dUtils::AigNodeID(pFanin1[nodeId]);\n            // printf(\"    checking fanin %d\\n\", faninId);\n            fDecRet = decrementRef(rootId, faninId, nPIs, nMaxCutSize, nMinLevel, \n                                   pNumFanouts, pLevels, vCutTable, vCutSizes, \n                                   &travSize, travIds, travRefs);\n            // printf(\"    checked fanin %d\\n\", faninId);\n            if (fDecRet == -100)\n                break;             // cut size reached maximum\n            else if (fDecRet == 0)\n                stack[++stackTop] = faninId;\n            assert(stackTop < STACK_SIZE);\n\n            faninId = dUtils::AigNodeID(pFanin0[nodeId]);\n            // printf(\"    checking fanin %d\\n\", faninId);\n            fDecRet = decrementRef(rootId, faninId, nPIs, nMaxCutSize, nMinLevel, \n                                   pNumFanouts, pLevels, vCutTable, vCutSizes, \n                                   &travSize, travIds, travRefs);\n            // printf(\"    checked fanin %d\\n\", faninId);\n            if (fDecRet == -100)\n                break;\n            else if (fDecRet == 0)\n                stack[++stackTop] = faninId;\n            assert(stackTop < STACK_SIZE);\n\n            // printf(\"  iteration end\\n\");\n        }\n\n        if (vCutSizes[rootId] != -1) {\n            // add all nodes in the trav list with ref > 0 into the cut list\n            for (int i = 0; i < travSize; i++) {\n                assert(travRefs[i] >= 0);\n                if (travRefs[i] == 0) continue;\n\n                if (vCutSizes[rootId] < nMaxCutSize) {\n                    // add into cut list\n                    vCutTable[rootId * CUT_TABLE_SIZE + vCutSizes[rootId]] = travIds[i];\n                    vCutSizes[rootId]++;\n                } else {\n                    // the cut has reached max size\n                    vCutSizes[rootId] = -1;\n                    break;\n                }\n            }\n            assert(vCutSizes[rootId] <= MAX_CUT_SIZE);\n            // save coneSize\n            vConeSizes[rootId] = coneSize;\n        }\n\n        // printf(\"node: %d, cone size: %d | \", rootId, vConeSizes[rootId]);\n        // for (int j = 0; j < vCutSizes[rootId]; j++) {\n        //     printf(\"%d \", vCutTable[rootId * CUT_TABLE_SIZE + j]);\n        // }\n        // printf(\"\\n\");\n    }\n}",
            "__device__ int decrementReconvRef(int rootId, int nodeId, const int * pNumFanouts, \n                                  const int * vCutTable, const int * vCutSizes, \n                                  int * pTravSize, int * travIds, int * travRefs) {\n    // terminate when reaching the cut nodes\n    for (int i = 0; i < vCutSizes[rootId]; i++)\n        if (vCutTable[rootId * CUT_TABLE_SIZE + i] == nodeId)\n            return 1;\n    \n    // check whether nodeId is already in the trav list\n    for (int i = 0; i < *pTravSize; i++)\n        if (travIds[i] == nodeId)\n            return --travRefs[i];\n    assert(*pTravSize < STACK_SIZE);\n\n    // nodeId is not in the trav list; insert it\n    travIds[*pTravSize] = nodeId;\n    travRefs[*pTravSize] = pNumFanouts[nodeId] - 1;\n    (*pTravSize)++;\n    return pNumFanouts[nodeId] - 1;\n}\n\n__global__ void getReconvNumSaved(const int * pFanin0, const int * pFanin1, const int * pNumFanouts, \n                                  const int * vReconvInd, const int * vCutTable, const int * vCutSizes, \n                                  int * vNumSaved, int nReconv) {\n    int nThreads = gridDim.x * blockDim.x;\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    int stack[STACK_SIZE], travIds[STACK_SIZE], travRefs[STACK_SIZE];\n    int stackTop, travSize, savedSize;\n    int nodeId, rootId, faninId;\n\n    for (; idx < nReconv; idx += nThreads) {\n        rootId = vReconvInd[idx];\n        stackTop = -1, travSize = 0, savedSize = 0;\n        stack[++stackTop] = rootId;\n\n        while (stackTop != -1) {\n            nodeId = stack[stackTop--];\n            savedSize++;\n\n            // check two fanins\n            faninId = dUtils::AigNodeID(pFanin1[nodeId]);\n            if (decrementReconvRef(rootId, faninId, pNumFanouts, vCutTable, vCutSizes, \n                                   &travSize, travIds, travRefs) == 0) {\n                assert(stackTop < STACK_SIZE);\n                stack[++stackTop] = faninId;\n            }\n\n            faninId = dUtils::AigNodeID(pFanin0[nodeId]);\n            if (decrementReconvRef(rootId, faninId, pNumFanouts, vCutTable, vCutSizes, \n                                   &travSize, travIds, travRefs) == 0) {\n                assert(stackTop < STACK_SIZE);\n                stack[++stackTop] = faninId;\n            }\n        }\n        vNumSaved[rootId] = savedSize;\n    }\n}",
            "__device__ int getReconvCutIter(int rootId,\n                                const int * pFanin0, const int * pFanin1, \n                                const int * pNumFanouts, const int * pLevels, \n                                int * visited, int * pVisitedSize,\n                                int * vCutTable, int * vCutSizes, \n                                int nPIs, int nMaxCutSize, int nFanoutLimit) {\n    int nodeId, faninId, bestId = -1, bestIdx = -1;\n    int bestCost = 100, currCost;\n    int fFanin0Visited, fFanin1Visited, fBestFanin0Visited = 0, fBestFanin1Visited = 0;\n\n    // find the best cost cut node to expand\n    for (int i = 0; i < vCutSizes[rootId]; i++) {\n        nodeId = vCutTable[rootId * CUT_TABLE_SIZE + i];\n\n        // get the number of new leaves\n        fFanin0Visited = fFanin1Visited = 0;\n        if (dUtils::AigIsPIConst(nodeId, nPIs))\n            currCost = 999;\n        else {\n            faninId = dUtils::AigNodeID(pFanin0[nodeId]);\n            for (int j = 0; j < *pVisitedSize; j++)\n                if (visited[j] == faninId) {\n                    fFanin0Visited = 1;\n                    break;\n                }\n            \n            faninId = dUtils::AigNodeID(pFanin1[nodeId]);\n            for (int j = 0; j < *pVisitedSize; j++)\n                if (visited[j] == faninId) {\n                    fFanin1Visited = 1;\n                    break;\n                }\n            \n            currCost = (1 - fFanin0Visited) + (1 - fFanin1Visited);\n            if (currCost >= 2) {\n                if (pNumFanouts[nodeId] > nFanoutLimit)\n                    currCost = 999;\n            }\n        }\n\n        // update best node\n        if (bestCost > currCost || (bestCost == currCost && pLevels[nodeId] > pLevels[bestId])) {\n            bestCost = currCost, bestId = nodeId, bestIdx = i;\n            fBestFanin0Visited = fFanin0Visited, fBestFanin1Visited = fFanin1Visited;\n        }\n        if (bestCost == 0)\n            break;\n    }\n\n    if (bestId == -1)\n        return 0;\n    assert(bestCost < 3);\n\n    if (vCutSizes[rootId] - 1 + bestCost > nMaxCutSize)\n        return 0;\n    assert(dUtils::AigIsNode(bestId, nPIs));\n\n    // remove the best node from cut list\n    for (int i = bestIdx + 1; i < vCutSizes[rootId]; i++)\n        vCutTable[rootId * CUT_TABLE_SIZE + i - 1] = vCutTable[rootId * CUT_TABLE_SIZE + i];\n    vCutSizes[rootId]--;\n\n    if (!fBestFanin0Visited) {\n        assert(*pVisitedSize < STACK_SIZE);\n        faninId = dUtils::AigNodeID(pFanin0[bestId]);\n        vCutTable[rootId * CUT_TABLE_SIZE + (vCutSizes[rootId]++)] = faninId;\n        visited[(*pVisitedSize)++] = faninId;\n    }\n    if (!fBestFanin1Visited) {\n        assert(*pVisitedSize < STACK_SIZE);\n        faninId = dUtils::AigNodeID(pFanin1[bestId]);\n        vCutTable[rootId * CUT_TABLE_SIZE + (vCutSizes[rootId]++)] = faninId;\n        visited[(*pVisitedSize)++] = faninId;\n    }\n    assert(vCutSizes[rootId] <= nMaxCutSize);\n    return 1;\n}\n\n__global__ void getReconvCut(const int * pFanin0, const int * pFanin1, \n                             const int * pNumFanouts, const int * pLevels, \n                             const int * vReconvInd, int * vCutTable, int * vCutSizes, \n                             int nReconv, int nPIs, int nMaxCutSize, int nFanoutLimit) {\n    int nThreads = gridDim.x * blockDim.x;\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    int visited[STACK_SIZE];\n    int visitedSize;\n    int rootId, faninId;\n\n    for (; idx < nReconv; idx += nThreads) {\n        visitedSize = 0;\n        rootId = vReconvInd[idx];\n        vCutSizes[rootId] = 0;\n\n        // initialize the cut list and visited list\n        visited[visitedSize++] = rootId;\n\n        faninId = dUtils::AigNodeID(pFanin0[rootId]);\n        vCutTable[rootId * CUT_TABLE_SIZE + (vCutSizes[rootId]++)] = faninId;\n        visited[visitedSize++] = faninId;\n\n        faninId = dUtils::AigNodeID(pFanin1[rootId]);\n        vCutTable[rootId * CUT_TABLE_SIZE + (vCutSizes[rootId]++)] = faninId;\n        visited[visitedSize++] = faninId;\n\n        // iteratively expand the cut\n        while (getReconvCutIter(rootId, pFanin0, pFanin1, pNumFanouts, pLevels, visited, &visitedSize,\n                   vCutTable, vCutSizes, nPIs, nMaxCutSize, nFanoutLimit));\n        assert(vCutSizes[rootId] <= nMaxCutSize);\n    }\n}",
            "__global__ void getTruthWordNums(const int * vResynInd, const int * vCutSizes, \n                                 int * vTruthRanges, int nResyn) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < nResyn) {\n        int nodeId = vResynInd[idx];\n        assert(vCutSizes[nodeId] > 0);\n        vTruthRanges[idx] = dUtils::TruthWordNum(vCutSizes[nodeId]);\n    }\n}",
            "#define uint64 uint64_hack_\n\n\n__device__ __forceinline__ uint32 hash(int x) {\n    return hash((uint32)x);\n}\n\n__device__ __forceinline__ uint64 formAndNodeKey(const int lit1, const int lit2) {\n    // make sure lit1 is smaller than lit2\n    uint32 uLit1 = (uint32)lit1;\n    uint32 uLit2 = (uint32)lit2;\n    uint64 key = ((uint64)uLit1) << 32 | uLit2;\n    return key;\n}\n\n__device__ __forceinline__ uint32 insert_single_no_update_volatile(volatile KeyT * ht_keys, volatile ValueT * ht_values, \n                                        const KeyT key, const ValueT value, const int capacity) {\n    KeyT loc = hash(key);\n    loc = loc % capacity;\n\n    while (true) {\n        KeyT prev = atomicCAS((KeyT *)&ht_keys[loc], HASHTABLE_EMPTY_KEY<KeyT, ValueT>, key);\n        if (prev == HASHTABLE_EMPTY_KEY<KeyT, ValueT>) {\n            // found a empty entry\n            ht_values[loc] = value;\n            return 2;\n        } else if (prev == key) {\n            // already have key inserted, no update\n            return ht_values[loc];\n        }\n\n        loc = (loc + 1) % capacity;\n    }\n}\n\n__device__ __forceinline__ ValueT retrieve_single_volatile(volatile const KeyT * ht_keys, volatile const ValueT * ht_values, \n                                  const KeyT key, const int capacity) {\n    KeyT loc = hash(key);\n    loc = loc % capacity;\n\n    while (true) {\n        if (ht_keys[loc] == key) {\n            return ht_values[loc];\n        } else if (ht_keys[loc] == HASHTABLE_EMPTY_KEY<KeyT, ValueT>) {\n            return HASHTABLE_EMPTY_VALUE<KeyT, ValueT>;\n        }\n        loc = (loc + 1) % capacity;\n    }\n}\n\n__global__ void insertChoiceGraphs(const uint64 * vSubgTable, const int * vSubgLinks, const int * vSubgLens, \n                                   const int * vResynInd, const int * vCutTable, const int * vCutSizes, \n                                   uint64 * htKeys, uint32 * htValues, int htCapacity, int * vChoicesLit, \n                                   int nResyn, int nObjs, int nPIs, int * pnNewObjs) {\n    // vChoicesLit should be initialized as -1\n    int nThreads = gridDim.x * blockDim.x;\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    int rootId, subgId, newId, fanin0Id, fanin1Id;\n    int nCutSize, nSubgSize;\n    int currRowIdx, columnPtr;\n    int mapId[MAX_SUBG_SIZE]; // map subgraph id of a node to old node id in the original graph\n    uint32 nodeId, realNewId;\n    int lit0, lit1, lit2, lit3, fComp, temp;\n    uint64 key;\n\n    for (; idx < nResyn; idx += nThreads) {\n        rootId = vResynInd[idx];\n        nSubgSize = vSubgLens[idx];\n        nCutSize = vCutSizes[rootId];\n\n        assert(nSubgSize + nCutSize <= MAX_SUBG_SIZE);\n        // nSubgSize == 0 means that in pre-evaluation the resyned subgraphs are no better than the original one\n        // vChoicesLit is still -1\n        if (nSubgSize == 0)\n            continue;\n\n        for (int i = nCutSize; i < nCutSize + nSubgSize; i++)\n            mapId[i] = -1;\n        for (int i = 0; i < nCutSize; i++)\n            mapId[i] = vCutTable[rootId * CUT_TABLE_SIZE + i];\n        \n        // printf(\" *** root id %d, cut nodes: \", rootId);\n        // for (int k = 0; k < nCutSize; k++)\n        //     printf(\"%d \", mapId[k]);\n        // printf(\"| subg: \");\n        // currRowIdx = idx, columnPtr = 0;\n        // for (int k = nCutSize; k < nCutSize + nSubgSize; k++) {\n        //     if (columnPtr == SUBG_TABLE_SIZE) {\n        //         columnPtr = 0;\n        //         currRowIdx = vSubgLinks[currRowIdx];\n        //         assert(currRowIdx > 0);\n        //     }\n        //     subgUtil::unbindAndNodeKeyFlag(vSubgTable[currRowIdx * SUBG_TABLE_SIZE + (columnPtr++)], \n        //                             &lit0, &lit1, &fComp);\n        //     fanin0Id = dUtils::AigNodeID(lit0);\n        //     fanin1Id = dUtils::AigNodeID(lit1);\n        //     printf(\"%s%d,%s%d \", dUtils::AigNodeIsComplement(lit0) ? \"!\" : \"\", fanin0Id, dUtils::AigNodeIsComplement(lit1) ? \"!\" : \"\", fanin1Id);\n        //     if (k == nCutSize + nSubgSize - 1)\n        //         printf(\" root complemented: %s\", fComp ? \"y\" : \"n\");\n        // }\n        // printf(\"\\n\");\n\n        // take care of the case that the resyned cut is a const or a single var of cut nodes\n        if (nSubgSize == 1) {\n            subgUtil::unbindAndNodeKeyFlag(vSubgTable[idx * SUBG_TABLE_SIZE], &lit0, &lit1, &fComp);\n            // having two same lits is the indicator of these cases\n            if (lit0 == lit1) {\n                fanin0Id = dUtils::AigNodeID(lit0); // in this case lit0 is using the original id\n                assert(fanin0Id < nObjs);\n\n                // debug assertion\n                // int fCutNode = 0;\n                // for (int i = 0; i < nCutSize; i++)\n                //     if (mapId[i] == fanin0Id) {\n                //         fCutNode = 1;\n                //         break;\n                //     }\n                // assert(fCutNode || fanin0Id == 0);\n\n                assert(fComp == dUtils::AigNodeIsComplement(lit0));\n                vChoicesLit[idx] = lit0;\n                continue;\n            }\n        }\n\n        // in topo order of the subgraph, check whether each node has corresponding old node\n        currRowIdx = idx, columnPtr = 0;\n        for (int i = 0; i < nSubgSize; i++) {\n            // change row\n            if (columnPtr == SUBG_TABLE_SIZE) {\n                columnPtr = 0;\n                currRowIdx = vSubgLinks[currRowIdx];\n            }\n\n            subgId = i + nCutSize; // new id of the node in subgraph\n            subgUtil::unbindAndNodeKeyFlag(vSubgTable[currRowIdx * SUBG_TABLE_SIZE + (columnPtr++)], \n                                           &lit0, &lit1, &fComp);\n            fanin0Id = dUtils::AigNodeID(lit0);\n            fanin1Id = dUtils::AigNodeID(lit1);\n            // if one of the fanins is a new node, then this is also a new node\n            if (mapId[fanin0Id] == -1 || mapId[fanin1Id] == -1)\n                continue;\n            \n            // note that the old node will never be a PI, since this is an AND node in the new subgraph\n            assert(lit0 <= lit1);\n            lit2 = dUtils::AigNodeLitCond(mapId[fanin0Id], dUtils::AigNodeIsComplement(lit0));\n            lit3 = dUtils::AigNodeLitCond(mapId[fanin1Id], dUtils::AigNodeIsComplement(lit1));\n            if (lit2 > lit3)\n                temp = lit2, lit2 = lit3, lit3 = temp;\n            key = formAndNodeKey(lit2, lit3);\n            nodeId = retrieve_single_volatile<uint64, uint32>(htKeys, htValues, key, htCapacity);\n            // nodeId = retrieve_single<uint64, uint32>(htKeys, htValues, key, htCapacity);\n            if (nodeId != (HASHTABLE_EMPTY_VALUE<uint64, uint32>) && (int)nodeId > 0 && (int)nodeId < nObjs)\n                mapId[subgId] = (int)nodeId;\n            // nodeId < nObjs is to filter out the subsequently inserted new nodes by other threads\n        }\n\n        if (mapId[nCutSize + nSubgSize - 1] != -1) {\n            // the whole new subgraph is the same as the old one\n            // note, vChoicesLit is still -1 for this root\n            // if (mapId[nCutSize + nSubgSize - 1] != rootId && rootId == 203) {\n            //     printf(\" *** error: root id %d, subg \", rootId);\n            //     currRowIdx = idx, columnPtr = 0;\n            //     for (int k = nCutSize; k < nCutSize + nSubgSize; k++) {\n            //         if (columnPtr == SUBG_TABLE_SIZE) {\n            //             columnPtr = 0;\n            //             currRowIdx = vSubgLinks[currRowIdx];\n            //             assert(currRowIdx > 0);\n            //         }\n            //         subgUtil::unbindAndNodeKeyFlag(vSubgTable[currRowIdx * SUBG_TABLE_SIZE + (columnPtr++)], \n            //                                &lit0, &lit1, &fComp);\n            //         fanin0Id = dUtils::AigNodeID(lit0);\n            //         fanin1Id = dUtils::AigNodeID(lit1);\n            //         printf(\"%s%d,%s%d \", dUtils::AigNodeIsComplement(lit0) ? \"!\" : \"\", fanin0Id, dUtils::AigNodeIsComplement(lit1) ? \"!\" : \"\", fanin1Id);\n            //     }\n            //     printf(\"; mapId: \");\n            //     for (int k = 0; k < nCutSize; k++)\n            //         printf(\"%d \", mapId[k]);\n            //     printf(\"| \");\n            //     for (int k = nCutSize; k < nCutSize + nSubgSize; k++)\n            //         printf(\"%d \", mapId[k]);\n            //     printf(\"\\n\");\n            //     assert(0);\n            // }\n            // assert(mapId[nCutSize + nSubgSize - 1] == rootId);\n            // printf(\"   found whole new subgraph has nodes in the original graph, original root = %d, chioce root = %d\\n\", rootId, mapId[nCutSize + nSubgSize - 1]);\n            \n            // FIXME if its not the same as root, we found a real pair of choice nodes between two old nodes\n            if (mapId[nCutSize + nSubgSize - 1] != rootId) {\n                assert(mapId[nCutSize + nSubgSize - 1] < nObjs);\n                vChoicesLit[idx] = dUtils::AigNodeLitCond(mapId[nCutSize + nSubgSize - 1], fComp);\n            }\n            continue;\n        }\n\n        __threadfence();\n\n        // printf(\"** begin insertion of root id %d, subg \", rootId);\n        // currRowIdx = idx, columnPtr = 0;\n        // for (int k = nCutSize; k < nCutSize + nSubgSize; k++) {\n        //     if (columnPtr == SUBG_TABLE_SIZE) {\n        //         columnPtr = 0;\n        //         currRowIdx = vSubgLinks[currRowIdx];\n        //         assert(currRowIdx > 0);\n        //     }\n        //     subgUtil::unbindAndNodeKeyFlag(vSubgTable[currRowIdx * SUBG_TABLE_SIZE + (columnPtr++)], \n        //                             &lit0, &lit1, &fComp);\n        //     fanin0Id = dUtils::AigNodeID(lit0);\n        //     fanin1Id = dUtils::AigNodeID(lit1);\n        //     printf(\"%s%d,%s%d \", dUtils::AigNodeIsComplement(lit0) ? \"!\" : \"\", fanin0Id, dUtils::AigNodeIsComplement(lit1) ? \"!\" : \"\", fanin1Id);\n        // }\n        // printf(\"; mapId: \");\n        // for (int k = 0; k < nCutSize; k++)\n        //     printf(\"%d \", mapId[k]);\n        // printf(\"| \");\n        // for (int k = nCutSize; k < nCutSize + nSubgSize; k++)\n        //     printf(\"%d \", mapId[k]);\n        // printf(\"\\n\");\n\n        // add new nodes without corresponding old nodes to the hash table\n        currRowIdx = idx, columnPtr = 0;\n        for (int i = 0; i < nSubgSize; i++) {\n            // change row\n            if (columnPtr == SUBG_TABLE_SIZE) {\n                columnPtr = 0;\n                currRowIdx = vSubgLinks[currRowIdx];\n                assert(currRowIdx > 0);\n            }\n\n            subgUtil::unbindAndNodeKeyFlag(vSubgTable[currRowIdx * SUBG_TABLE_SIZE + (columnPtr++)], \n                                           &lit0, &lit1, &fComp);\n            fanin0Id = dUtils::AigNodeID(lit0);\n            fanin1Id = dUtils::AigNodeID(lit1);\n\n            subgId = i + nCutSize;\n            if (mapId[subgId] != -1)\n                continue;\n            \n            // if (mapId[fanin0Id] == -1 || mapId[fanin1Id] == -1) {\n            //     printf(\" *** error (curr i=%d): root id %d, subg \", i, rootId);\n            //     currRowIdx = idx, columnPtr = 0;\n            //     for (int k = nCutSize; k < nCutSize + nSubgSize; k++) {\n            //         if (columnPtr == SUBG_TABLE_SIZE) {\n            //             columnPtr = 0;\n            //             currRowIdx = vSubgLinks[currRowIdx];\n            //             assert(currRowIdx > 0);\n            //         }\n            //         subgUtil::unbindAndNodeKeyFlag(vSubgTable[currRowIdx * SUBG_TABLE_SIZE + (columnPtr++)], \n            //                                &lit0, &lit1, &fComp);\n            //         fanin0Id = dUtils::AigNodeID(lit0);\n            //         fanin1Id = dUtils::AigNodeID(lit1);\n            //         printf(\"%s%d,%s%d \", dUtils::AigNodeIsComplement(lit0) ? \"!\" : \"\", fanin0Id, dUtils::AigNodeIsComplement(lit1) ? \"!\" : \"\", fanin1Id);\n            //     }\n            //     printf(\"; mapId: \");\n            //     for (int k = 0; k < nCutSize; k++)\n            //         printf(\"%d \", mapId[k]);\n            //     printf(\"| \");\n            //     for (int k = nCutSize; k < nCutSize + nSubgSize; k++)\n            //         printf(\"%d \", mapId[k]);\n            //     printf(\"\\n\");\n            //     assert(0);\n            // }\n\n            assert(mapId[fanin0Id] != -1 && mapId[fanin1Id] != -1);\n\n            newId = atomicAdd(pnNewObjs, 1);\n            lit2 = dUtils::AigNodeLitCond(mapId[fanin0Id], dUtils::AigNodeIsComplement(lit0));\n            lit3 = dUtils::AigNodeLitCond(mapId[fanin1Id], dUtils::AigNodeIsComplement(lit1));\n            if (lit2 > lit3)\n                temp = lit2, lit2 = lit3, lit3 = temp;\n            key = formAndNodeKey(lit2, lit3);\n            uint32 hashRet = insert_single_no_update_volatile<uint64, uint32>(htKeys, htValues, key, (uint32)newId, htCapacity);\n            // uint32 hashRet = insert_single_no_update<uint64, uint32>(htKeys, htValues, key, (uint32)newId, htCapacity);\n\n            __threadfence(); // ensure strong memory order\n            \n            // immediate retrieval to check whether a same node is also created by other threads\n            // NOTE the assignments of key and value are not in a single atomic transaction,\n            // so it could happen that when another thread has the same key, \n            // the key is already assigned in hashtable but the value is not, so the retrieved value is still empty_value\n            do {\n                realNewId = retrieve_single_volatile<uint64, uint32>(htKeys, htValues, key, htCapacity);\n                // realNewId = retrieve_single<uint64, uint32>(htKeys, htValues, key, htCapacity);\n                __threadfence();\n            } while (realNewId == (HASHTABLE_EMPTY_VALUE<uint64, uint32>));\n            // realNewId = retrieve_single<uint64, uint32>(htKeys, htValues, key, htCapacity);\n            // if ((int)realNewId < nObjs) {\n            //     printf(\" *** error (hash key=%llu, hash ret=%u, curr i=%d): root id %d, new id %u, real new id %u, subg \", key, hashRet, i, rootId, (uint32)newId, realNewId);\n            //     currRowIdx = idx, columnPtr = 0;\n            //     for (int k = nCutSize; k < nCutSize + nSubgSize; k++) {\n            //         if (columnPtr == SUBG_TABLE_SIZE) {\n            //             columnPtr = 0;\n            //             currRowIdx = vSubgLinks[currRowIdx];\n            //             assert(currRowIdx > 0);\n            //         }\n            //         subgUtil::unbindAndNodeKeyFlag(vSubgTable[currRowIdx * SUBG_TABLE_SIZE + (columnPtr++)], \n            //                                &lit0, &lit1, &fComp);\n            //         fanin0Id = dUtils::AigNodeID(lit0);\n            //         fanin1Id = dUtils::AigNodeID(lit1);\n            //         printf(\"%s%d,%s%d \", dUtils::AigNodeIsComplement(lit0) ? \"!\" : \"\", fanin0Id, dUtils::AigNodeIsComplement(lit1) ? \"!\" : \"\", fanin1Id);\n            //     }\n            //     printf(\"; mapId: \");\n            //     for (int k = 0; k < nCutSize; k++)\n            //         printf(\"%d \", mapId[k]);\n            //     printf(\"| \");\n            //     for (int k = nCutSize; k < nCutSize + nSubgSize; k++)\n            //         printf(\"%d \", mapId[k]);\n            //     printf(\"\\n\");\n            //     assert(0);\n            // }\n            \n            assert((int)realNewId >= nObjs);\n            assert(realNewId != (HASHTABLE_EMPTY_VALUE<uint64, uint32>));\n\n            // update id mapping since this new node is now in the hash table\n            mapId[subgId] = (int)realNewId;\n            \n            // printf(\"  Attempted newId %u, realNewId %u %s; mapId: \", newId, realNewId, newId == realNewId ? \"(added)\" : \"       \");\n            // for (int k = 0; k < nCutSize; k++)\n            //     printf(\"%d \", mapId[k]);\n            // printf(\"| \");\n            // for (int k = nCutSize; k <= nCutSize + i; k++)\n            //     printf(\"%d \", mapId[k]);\n            // printf(\"\\n\");\n        }\n        // save choice node lit of the root\n        assert(mapId[nCutSize + nSubgSize - 1] == (int)realNewId);\n        vChoicesLit[idx] = dUtils::AigNodeLitCond((int)realNewId, fComp);\n        // printf(\"  Choice Lit: %d\\n\", vChoicesLit[idx]);\n    }\n}",
            "#define uint64 uint64_hack_\n\n\n__device__ __forceinline__ void unbindAndNodeKeys(const uint64 key, uint32 * lit1, uint32 * lit2) {\n    *lit2 = (uint32)(key & 0xffffffffUL);\n    *lit1 = (uint32)(key >> 32);\n}\n\n__global__ void unbindAllKeys(const uint64 * vReconstructedKeys, const uint32 * vReconstructedIds,\n                              int * vFanin0New, int * vFanin1New, int nEntries, int nObjs, int nBufferLen) {\n    // NOTE the four pointers should point to the end of old nObjs, i.e., the begin of new objs\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    // to ensure that the pointer is correctly pointed to the begin of new objs\n    if (idx == 0) {\n        assert(*vReconstructedIds >= nObjs);\n        assert(*(vReconstructedIds - 1) < nObjs);\n    }\n    if (idx < nEntries) {\n        uint64 key = vReconstructedKeys[idx];\n        uint32 lit0, lit1, nodeId;\n        unbindAndNodeKeys(key, &lit0, &lit1);\n        nodeId = vReconstructedIds[idx];\n        assert(nodeId < nBufferLen);\n\n        // for a value that does not correspond to a valid nodeId, its fanin = -1\n        vFanin0New[nodeId - nObjs] = (int)lit0;\n        vFanin1New[nodeId - nObjs] = (int)lit1;\n    }\n}"
        ],
        "/Users/gbolet/hecbench-roofline/src/logic-rewrite-cuda/tables.cuh": [
            "#define T ((int)32)\n\n\n__global__ void gatherTableToConsecutive(const T * vTable, const int * vTableLens, \n                                         const int * vIndices, const int * vRanges, \n                                         T * vArray, int nIndices) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < nIndices) {\n        int nodeId = vIndices[idx];\n        const T * vTableEntry = &vTable[nodeId * TABLE_NUM_COLS];\n        int startIdx = (idx == 0 ? 0 : vRanges[idx - 1]);\n        int length = vTableLens[nodeId];\n\n        assert(length == vRanges[idx] - startIdx);\n\n        for (int i = 0; i < length; i++) {\n            vArray[startIdx + i] = vTableEntry[i];\n        }\n    }\n}"
        ],
        "/Users/gbolet/hecbench-roofline/src/logic-rewrite-cuda/hash_table.h": [
            "__device__ __forceinline__ uint32 hash(int x) {\n    return hash((uint32)x);\n}\n\n__global__ void insert_batch_no_update_kernel(KeyT * ht_keys, ValueT * ht_values, \n                                              const KeyT * keys, const ValueT * values, \n                                              const int len, const int capacity) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < len) {\n        KeyT key = keys[idx];\n        ValueT value = values[idx];\n        KeyT loc = hash(key);\n        loc = loc % capacity;\n\n        while (true) {\n            KeyT prev = atomicCAS(&ht_keys[loc], HASHTABLE_EMPTY_KEY<KeyT, ValueT>, key);\n            if (prev == HASHTABLE_EMPTY_KEY<KeyT, ValueT>) {\n                // found a empty entry\n                ht_values[loc] = value;\n                return;\n            } else if (prev == key) {\n                // already have key inserted, no update\n                return;\n            }\n\n            loc = (loc + 1) % capacity;\n        }\n    }\n}",
            "__device__ __forceinline__ uint32 hash(int x) {\n    return hash((uint32)x);\n}\n\n__global__ void insert_batch_no_update_masked_kernel(KeyT * ht_keys, ValueT * ht_values, \n                                                     const KeyT * keys, const ValueT * values, const int * mask,\n                                                     const int len, const int capacity) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < len && mask[idx] == 1) {\n        KeyT key = keys[idx];\n        ValueT value = values[idx];\n        KeyT loc = hash(key);\n        loc = loc % capacity;\n\n        while (true) {\n            KeyT prev = atomicCAS(&ht_keys[loc], HASHTABLE_EMPTY_KEY<KeyT, ValueT>, key);\n            if (prev == HASHTABLE_EMPTY_KEY<KeyT, ValueT>) {\n                // found a empty entry\n                ht_values[loc] = value;\n                return;\n            } else if (prev == key) {\n                // already have key inserted, no update\n                return;\n            }\n\n            loc = (loc + 1) % capacity;\n        }\n    }\n}",
            "__device__ __forceinline__ uint32 hash(int x) {\n    return hash((uint32)x);\n}\n\n__global__ void retrieve_batch_kernel(KeyT * ht_keys, ValueT * ht_values, \n                                      const KeyT * keys, ValueT * values,\n                                      const int len, const int capacity) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < len) {\n        KeyT key = keys[idx];\n        KeyT loc = hash(key);\n        loc = loc % capacity;\n\n        while (true) {\n            if (ht_keys[loc] == key) {\n                values[idx] = ht_values[loc];\n                return;\n            } else if (ht_keys[loc] == HASHTABLE_EMPTY_KEY<KeyT, ValueT>) {\n                values[idx] = HASHTABLE_EMPTY_VALUE<KeyT, ValueT>;\n                return;\n            }\n\n            loc = (loc + 1) % capacity;\n        }\n    }\n}",
            "__device__ __forceinline__ uint32 hash(int x) {\n    return hash((uint32)x);\n}\n\n__global__ void retrieve_batch_masked_kernel(KeyT * ht_keys, ValueT * ht_values, \n                                             const KeyT * keys, ValueT * values, const int * mask,\n                                             const int len, const int capacity) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < len  && mask[idx] == 1) {\n        KeyT key = keys[idx];\n        KeyT loc = hash(key);\n        loc = loc % capacity;\n\n        while (true) {\n            if (ht_keys[loc] == key) {\n                values[idx] = ht_values[loc];\n                return;\n            } else if (ht_keys[loc] == HASHTABLE_EMPTY_KEY<KeyT, ValueT>) {\n                values[idx] = HASHTABLE_EMPTY_VALUE<KeyT, ValueT>;\n                return;\n            }\n\n            loc = (loc + 1) % capacity;\n        }\n    }\n}",
            "__global__ void getEntryIndicator(int * indicator, KeyT * ht_keys, const int capacity) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < capacity) {\n        indicator[idx] = (ht_keys[idx] != HASHTABLE_EMPTY_KEY<KeyT, ValueT>);\n    }\n}",
            "__global__ void gatherKeyValues(const int * scanned_indicator, const KeyT * ht_keys, const ValueT * ht_values, \n                                KeyT * keys, ValueT * values, const int capacity) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx == 0) {\n        if (scanned_indicator[0] == 1) {\n            keys[0] = ht_keys[0];\n            values[0] = ht_values[0];\n        }\n    } else if (idx < capacity) {\n        if (scanned_indicator[idx] > scanned_indicator[idx - 1]) {\n            int dst_idx = scanned_indicator[idx] - 1;\n            keys[dst_idx] = ht_keys[idx];\n            values[dst_idx] = ht_values[idx];\n        }\n    }\n}"
        ],
        "/Users/gbolet/hecbench-roofline/src/logic-rewrite-cuda/rewrite.cu": [
            "__device__ int CutFindValue(Cut *cut, int *nRef) {\n    int value = 0, nOnes = 0;\n    for(int i = 0; i < cut->nLeaves; i++) {\n        value += nRef[cut->leaves[i]];\n        nOnes += (nRef[cut->leaves[i]] == 1);\n    }\n    if(cut->nLeaves < 2) return 1001;\n    if(value > 1000)\n        value = 1000;\n    if(nOnes > 3)\n        value = 5 - nOnes;\n    return value;\n}\n\n__global__ void Inputs(int *nRef, Cut *cuts, int n) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x + 1;\n    if(idx <= n) {\n        Cut* cut = cuts + ID(idx, 0);\n        cut->used = 1;\n        cut->sign = 1 << (idx & 31);\n        cut->truthtable = 0xAAAA;\n        cut->nLeaves = 1;\n        cut->leaves[0] = idx;\n        cut->value = CutFindValue(cut, nRef);\n    }\n}",
            "__device__ int CutDominance(Cut a, Cut b) {\n    for(int i = 0; i < a.nLeaves; i++) {\n        int ok = 0;\n        for(int j = 0; j < b.nLeaves; j++)\n            if(b.leaves[j] == a.leaves[i]) ok = 1;\n        if(!ok) return 0;\n    }\n    return 1;\n}\n\n__device__ int CutTruthSwapAdjacentVars(int uTruth, int iVar) {\n    if ( iVar == 0 )\n        return (uTruth & 0x99999999) | ((uTruth & 0x22222222) << 1) | ((uTruth & 0x44444444) >> 1);\n    if ( iVar == 1 )\n        return (uTruth & 0xC3C3C3C3) | ((uTruth & 0x0C0C0C0C) << 2) | ((uTruth & 0x30303030) >> 2);\n    if ( iVar == 2 )\n        return (uTruth & 0xF00FF00F) | ((uTruth & 0x00F000F0) << 4) | ((uTruth & 0x0F000F00) >> 4);\n    return 0;\n}\n\n__device__ int CutTruthPhase(Cut x, Cut cut) {\n    int phase = 0;\n    for(int i = 0; i < cut.nLeaves; i++)\n        for(int j = 0; j < x.nLeaves; j++)\n            if(x.leaves[j] == cut.leaves[i]) phase |= 1 << i;\n    return phase;\n}\n\n__device__ int CutTruthStretch(int truthtable, int nVar, int phase) {\n    int Var = nVar - 1;\n    for(int i = 3; i >= 0; i--) if(phase >> i & 1) {\n        for(int k = Var; k < i; k++)\n            truthtable = CutTruthSwapAdjacentVars(truthtable, k);\n        Var--;\n    }\n    return truthtable;\n}\n\n__device__ int MergeCutOrdered(Cut a, Cut b, Cut* cut) {\n    int c = 0, i = 0, k = 0;\n    for (c = 0; c < 4; c++)\n    {\n        if (k == b.nLeaves)\n        {\n            if (i == a.nLeaves)\n            {\n                cut->nLeaves = c;\n                return 1;\n            }\n            cut->leaves[c] = a.leaves[i++];\n            continue;\n        }\n        if (i == a.nLeaves)\n        {\n            if (k == b.nLeaves)\n            {\n                cut->nLeaves = c;\n                return 1;\n            }\n            cut->leaves[c] = b.leaves[k++];\n            continue;\n        }\n        if (a.leaves[i] < b.leaves[k])\n        {\n            cut->leaves[c] = a.leaves[i++];\n            continue;\n        }\n        if (a.leaves[i] > b.leaves[k])\n        {\n            cut->leaves[c] = b.leaves[k++];\n            continue;\n        }\n        cut->leaves[c] = a.leaves[i++];\n        k++;\n    }\n    if (i < a.nLeaves || k < b.nLeaves)\n        return 0;\n    cut->nLeaves = c;\n    return 1;\n}\n\n__device__ int CutTruthShrink(int uTruth, int nVars, int Phase) {\n    int i, k, Var = 0;\n    for ( i = 0; i < 4; i++ )\n        if ( Phase & (1 << i) )\n        {\n            for ( k = i-1; k >= Var; k-- )\n                uTruth = CutTruthSwapAdjacentVars( uTruth, k );\n            Var++;\n        }\n    return uTruth;\n}\n\n__device__ int CountOnes(unsigned uWord) {\n    uWord = (uWord & 0x55555555) + ((uWord>>1) & 0x55555555);\n    uWord = (uWord & 0x33333333) + ((uWord>>2) & 0x33333333);\n    uWord = (uWord & 0x0F0F0F0F) + ((uWord>>4) & 0x0F0F0F0F);\n    uWord = (uWord & 0x00FF00FF) + ((uWord>>8) & 0x00FF00FF);\n    return  (uWord & 0x0000FFFF) + (uWord>>16);\n}\n\n__device__ int CutFilter(Cut *cut, int id) {\n    for(int i = 0; i < CUT_SET_SIZE; i++) if(i != id && cut[i].used) {\n        if(cut[i].nLeaves > cut[id].nLeaves) {\n            if((cut[i].sign & cut[id].sign) != cut[id].sign) continue;\n            if(CutDominance(cut[id], cut[i]))\n                cut[i].used = 0;\n        } else {\n            if((cut[i].sign & cut[id].sign) != cut[i].sign) continue;\n            if(CutDominance(cut[i], cut[id])) {\n                cut[id].used = 0;\n                return 1;\n            }\n        }\n    }\n    return 0;\n}\n\n__device__ int CutFindValue(Cut *cut, int *nRef) {\n    int value = 0, nOnes = 0;\n    for(int i = 0; i < cut->nLeaves; i++) {\n        value += nRef[cut->leaves[i]];\n        nOnes += (nRef[cut->leaves[i]] == 1);\n    }\n    if(cut->nLeaves < 2) return 1001;\n    if(value > 1000)\n        value = 1000;\n    if(nOnes > 3)\n        value = 5 - nOnes;\n    return value;\n}\n\n__device__ int CutTruthtable(Cut cut, Cut a, Cut b, int aComplement, int bComplement) {\n    int tt0 = aComplement ? ~a.truthtable : a.truthtable;\n    int tt1 = bComplement ? ~b.truthtable : b.truthtable;\n    tt0 = CutTruthStretch(tt0, a.nLeaves, CutTruthPhase(a, cut));\n    tt1 = CutTruthStretch(tt1, b.nLeaves, CutTruthPhase(b, cut));\n    return tt0 & tt1;\n}\n\n__device__ int FindCut(int idx, Cut *cuts) {\n    for(int i = 0; i < CUT_SET_SIZE; i++)\n        if(cuts[ID(idx, i)].used == 0) return i;\n    int ans = -1;\n    for(int i = 0; i < CUT_SET_SIZE; i++)\n        if(cuts[ID(idx, i)].nLeaves > 2) {\n            if(ans == -1 || cuts[ID(idx, i)].value < cuts[ID(idx, ans)].value)\n                ans = i;\n        }\n    if(ans == -1) {\n        for(int i = 0; i < CUT_SET_SIZE; i++)\n            if(cuts[ID(idx, i)].nLeaves == 2) {\n                if(ans == -1 || cuts[ID(idx, i)].value < cuts[ID(idx, ans)].value)\n                    ans = i;\n            }\n    }\n    if(ans == -1) {\n        for(int i = 0; i < CUT_SET_SIZE; i++)\n            if(cuts[ID(idx, i)].nLeaves < 2) {\n                if(ans == -1 || cuts[ID(idx, i)].value < cuts[ID(idx, ans)].value)\n                    ans = i;\n            }\n    }\n    cuts[ID(idx, ans)].used = 0;\n    return ans;\n}\n\n__device__ int MergeCut(Cut a, Cut b, Cut *cut) {\n    if(a.nLeaves >= b.nLeaves) {\n        if(MergeCutOrdered(a, b, cut) == 0) return 0;\n    } else {\n        if(MergeCutOrdered(b, a, cut) == 0) return 0;\n    }\n    cut->sign = a.sign | b.sign;\n    cut->used = 1;\n    return 1;\n}\n\n__device__ int MinimizeCutSupport(Cut* cut) {\n    int masks[4][2] = {\n        { 0x5555, 0xAAAA },\n        { 0x3333, 0xCCCC },\n        { 0x0F0F, 0xF0F0 },\n        { 0x00FF, 0xFF00 }\n    };\n    int phase = 0, truth = cut->truthtable & 0xFFFF, nLeaves = cut->nLeaves;\n    for(int i = 0; i < cut->nLeaves; i++)\n        if((truth & masks[i][0]) == ((truth & masks[i][1]) >> (1 << i)))\n            nLeaves--;\n        else\n            phase |= 1 << i;\n    if(nLeaves == cut->nLeaves) return 0;\n    truth = CutTruthShrink(truth, cut->nLeaves, phase);\n    cut->truthtable = truth & 0xFFFF;\n    cut->sign = 0;\n    for(int i = 0, k = 0; i < cut->nLeaves; i++) if(phase >> i & 1) {\n        cut->leaves[k++] = cut->leaves[i];\n        cut->sign |= 1 << (31 & cut->leaves[i]);\n    }\n    cut->nLeaves = nLeaves;\n    return 1;\n}\n\n__global__ void CutEnumerate(int *fanin0, int *fanin1, int* isC0, int *isC1, int *nRef, Cut *cuts, int delta, int n) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if(idx < n) {\n        idx += delta + 1;\n        Cut* cut = cuts + ID(idx, 0);\n        cut->used = 1;\n        cut->sign = 1 << (idx & 31);\n        cut->truthtable = 0xAAAA;\n        cut->nLeaves = 1;\n        cut->leaves[0] = idx;\n        cut->value = CutFindValue(cut, nRef);\n        int in0 = fanin0[idx], in1 = fanin1[idx];\n        for(int i = 0; i < CUT_SET_SIZE; i++) if(cuts[ID(in0, i)].used == 1)\n            for(int j = 0; j < CUT_SET_SIZE; j++) if(cuts[ID(in1, j)].used == 1) {\n                Cut a = cuts[ID(in0, i)], b = cuts[ID(in1, j)];\n                if(CountOnes(a.sign | b.sign) > 4) continue;\n                int cutId = FindCut(idx, cuts);\n                if(!MergeCut(a, b, cut + cutId)) continue;\n                if(CutFilter(cut, cutId)) continue;\n                cut[cutId].truthtable = 0xFFFF & CutTruthtable(cut[cutId], a, b, isC0[idx], isC1[idx]);\n                if(MinimizeCutSupport(cut + cutId))\n                    CutFilter(cut, cutId);\n                cut[cutId].value = CutFindValue(cut + cutId, nRef);\n                if(cut[cutId].nLeaves < 2) return;\n            }\n    }\n}",
            "__device__ int CalcMFFC(int cur, Cut* cut, int *fanin0, int *fanin1, int *tableSize, int *tableId, int *tableNum, int *nRef, int root) {\n    int ans = 1;\n    if(Decrease(fanin0[cur], tableSize, tableId, tableNum, nRef, cut) == 0)\n        ans += CalcMFFC(fanin0[cur], cut, fanin0, fanin1, tableSize, tableId, tableNum, nRef, root);\n    if(Decrease(fanin1[cur], tableSize, tableId, tableNum, nRef, cut) == 0)\n        ans += CalcMFFC(fanin1[cur], cut, fanin0, fanin1, tableSize, tableId, tableNum, nRef, root);\n    return ans;\n}\n\n__device__ int Decrease(int id, int *tableSize, int *tableId, int *tableNum, int *nRef, Cut *cut) {\n    for(int i = 0; i < cut->nLeaves; i++)\n        if(id == cut->leaves[i]) return 1;\n    for(int i = 0; i < tableSize[0]; i++)\n        if(tableId[i] == id) return --tableNum[i];\n    if(tableSize[0] == TABLE_SIZE) {\n        //printf(\"Warning in Decrease\\n\");\n        return 1;\n    }\n    tableId[tableSize[0]] = id;\n    tableNum[tableSize[0]] = nRef[id] - 1;\n    tableSize[0]++;\n    return nRef[id] - 1;\n}\n\n__device__ int Eval(int cur, int *match, int Class, Library *lib, int curId) {\n    if(match[cur] > -1) return 0;\n    if(match[cur] == curId) return 0;\n    match[cur] = curId;\n    return 1 + Eval(lib->fanin0[Class][cur - 4], match, Class, lib, curId) + Eval(lib->fanin1[Class][cur - 4], match, Class, lib, curId);\n}\n\n__device__ int IsDeleted(int id, int *tableSize, int *tableId, int *tableNum) {\n    for(int i = 0; i < tableSize[0]; i++)\n        if(tableId[i] == id) return tableNum[i] == 0;\n    return 0;\n}\n\n__device__ int TableLookup(int in0, int in1, int C0, int C1, TableNode* hashTable, int *fanin0, int *fanin1, int *isC0, int *isC1) {\n    unsigned long key = 0;\n    if(in0 > in1) {\n        int temp = in0;\n        in0 = in1;\n        in1 = temp;\n        temp = C0;\n        C0 = C1;\n        C1 = temp;\n    }\n    key ^= in0 * 7937;\n    key ^= in1 * 2971;\n    key ^= C0 * 911;\n    key ^= C1 * 353;\n    key %= P;\n    for(int cur = hashTable[key].next; cur != -1; cur = hashTable[cur].next) {\n        int val = hashTable[cur].val;\n        if(fanin0[val] == in0 && fanin1[val] == in1 && isC0[val] == C0 && isC1[val] == C1)\n            return val;\n    }\n    return -1;\n}\n\n__global__ void EvaluateNode(int sz, int *bestout, int *fanin0, int *fanin1, int *isC0, int *isC1, int *nodeLevels, Cut *cuts, Cut* selectedCuts, int *nRef, \n                             Library *lib, TableNode* hashTable, int fUseZeros) {\n    if(blockIdx.x * blockDim.x + threadIdx.x >= sz) return;\n    int id = 1 + blockIdx.x * blockDim.x + threadIdx.x, reduction = -1, bestLevel = 99999999, bestCut = -1, bestOut;\n    int match[54], tableSize, tableId[TABLE_SIZE], tableNum[TABLE_SIZE];\n    int matchLevel[54];\n    for(int i = 0; i < CUT_SET_SIZE; i++) {\n        Cut *cut = cuts + ID(id, i);\n        if(cut->used == 0 || cut->nLeaves < 3) continue;\n        int nleaves = cut->nLeaves;\n        if(nleaves == 3)\n            cut->leaves[cut->nLeaves++] = 0;\n        tableSize = 0;\n        int saved = CalcMFFC(id, cut, fanin0, fanin1, &tableSize, tableId, tableNum, nRef, id);\n        int uPhase = lib->pPhases[cut->truthtable];\n        int Class = lib->pMap[cut->truthtable];\n        int *pPerm = lib->pPerms4[lib->pPerms[cut->truthtable]];\n        for(int j = 0; j < 54; j++)\n            match[j] = matchLevel[j] = -1;\n\t      uint64_t isC = 0;\n        for(int j = 0; j < 4; j++) {\n            match[j] = cut->leaves[pPerm[j]];\n            matchLevel[j] = nodeLevels[match[j]];\n            if(uPhase >> j & 1)\n\t\t            isC |= 1LL << j;\n        }\n        for(int j = 0; j < lib->nNodes[Class]; j++) {\n            int num = j + 4;\n            int in0 = lib->fanin0[Class][j], in1 = lib->fanin1[Class][j];\n            assert(matchLevel[in0] != -1 && matchLevel[in1] != -1);\n            matchLevel[num] = 1 + (matchLevel[in0] > matchLevel[in1] ? matchLevel[in0] : matchLevel[in1]);\n\n            if(match[in0] == -1 || match[in1] == -1 || match[in0] == id || match[in1] == id) continue;\n            int nodeId = TableLookup(match[in0], match[in1], (isC >> in0 & 1) ^ lib->isC0[Class][j], (isC >> in1 & 1) ^ lib->isC1[Class][j], hashTable, fanin0, fanin1, isC0, isC1);\n            if(nodeId != -1 && !IsDeleted(nodeId, &tableSize, tableId, tableNum)) {\n                match[num] = nodeId;\n                matchLevel[num] = nodeLevels[nodeId];\n            }\n        }\n        for(int out = 0; out < lib->nSubgr[Class]; out++) {\n            int rt = lib->pSubgr[Class][out];\n            if(match[rt] == id) continue;\n            int nodesAdded = Eval(rt, match, Class, lib, -out - 2);\n            int rtLevel = matchLevel[rt];\n            assert(rtLevel != -1);\n            // if(saved - nodesAdded > reduction || (fUseZeros && saved == nodesAdded && bestCut == -1)) {\n            // if(saved - nodesAdded > reduction || (fUseZeros && saved == nodesAdded)) {\n            //     reduction = saved - nodesAdded;\n            //     bestCut = i;\n            //     bestOut = out;\n            // }\n            if (saved - nodesAdded < 0 || (saved - nodesAdded == 0 && !fUseZeros))\n                continue;\n            if (saved - nodesAdded < reduction || (saved - nodesAdded == reduction && rtLevel >= bestLevel))\n                continue;\n            reduction = saved - nodesAdded;\n            bestLevel = rtLevel;\n            bestCut = i;\n            bestOut = out;\n        }\n        cut->nLeaves = nleaves;\n    }\n    if(bestCut != -1) {\n\t// atomicAdd(&GPUexpected, 1);\n        selectedCuts[id] = cuts[ID(id, bestCut)];\n        selectedCuts[id].used = 1;\n        bestout[id] = bestOut;\n    } else\n        selectedCuts[id].used = 0;\n}",
            "__global__ void BuildHashTable(TableNode *hashTable, int sz, int *fanin0, int *fanin1, int *isC0, int *isC1) {\n    int index = blockDim.x * blockIdx.x + threadIdx.x;\n    if(index < sz) {\n        int id = index + 1;\n        if(fanin0[id] == 0 && fanin1[id] == 0) return;\n        unsigned long key = 0;\n        key ^= fanin0[id] * 7937;\n        key ^= fanin1[id] * 2971;\n        key ^= isC0[id] * 911;\n        key ^= isC1[id] * 353;\n        key %= P;\n        hashTable[P + index].val = id;\n        while(true) {\n            int res = atomicCAS(&hashTable[key].next, -1, P + index);\n            if(res == -1)\n                break;\n            else\n                key = res;\n        }\n    }\n\n}",
            "__device__ void BuildSubgr(int cur, int Class, Library *lib, int *match, uint64_t isC, int *fanin0, int *fanin1, int *isC0, int *isC1, int *phase, TableNode* newTable, int sz) {\n    // only create the node that has no correspondence in original table, or not created by this thread (in this subgraph)\n    if (match[cur] != -1) return;\n    int in0 = lib->fanin0[Class][cur - 4];\n    int in1 = lib->fanin1[Class][cur - 4];\n    BuildSubgr(in0, Class, lib, match, isC, fanin0, fanin1, isC0, isC1, phase, newTable, sz);\n    BuildSubgr(in1, Class, lib, match, isC, fanin0, fanin1, isC0, isC1, phase, newTable, sz);\n\n    // Build the node\n    int node = atomicAdd(&N, 1);\n    // assert(node < (int) (sz * RATIO));\n    node += 1;\n    fanin0[node] = match[in0];\n    fanin1[node] = match[in1];\n    isC0[node] = lib->isC0[Class][cur - 4] ^ (isC >> in0 & 1);\n    isC1[node] = lib->isC1[Class][cur - 4] ^ (isC >> in1 & 1);\n    phase[node] = (phase[fanin0[node]] ^ isC0[node]) & (phase[fanin1[node]] ^ isC1[node]);\n\n    if (fanin0[node] > fanin1[node]) {\n        int temp = fanin0[node];\n        fanin0[node] = fanin1[node];\n        fanin1[node] = temp;\n        temp = isC0[node];\n        isC0[node] = isC1[node];\n        isC1[node] = temp;\n    }\n\n    // newTable is used for ensuring unique insertion\n    TableInsert(fanin0[node], fanin1[node], isC0[node], isC1[node], newTable, node, sz);\n    int actual_node = TableLookup(fanin0[node], fanin1[node], isC0[node], isC1[node], newTable, fanin0, fanin1, isC0, isC1);\n    if (actual_node != node) { // The node is already created (by other threads, in other subgraphs)\n        fanin0[node] = fanin1[node] = -1;\n        match[cur] = actual_node;\n    } else {\n        match[cur] = node;\n    }\n}\n\n__device__ void TableInsert(int in0, int in1, int C0, int C1, TableNode* hashTable, int idx, int offset=0) {\n    int index = idx - offset - 1;\n    unsigned long key = 0;\n    key ^= in0 * 7937;\n    key ^= in1 * 2971;\n    key ^= C0 * 911;\n    key ^= C1 * 353;\n    key %= P;\n    hashTable[P + index].val = idx;\n    while(true) {\n        int res = atomicCAS(&hashTable[key].next, -1, P + index);\n        if(res == -1)\n            break;\n        else\n            key = res;\n    }\n}\n\n__device__ int TableLookup(int in0, int in1, int C0, int C1, TableNode* hashTable, int *fanin0, int *fanin1, int *isC0, int *isC1) {\n    unsigned long key = 0;\n    if(in0 > in1) {\n        int temp = in0;\n        in0 = in1;\n        in1 = temp;\n        temp = C0;\n        C0 = C1;\n        C1 = temp;\n    }\n    key ^= in0 * 7937;\n    key ^= in1 * 2971;\n    key ^= C0 * 911;\n    key ^= C1 * 353;\n    key %= P;\n    for(int cur = hashTable[key].next; cur != -1; cur = hashTable[cur].next) {\n        int val = hashTable[cur].val;\n        if(fanin0[val] == in0 && fanin1[val] == in1 && isC0[val] == C0 && isC1[val] == C1)\n            return val;\n    }\n    return -1;\n}\n\n__global__ void ReplaceSubgr(int sz, int *bestout, int *fanin0, int *fanin1, int *isC0, int *isC1, Cut *selectedCuts, Library *lib, TableNode* hashTable, TableNode* newTable, int *phase, int *replace) {\n    if(blockIdx.x * blockDim.x + threadIdx.x >= sz) return;\n    //if (N >= (int) sz * RATIO) printf(\"Warning: too many new nodes to handle...\\n\");\n    int id = 1 + blockIdx.x * blockDim.x + threadIdx.x;\n    Cut cut = selectedCuts[id];\n    if (cut.used == 0 || cut.nLeaves < 3) return;\n\t//atomicAdd(&CNT, 1);\n    if(cut.nLeaves == 3)\n        cut.leaves[cut.nLeaves++] = 0;\n\n    int uPhase = lib->pPhases[cut.truthtable];\n    int Class = lib->pMap[cut.truthtable];\n    int *pPerm = lib->pPerms4[lib->pPerms[cut.truthtable]];\n    // match maps the new node id into the id in the original hashtable;\n    // if there is no corresponding node in the original table, then match = -1\n    int match[54];\n\t  uint64_t isC = 0;\n    for(int j = 0; j < 54; j++) match[j] = -1;\n    for(int j = 0; j < 4; j++) {\n        match[j] = cut.leaves[pPerm[j]];\n        if(uPhase >> j & 1)\n        \t  isC |= 1LL << j;\n    }\n    int rt = lib->pSubgr[Class][bestout[id]];\n\t  uint64_t used = 1LL << rt;\n    for(int j = rt; j >= 4; j--) if(used >> j & 1) {\n\t      used |= 1LL << lib->fanin0[Class][j - 4];\n\t      used |= 1LL << lib->fanin1[Class][j - 4];\n    }\n    for(int j = 0; j < lib->nNodes[Class]; j++) if(used >> (j + 4) & 1) {\n      \tint num = j + 4;\n      \tint in0 = lib->fanin0[Class][j], in1 = lib->fanin1[Class][j];\n      \tif(match[in0] == -1 || match[in1] == -1) continue;\n      \tint nodeId = TableLookup(match[in0], match[in1], (isC >> in0 & 1) ^ lib->isC0[Class][j], (isC >> in1 & 1) ^ lib->isC1[Class][j], hashTable, fanin0, fanin1, isC0, isC1);\n      \tif(nodeId != -1) match[num] = nodeId;\n    }\n\n    // the root of the new graph has correspondence, so the whole new graph exists in the original table\n    // if (match[rt] == id) {atomicAdd(&replaceHasFullCorrsp, 1);  return;}\n    if (match[rt] == id) return;\n    BuildSubgr(rt, Class, lib, match, isC, fanin0, fanin1, isC0, isC1, phase, newTable, sz);\n    replace[id] = (match[rt] * 2) + (phase[match[rt]] ^ phase[id]); // the corresponding new node\n    // if (id > 100300 && id < 100400)\n    //     printf(\"replace[%d] = %d\\n\", id, replace[id]);\n}",
            "__global__ void DetachAndAttach(int sz, int *fanin0, int *fanin1, int *replace) {\n    if(blockIdx.x * blockDim.x + threadIdx.x >= sz) return;\n    int id = 1 + blockIdx.x * blockDim.x + threadIdx.x;\n    if (replace[id] == -1) return;\n    fanin0[id] = 1;\n    fanin1[id] = replace[id];\n}",
            "__global__ void printCuts(int id, Cut *cuts) {\n    for(int i = 0; i < CUT_SET_SIZE; i++) {\n        Cut *cut = cuts + ID(id, i);\n        printf(\"cut#%d details: truthtable %d, used%d, nLeaves=%d, leaves=%d %d %d %d\\n\", i, cut->truthtable, cut->used, cut->nLeaves, cut->leaves[0], cut->leaves[1], cut->leaves[2], cut->leaves[3]);\n    }\n}",
            "__global__ void Convert(int *fanin, int *isC, int n) {\n    int id = blockIdx.x * blockDim.x + threadIdx.x;\n    if(id < n) {\n        isC[id] = fanin[id] & 1;\n        fanin[id] >>= 1;\n    }\n}",
            "__global__ void Revert(int *fanin, int *isC, int n) {\n    int id = blockIdx.x * blockDim.x + threadIdx.x;\n    if(id <= n) {\n        fanin[id] <<= 1;\n        fanin[id] += isC[id];\n    }\n}",
            "__global__ void print(int n, Cut *selected, int *bestOut) {\n    for(int i = 1; i <= n; i++) if(selected[i].used == 1)\n        printf(\"Selected %d: %d %d\\n\", i, selected[i].truthtable, selected[i].sign);\n}",
            "__device__ int CalcMFFC(int cur, Cut* cut, int *fanin0, int *fanin1, int *tableSize, int *tableId, int *tableNum, int *nRef, int root) {\n    int ans = 1;\n    if(Decrease(fanin0[cur], tableSize, tableId, tableNum, nRef, cut) == 0)\n        ans += CalcMFFC(fanin0[cur], cut, fanin0, fanin1, tableSize, tableId, tableNum, nRef, root);\n    if(Decrease(fanin1[cur], tableSize, tableId, tableNum, nRef, cut) == 0)\n        ans += CalcMFFC(fanin1[cur], cut, fanin0, fanin1, tableSize, tableId, tableNum, nRef, root);\n    return ans;\n}\n\n__device__ int Decrease(int id, int *tableSize, int *tableId, int *tableNum, int *nRef, Cut *cut) {\n    for(int i = 0; i < cut->nLeaves; i++)\n        if(id == cut->leaves[i]) return 1;\n    for(int i = 0; i < tableSize[0]; i++)\n        if(tableId[i] == id) return --tableNum[i];\n    if(tableSize[0] == TABLE_SIZE) {\n        //printf(\"Warning in Decrease\\n\");\n        return 1;\n    }\n    tableId[tableSize[0]] = id;\n    tableNum[tableSize[0]] = nRef[id] - 1;\n    tableSize[0]++;\n    return nRef[id] - 1;\n}\n\n__device__ int Eval(int cur, int *match, int Class, Library *lib, int curId) {\n    if(match[cur] > -1) return 0;\n    if(match[cur] == curId) return 0;\n    match[cur] = curId;\n    return 1 + Eval(lib->fanin0[Class][cur - 4], match, Class, lib, curId) + Eval(lib->fanin1[Class][cur - 4], match, Class, lib, curId);\n}\n\n__device__ int IsDeleted(int id, int *tableSize, int *tableId, int *tableNum) {\n    for(int i = 0; i < tableSize[0]; i++)\n        if(tableId[i] == id) return tableNum[i] == 0;\n    return 0;\n}\n\n__device__ int TableLookup(int in0, int in1, int C0, int C1, TableNode* hashTable, int *fanin0, int *fanin1, int *isC0, int *isC1) {\n    unsigned long key = 0;\n    if(in0 > in1) {\n        int temp = in0;\n        in0 = in1;\n        in1 = temp;\n        temp = C0;\n        C0 = C1;\n        C1 = temp;\n    }\n    key ^= in0 * 7937;\n    key ^= in1 * 2971;\n    key ^= C0 * 911;\n    key ^= C1 * 353;\n    key %= P;\n    for(int cur = hashTable[key].next; cur != -1; cur = hashTable[cur].next) {\n        int val = hashTable[cur].val;\n        if(fanin0[val] == in0 && fanin1[val] == in1 && isC0[val] == C0 && isC1[val] == C1)\n            return val;\n    }\n    return -1;\n}\n\n__global__ void EvaluateNodeWave(int sz, int *bestout, int *fanin0, int *fanin1, int *isC0, int *isC1, int *nodeLevels, Cut *cuts, Cut* selectedCuts, int *nRef, \n                                 Library * lib, TableNode* hashTable, const int * vWaveMask, int fUseZeros) {\n    if(blockIdx.x * blockDim.x + threadIdx.x >= sz) return;\n    int id = 1 + blockIdx.x * blockDim.x + threadIdx.x;\n    if (!vWaveMask[id]) {\n        // node not in this wave\n        selectedCuts[id].used = 0;\n        return;\n    }\n\n    int reduction = -1, bestLevel = 99999999, bestCut = -1, bestOut;\n    int match[54], tableSize, tableId[TABLE_SIZE], tableNum[TABLE_SIZE];\n    int matchLevel[54];\n    for(int i = 0; i < CUT_SET_SIZE; i++) {\n        Cut *cut = cuts + ID(id, i);\n        if(cut->used == 0 || cut->nLeaves < 3) continue;\n        int nleaves = cut->nLeaves;\n        if(nleaves == 3)\n            cut->leaves[cut->nLeaves++] = 0;\n        tableSize = 0;\n        int saved = CalcMFFC(id, cut, fanin0, fanin1, &tableSize, tableId, tableNum, nRef, id);\n        int uPhase = lib->pPhases[cut->truthtable];\n        int Class = lib->pMap[cut->truthtable];\n        int *pPerm = lib->pPerms4[lib->pPerms[cut->truthtable]];\n        for(int j = 0; j < 54; j++)\n            match[j] = matchLevel[j] = -1;\n\t      uint64_t isC = 0;\n        for(int j = 0; j < 4; j++) {\n            match[j] = cut->leaves[pPerm[j]];\n            matchLevel[j] = nodeLevels[match[j]];\n            if(uPhase >> j & 1)\n\t\t            isC |= 1LL << j;\n        }\n        for(int j = 0; j < lib->nNodes[Class]; j++) {\n            int num = j + 4;\n            int in0 = lib->fanin0[Class][j], in1 = lib->fanin1[Class][j];\n            assert(matchLevel[in0] != -1 && matchLevel[in1] != -1);\n            matchLevel[num] = 1 + (matchLevel[in0] > matchLevel[in1] ? matchLevel[in0] : matchLevel[in1]);\n\n            if(match[in0] == -1 || match[in1] == -1 || match[in0] == id || match[in1] == id) continue;\n            int nodeId = TableLookup(match[in0], match[in1], (isC >> in0 & 1) ^ lib->isC0[Class][j], (isC >> in1 & 1) ^ lib->isC1[Class][j], hashTable, fanin0, fanin1, isC0, isC1);\n            if(nodeId != -1 && !IsDeleted(nodeId, &tableSize, tableId, tableNum)) {\n                match[num] = nodeId;\n                matchLevel[num] = nodeLevels[nodeId];\n            }\n        }\n        for(int out = 0; out < lib->nSubgr[Class]; out++) {\n            int rt = lib->pSubgr[Class][out];\n            if(match[rt] == id) continue;\n            int nodesAdded = Eval(rt, match, Class, lib, -out - 2);\n            int rtLevel = matchLevel[rt];\n            assert(rtLevel != -1);\n            // if(saved - nodesAdded > reduction || (fUseZeros && saved == nodesAdded && bestCut == -1)) {\n            // if(saved - nodesAdded > reduction || (fUseZeros && saved == nodesAdded)) {\n            //     reduction = saved - nodesAdded;\n            //     bestCut = i;\n            //     bestOut = out;\n            // }\n            if (saved - nodesAdded < 0 || (saved - nodesAdded == 0 && !fUseZeros))\n                continue;\n            if (saved - nodesAdded < reduction || (saved - nodesAdded == reduction && rtLevel >= bestLevel))\n                continue;\n            reduction = saved - nodesAdded;\n            bestLevel = rtLevel;\n            bestCut = i;\n            bestOut = out;\n        }\n        cut->nLeaves = nleaves;\n    }\n    if(bestCut != -1) {\n\tatomicAdd(&GPUexpected, 1);\n        selectedCuts[id] = cuts[ID(id, bestCut)];\n        selectedCuts[id].used = 1;\n        bestout[id] = bestOut;\n    } else\n        selectedCuts[id].used = 0;\n}"
        ],
        "/Users/gbolet/hecbench-roofline/src/logic-rewrite-cuda/strash.cu": [
            "#define uint64 uint64_hack_\n\n\n__device__ __forceinline__ uint32 hash(int x) {\n    return hash((uint32)x);\n}\n\n__device__ __forceinline__ uint64 formAndNodeKey(const int lit1, const int lit2) {\n    // make sure lit1 is smaller than lit2\n    uint32 uLit1 = (uint32)lit1;\n    uint32 uLit2 = (uint32)lit2;\n    uint64 key = ((uint64)uLit1) << 32 | uLit2;\n    return key;\n}\n\n__device__ __forceinline__ uint32 insert_single_no_update(KeyT * ht_keys, ValueT * ht_values, \n                                        const KeyT key, const ValueT value, const int capacity) {\n    KeyT loc = hash(key);\n    loc = loc % capacity;\n\n    while (true) {\n        KeyT prev = atomicCAS(&ht_keys[loc], HASHTABLE_EMPTY_KEY<KeyT, ValueT>, key);\n        if (prev == HASHTABLE_EMPTY_KEY<KeyT, ValueT>) {\n            // found a empty entry\n            ht_values[loc] = value;\n            return 2;\n        } else if (prev == key) {\n            // already have key inserted, no update\n            return ht_values[loc];\n        }\n\n        loc = (loc + 1) % capacity;\n    }\n}\n\n__global__ void Aig::buildHashTable(const int * pFanin0, const int * pFanin1, \n                                    uint64 * htKeys, uint32 * htValues, int htCapacity,\n                                    int nNodes, int nPIs) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    int lit0, lit1, temp;\n    if (idx < nNodes) {\n        uint32 id = idx + nPIs + 1;\n        lit0 = pFanin0[id], lit1 = pFanin1[id];\n        if (lit0 > lit1)\n            temp = lit0, lit0 = lit1, lit1 = temp;\n        uint64 key = formAndNodeKey(lit0, lit1);\n        insert_single_no_update<uint64, uint32>(htKeys, htValues, key, id, htCapacity);\n    }\n}",
            "__global__ void markReadyNodes(const int * pFanin0, const int * pFanin1,\n                               const int * vRemainNodes, const int * vOld2NewLit, int * vMarks, int nRemain) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    int nodeId;\n    int id0, id1;\n    if (idx < nRemain) {\n        nodeId = vRemainNodes[idx];\n        id0 = dUtils::AigNodeID(pFanin0[nodeId]), id1 = dUtils::AigNodeID(pFanin1[nodeId]);\n        if (vOld2NewLit[id0] != -1 && vOld2NewLit[id1] != -1) {\n            // both of its two fanins are already reconstructed\n            vMarks[idx] = 1;\n        }\n    }\n}",
            "#define uint64 uint64_hack_\n\n\n__device__ __forceinline__ uint32 hash(int x) {\n    return hash((uint32)x);\n}\n\n__device__ __forceinline__ uint32 checkTrivialAndCases(int lit1, int lit2) {\n    if (lit1 == lit2)\n        return (uint32)lit1;\n    if (lit1 == dUtils::AigNodeNot(lit2))\n        return (uint32)dUtils::AigConst0;\n    if (lit1 == dUtils::AigConst1)\n        return (uint32)lit2;\n    if (lit2 == dUtils::AigConst1)\n        return (uint32)lit1;\n    if (lit1 == dUtils::AigConst0 || lit2 == dUtils::AigConst0)\n        return (uint32)dUtils::AigConst0;\n    \n    // non-trivial\n    return HASHTABLE_EMPTY_VALUE<uint64, uint32>;\n}\n\n__device__ __forceinline__ uint64 formAndNodeKey(const int lit1, const int lit2) {\n    // make sure lit1 is smaller than lit2\n    uint32 uLit1 = (uint32)lit1;\n    uint32 uLit2 = (uint32)lit2;\n    uint64 key = ((uint64)uLit1) << 32 | uLit2;\n    return key;\n}\n\n__device__ __forceinline__ uint32 insert_single_no_update(KeyT * ht_keys, ValueT * ht_values, \n                                        const KeyT key, const ValueT value, const int capacity) {\n    KeyT loc = hash(key);\n    loc = loc % capacity;\n\n    while (true) {\n        KeyT prev = atomicCAS(&ht_keys[loc], HASHTABLE_EMPTY_KEY<KeyT, ValueT>, key);\n        if (prev == HASHTABLE_EMPTY_KEY<KeyT, ValueT>) {\n            // found a empty entry\n            ht_values[loc] = value;\n            return 2;\n        } else if (prev == key) {\n            // already have key inserted, no update\n            return ht_values[loc];\n        }\n\n        loc = (loc + 1) % capacity;\n    }\n}\n\n__global__ void insertLevelNodes(const int * vReadyNodes, const int * vOld2NewLit,\n                                 const int * pFanin0, const int * pFanin1,\n                                 uint64 * htKeys, uint32 * htValues, int htCapacity,\n                                 uint64 * vKeysBuffer, int idCounter, int nReady) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    int nodeId;\n    int lit0, lit1, id0, id1, temp;\n    uint32 temp0;\n    uint64 key;\n    if (idx < nReady) {\n        nodeId = vReadyNodes[idx];\n        \n        lit0 = pFanin0[nodeId], lit1 = pFanin1[nodeId]; // old lits\n        id0 = dUtils::AigNodeID(lit0), id1 = dUtils::AigNodeID(lit1); // old ids\n        // convert to new lit\n        lit0 = dUtils::AigNodeNotCond(vOld2NewLit[id0], dUtils::AigNodeIsComplement(lit0));\n        lit1 = dUtils::AigNodeNotCond(vOld2NewLit[id1], dUtils::AigNodeIsComplement(lit1));\n        if (lit0 > lit1)\n            temp = lit0, lit0 = lit1, lit1 = temp;\n        \n        assert(dUtils::AigNodeID(lit0) < idCounter + idx);\n        assert(dUtils::AigNodeID(lit1) < idCounter + idx);\n        \n        key = formAndNodeKey(lit0, lit1);\n        vKeysBuffer[idx] = key;\n        \n        // check trivial\n        temp0 = checkTrivialAndCases(lit0, lit1);\n        if (temp0 == HASHTABLE_EMPTY_VALUE<uint64, uint32>) {\n            // non-trivial, insert into hashtable\n            // assign new (tentative) id as idCounter + idx, which is unique\n            insert_single_no_update<uint64, uint32>(htKeys, htValues, key, \n                                                    (uint32)(idCounter + idx), htCapacity);\n        }\n    }\n}",
            "#define uint64 uint64_hack_\n\n\n__device__ __forceinline__ uint32 hash(int x) {\n    return hash((uint32)x);\n}\n\n__device__ __forceinline__ uint32 checkTrivialAndCases(int lit1, int lit2) {\n    if (lit1 == lit2)\n        return (uint32)lit1;\n    if (lit1 == dUtils::AigNodeNot(lit2))\n        return (uint32)dUtils::AigConst0;\n    if (lit1 == dUtils::AigConst1)\n        return (uint32)lit2;\n    if (lit2 == dUtils::AigConst1)\n        return (uint32)lit1;\n    if (lit1 == dUtils::AigConst0 || lit2 == dUtils::AigConst0)\n        return (uint32)dUtils::AigConst0;\n    \n    // non-trivial\n    return HASHTABLE_EMPTY_VALUE<uint64, uint32>;\n}\n\n__device__ __forceinline__ ValueT retrieve_single(const KeyT * ht_keys, const ValueT * ht_values, \n                                  const KeyT key, const int capacity) {\n    KeyT loc = hash(key);\n    loc = loc % capacity;\n\n    while (true) {\n        if (ht_keys[loc] == key) {\n            return ht_values[loc];\n        } else if (ht_keys[loc] == HASHTABLE_EMPTY_KEY<KeyT, ValueT>) {\n            return HASHTABLE_EMPTY_VALUE<KeyT, ValueT>;\n        }\n        loc = (loc + 1) % capacity;\n    }\n}\n\n__device__ __forceinline__ void unbindAndNodeKeys(const uint64 key, uint32 * lit1, uint32 * lit2) {\n    *lit2 = (uint32)(key & 0xffffffffUL);\n    *lit1 = (uint32)(key >> 32);\n}\n\n__global__ void updateLevelNodesNewIds(const int * vReadyNodes, int * vOld2NewLit, const uint64 * vKeysBuffer,\n                                       const uint64 * htKeys, const uint32 * htValues, int htCapacity,\n                                       int nReady) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    int nodeId;\n    uint32 lit0, lit1;\n    uint32 temp0;\n    uint64 key;\n    if (idx < nReady) {\n        nodeId = vReadyNodes[idx];\n        key = vKeysBuffer[idx];\n        unbindAndNodeKeys(key, &lit0, &lit1);\n\n        // check trivial\n        temp0 = checkTrivialAndCases((int)lit0, (int)lit1);\n        if (temp0 == HASHTABLE_EMPTY_VALUE<uint64, uint32>) {\n            // non-trivial\n            temp0 = retrieve_single<uint64, uint32>(htKeys, htValues, key, htCapacity); // id\n            temp0 = temp0 << 1; // convert to lit\n        }\n\n        assert(vOld2NewLit[nodeId] == -1);\n        vOld2NewLit[nodeId] = temp0;\n    }\n}",
            "__global__ void assignOld2NewConsecutiveIds(const uint32 * vOldIds, int * vOld2NewIdConsecutive, \n                                            int nPIs, int nObjsNew) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx <= nPIs + 1) {\n        vOld2NewIdConsecutive[idx] = idx;\n    } else if (idx < nObjsNew) {\n        uint32 nodeIdOld = vOldIds[idx - nPIs - 1];\n        assert(nodeIdOld > (uint32)nPIs);\n        vOld2NewIdConsecutive[nodeIdOld] = idx;\n    }\n}",
            "#define uint64 uint64_hack_\n\n\n__device__ __forceinline__ void unbindAndNodeKeys(const uint64 key, uint32 * lit1, uint32 * lit2) {\n    *lit2 = (uint32)(key & 0xffffffffUL);\n    *lit1 = (uint32)(key >> 32);\n}\n\n__global__ void unbindKeysReId(const uint64 * vOldKeys, const int * vOld2NewIdConsecutive,\n                               int * vFanin0New, int * vFanin1New, int * vNumFanoutsNew,\n                               int nNodesNew) {\n    // NOTE vFanin0New, vFanin1New should point to the begin of AND node storage;\n    //      vNumFanoutsNew should point to the begin of PIs + AND node storage!\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < nNodesNew) {\n        uint32 oldLit0, oldLit1;\n        int oldId0, oldId1, newId0, newId1;\n        unbindAndNodeKeys(vOldKeys[idx], &oldLit0, &oldLit1);\n\n        oldId0 = dUtils::AigNodeID((int)oldLit0), oldId1 = dUtils::AigNodeID((int)oldLit1);\n        newId0 = vOld2NewIdConsecutive[oldId0], newId1 = vOld2NewIdConsecutive[oldId1];\n\n        vFanin0New[idx] = dUtils::AigNodeLitCond(newId0, dUtils::AigNodeIsComplement((int)oldLit0));\n        vFanin1New[idx] = dUtils::AigNodeLitCond(newId1, dUtils::AigNodeIsComplement((int)oldLit1));\n        atomicAdd(&vNumFanoutsNew[newId0], 1);\n        atomicAdd(&vNumFanoutsNew[newId1], 1);\n    }\n}",
            "__global__ void poReId(const int * vOld2NewIdConsecutive,\n                       int * vOutsNew, int * vNumFanoutsNew, int nPOs) {\n    // strashed id -> consecutive id\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < nPOs) {\n        int oldLit, oldId, newId;\n        \n        oldLit = vOutsNew[idx];\n        oldId = dUtils::AigNodeID(oldLit);\n        newId = vOld2NewIdConsecutive[oldId];\n\n        vOutsNew[idx] = dUtils::AigNodeLitCond(newId, dUtils::AigNodeIsComplement(oldLit));\n        atomicAdd(&vNumFanoutsNew[newId], 1);\n    }\n}",
            "__global__ void poUpdateId(const int * pOuts, const int * vOld2NewLit,\n                           int * vOutsNew, int nPOs) {\n    // old id -> strashed id\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < nPOs) {\n        int oldLit, oldId;\n\n        oldLit = pOuts[idx];\n        oldId = dUtils::AigNodeID(oldLit);\n        vOutsNew[idx] = dUtils::AigNodeNotCond(vOld2NewLit[oldId], dUtils::AigNodeIsComplement(oldLit));\n    }\n}",
            "__global__ void markDanglingNodesIter(const int * pFanin0, const int * pFanin1, int * pNumFanouts,\n                                      int * vDanglingMarks, int nNodes, int nPIs) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < nNodes) {\n        int nodeId = idx + nPIs + 1, id0, id1;\n        assert(pNumFanouts[nodeId] >= 0);\n        if (!vDanglingMarks[idx] && pNumFanouts[nodeId] == 0) {\n            vDanglingMarks[idx] = 1;\n            id0 = dUtils::AigNodeID(pFanin0[nodeId]), id1 = dUtils::AigNodeID(pFanin1[nodeId]);\n            atomicSub(&pNumFanouts[id0], 1);\n            atomicSub(&pNumFanouts[id1], 1);\n        }\n    }\n}"
        ],
        "/Users/gbolet/hecbench-roofline/src/logic-rewrite-cuda/balance.cu": [
            "__global__ void checkCoverTravEnd(int * isEnd, int * vInputs, int * vInputsFilter, \n                                  int * vCanonTable, const int nPIs, const int arrayLen) {\n    /**\n     * This is a full-filtering marking both \"PI\" and \"R\" (in-level redundancy). \n     **/\n\n    // isEnd should be 1 when passed in!\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < arrayLen) {\n        int thisLit = vInputs[idx];\n        int nodeId = dUtils::AigNodeID(thisLit);\n        int nodeIsComplement = dUtils::AigNodeIsComplement(thisLit);\n\n        // convert vInputs from literal to nodeId\n        vInputs[idx] = nodeId;\n\n        if (dUtils::AigIsNode(nodeId, nPIs)) {\n            // has at least one AND node, cannot stop traversal\n            *isEnd = 0;\n            // 1 indicates non-duplicate AND node\n            vInputsFilter[idx] = 1;\n\n            // only the first attempt thread will change vCanonTable[nodeId] to its idx + 1 (> 0),\n            // and get old = 0; later threads will not change vCanonTable[nodeId], and get\n            // old = idx of first thread + 1\n            int old = atomicCAS(&vCanonTable[nodeId], 0, idx + 1);\n            if (old > 0) {\n                // later threads\n                vInputsFilter[idx] = -old;\n            }\n            // first thread keeps vInputsFilter[idx] = 1 unchanged\n        } else {\n            // vInputs[idx] is PI\n            vInputsFilter[idx] = 0;\n        }\n    }\n}",
            "#define uint64 uint64_hack_\n\n\ninline __host__ __device__ uint4 make_uint4(int4 a)\n{\n    return make_uint4(uint(a.x), uint(a.y), uint(a.z), uint(a.w));\n}\n\ninline __host__ __device__ uint4 max(uint4 a, uint4 b)\n{\n    return make_uint4(max(a.x,b.x), max(a.y,b.y), max(a.z,b.z), max(a.w,b.w));\n}\n\n__device__ __forceinline__ uint32 hash(int x) {\n    return hash((uint32)x);\n}\n\n__device__ __forceinline__ uint32 checkTrivialAndCases(int lit1, int lit2) {\n    if (lit1 == lit2)\n        return (uint32)lit1;\n    if (lit1 == dUtils::AigNodeNot(lit2))\n        return (uint32)dUtils::AigConst0;\n    if (lit1 == dUtils::AigConst1)\n        return (uint32)lit2;\n    if (lit2 == dUtils::AigConst1)\n        return (uint32)lit1;\n    if (lit1 == dUtils::AigConst0 || lit2 == dUtils::AigConst0)\n        return (uint32)dUtils::AigConst0;\n    \n    // non-trivial\n    return HASHTABLE_EMPTY_VALUE<uint64, uint32>;\n}\n\n__device__ __forceinline__ uint64 formAndNodeKey(const int lit1, const int lit2) {\n    // make sure lit1 is smaller than lit2\n    uint32 uLit1 = (uint32)lit1;\n    uint32 uLit2 = (uint32)lit2;\n    uint64 key = ((uint64)uLit1) << 32 | uLit2;\n    return key;\n}\n\n__device__ __forceinline__ ValueT retrieve_single(const KeyT * ht_keys, const ValueT * ht_values, \n                                  const KeyT key, const int capacity) {\n    KeyT loc = hash(key);\n    loc = loc % capacity;\n\n    while (true) {\n        if (ht_keys[loc] == key) {\n            return ht_values[loc];\n        } else if (ht_keys[loc] == HASHTABLE_EMPTY_KEY<KeyT, ValueT>) {\n            return HASHTABLE_EMPTY_VALUE<KeyT, ValueT>;\n        }\n        loc = (loc + 1) % capacity;\n    }\n}\n\n__device__ uint32 retrieveAndNode(int lit1, int lit2,\n                                  const uint64 * htReconstructKeys, const uint32 * htReconstructValues, const int htReconstructCapacity) {\n    // check trivial cases\n    // Note that when encountering these cases during cover reconstruction,\n    // the hashtable remains unaffected while the number of inputs is reduced by one,\n    // with the cover function unchanged.\n    uint32 trivialResult = checkTrivialAndCases(lit1, lit2);\n    if (trivialResult != HASHTABLE_EMPTY_VALUE<uint64, uint32>)\n        return trivialResult;\n    \n    // make sure lit1 is smaller than lit2\n    if (lit1 > lit2) {\n        int temp;\n        temp = lit1;\n        lit1 = lit2;\n        lit2 = temp;\n    }\n    uint64 key = formAndNodeKey(lit1, lit2);\n\n    // check hash table\n    uint32 value = retrieve_single<uint64, uint32>(htReconstructKeys, htReconstructValues, key, htReconstructCapacity);\n    // note that retrieved is id, so convert to literal\n    if (value != HASHTABLE_EMPTY_VALUE<uint64, uint32>)\n        value = value << 1;\n\n    // return HASHTABLE_EMPTY_VALUE if not found\n    return value;\n}\n\n__global__ void prepareDataToInsert(int * vLocalReconstructArrays, int * vLocalReconstructLevels, int * vLocalReconstructLens, \n                                    uint64 * vStepNodeKeys, uint32 * vStepNodeValues, int * vStepNodeMask, uint32 * vStepNodeLevels,\n                                    const uint64 * htReconstructKeys, const uint32 * htReconstructValues, const int htReconstructCapacity,\n                                    const uint32 * htLevelKeys, const uint32 * htLevelValues, const int htLevelCapacity,\n                                    const int newIdCounter, const int nReadyCovers, const int nPIs, const int nPOs, const int maxCoverLen) {\n    /**\n     * Prepare one batch of AND node to be inserted to hash table. \n     * \n     * Result of this kernel function:\n     * vStepNodeMask = 0 (already complete including constant false, invalid entry): vStepNodeKeys, vStepNodeValues, vStepNodeLevels are all invalid\n     * vStepNodeMask = 1 (non-existing nodes): vStepNodeKeys, vStepNodeValues, vStepNodeLevels are all valid\n     * vStepNodeMask = 2 (existing nodes): vStepNodeValues, vStepNodeLevels are valid, but vStepNodeKeys are invalid\n     * */\n    // newIdCounter indicates the max id in reconstruct hashtable + 1, i.e., first available new id.\n\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < nReadyCovers) {\n        int length = vLocalReconstructLens[idx];\n        int localArrayStartIdx = idx * maxCoverLen;\n        assert(length >= 0);\n\n        if (length <= 1) {\n            // mask zero\n            vStepNodeMask[idx] = 0; // 0 as no insert to hash table and reconstruct array\n            return;\n        }\n\n        // pop out two last nodes\n        int lit1 = vLocalReconstructArrays[localArrayStartIdx + length - 1];\n        int lit2 = vLocalReconstructArrays[localArrayStartIdx + length - 2];\n        int lv1 = vLocalReconstructLevels[localArrayStartIdx + length - 1];\n        int lv2 = vLocalReconstructLevels[localArrayStartIdx + length - 2];\n        vLocalReconstructLens[idx] -= 2;\n\n        // perform a retrieval from reconstruct hashtable first\n        // if found, update its level and mask zero\n        uint32 retrieveRes = retrieveAndNode(lit1, lit2, htReconstructKeys, htReconstructValues, htReconstructCapacity);\n        if (retrieveRes != HASHTABLE_EMPTY_VALUE<uint64, uint32>) {\n            // printf(\" * %s%d,%s%d \", dUtils::AigNodeIsComplement(lit1) ? \"!\" : \"\", dUtils::AigNodeIDDebug(lit1, nPIs, nPOs),\n            //                        dUtils::AigNodeIsComplement(lit2) ? \"!\" : \"\", dUtils::AigNodeIDDebug(lit2, nPIs, nPOs));\n\n            // already exist node\n            vStepNodeMask[idx] = 2; // 2 as no insert to hash table but to reconstruct array\n            vStepNodeValues[idx] = retrieveRes; // note that literal is saved here, but for new node entries id is saved\n\n            // note that if retrieveRes is a trivial case, then it might be PI\n            int retrievedId = dUtils::AigNodeID(retrieveRes);\n            if (dUtils::AigIsNode(retrievedId, nPIs)) {\n                retrieveRes = retrieve_single<uint32, uint32>( // retrieve level from level hashtable\n                    htLevelKeys, htLevelValues, (uint32)retrievedId, htLevelCapacity\n                );\n            } else {\n                retrieveRes = 0; // PI has zero level\n            }\n            assert(retrieveRes != (HASHTABLE_EMPTY_VALUE<uint32, uint32>));\n\n            vStepNodeLevels[idx] = retrieveRes;\n            return;\n        }\n\n        // non-existing nodes\n        if (lit1 > lit2) {\n            int temp;\n            temp = lit1, lit1 = lit2, lit2 = temp;\n        }\n        \n        vStepNodeMask[idx] = 1; // 1 as insert to hash table and to reconstruct array\n        vStepNodeKeys[idx] = formAndNodeKey(lit1, lit2);\n        vStepNodeValues[idx] = (uint32)(newIdCounter + idx); // save new id, instead of literal as for mask = 2 nodes\n        vStepNodeLevels[idx] = (uint32)max(lv1, lv2) + 1;\n    }\n}",
            "#define uint64 uint64_hack_\n\n\n__device__ __forceinline__ uint32 hash(int x) {\n    return hash((uint32)x);\n}\n\n__device__ __forceinline__ uint32 checkTrivialAndCases(int lit1, int lit2) {\n    if (lit1 == lit2)\n        return (uint32)lit1;\n    if (lit1 == dUtils::AigNodeNot(lit2))\n        return (uint32)dUtils::AigConst0;\n    if (lit1 == dUtils::AigConst1)\n        return (uint32)lit2;\n    if (lit2 == dUtils::AigConst1)\n        return (uint32)lit1;\n    if (lit1 == dUtils::AigConst0 || lit2 == dUtils::AigConst0)\n        return (uint32)dUtils::AigConst0;\n    \n    // non-trivial\n    return HASHTABLE_EMPTY_VALUE<uint64, uint32>;\n}\n\n__device__ __forceinline__ uint64 formAndNodeKey(const int lit1, const int lit2) {\n    // make sure lit1 is smaller than lit2\n    uint32 uLit1 = (uint32)lit1;\n    uint32 uLit2 = (uint32)lit2;\n    uint64 key = ((uint64)uLit1) << 32 | uLit2;\n    return key;\n}\n\n__device__ __forceinline__ ValueT retrieve_single(const KeyT * ht_keys, const ValueT * ht_values, \n                                  const KeyT key, const int capacity) {\n    KeyT loc = hash(key);\n    loc = loc % capacity;\n\n    while (true) {\n        if (ht_keys[loc] == key) {\n            return ht_values[loc];\n        } else if (ht_keys[loc] == HASHTABLE_EMPTY_KEY<KeyT, ValueT>) {\n            return HASHTABLE_EMPTY_VALUE<KeyT, ValueT>;\n        }\n        loc = (loc + 1) % capacity;\n    }\n}\n\n__device__ uint32 retrieveAndNode(int lit1, int lit2,\n                                  const uint64 * htReconstructKeys, const uint32 * htReconstructValues, const int htReconstructCapacity) {\n    // check trivial cases\n    // Note that when encountering these cases during cover reconstruction,\n    // the hashtable remains unaffected while the number of inputs is reduced by one,\n    // with the cover function unchanged.\n    uint32 trivialResult = checkTrivialAndCases(lit1, lit2);\n    if (trivialResult != HASHTABLE_EMPTY_VALUE<uint64, uint32>)\n        return trivialResult;\n    \n    // make sure lit1 is smaller than lit2\n    if (lit1 > lit2) {\n        int temp;\n        temp = lit1;\n        lit1 = lit2;\n        lit2 = temp;\n    }\n    uint64 key = formAndNodeKey(lit1, lit2);\n\n    // check hash table\n    uint32 value = retrieve_single<uint64, uint32>(htReconstructKeys, htReconstructValues, key, htReconstructCapacity);\n    // note that retrieved is id, so convert to literal\n    if (value != HASHTABLE_EMPTY_VALUE<uint64, uint32>)\n        value = value << 1;\n\n    // return HASHTABLE_EMPTY_VALUE if not found\n    return value;\n}\n\n__global__ void sharedNodeDrivenPermute(int * vLocalReconstructArrays, int * vLocalReconstructLevels, int * vLocalReconstructLens, \n                                        const uint64 * htReconstructKeys, const uint32 * htReconstructValues, const int htReconstructCapacity,\n                                        const int nReadyCovers, const int maxCoverLen) {\n    // this can be optional. \n    // In original ABC we tested that without this step, num of node reduction dropped around 2.5%.\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < nReadyCovers) {\n        int leftMostIdx, rightBoundIdx;\n        int length = vLocalReconstructLens[idx];\n        int localArrayStartIdx = idx * maxCoverLen;\n        int borderLevel, currLevel;\n\n        if (length <= 1)\n            return;\n\n        // part 1. find left most node with same level as right border one\n\n        if (length < 3) {\n            leftMostIdx = 0;\n        } else {\n            leftMostIdx = length - 2;\n            borderLevel = vLocalReconstructLevels[localArrayStartIdx + leftMostIdx];\n\n            for (leftMostIdx--; leftMostIdx >= 0; leftMostIdx--) {\n                currLevel = vLocalReconstructLevels[localArrayStartIdx + leftMostIdx];\n                if (currLevel != borderLevel)\n                    break;\n            }\n            leftMostIdx++;\n            assert(vLocalReconstructLevels[localArrayStartIdx + leftMostIdx] == borderLevel);\n        }\n\n        // part 2. do shared-node driven permutation\n        rightBoundIdx = length - 2;\n        assert(leftMostIdx <= rightBoundIdx);\n        if (leftMostIdx < rightBoundIdx) {\n            int i, lit1, lit2, lit3;\n            lit1 = vLocalReconstructArrays[localArrayStartIdx + rightBoundIdx + 1];\n            lit2 = vLocalReconstructArrays[localArrayStartIdx + rightBoundIdx];\n\n            for (i = rightBoundIdx; i >= leftMostIdx; i--) {\n                lit3 = vLocalReconstructArrays[localArrayStartIdx + i];\n                if (\n                    retrieveAndNode(lit1, lit3, htReconstructKeys, htReconstructValues, htReconstructCapacity) \n                    != HASHTABLE_EMPTY_VALUE<uint64, uint32>\n                ) {\n                    if (lit3 != lit2) {\n                        int tempLevel;\n                        vLocalReconstructArrays[localArrayStartIdx + i] = lit2;\n                        vLocalReconstructArrays[localArrayStartIdx + rightBoundIdx] = lit3;\n\n                        tempLevel = vLocalReconstructLevels[localArrayStartIdx + i];\n                        vLocalReconstructLevels[localArrayStartIdx + i] = vLocalReconstructLevels[localArrayStartIdx + rightBoundIdx];\n                        vLocalReconstructLevels[localArrayStartIdx + rightBoundIdx] = tempLevel;\n                    }\n                    break;\n                }\n            }\n        }\n    }\n}",
            "__device__ __forceinline__ uint32 hash(int x) {\n    return hash((uint32)x);\n}\n\n__device__ __forceinline__ ValueT retrieve_single(const KeyT * ht_keys, const ValueT * ht_values, \n                                  const KeyT key, const int capacity) {\n    KeyT loc = hash(key);\n    loc = loc % capacity;\n\n    while (true) {\n        if (ht_keys[loc] == key) {\n            return ht_values[loc];\n        } else if (ht_keys[loc] == HASHTABLE_EMPTY_KEY<KeyT, ValueT>) {\n            return HASHTABLE_EMPTY_VALUE<KeyT, ValueT>;\n        }\n        loc = (loc + 1) % capacity;\n    }\n}\n\n__global__ void prepareReconstructArrays(const int * vGatheredReadyCovers, const int * vNodeCoverIdMapping, \n                                         const int * vCoverNodeIdMapping, const int * vCoverNodeNewLitMapping, \n                                         const int * vCoverTable, const int * vCoverTableLinks, const int * vCoverTableLens, \n                                         const uint32 * htLevelKeys, const uint32 * htLevelValues, const int htLevelCapacity,\n                                         int * vLocalReconstructArrays, int * vLocalReconstructLevels, int * vLocalReconstructLens, \n                                         const int nReadyCovers, const int nPIs, const int maxCoverLen,\n                                         const int sortDecId) {\n    // gather cover input list from global cover table to local reconstruct arrays,\n    // then do sorting\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < nReadyCovers) {\n        // int coverId = vGatheredReadyCovers[idx];\n        // int oldOutputNodeId = vCoverNodeIdMapping[coverId];\n        int oldOutputNodeId = vGatheredReadyCovers[idx];\n        int currRowIdx = oldOutputNodeId;\n        int columnPtr = 0;\n        int length = vCoverTableLens[oldOutputNodeId];\n        int currInputLit, currInputOldId, currInputNewLit, currInputNewId;\n        int localArrayStartIdx = idx * maxCoverLen;\n        assert(length <= maxCoverLen);\n\n        // if (oldOutputNodeId == 480634 || oldOutputNodeId == 480396)\n        //     printf(\"prepare reconstruct %d, len = %d\\n\", oldOutputNodeId, length);\n\n        vLocalReconstructLens[idx] = length;\n        if (length == 0) return;\n\n        // int vRand[120];\n        // assert(length <= 120);\n        thrust::default_random_engine rng(idx);\n        // thrust::uniform_int_distribution<int> dist(0, 9999999);\n\n\n        for (int i = 0; i < length; i++) {\n            // vRand[i] = dist(rng);\n\n            // change row\n            if (columnPtr == COVER_TABLE_NUM_COLS) {\n                columnPtr = 0;\n                currRowIdx = vCoverTableLinks[currRowIdx];\n            }\n            currInputLit = vCoverTable[currRowIdx * COVER_TABLE_NUM_COLS + (columnPtr++)];\n            currInputOldId = dUtils::AigNodeID(currInputLit);\n            // if (oldOutputNodeId == 480634 || oldOutputNodeId == 480396)\n            //     printf(\"%s%d -> \", dUtils::AigNodeIsComplement(currInputLit) ? \"!\" : \"\", currInputOldId);\n\n            // // new id of PIs are same as old id, and also not saved in new id mapping\n            // if (isPI)\n            //     currInputNewId = currInputOldId;\n            // else\n            //     currInputNewId = vCoverNodeNewIdMapping[vNodeCoverIdMapping[currInputOldId]];\n            // assert(currInputNewId != -1);\n            \n            // // save new literal into local reconstruct array\n            // vLocalReconstructArrays[localArrayStartIdx + i] = dUtils::AigNodeLitCond(\n            //     currInputNewId, dUtils::AigNodeIsComplement(currInputLit)\n            // );\n\n            // // save level info\n            // if (isPI) {\n            //     // zero level for PIs\n            //     vLocalReconstructLevels[localArrayStartIdx + i] = 0;\n            // } else {\n            //     // retrieve level info from level hash table\n            //     int retrieveRes;\n            //     retrieveRes = retrieve_single<uint32, uint32>(\n            //         htLevelKeys, htLevelValues, (uint32)currInputNewId, htLevelCapacity\n            //     );\n            //     assert(retrieveRes != (HASHTABLE_EMPTY_VALUE<uint32, uint32>));\n            //     vLocalReconstructLevels[localArrayStartIdx + i] = retrieveRes;\n            // }\n\n            if (dUtils::AigIsPIConst(currInputOldId, nPIs)) {\n                // save new literal into local reconstruct array\n                vLocalReconstructArrays[localArrayStartIdx + i] = currInputLit;\n                // zero level for PIs\n                vLocalReconstructLevels[localArrayStartIdx + i] = 0;\n            } else {\n                currInputNewLit = vCoverNodeNewLitMapping[vNodeCoverIdMapping[currInputOldId]];\n                // save new literal into local reconstruct array\n                vLocalReconstructArrays[localArrayStartIdx + i] = dUtils::AigNodeNotCond(\n                    currInputNewLit, dUtils::AigNodeIsComplement(currInputLit)\n                );\n                currInputNewId = dUtils::AigNodeID(currInputNewLit);\n                // if (oldOutputNodeId == 480634 || oldOutputNodeId == 480396)\n                //     printf(\"%s%d, \", dUtils::AigNodeIsComplement(currInputLit) ? \"!\" : \"\", currInputNewId);\n\n                // save level info\n                if (dUtils::AigIsPIConst(currInputNewId, nPIs))\n                    vLocalReconstructLevels[localArrayStartIdx + i] = 0;\n                else {\n                    // retrieve level info from level hash table\n                    int retrieveRes;\n                    retrieveRes = retrieve_single<uint32, uint32>(\n                        htLevelKeys, htLevelValues, (uint32)currInputNewId, htLevelCapacity\n                    );\n                    assert(retrieveRes != (HASHTABLE_EMPTY_VALUE<uint32, uint32>));\n                    vLocalReconstructLevels[localArrayStartIdx + i] = retrieveRes;\n                }\n                \n            }\n        }\n        // if (oldOutputNodeId == 480634 || oldOutputNodeId == 480396)\n        //     printf(\"\\n\");\n\n        // sort in desecending level order, no further parallel\n        // thrust::sort_by_key(\n        //     thrust::seq, &vLocalReconstructLevels[localArrayStartIdx], \n        //     &vLocalReconstructLevels[localArrayStartIdx + length], \n        //     &vLocalReconstructArrays[localArrayStartIdx],\n        //     thrust::greater<int>()\n        // );\n\n        if (sortDecId) {\n            thrust::sort(\n                thrust::seq, \n                thrust::make_zip_iterator(thrust::make_tuple(&vLocalReconstructArrays[localArrayStartIdx], \n                                          &vLocalReconstructLevels[localArrayStartIdx])), \n                thrust::make_zip_iterator(thrust::make_tuple(&vLocalReconstructArrays[localArrayStartIdx + length], \n                                          &vLocalReconstructLevels[localArrayStartIdx + length])), \n                dUtils::decreaseLevelIds<int, int>()\n            );\n\n            // thrust::sort(\n            //     thrust::seq, \n            //     thrust::make_zip_iterator(thrust::make_tuple(&vLocalReconstructArrays[localArrayStartIdx], \n            //                               &vLocalReconstructLevels[localArrayStartIdx],\n            //                               &vRand[0])), \n            //     thrust::make_zip_iterator(thrust::make_tuple(&vLocalReconstructArrays[localArrayStartIdx + length], \n            //                               &vLocalReconstructLevels[localArrayStartIdx + length],\n            //                               &vRand[length])), \n            //     dUtils::decreaseLevelsPerm<int, int>()\n            // );\n        } else {\n            thrust::shuffle(thrust::seq, \n                        thrust::make_zip_iterator(thrust::make_tuple(&vLocalReconstructArrays[localArrayStartIdx], \n                                          &vLocalReconstructLevels[localArrayStartIdx])), \n                        thrust::make_zip_iterator(thrust::make_tuple(&vLocalReconstructArrays[localArrayStartIdx + length], \n                                          &vLocalReconstructLevels[localArrayStartIdx + length])), \n                        rng);\n\n            thrust::sort(\n                thrust::seq, \n                thrust::make_zip_iterator(thrust::make_tuple(&vLocalReconstructArrays[localArrayStartIdx], \n                                          &vLocalReconstructLevels[localArrayStartIdx])), \n                thrust::make_zip_iterator(thrust::make_tuple(&vLocalReconstructArrays[localArrayStartIdx + length], \n                                          &vLocalReconstructLevels[localArrayStartIdx + length])), \n                dUtils::decreaseLevels<int, int>()\n            );\n        }\n        \n\n        \n    }\n}",
            "__global__ void addBackLocalArrays(int * vLocalReconstructArrays, int * vLocalReconstructLevels, int * vLocalReconstructLens, \n                                   const uint32 * vStepNodeValues, const int * vStepNodeMask, const uint32 * vStepNodeLevels,\n                                   const int nReadyCovers, const int maxCoverLen) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    // mask = 0 means that idx has complete reconstruction in previous steps\n    if (idx < nReadyCovers && vStepNodeMask[idx] != 0) {\n        int litToInsert = (vStepNodeMask[idx] == 2 ? vStepNodeValues[idx] : (vStepNodeValues[idx] << 1));\n        int localArrayStartIdx = idx * maxCoverLen;\n        int length = vLocalReconstructLens[idx];\n        int i;\n        int temp;\n\n        // check whether there alreay exists a same node\n        for (i = 0; i < length; i++) {\n            if (vLocalReconstructArrays[localArrayStartIdx + i] == litToInsert)\n                return;\n        }\n\n        // insert\n        vLocalReconstructArrays[localArrayStartIdx + length] = litToInsert;\n        vLocalReconstructLevels[localArrayStartIdx + length] = vStepNodeLevels[idx];\n        vLocalReconstructLens[idx]++;\n\n        // adjust new node to proper location in terms of level\n        for (i = length; i > 0; i--) {\n            if (vLocalReconstructLevels[localArrayStartIdx + i] <= \n                vLocalReconstructLevels[localArrayStartIdx + i - 1])\n                break;\n\n            temp = vLocalReconstructArrays[localArrayStartIdx + i];\n            vLocalReconstructArrays[localArrayStartIdx + i] = vLocalReconstructArrays[localArrayStartIdx + i - 1];\n            vLocalReconstructArrays[localArrayStartIdx + i - 1] = temp;\n\n            temp = vLocalReconstructLevels[localArrayStartIdx + i];\n            vLocalReconstructLevels[localArrayStartIdx + i] = vLocalReconstructLevels[localArrayStartIdx + i - 1];\n            vLocalReconstructLevels[localArrayStartIdx + i - 1] = temp;\n        }\n    }\n}",
            "__global__ void recordReconstructedCovers(const int * vLocalReconstructArrays, const int * vLocalReconstructLens, \n                                          const int * vGatheredReadyCovers, const int * vNodeCoverIdMapping, int * vCoverNodeNewLitMapping, \n                                          const int nReadyCovers, const int maxCoverLen) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < nReadyCovers) {\n        int newLit;\n        if (vLocalReconstructLens[idx] == 0) {\n            // constant false\n            newLit = dUtils::AigConst0;\n            // if (vGatheredReadyCovers[idx] == 480634)\n            //     printf(\"** constant false!\\n\");\n        } else {\n            assert(vLocalReconstructLens[idx] == 1);\n            newLit = vLocalReconstructArrays[idx * maxCoverLen];\n            // if (vGatheredReadyCovers[idx] == 480634)\n            //     printf(\"** normal branch, newLit=%d!\\n\", newLit);\n        }\n        \n        int nodeId = vGatheredReadyCovers[idx];\n        int coverId = vNodeCoverIdMapping[nodeId];\n        vCoverNodeNewLitMapping[coverId] = newLit;\n    }\n}",
            "__global__ void genReadyMask(const int * vCoverNodeIdMapping, const int * vNodeCoverIdMapping, \n                             const int * vCoverTable, const int * vCoverTableLinks, const int * vCoverTableLens, \n                             const int * vCoverNodeNewLitMapping, int * vThisIterReady, const int arrayLen, const int nPIs) {\n    // vCoverNodeNewLitMapping indicates ready or not\n    // generate a mask vThisIterReady for all covers that can be re-constructed in this iter\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx > 0 && idx < arrayLen) {\n        // if cover to new id mapping is not -1, meaning the cover is already re-constructed\n        if (vCoverNodeNewLitMapping[idx] >= 0) {\n            // already constructed covers, should not construct again\n            vThisIterReady[idx] = 0;\n        } else {\n            // check ready status by going through all its inputs\n            int nodeId = vCoverNodeIdMapping[idx];\n            int ready = 1;\n            int currRowIdx = nodeId;\n            int columnPtr = 0;\n            int length = vCoverTableLens[nodeId];\n            int currInputId;\n\n            for (int i = 0; i < length; i++) {\n                // change row\n                if (columnPtr == COVER_TABLE_NUM_COLS) {\n                    columnPtr = 0;\n                    currRowIdx = vCoverTableLinks[currRowIdx];\n                }\n                currInputId = dUtils::AigNodeID(\n                    vCoverTable[currRowIdx * COVER_TABLE_NUM_COLS + (columnPtr++)]\n                );\n\n                if (dUtils::AigIsPIConst(currInputId, nPIs))\n                    continue;\n\n                if (vCoverNodeNewLitMapping[vNodeCoverIdMapping[currInputId]] == -1) {\n                    // this input is not re-constructed\n                    ready = 0;\n                    break;\n                }\n            }\n\n            vThisIterReady[idx] = ready;\n        }\n    }\n}",
            "__global__ void gatherByScannedMask(int * vScannedMask, int * vGathered, const int arrayLen, const int idOffset=0) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < arrayLen) {\n        if (idx == 0) {\n            // vGathered[vScannedMask[idx] - 1] = idx + idOffset\n            if (vScannedMask[0] == 1)\n                vGathered[0] = idOffset;\n        } else {\n            // + idOffset indicates id start from idOffset\n            if (vScannedMask[idx] > vScannedMask[idx - 1])\n                vGathered[vScannedMask[idx] - 1] = idx + idOffset;\n        }\n    }\n}",
            "__global__ void markIsCoverOutput(const int * vCoverTableLens, int * vMarks, const int arrayLen) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < arrayLen) {\n        vMarks[idx] = (vCoverTableLens[idx] != -1 ? 1 : 0);\n    }\n}",
            "__global__ void getCoverToNodeIdMapping(const int * vNodeCoverIdMapping, const int * vCoverTableLens, \n                                        int * vCoverNodeIdMapping, const int arrayLen) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < arrayLen) {\n        if (vCoverTableLens[idx] != -1) {\n            // idx is output node id that corresponds to a cover\n            vCoverNodeIdMapping[vNodeCoverIdMapping[idx]] = idx;\n        }\n    }\n}",
            "__global__ void gatherWithFilter(int * isEnd, \n                                 const int * vCoverTable, const int * vCoverTableLinks, \n                                 const int * vNodes, const int * vCoverRanges, int * vCanonTable, \n                                 int * vNewGlobalList, int * vNodesFilter, const int nPIs, const int arrayLen) {\n    // vCanonTable should be all zero before this kernel call\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < arrayLen) {\n        int dstStartIdx = (idx == 0 ? 0 : vCoverRanges[idx - 1]);\n        // int length =  vCoverRanges[idx] - dstStartIdx;\n        int dstEndIdx = vCoverRanges[idx];\n\n        int fatherLit = vNodes[idx];\n        int fatherId = dUtils::AigNodeID(fatherLit);\n        // if (length != vCoverTableLens[fatherId]) {\n        //     printf(\"father id %d: range %d, table len %d\\n\", fatherId, length, vCoverTableLens[fatherId]);\n        // }\n        // assert(length == vCoverTableLens[fatherId]);\n\n        int currRowIdx, columnPtr;\n        currRowIdx = fatherId;\n        columnPtr = 0;\n\n        int laterThread, currLit, currId;\n        for (int i = dstStartIdx; i < dstEndIdx; i++) {\n            // change row\n            if (columnPtr == COVER_TABLE_NUM_COLS) {\n                columnPtr = 0;\n                currRowIdx = vCoverTableLinks[currRowIdx];\n            }\n            currLit = vCoverTable[currRowIdx * COVER_TABLE_NUM_COLS + columnPtr];\n            columnPtr++;\n            currId = dUtils::AigNodeID(currLit);\n            vNewGlobalList[i] = currLit;\n\n            // check whether this is PI\n            if (!dUtils::AigIsNode(currId, nPIs)) {\n                // mark in filter\n                vNodesFilter[i] = 0;\n                continue;\n            }\n\n            // update father, only one entry corresponds to currId will succeed\n            laterThread = atomicCAS(&vCanonTable[currId], 0, 1);\n            // laterThread = atomicAdd(&vCanonTable[currId], 1);\n            if (!laterThread) {\n                // first thread, perform update\n                // mark as non-PI, non-R\n                vNodesFilter[i] = 1;\n                // *isEnd = 0;\n            } else {\n                // later thread, mark itself as duplicate in filter\n                vNodesFilter[i] = -1;\n            }\n        }\n    }\n}",
            "__device__ int localCoverTravToTable(int nodeLit, const int * pFanin0, const int * pFanin1, const int * pNumFanouts, \n                                     int * vCoverTable, int * vCoverTableLinks, int * vCoverTableLens, int * nCoverTableNext,\n                                     int * superLen, const int nPIs) {\n    // because of the condition \"pNumFanouts[nodeId] > 1\", the traversal only visits a tree-structure, \n    // therefore even there is no visited marks, we will never encounter repeated nodes.\n    // also saves traversal results into the table\n    int nodeId, outputNodeId;\n    int i, currLit, checkLit, samePolarFlag;\n    int currRowIdx, lastRowIdx, columnPtr;\n    int checkCurrRowIdx, checkColumnPtr; // used in checking existing nodes\n\n    int stack[DFS_COVER_STACK_SIZE];\n    int stackTop = -1;\n    stack[++stackTop] = nodeLit;\n\n    // initial changes on cover table\n    outputNodeId = dUtils::AigNodeID(nodeLit);\n    vCoverTableLinks[outputNodeId] = 0;\n    // writing pointers\n    currRowIdx = outputNodeId;\n    columnPtr = 0;\n\n    while (stackTop != -1) {\n        // pop\n        currLit = stack[stackTop--];\n\n        // check whether currLit is already in cover table\n        samePolarFlag = 0;\n        checkCurrRowIdx = outputNodeId;\n        for (i = 0, checkColumnPtr = 0; i < *superLen; i++, checkColumnPtr++) {\n            // change row\n            if (checkColumnPtr == COVER_TABLE_NUM_COLS) {\n                checkColumnPtr = 0;\n                assert(vCoverTableLinks[checkCurrRowIdx] > checkCurrRowIdx);\n                checkCurrRowIdx = vCoverTableLinks[checkCurrRowIdx];\n            }\n            checkLit = vCoverTable[checkCurrRowIdx * COVER_TABLE_NUM_COLS + checkColumnPtr];\n\n            if (checkLit == currLit) {\n                // same polarity\n                samePolarFlag = 1;\n                break;\n            } else if (checkLit == dUtils::AigNodeNot(currLit)) {\n                // opposite polarity, modify cover table and directly return false\n                vCoverTableLens[outputNodeId] = 0;\n                return 0;\n            }\n        }\n\n        // if currLit has a replica in cover table, then prune this branch\n        if (samePolarFlag)\n            continue;\n        \n        // encounter cover input nodes\n        nodeId = dUtils::AigNodeID(currLit);\n        // if (currLit != nodeLit && (dUtils::AigNodeIsComplement(currLit) || !dUtils::AigIsNode(nodeId, nPIs) || pNumFanouts[nodeId] > 1)) {\n        if (currLit != nodeLit && \n            (dUtils::AigNodeIsComplement(currLit) || !dUtils::AigIsNode(nodeId, nPIs) || pNumFanouts[nodeId] > 1 \n                || stackTop+2>=DFS_COVER_STACK_SIZE || *superLen+2*(stackTop+1)+1 >= MAX_LOCAL_COVER_SIZE )\n            ) {\n            // add one node into cover table\n            if (columnPtr == COVER_TABLE_NUM_COLS) {\n                // expand a new row\n                lastRowIdx = currRowIdx;\n                currRowIdx = atomicAdd(nCoverTableNext, 1);\n                vCoverTableLinks[lastRowIdx] = currRowIdx;\n                vCoverTableLinks[currRowIdx] = 0;\n                columnPtr = 0;\n            }\n            vCoverTable[currRowIdx * COVER_TABLE_NUM_COLS + columnPtr] = currLit;\n            columnPtr++;\n\n            (*superLen)++;\n            continue;\n        }\n\n        // in reversed order to get identical result as the recursive algorithm\n        stack[++stackTop] = pFanin1[nodeId];\n        stack[++stackTop] = pFanin0[nodeId];\n\n        assert(stackTop < DFS_COVER_STACK_SIZE);\n    }\n\n    vCoverTableLens[outputNodeId] = *superLen;\n    return 1;\n}\n\n__global__ void coverFindingToTable(int * vNodes, int * vNodesStatus, int * vLastAppearLevel, \n                                    const int * pFanin0, const int * pFanin1, const int * pNumFanouts, \n                                    int * vCoverTable, int * vCoverTableLinks, int * vCoverTableLens, int * nCoverTableNext,\n                                    const int nPIs, const int arrayLen, const int levelCount) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < arrayLen) {\n        // only perform traversal for vNodesFilter[idx] = 1\n        // if (vNodesFilter[idx] != 1) {\n        //     vCoverWiseLens[idx] = 0;\n        //     return;\n        // }\n\n        // int thisLit = vNodes[idx];\n        // int nodeId = dUtils::AigNodeID(thisLit); // note, we assume that there is no concurrent thread with same nodeId\n        int nodeId = vNodes[idx];\n        int thisLit = (nodeId << 1); // the polarity of nodeId will not affect cover traversal\n        int superLen = 0;\n        int ret;\n\n        // update last appear level of this cover output node\n        vLastAppearLevel[nodeId] = levelCount;\n\n        if (vCoverTableLinks[nodeId] != -1) {\n            // assuming that there are no duplicates in vNodes, this means that\n            // this cover has already been discovered in the previous levels;\n            // if there are duplicates, then this part should be modified\n            superLen = vCoverTableLens[nodeId];\n            // if superLen is zero, then this cover should be constant true/false\n            ret = (superLen != 0);\n        } else {\n            // do cover traversal and write into cover table\n            ret = localCoverTravToTable(\n                thisLit, pFanin0, pFanin1, pNumFanouts, \n                vCoverTable, vCoverTableLinks, vCoverTableLens, nCoverTableNext,\n                &superLen, nPIs\n            );\n        }\n\n        if (!ret) {\n            // has two opposite edges\n            // note that for constant false covers, the status array is not updated\n\n            // printf(\"constant false triggered! nodeId: %d\\n\", nodeId);\n            // vCoverWiseLens[idx] = 0;\n            // vNodesFilter[idx] = 2; // 2 indicates constant true/false\n            // if (dUtils::AigNodeIsComplement(thisLit)) // constant true\n            //     vNodes[idx] = dUtils::AigConst1;\n            // else                                      // constant false\n            //     vNodes[idx] = dUtils::AigConst0;\n            vNodes[idx] = 0;\n        } else {\n            assert(superLen > 1);\n            assert(superLen <= MAX_LOCAL_COVER_SIZE);\n            // vCoverWiseLens[idx] = superLen;\n            \n            // assign corresponding entries in the status array to 1\n            int currRowIdx, columnPtr;\n            currRowIdx = nodeId;\n            columnPtr = 0;\n\n            int currLit, currId;\n            for (int i = 0; i < superLen; i++) {\n                // change row\n                if (columnPtr == COVER_TABLE_NUM_COLS) {\n                    columnPtr = 0;\n                    currRowIdx = vCoverTableLinks[currRowIdx];\n                }\n                currLit = vCoverTable[currRowIdx * COVER_TABLE_NUM_COLS + columnPtr];\n                columnPtr++;\n                currId = dUtils::AigNodeID(currLit);\n                if (dUtils::AigIsNode(currId, nPIs))\n                    vNodesStatus[currId] = 1;\n                // if (nodeId == 480634) {\n                //     printf(\"%d \", currId);\n                // }\n            }\n            // if (nodeId == 480634)\n            //     printf(\"\\n\");\n        }\n\n    }\n}",
            "__global__ void findLevelNodeRanges(const int * vLastAppearLevel, int * vLastAppearLevelRanges, const int nCovers) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx == nCovers - 1) {\n        printf(\"*** Max Level: %d\\n\", vLastAppearLevel[idx]);\n        vLastAppearLevelRanges[vLastAppearLevel[idx]] = idx + 1;\n    } else if (idx < nCovers - 1) {\n        if (vLastAppearLevel[idx] < vLastAppearLevel[idx + 1]) {\n            vLastAppearLevelRanges[vLastAppearLevel[idx]] = idx + 1;\n        }\n    }\n}",
            "#define uint64 uint64_hack_\n\n\n__device__ __forceinline__ uint32 hash(int x) {\n    return hash((uint32)x);\n}\n\n__device__ __forceinline__ ValueT retrieve_single(const KeyT * ht_keys, const ValueT * ht_values, \n                                  const KeyT key, const int capacity) {\n    KeyT loc = hash(key);\n    loc = loc % capacity;\n\n    while (true) {\n        if (ht_keys[loc] == key) {\n            return ht_values[loc];\n        } else if (ht_keys[loc] == HASHTABLE_EMPTY_KEY<KeyT, ValueT>) {\n            return HASHTABLE_EMPTY_VALUE<KeyT, ValueT>;\n        }\n        loc = (loc + 1) % capacity;\n    }\n}\n\n__device__ __forceinline__ void unbindAndNodeKeys(const uint64 key, uint32 * lit1, uint32 * lit2) {\n    *lit2 = (uint32)(key & 0xffffffffUL);\n    *lit1 = (uint32)(key >> 32);\n}\n\n__global__ void parseOutputRes(const uint64 * vReconstructedKeys, \n                               const uint32 * htOutKeys, const uint32 * htOutValues, const int htOutCapacity,\n                               int * vFanin0New, int * vFanin1New, int * vNumFanoutsNew, \n                               const int nEntries, const int nPIs) {\n    // NOTE vFanin0New, vFanin1New should point to the begin of AND node storage;\n    //      vNumFanoutsNew should point to the begin of PIs + AND node storage!\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < nEntries) {\n        uint64 key = vReconstructedKeys[idx];\n        uint32 oldLit0, oldLit1, oldId0, oldId1, newId0, newId1;\n        unbindAndNodeKeys(key, &oldLit0, &oldLit1);\n\n        oldId0 = dUtils::AigNodeID(oldLit0);\n        oldId1 = dUtils::AigNodeID(oldLit1);\n\n        if (dUtils::AigIsPIConst(oldId0, nPIs))\n            newId0 = oldId0;\n        else\n            newId0 = retrieve_single<uint32, uint32>(htOutKeys, htOutValues, oldId0, htOutCapacity);\n        \n        if (dUtils::AigIsPIConst(oldId1, nPIs))\n            newId1 = oldId1;\n        else\n            newId1 = retrieve_single<uint32, uint32>(htOutKeys, htOutValues, oldId1, htOutCapacity);\n        assert(newId0 != (HASHTABLE_EMPTY_VALUE<uint32, uint32>));\n        assert(newId1 != (HASHTABLE_EMPTY_VALUE<uint32, uint32>));\n\n        vFanin0New[idx] = dUtils::AigNodeLitCond(newId0, dUtils::AigNodeIsComplement(oldLit0));\n        vFanin1New[idx] = dUtils::AigNodeLitCond(newId1, dUtils::AigNodeIsComplement(oldLit1));\n        atomicAdd(&vNumFanoutsNew[newId0], 1);\n        atomicAdd(&vNumFanoutsNew[newId1], 1);\n    }\n}",
            "__device__ __forceinline__ uint32 hash(int x) {\n    return hash((uint32)x);\n}\n\n__device__ __forceinline__ ValueT retrieve_single(const KeyT * ht_keys, const ValueT * ht_values, \n                                  const KeyT key, const int capacity) {\n    KeyT loc = hash(key);\n    loc = loc % capacity;\n\n    while (true) {\n        if (ht_keys[loc] == key) {\n            return ht_values[loc];\n        } else if (ht_keys[loc] == HASHTABLE_EMPTY_KEY<KeyT, ValueT>) {\n            return HASHTABLE_EMPTY_VALUE<KeyT, ValueT>;\n        }\n        loc = (loc + 1) % capacity;\n    }\n}\n\n__global__ void processPO(const int * d_pOuts, const int * vNodeCoverIdMapping, const int * vCoverNodeNewLitMapping, \n                          const uint32 * htOutKeys, const uint32 * htOutValues, const int htOutCapacity,\n                          int * vNewOuts, int * vNumFanoutsNew, const int nPOs, const int nPIs) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < nPOs) {\n        int oldLit = d_pOuts[idx];\n        int oldId = dUtils::AigNodeID(oldLit);\n\n        // increase number of fanout for POs, which ensures consistency with ABC\n        if (dUtils::AigIsPIConst(oldId, nPIs)) {\n            vNewOuts[idx] = oldLit;\n            atomicAdd(&vNumFanoutsNew[oldId], 1);\n        } else {\n            int newLit = vCoverNodeNewLitMapping[vNodeCoverIdMapping[oldId]];\n            assert(newLit != -1);\n            int outId = retrieve_single<uint32, uint32>(htOutKeys, htOutValues, dUtils::AigNodeID(newLit), htOutCapacity);\n            if (outId == (HASHTABLE_EMPTY_VALUE<uint32, uint32>)) {\n                // should be PI or const\n                printf(\"oldId: %d, coverId: %d, newLit: %d\\n\", oldId, vNodeCoverIdMapping[oldId], newLit);\n                outId = dUtils::AigNodeID(newLit);\n                assert(dUtils::AigIsPIConst(outId, nPIs));\n            }\n            assert(outId != (HASHTABLE_EMPTY_VALUE<uint32, uint32>));\n\n            vNewOuts[idx] = dUtils::AigNodeLitCond(outId, dUtils::AigNodeIsComplement(oldLit)^dUtils::AigNodeIsComplement(newLit));\n            atomicAdd(&vNumFanoutsNew[outId], 1);\n        }\n    }\n}",
            "__global__ void printLevelInfo(int * vNodes, int * vNodesFilter, int levelCount, int len, int nPIs, int nPOs) {\n    printf(\"---Level %d global input list [len = %d]---\\n\", levelCount, len);\n    int lit;\n    for (int i = 0; i < len; i++) {\n        lit = vNodes[i];\n        printf(\"%s%d\\t\", dUtils::AigNodeIsComplement(lit) ? \"!\" : \"\", dUtils::AigNodeIDDebug(lit, nPIs, nPOs));\n    }\n    printf(\"\\n\");\n    printf(\"---Level %d filter---\\n\", levelCount);\n    for (int i = 0; i < len; i++) {\n        printf(\"%d\\t\", vNodesFilter[i]);\n    }\n    printf(\"\\n\");\n}",
            "__global__ void printArray(const int * array, const int len) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx == 0) {\n        for (int i = 0; i < len; i++) {\n            printf(\"%d \", array[i]);\n        }\n        printf(\"\\n\");\n    }\n}",
            "__global__ void printLocalReconstructArray(int * vLocalReconstructArrays, int * vLocalReconstructLens, \n                                           const int * vLocalReconstructLevels, const int nReadyCovers,\n                                           const int maxCoverLen, int nPIs, int nPOs) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    int lit;\n    if (idx == 0) {\n        for (int i = 0; i < nReadyCovers; i++) {\n            printf(\"    Cover: \");\n            for (int j = 0; j < vLocalReconstructLens[i]; j++) {\n                lit = vLocalReconstructArrays[i * maxCoverLen + j];\n                printf(\"%s%d,%d \", dUtils::AigNodeIsComplement(lit) ? \"!\" : \"\", \n                       dUtils::AigNodeIDDebug(lit, nPIs, nPOs), vLocalReconstructLevels[i * maxCoverLen + j]);\n            }\n            printf(\"\\n\");\n        }\n    }\n}",
            "__global__ void printCoverTable(const int * vCoverTable, const int * vCoverTableLinks, const int * vCoverTableLens) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    int checklist[] = {480379, 480394, 480396};\n    if (idx == 0) {\n        for (int nodeId : checklist) {\n            printf(\"  Cover of %d: \", nodeId);\n            int length = vCoverTableLens[nodeId];\n            int currRowIdx = nodeId;\n            int columnPtr = 0;\n            int currLit, currId;\n            for (int i = 0; i < length; i++) {\n                if (columnPtr == COVER_TABLE_NUM_COLS) {\n                    columnPtr = 0;\n                    currRowIdx = vCoverTableLinks[currRowIdx];\n                }\n                currLit = vCoverTable[currRowIdx * COVER_TABLE_NUM_COLS + (columnPtr++)];\n                currId = dUtils::AigNodeID(currLit);\n\n                printf(\"%s%d \", dUtils::AigNodeIsComplement(currLit) ? \"!\" : \"\", currId);\n            }\n            printf(\"\\n\");\n        }\n    }\n}"
        ],
        "/Users/gbolet/hecbench-roofline/src/logic-rewrite-cuda/truth.cu": [
            "__global__ void Aig::getElemTruthTable(unsigned * vTruthElem, int nVars) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    unsigned masks[5] = { 0xAAAAAAAA, 0xCCCCCCCC, 0xF0F0F0F0, 0xFF00FF00, 0xFFFF0000 };\n    unsigned * pTruth;\n\n    if (idx == 0) {\n        int nWords = dUtils::TruthWordNum(nVars);\n\n        for (int i = 0; i < nVars; i++) {\n            pTruth = vTruthElem + i * nWords;\n            if (i < 5) {\n                for (int k = 0; k < nWords; k++)\n                    pTruth[k] = masks[i];\n            } else {\n                for (int k = 0; k < nWords; k++) {\n                    if (k & (1 << (i-5)))\n                        pTruth[k] = ~(unsigned)0;\n                    else\n                        pTruth[k] = 0;\n                }\n            }\n        }\n    }\n}"
        ],
        "/Users/gbolet/hecbench-roofline/src/logic-rewrite-cuda/truth.cuh": [
            "#define STACK_SIZE\t(8 * sizeof(unsigned long int))\n\n\n__device__ __forceinline__\nvoid cutTruthIter(int nodeId, const int * pFanin0, const int * pFanin1, \n                  unsigned * vTruthMem, int * visited, int * pVisitedSize, int nWords) {\n    int fanin0Id, fanin1Id, fanin0TruthIdx, fanin1TruthIdx;\n    int lit0, lit1;\n\n    fanin0Id = dUtils::AigNodeID(pFanin0[nodeId]);\n    fanin1Id = dUtils::AigNodeID(pFanin1[nodeId]);\n    fanin0TruthIdx = fanin1TruthIdx = -1;\n    for (int j = 0; j < *pVisitedSize; j++)\n        if (visited[j] == fanin0Id) {\n            fanin0TruthIdx = j;\n            break;\n        }\n    for (int j = 0; j < *pVisitedSize; j++)\n        if (visited[j] == fanin1Id) {\n            fanin1TruthIdx = j;\n            break;\n        }\n    assert(fanin0TruthIdx != -1 && fanin1TruthIdx != -1);\n\n    lit0 = pFanin0[nodeId], lit1 = pFanin1[nodeId];\n    if (!dUtils::AigNodeIsComplement(lit0) && !dUtils::AigNodeIsComplement(lit1))\n        for (int i = 0; i < nWords; i++)\n            vTruthMem[(*pVisitedSize) * nWords + i] = \n                vTruthMem[fanin0TruthIdx * nWords + i] & vTruthMem[fanin1TruthIdx * nWords + i];\n    else if (!dUtils::AigNodeIsComplement(lit0) && dUtils::AigNodeIsComplement(lit1))\n        for (int i = 0; i < nWords; i++)\n            vTruthMem[(*pVisitedSize) * nWords + i] = \n                vTruthMem[fanin0TruthIdx * nWords + i] & ~vTruthMem[fanin1TruthIdx * nWords + i];\n    else if (dUtils::AigNodeIsComplement(lit0) && !dUtils::AigNodeIsComplement(lit1))\n        for (int i = 0; i < nWords; i++)\n            vTruthMem[(*pVisitedSize) * nWords + i] = \n                ~vTruthMem[fanin0TruthIdx * nWords + i] & vTruthMem[fanin1TruthIdx * nWords + i];\n    else\n        for (int i = 0; i < nWords; i++)\n            vTruthMem[(*pVisitedSize) * nWords + i] = \n                ~vTruthMem[fanin0TruthIdx * nWords + i] & ~vTruthMem[fanin1TruthIdx * nWords + i];\n\n    visited[(*pVisitedSize)++] = nodeId;\n}\n\n__device__ __forceinline__\nvoid getCutTruthTableSingle(const int * pFanin0, const int * pFanin1, \n                            unsigned * pTruth, const unsigned * vTruthElem, \n                            const int * pCut, int nVars, int rootId, int nMaxCutSize, int nPIs,\n                            unsigned * vTruthMem = NULL, const int * vNumSaved = NULL) {\n    int nodeId;\n    int fVisited, fAllocated, nWords, nWordsElem, nIntNodes;\n    int stack[STACK_SIZE], stackRes[STACK_SIZE], visited[STACK_SIZE];\n    int stackTop, stackResTop, visitedSize;\n\n    nWords = dUtils::TruthWordNum(nVars);\n    nWordsElem = dUtils::TruthWordNum(nMaxCutSize);\n\n    visitedSize = 0;\n    for (int i = 0; i < nVars; i++) {\n        visited[visitedSize++] = pCut[i];\n    }\n\n    // 1. traversal to collect intermediate nodes\n    stackTop = stackResTop = -1;\n    stack[++stackTop] = rootId;\n    while (stackTop != -1) {\n        // skip if already visited\n        nodeId = stack[stackTop--];\n        fVisited = 0;\n        for (int i = 0; i < visitedSize; i++) {\n            if (visited[i] == nodeId) {\n                fVisited = 1;\n                break;\n            }\n        }\n        if (fVisited) continue;\n\n        assert(dUtils::AigIsNode(nodeId, nPIs));\n        assert(visitedSize < STACK_SIZE);\n        visited[visitedSize++] = nodeId;\n\n        // save result. make sure the nodes in stackRes are in reversed topo order (decreasing id)\n        int j = stackResTop++;\n        for (; j >= 0 && stackRes[j] < nodeId; j--) // insertion sort\n            stackRes[j + 1] = stackRes[j];\n        stackRes[j + 1] = nodeId;\n\n        // push fanins into stack\n        assert(stackTop < STACK_SIZE - 2);\n        stack[++stackTop] = dUtils::AigNodeID(pFanin1[nodeId]);\n        stack[++stackTop] = dUtils::AigNodeID(pFanin0[nodeId]);\n    }\n    nIntNodes = stackResTop + 1;\n    assert(stackRes[0] == rootId);\n    // debug\n    if (vNumSaved != NULL)\n        assert(nIntNodes >= vNumSaved[rootId] && nIntNodes + nVars < STACK_SIZE);\n    else\n        assert(nIntNodes + nVars < STACK_SIZE);\n    \n    // 2. compute truth table\n    // allocate memory for intermediate truth tables, if it is not provided\n    if (vTruthMem == NULL) {\n        vTruthMem = (unsigned *) malloc((nVars + nIntNodes) * nWords * sizeof(unsigned));\n        assert(vTruthMem != NULL); // if NULL, then not enough heap memory\n        fAllocated = 1;\n    } else\n        fAllocated = 0;\n    \n    visitedSize = 0;\n    // collect elementary truth tables for the cut nodes\n    for (int i = 0; i < nVars; i++) {\n        for (int j = 0; j < nWords; j++)\n            vTruthMem[i * nWords + j] = vTruthElem[i * nWordsElem + j];\n        visited[visitedSize++] = pCut[i];\n    }\n    for (int i = stackResTop; i >= 0; i--) {\n        cutTruthIter(stackRes[i], pFanin0, pFanin1, vTruthMem, visited, &visitedSize, nWords);\n    }\n    assert(visited[visitedSize - 1] == rootId);\n\n    // copy the truth table of rootId to pTruth\n    for (int i = 0; i < nWords; i++)\n        pTruth[i] = vTruthMem[(visitedSize - 1) * nWords + i];\n    \n    if (fAllocated)\n        free(vTruthMem);\n}\n\n__global__ void getCutTruthTable(const int * pFanin0, const int * pFanin1, const int * vNumSaved,\n                                 const int * vIndices, const int * vCutTable, const int * vCutSizes, \n                                 unsigned * vTruth, const int * vTruthRanges, const unsigned * vTruthElem,\n                                 int nIndices, int nPIs, int nMaxCutSize) {\n    int nThreads = gridDim.x * blockDim.x;\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    int rootId, nVars;\n    int startIdx;\n\n    for (; idx < nIndices; idx += nThreads) {\n        rootId = vIndices[idx];\n        nVars = vCutSizes[rootId];\n        startIdx = (idx == 0 ? 0 : vTruthRanges[idx - 1]);\n        getCutTruthTableSingle<STACK_SIZE>(pFanin0, pFanin1, vTruth + startIdx, vTruthElem,\n                                           vCutTable + rootId * CUT_TABLE_NUM_COLS, nVars,\n                                           rootId, nMaxCutSize, nPIs, NULL, vNumSaved);\n    }\n}",
            "__device__ __forceinline__\nvoid cutTruthIter(int nodeId, const int * pFanin0, const int * pFanin1, \n                  unsigned * vTruthMem, int * visited, int * pVisitedSize, int nWords) {\n    int fanin0Id, fanin1Id, fanin0TruthIdx, fanin1TruthIdx;\n    int lit0, lit1;\n\n    fanin0Id = dUtils::AigNodeID(pFanin0[nodeId]);\n    fanin1Id = dUtils::AigNodeID(pFanin1[nodeId]);\n    fanin0TruthIdx = fanin1TruthIdx = -1;\n    for (int j = 0; j < *pVisitedSize; j++)\n        if (visited[j] == fanin0Id) {\n            fanin0TruthIdx = j;\n            break;\n        }\n    for (int j = 0; j < *pVisitedSize; j++)\n        if (visited[j] == fanin1Id) {\n            fanin1TruthIdx = j;\n            break;\n        }\n    assert(fanin0TruthIdx != -1 && fanin1TruthIdx != -1);\n\n    lit0 = pFanin0[nodeId], lit1 = pFanin1[nodeId];\n    if (!dUtils::AigNodeIsComplement(lit0) && !dUtils::AigNodeIsComplement(lit1))\n        for (int i = 0; i < nWords; i++)\n            vTruthMem[(*pVisitedSize) * nWords + i] = \n                vTruthMem[fanin0TruthIdx * nWords + i] & vTruthMem[fanin1TruthIdx * nWords + i];\n    else if (!dUtils::AigNodeIsComplement(lit0) && dUtils::AigNodeIsComplement(lit1))\n        for (int i = 0; i < nWords; i++)\n            vTruthMem[(*pVisitedSize) * nWords + i] = \n                vTruthMem[fanin0TruthIdx * nWords + i] & ~vTruthMem[fanin1TruthIdx * nWords + i];\n    else if (dUtils::AigNodeIsComplement(lit0) && !dUtils::AigNodeIsComplement(lit1))\n        for (int i = 0; i < nWords; i++)\n            vTruthMem[(*pVisitedSize) * nWords + i] = \n                ~vTruthMem[fanin0TruthIdx * nWords + i] & vTruthMem[fanin1TruthIdx * nWords + i];\n    else\n        for (int i = 0; i < nWords; i++)\n            vTruthMem[(*pVisitedSize) * nWords + i] = \n                ~vTruthMem[fanin0TruthIdx * nWords + i] & ~vTruthMem[fanin1TruthIdx * nWords + i];\n\n    visited[(*pVisitedSize)++] = nodeId;\n}\n\n__global__ void getCutTruthTableConsecutive(const int * pFanin0, const int * pFanin1, const int * vNumSaved,\n                                            const int * vIndices, const int * vCuts, const int * vCutRanges,\n                                            unsigned * vTruth, const int * vTruthRanges, const unsigned * vTruthElem,\n                                            int nIndices, int nPIs, int nMaxCutSize, int * vNode2ConeResynIdx = NULL) {\n    typedef cub::WarpScan<int> WarpScan;\n    __shared__ typename WarpScan::TempStorage temp_storage[THREAD_PER_BLOCK / 32];\n    __shared__ unsigned * vTruthMemAlloc[THREAD_PER_BLOCK / 32];\n    \n    int nThreads = NUM_BLOCKS(nIndices, 32) * 32;\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    int warpIdx = threadIdx.x / 32, laneIdx = threadIdx.x % 32;\n    int stack[STACK_SIZE], stackRes[STACK_SIZE], visited[STACK_SIZE];\n    int stackTop, stackResTop, visitedSize;\n    int nodeId, rootId;\n    int fVisited, nWords, nWordsElem, nVars, nIntNodes;\n    unsigned * vTruthMem;\n    int startIdx, endIdx;\n    int localMemLen, totalMemLen = -1, startMemIdx = -1;\n\n    assert(nIndices <= nThreads && nThreads - nIndices < 32);\n\n    if (idx < nThreads) {\n        if (idx < nIndices) {\n            rootId = vIndices[idx];\n            startIdx = (idx == 0 ? 0 : vCutRanges[idx - 1]);\n            endIdx = vCutRanges[idx];\n            nVars = endIdx - startIdx;\n            nWords = dUtils::TruthWordNum(nVars);\n            nWordsElem = dUtils::TruthWordNum(nMaxCutSize);\n\n            // set the leaves as visited\n            visitedSize = 0;\n            for (int i = 0; i < nVars; i++) {\n                visited[visitedSize++] = vCuts[startIdx + i];\n            }\n\n            // 1. traversal to collect intermediate nodes\n            stackTop = stackResTop = -1;\n            stack[++stackTop] = rootId;\n            while (stackTop != -1) {\n                // skip if already visited\n                nodeId = stack[stackTop--];\n                fVisited = 0;\n                for (int i = 0; i < visitedSize; i++) {\n                    if (visited[i] == nodeId) {\n                        fVisited = 1;\n                        break;\n                    }\n                }\n                if (fVisited) continue;\n\n                assert(dUtils::AigIsNode(nodeId, nPIs));\n                assert(visitedSize < STACK_SIZE);\n                visited[visitedSize++] = nodeId;\n\n                // mark the node if vNode2ConeResynIdx is provided\n                if (vNode2ConeResynIdx && nodeId != rootId) {\n                    vNode2ConeResynIdx[nodeId] = idx;\n                }\n\n                // save result. make sure the nodes in stackRes are in reversed topo order (decreasing id)\n                int j = stackResTop++;\n                for (; j >= 0 && stackRes[j] < nodeId; j--) // insertion sort\n                    stackRes[j + 1] = stackRes[j];\n                stackRes[j + 1] = nodeId;\n\n                // push fanins into stack\n                assert(stackTop < STACK_SIZE - 2);\n                stack[++stackTop] = dUtils::AigNodeID(pFanin1[nodeId]);\n                stack[++stackTop] = dUtils::AigNodeID(pFanin0[nodeId]);\n            }\n            nIntNodes = stackResTop + 1;\n            assert(nIntNodes >= vNumSaved[rootId] && nIntNodes + nVars < STACK_SIZE);\n            assert(stackRes[0] == rootId);\n        }\n\n        // 2. compute truth table\n        // allocate memory for intermediate truth tables\n        // allocate once per warp to reduce overhead\n        localMemLen = (idx < nIndices ? (nVars + nIntNodes) * nWords : 0);\n        WarpScan(temp_storage[warpIdx]).ExclusiveSum(localMemLen, startMemIdx, totalMemLen);\n        assert(startMemIdx != -1 && totalMemLen != -1);\n\n        if (laneIdx == 0) {\n            vTruthMemAlloc[warpIdx] = (unsigned *) malloc(totalMemLen * sizeof(unsigned));\n            assert(vTruthMemAlloc[warpIdx] != NULL); // if NULL, then not enough heap memory\n        }\n        __syncwarp();\n        vTruthMem = vTruthMemAlloc[warpIdx] + startMemIdx;\n        \n\n        if (idx < nIndices) {\n            visitedSize = 0;\n            // collect elementary truth tables for the cut nodes\n            for (int i = 0; i < nVars; i++) {\n                for (int j = 0; j < nWords; j++)\n                    vTruthMem[i * nWords + j] = vTruthElem[i * nWordsElem + j];\n                visited[visitedSize++] = vCuts[startIdx + i];\n            }\n            for (int i = stackResTop; i >= 0; i--) {\n                cutTruthIter(stackRes[i], pFanin0, pFanin1, vTruthMem, visited, &visitedSize, nWords);\n            }\n            assert(visited[visitedSize - 1] == rootId);\n\n            // copy the truth table of rootId to vTruth\n            startIdx = (idx == 0 ? 0 : vTruthRanges[idx - 1]);\n            endIdx = vTruthRanges[idx];\n            assert(endIdx - startIdx == nWords);\n            for (int i = 0; i < nWords; i++)\n                vTruth[startIdx + i] = vTruthMem[(visitedSize - 1) * nWords + i];\n        }\n\n        __syncwarp();\n        if (laneIdx == 0)\n            free(vTruthMemAlloc[warpIdx]);\n    }\n}"
        ],
        "/Users/gbolet/hecbench-roofline/src/logic-rewrite-cuda/refactor_mffc.cu": [
            "__global__ void printMffcCut(int * vCutTable, int * vCutSizes, int * vConeSizes,\n                             const int * pFanin0, const int * pFanin1, \n                             int nNodes, int nPIs, int nPOs) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx != 0)\n        return;\n\n    int counter = 0;\n    for (int i = 0; i < nNodes; i++) {\n        int id = i + nPIs + 1;\n        if (vCutSizes[id] == -1)\n            continue;\n        counter++;\n        \n        printf(\"root: %d, cone size: %d | \", id, vConeSizes[id]);\n        for (int j = 0; j < vCutSizes[id]; j++) {\n            printf(\"%d \", vCutTable[id * CUT_TABLE_SIZE + j]);\n        }\n        printf(\"\\n\");\n    }\n    printf(\"Total number of MFFCs: %d\\n\", counter);\n}",
            "#define STACK_SIZE\t(8 * sizeof(unsigned long int))\n\n\n__global__ void recordMFFC(const int * vRoots, \n                           const int * pFanin0, const int * pFanin1, \n                           const int * pNumFanouts, const int * pLevels, \n                           int * vCutTable, int * vCutSizes, int * vConeSizes, \n                           int nPIs, int nMaxCutSize, int nRoots) {\n    int nThreads = gridDim.x * blockDim.x;\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    int rootId, nSaved;\n\n    for (; idx < nRoots; idx += nThreads) {\n        rootId = vRoots[idx];\n\n        nSaved = Aig::findReconvMFFCCut<CUT_TABLE_SIZE, STACK_SIZE, useHashtable>(\n            rootId, pFanin0, pFanin1, pNumFanouts, pLevels, \n            vCutTable, vCutSizes, nPIs, nMaxCutSize\n        );\n        vConeSizes[rootId] = nSaved;\n    }\n}",
            "__global__ void setStatus(const int * vRoots,\n                          const int * vCutTable, const int * vCutSizes,\n                          int * vNodesStatus,\n                          int nPIs, int nRoots) {\n    // vNodesStatus should be set to all zeros before launching this kernel\n    int nThreads = gridDim.x * blockDim.x;\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    int rootId, nodeId, nCutSize;\n\n    for (; idx < nRoots; idx += nThreads) {\n        rootId = vRoots[idx];\n\n        const int * vCutRoot = &vCutTable[rootId * CUT_TABLE_SIZE];\n        nCutSize = vCutSizes[rootId];\n        for (int i = 0; i < nCutSize; i++) {\n            nodeId = vCutRoot[i];\n            // update the status array if the MFFC rooted at the node is not explored\n            if (dUtils::AigIsNode(nodeId, nPIs) && vCutSizes[nodeId] == -1)\n                vNodesStatus[nodeId] = 1;\n        }\n    }\n}",
            "__global__ void getCutTruthRanges(const int * vResynRoots, const int * vCutSizes, \n                                  int * vCutRanges, int * vTruthRanges, int nResyn) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < nResyn) {\n        int nodeId = vResynRoots[idx];\n        int cutSize = vCutSizes[nodeId];\n        assert(cutSize > 0);\n        vCutRanges[idx] = cutSize;\n        vTruthRanges[idx] = dUtils::TruthWordNum(cutSize);\n    }\n}",
            "#define uint64 uint64_hack_\n\n\ninline __host__ __device__ uint4 make_uint4(int4 a)\n{\n    return make_uint4(uint(a.x), uint(a.y), uint(a.z), uint(a.w));\n}\n\ninline __host__ __device__ uint4 max(uint4 a, uint4 b)\n{\n    return make_uint4(max(a.x,b.x), max(a.y,b.y), max(a.z,b.z), max(a.w,b.w));\n}\n\n__device__ int evalSubgNumAdded(int rootId, int * pNewRootLevel, const int * vCurrCut, int nVars, \n                                int nNodeMax, int nLevelMax, const int * pLevels, const int * vNode2ConeResynIdx,\n                                const uint64 * htKeys, const uint32 * htValues, int htCapacity,\n                                const uint64 * vSubgTable, const int * vSubgLinks, const int * vSubgLens, \n                                int subgIdx) {\n    int i, counter, temp;\n    int lit0, lit1, id0, id1, func0, func1, currId, fCompRoot, newLevel;\n    uint32 retrId;\n    int vFuncs[SUBG_CAP], vLevels[SUBG_CAP];\n    int length = vSubgLens[subgIdx] + nVars;\n    int currRowIdx, columnPtr;\n\n    assert(vSubgLens[subgIdx] > 0);\n\n    // check the case of the resyned cut is a const or a single var of cut nodes\n    if (vSubgLens[subgIdx] == 1) {\n        subgUtil::unbindAndNodeKeyFlag(vSubgTable[subgIdx * SUBG_TABLE_SIZE], &lit0, &lit1, &fCompRoot);\n        if (lit0 == lit1)\n            return 0;\n    }\n\n    // initialize funcs (ids) and levels for the leaves\n    for (i = 0; i < nVars; i++) {\n        vFuncs[i] = vCurrCut[i];\n        vLevels[i] = pLevels[vCurrCut[i]];\n    }\n\n    counter = 0;\n    currRowIdx = subgIdx, columnPtr = 0;\n    for (i = nVars; i < length; i++) {\n        if (columnPtr == SUBG_TABLE_SIZE) {\n            // expand a new row\n            columnPtr = 0;\n            currRowIdx = vSubgLinks[currRowIdx];\n        }\n        // get the children of the current subgraph node\n        subgUtil::unbindAndNodeKeyFlag(\n            vSubgTable[currRowIdx * SUBG_TABLE_SIZE + (columnPtr++)], \n            &lit0, &lit1, &fCompRoot\n        );\n        assert(lit0 < lit1);\n        id0 = dUtils::AigNodeID(lit0), id1 = dUtils::AigNodeID(lit1);\n        assert(id0 < i && id1 < i);\n        func0 = vFuncs[id0], func1 = vFuncs[id1]; // ids of its children in the original AIG\n\n        // if they are both present, find the resulting node in hashtable\n        if (func0 != -1 && func1 != -1) {\n            func0 = dUtils::AigNodeLitCond(func0, dUtils::AigNodeIsComplement(lit0));\n            func1 = dUtils::AigNodeLitCond(func1, dUtils::AigNodeIsComplement(lit1));\n            if (func0 > func1) // though they are properly ordered in subgraph id, in AIG id they may not\n                temp = func0, func0 = func1, func1 = temp;\n            // if (func0 >= func1) {\n            //     printf(\"func0: %d, func1: %d\\n\", func0, func1);\n            //     printf(\"  cuts: \");\n            //     for (int j = 0; j < nVars; j++)\n            //         printf(\"%d \", vCurrCut[j]);\n            //     printf(\"\\n  subg: \");\n            //     int currRowIdx0 = subgIdx, columnPtr0 = 0;\n            //     for (int j = 0; j < vSubgLens[subgIdx]; j++) {\n            //         if (columnPtr0 == SUBG_TABLE_SIZE) {\n            //             // expand a new row\n            //             columnPtr0 = 0;\n            //             currRowIdx0 = vSubgLinks[currRowIdx0];\n            //         }\n            //         subgUtil::unbindAndNodeKeyFlag(\n            //             vSubgTable[currRowIdx0 * SUBG_TABLE_SIZE + (columnPtr0++)], \n            //             &lit0, &lit1, &fCompRoot\n            //         );\n            //         printf(\"%s%d,%s%d \", dUtils::AigNodeIsComplement(lit0) ? \"!\" : \"\", dUtils::AigNodeID(lit0),\n            //                              dUtils::AigNodeIsComplement(lit1) ? \"!\" : \"\", dUtils::AigNodeID(lit1));\n            //     }\n            //     printf(\"\\n\");\n            // }\n            assert(func0 <= func1);\n\n            retrId = Aig::retrieveHashTableCheckTrivial(func0, func1, htKeys, htValues, htCapacity);\n            if (retrId == (HASHTABLE_EMPTY_VALUE<uint64, uint32>))\n                currId = -1;\n            else\n                currId = (int)retrId;\n            // return -1 if the node is the same as the original root\n            if (retrId == (uint32)rootId)\n                return -1;\n        } else\n            currId = -1;\n        \n        // count one new node\n        // nodes whose vNode2ConeResynIdx are assigned are MFFC nodes and are to be removed,\n        // so do not count shareable logic with them\n        if (currId == -1 || vNode2ConeResynIdx[currId] != -1) {\n            if (++counter > nNodeMax)\n                return -1;\n        }\n        // count new level\n        newLevel = 1 + max(vLevels[id0], vLevels[id1]);\n        if (currId != -1) {\n            // previously went though the hashtable retrival\n            if (currId == 0) // const 0/1\n                newLevel = 0;\n            else if (currId == dUtils::AigNodeID(func0))\n                newLevel = vLevels[id0];\n            else if (currId == dUtils::AigNodeID(func1))\n                newLevel = vLevels[id1];\n        }\n        if (newLevel > nLevelMax)\n            return -1;\n        \n        // save new func (id) and level\n        vFuncs[i] = currId;\n        vLevels[i] = newLevel;\n    }\n    *pNewRootLevel = vLevels[length - 1];\n    return counter;\n}\n\n__global__ void evalFactoredForm(const int * vResynRoots, const int * vCuts, const int * vCutRanges,\n                                 const int * vNumSaved, const int * pLevels, const int * vNode2ConeResynIdx, \n                                 const uint64 * htKeys, const uint32 * htValues, int htCapacity,\n                                 const uint64 * vSubgTable, const int * vSubgLinks, const int * vSubgLens, \n                                 int * vSelectedSubgInd, int nResyn) {\n    int nThreads = gridDim.x * blockDim.x;\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    int coneIdx, rootId;\n    int nVars, nSaved, nAdded, nOtherAdded, nNewLevel, nOtherNewLevel;\n    int startIdx, endIdx;\n    unsigned warpMask;\n    int fSelectedSubg;\n\n    for (; idx < 2 * nResyn; idx += nThreads) {\n        warpMask = __activemask();\n        coneIdx = idx >> 1;\n        rootId = vResynRoots[coneIdx];\n        \n        startIdx = (coneIdx == 0 ? 0 : vCutRanges[coneIdx - 1]);\n        endIdx = vCutRanges[coneIdx];\n        nVars = endIdx - startIdx;\n        nSaved = vNumSaved[rootId];\n\n        nAdded = evalSubgNumAdded(\n            rootId, &nNewLevel, vCuts + startIdx, \n            nVars, nSaved, 1000000000, pLevels, vNode2ConeResynIdx, \n            htKeys, htValues, htCapacity, vSubgTable, vSubgLinks, vSubgLens, idx\n        );\n        nOtherAdded = __shfl_xor_sync(warpMask, nAdded, 1); // nAdded of the corresponding negated subgraph\n        nOtherNewLevel = __shfl_xor_sync(warpMask, nNewLevel, 1);\n        \n        if (idx % 2 == 0) {\n            // select a better subgraph among the pair\n            fSelectedSubg = -1;\n            if (nAdded > -1)\n                fSelectedSubg = 0;\n            if (nOtherAdded > -1) {\n                if (nAdded == -1)\n                    fSelectedSubg = 1;\n                else if (nOtherAdded < nAdded || (nOtherAdded == nAdded && \n                         (vSubgLens[idx + 1] == 1 || nOtherNewLevel < nNewLevel)))\n                    fSelectedSubg = 1;\n            }\n\n            // write to vSelectedSubgInd\n            vSelectedSubgInd[coneIdx] = (fSelectedSubg == -1 ? -1 : idx + fSelectedSubg);\n        }\n    }\n\n}",
            "#define uint64 uint64_hack_\n\n\n__device__ __forceinline__ uint32 hash(int x) {\n    return hash((uint32)x);\n}\n\n__device__ __forceinline__ uint32 insert_single_no_update(KeyT * ht_keys, ValueT * ht_values, \n                                        const KeyT key, const ValueT value, const int capacity) {\n    KeyT loc = hash(key);\n    loc = loc % capacity;\n\n    while (true) {\n        KeyT prev = atomicCAS(&ht_keys[loc], HASHTABLE_EMPTY_KEY<KeyT, ValueT>, key);\n        if (prev == HASHTABLE_EMPTY_KEY<KeyT, ValueT>) {\n            // found a empty entry\n            ht_values[loc] = value;\n            return 2;\n        } else if (prev == key) {\n            // already have key inserted, no update\n            return ht_values[loc];\n        }\n\n        loc = (loc + 1) % capacity;\n    }\n}\n\n__global__ void duplicateHashTableWithoutMFFCs(const int * vNode2ConeResynIdx, const int * vSelectedSubgInd,\n                                               const uint64 * htKeys, const uint32 * htValues, int htCapacity,\n                                               uint64 * htDestKeys, uint32 * htDestValues, int htDestCapacity) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < htCapacity && htKeys[idx] != HASHTABLE_EMPTY_KEY<uint64, uint32>) {\n        int nodeId = htValues[idx];\n        int coneResynIdx = vNode2ConeResynIdx[nodeId];\n        // in two cases the cone nodes should be kept: \n        // (1) the cone is not a explored MFFC, (2) the corresponding two resyned graphs are not better than\n        // the original cone (i.e. vSelectedSubgInd == -1)\n        if (coneResynIdx == -1 || vSelectedSubgInd[coneResynIdx] == -1) {\n            insert_single_no_update<uint64, uint32>(htDestKeys, htDestValues, \n                                                    htKeys[idx], htValues[idx], htDestCapacity);\n        }\n    }\n}",
            "#define uint64 uint64_hack_\n\n\n__global__ void checkSingleVarSubg(const uint64 * vSubgTable, const int * vSubgLinks, const int * vSubgLens,\n                                   const int * vResynRoots, const int * vSelectedSubgInd, int * vOldRoot2NewRootLits, \n                                   int * vFinishedMark, int nObjs, int nResyn) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < nResyn) {\n        int subgIdx = vSelectedSubgInd[idx];\n        int rootId = vResynRoots[idx];\n        int lit0, lit1, fComp;\n        \n        if (subgIdx == -1) {\n            // both of the two graphs are not better than the original one\n            vFinishedMark[idx] = 1;\n        } else if (vSubgLens[subgIdx] == 1) {\n            // take care of the case that the resyned cut is a const or a single var of cut nodes\n            subgUtil::unbindAndNodeKeyFlag(vSubgTable[subgIdx * SUBG_TABLE_SIZE], &lit0, &lit1, &fComp);\n            if (lit0 == lit1) {\n                assert(dUtils::AigNodeID(lit0) < nObjs); // in this case lit0 is using the AIG id\n                assert(fComp == dUtils::AigNodeIsComplement(lit0));\n\n                vOldRoot2NewRootLits[rootId] = lit0;\n                vFinishedMark[idx] = 1;\n            }\n        }\n    }\n}",
            "#define uint64 uint64_hack_\n\n\n__device__ __forceinline__ uint32 hash(int x) {\n    return hash((uint32)x);\n}\n\n__device__ __forceinline__ uint32 checkTrivialAndCases(int lit1, int lit2) {\n    if (lit1 == lit2)\n        return (uint32)lit1;\n    if (lit1 == dUtils::AigNodeNot(lit2))\n        return (uint32)dUtils::AigConst0;\n    if (lit1 == dUtils::AigConst1)\n        return (uint32)lit2;\n    if (lit2 == dUtils::AigConst1)\n        return (uint32)lit1;\n    if (lit1 == dUtils::AigConst0 || lit2 == dUtils::AigConst0)\n        return (uint32)dUtils::AigConst0;\n    \n    // non-trivial\n    return HASHTABLE_EMPTY_VALUE<uint64, uint32>;\n}\n\n__device__ __forceinline__ uint64 formAndNodeKey(const int lit1, const int lit2) {\n    // make sure lit1 is smaller than lit2\n    uint32 uLit1 = (uint32)lit1;\n    uint32 uLit2 = (uint32)lit2;\n    uint64 key = ((uint64)uLit1) << 32 | uLit2;\n    return key;\n}\n\n__device__ __forceinline__ uint32 insert_single_no_update(KeyT * ht_keys, ValueT * ht_values, \n                                        const KeyT key, const ValueT value, const int capacity) {\n    KeyT loc = hash(key);\n    loc = loc % capacity;\n\n    while (true) {\n        KeyT prev = atomicCAS(&ht_keys[loc], HASHTABLE_EMPTY_KEY<KeyT, ValueT>, key);\n        if (prev == HASHTABLE_EMPTY_KEY<KeyT, ValueT>) {\n            // found a empty entry\n            ht_values[loc] = value;\n            return 2;\n        } else if (prev == key) {\n            // already have key inserted, no update\n            return ht_values[loc];\n        }\n\n        loc = (loc + 1) % capacity;\n    }\n}\n\n__device__ __forceinline__ void unbindAndNodeKeys(const uint64 key, uint32 * lit1, uint32 * lit2) {\n    *lit2 = (uint32)(key & 0xffffffffUL);\n    *lit1 = (uint32)(key >> 32);\n}\n\n__global__ void insertSubgIter(int iter, const int * vResynIdSeq,\n                               const int * vCuts, const int * vCutRanges,\n                               uint64 * htDestKeys, uint32 * htDestValues, int htDestCapacity,\n                               uint64 * vSubgTable, const int * vSubgLinks, const int * vSubgLens,\n                               const int * vSelectedSubgInd, int idCounter, int nReplace) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < nReplace) {\n        int startIdx, endIdx, nVars;\n        int lit0, lit1, id0, id1, fanin0, fanin1, fComp, temp;\n        uint32 temp0, temp1;\n        uint64 key;\n        int currRowIdx, columnPtr;\n        int subgRows[SUBG_CAP / SUBG_TABLE_SIZE];\n        int resynIdx = vResynIdSeq[idx];\n        int subgIdx = vSelectedSubgInd[resynIdx];\n\n        startIdx = (resynIdx == 0 ? 0 : vCutRanges[resynIdx - 1]);\n        endIdx = vCutRanges[resynIdx];\n        nVars = endIdx - startIdx;\n        const int * vCurrCut = vCuts + startIdx;\n\n        assert(iter < vSubgLens[subgIdx]);\n        assert(iter < SUBG_CAP);\n        // fetch the iter-th node of the subgraph\n        currRowIdx = subgIdx, columnPtr = iter % SUBG_TABLE_SIZE;\n        subgRows[0] = currRowIdx;\n        for (int i = 0; i < (iter / SUBG_TABLE_SIZE); i++) {\n            currRowIdx = vSubgLinks[currRowIdx];\n            subgRows[i + 1] = currRowIdx;\n        }\n        subgUtil::unbindAndNodeKeyFlag(vSubgTable[currRowIdx * SUBG_TABLE_SIZE + columnPtr], \n                                       &lit0, &lit1, &fComp);\n        id0 = dUtils::AigNodeID(lit0), id1 = dUtils::AigNodeID(lit1);\n\n        // convert lit0/1 into AIG ids\n        if (id0 < nVars) {\n            fanin0 = vCurrCut[id0]; // cut saves id\n            fanin0 = dUtils::AigNodeLitCond(fanin0, dUtils::AigNodeIsComplement(lit0));\n        } else {\n            id0 -= nVars;\n            assert(id0 < iter);\n            currRowIdx = subgRows[id0 / SUBG_TABLE_SIZE], columnPtr = id0 % SUBG_TABLE_SIZE;\n            unbindAndNodeKeys(vSubgTable[currRowIdx * SUBG_TABLE_SIZE + columnPtr], &temp0, &temp1);\n            assert(temp0 == 0); // has already been processed in previous iterations\n            fanin0 = (int)temp1; // temp1 saves lit instead of id\n            fanin0 = dUtils::AigNodeNotCond(fanin0, dUtils::AigNodeIsComplement(lit0));\n        }\n        if (id1 < nVars) {\n            fanin1 = vCurrCut[id1]; // cut saves id\n            fanin1 = dUtils::AigNodeLitCond(fanin1, dUtils::AigNodeIsComplement(lit1));\n        } else {\n            id1 -= nVars;\n            assert(id1 < iter);\n            currRowIdx = subgRows[id1 / SUBG_TABLE_SIZE], columnPtr = id1 % SUBG_TABLE_SIZE;\n            unbindAndNodeKeys(vSubgTable[currRowIdx * SUBG_TABLE_SIZE + columnPtr], &temp0, &temp1);\n            assert(temp0 == 0); // has already been processed in previous iterations\n            fanin1 = (int)temp1; // temp1 saves lit instead of id\n            fanin1 = dUtils::AigNodeNotCond(fanin1, dUtils::AigNodeIsComplement(lit1));\n        }\n        if (fanin0 > fanin1) // though they are properly ordered in subgraph id, in AIG id they may not\n            temp = fanin0, fanin0 = fanin1, fanin1 = temp;\n\n        // check trivial\n        temp0 = checkTrivialAndCases(fanin0, fanin1);\n        if (temp0 == HASHTABLE_EMPTY_VALUE<uint64, uint32>) {\n            // non-trivial, insert into hashtable\n            assert(fanin0 < fanin1);\n            key = formAndNodeKey(fanin0, fanin1);\n            // assign new (tentative) id as idCounter + idx, which is unique\n            insert_single_no_update<uint64, uint32>(htDestKeys, htDestValues, key, \n                                                    (uint32)(idCounter + idx), htDestCapacity);\n        }\n        // save the converted key into the corresponding location in vSubgTable\n        key = subgUtil::formAndNodeKeyFlag(fanin0, fanin1, fComp);\n        currRowIdx = subgRows[iter / SUBG_TABLE_SIZE], columnPtr = iter % SUBG_TABLE_SIZE;\n        vSubgTable[currRowIdx * SUBG_TABLE_SIZE + columnPtr] = key;\n    }\n}",
            "#define uint64 uint64_hack_\n\n\n__device__ __forceinline__ uint32 hash(int x) {\n    return hash((uint32)x);\n}\n\n__device__ __forceinline__ uint32 checkTrivialAndCases(int lit1, int lit2) {\n    if (lit1 == lit2)\n        return (uint32)lit1;\n    if (lit1 == dUtils::AigNodeNot(lit2))\n        return (uint32)dUtils::AigConst0;\n    if (lit1 == dUtils::AigConst1)\n        return (uint32)lit2;\n    if (lit2 == dUtils::AigConst1)\n        return (uint32)lit1;\n    if (lit1 == dUtils::AigConst0 || lit2 == dUtils::AigConst0)\n        return (uint32)dUtils::AigConst0;\n    \n    // non-trivial\n    return HASHTABLE_EMPTY_VALUE<uint64, uint32>;\n}\n\n__device__ __forceinline__ uint64 formAndNodeKey(const int lit1, const int lit2) {\n    // make sure lit1 is smaller than lit2\n    uint32 uLit1 = (uint32)lit1;\n    uint32 uLit2 = (uint32)lit2;\n    uint64 key = ((uint64)uLit1) << 32 | uLit2;\n    return key;\n}\n\n__device__ __forceinline__ ValueT retrieve_single(const KeyT * ht_keys, const ValueT * ht_values, \n                                  const KeyT key, const int capacity) {\n    KeyT loc = hash(key);\n    loc = loc % capacity;\n\n    while (true) {\n        if (ht_keys[loc] == key) {\n            return ht_values[loc];\n        } else if (ht_keys[loc] == HASHTABLE_EMPTY_KEY<KeyT, ValueT>) {\n            return HASHTABLE_EMPTY_VALUE<KeyT, ValueT>;\n        }\n        loc = (loc + 1) % capacity;\n    }\n}\n\n__global__ void updateInsertedIdsIter(int iter, const int * vResynRoots, const int * vResynIdSeq,\n                                      const uint64 * htDestKeys, const uint32 * htDestValues, int htDestCapacity,\n                                      uint64 * vSubgTable, const int * vSubgLinks, const int * vSubgLens,\n                                      const int * vSelectedSubgInd, int * vOldRoot2NewRootLits, int * vFinishedMark, \n                                      int nReplace) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < nReplace) {\n        int fanin0, fanin1, fComp;\n        uint32 temp0;\n        uint64 key;\n        int currRowIdx, columnPtr;\n        int resynIdx = vResynIdSeq[idx];\n        int subgIdx = vSelectedSubgInd[resynIdx];\n        int oldRootId = vResynRoots[resynIdx];\n\n        assert(iter < vSubgLens[subgIdx]);\n        assert(iter < SUBG_CAP);\n        // fetch the iter-th node of the subgraph\n        currRowIdx = subgIdx, columnPtr = iter % SUBG_TABLE_SIZE;\n        for (int i = 0; i < (iter / SUBG_TABLE_SIZE); i++)\n            currRowIdx = vSubgLinks[currRowIdx];\n        subgUtil::unbindAndNodeKeyFlag(vSubgTable[currRowIdx * SUBG_TABLE_SIZE + columnPtr], \n                                       &fanin0, &fanin1, &fComp);\n        \n        // check trivial\n        temp0 = checkTrivialAndCases(fanin0, fanin1);\n        if (temp0 == HASHTABLE_EMPTY_VALUE<uint64, uint32>) {\n            // non-trivial, retrieve the corresponding id from hashtable\n            assert(fanin0 < fanin1);\n            key = formAndNodeKey(fanin0, fanin1);\n            temp0 = retrieve_single<uint64, uint32>(htDestKeys, htDestValues, key, htDestCapacity); // id\n            temp0 = temp0 << 1; // convert to lit\n        }\n        // save the updated literal of current node into the corresponding location in vSubgTable\n        // mark the first entry as 0 to indicate that this key represents the literal of current node\n        key = formAndNodeKey(0, temp0);\n        vSubgTable[currRowIdx * SUBG_TABLE_SIZE + columnPtr] = key;\n\n        // deal with finished subgraphs\n        if (iter == vSubgLens[subgIdx] - 1) {\n            vFinishedMark[idx] = 1;\n            // save the new root literal\n            vOldRoot2NewRootLits[oldRootId] = dUtils::AigNodeNotCond(temp0, fComp);\n        }\n    }\n}",
            "__global__ void checkInsertion(const int * vResynRoots, const int * vOldRoot2NewRootLits, \n                               const int * vSelectedSubgInd, int nResyn) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < nResyn) {\n        int subgIdx = vSelectedSubgInd[idx];\n        int rootId = vResynRoots[idx];\n        if (subgIdx != -1) {\n            assert(vOldRoot2NewRootLits[rootId] != -1);\n        }\n    }\n}",
            "#define uint64 uint64_hack_\n\n\n__device__ __forceinline__ void unbindAndNodeKeys(const uint64 key, uint32 * lit1, uint32 * lit2) {\n    *lit2 = (uint32)(key & 0xffffffffUL);\n    *lit1 = (uint32)(key >> 32);\n}\n\n__global__ void unbindKeysUpdateOldRoots(const int * vOldRoot2NewRootLits, \n                                         const uint64 * vReconstructedKeys, const uint32 * vReconstructedIds,\n                                         int * vFanin0New, int * vFanin1New, int nEntries, int nObjs, int nBufferLen) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < nEntries) {\n        uint32 lit0, lit1, nodeId;\n        int newLit0, newLit1;\n        unbindAndNodeKeys(vReconstructedKeys[idx], &lit0, &lit1);\n        nodeId = vReconstructedIds[idx];\n        assert(nodeId < nBufferLen);\n        if (nodeId < nObjs && vOldRoot2NewRootLits[nodeId] != -1) {\n            // nodeId is a reconstructed old root\n            // convert this old root node into a buffer of the new root literal\n            newLit0 = dUtils::AigConst1;\n            newLit1 = vOldRoot2NewRootLits[nodeId];\n        } else {\n            newLit0 = (int)lit0, newLit1 = (int)lit1;\n        }\n\n        vFanin0New[nodeId] = newLit0;\n        vFanin1New[nodeId] = newLit1;\n    }\n}"
        ],
        "/Users/gbolet/hecbench-roofline/src/logic-rewrite-cuda/print.cu": [
            "__global__ void printAIGA(const int * pFanin0, const int * pFanin1, \n                         const int * pOuts, int nPIs, int nPOs, int nObjs) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx != 0)\n        return;\n    \n    printf(\"id\\tfanin0\\tfanin1\\n\");\n    \n    for (int i = nPIs + 1; i < nObjs; i++) {\n        int lit1 = pFanin0[i];\n        int lit2 = pFanin1[i];\n\n        printf(\"%d\\t\", i);\n        printf(\"%s%d\\t\", (lit1 & 1) ? \"!\" : \"\", lit1 >> 1);\n        printf(\"%s%d\\n\", (lit2 & 1) ? \"!\" : \"\", lit2 >> 1);\n    }\n    for (int i = 0; i < nPOs; i++) {\n        int lit = pOuts[i];\n        int id = lit >> 1;\n        printf(\"%s%d\\n\", (lit & 1) ? \"!\" : \"\", id);\n    }\n\n}",
            "__global__ void printAIG(const int * vFanin0, const int * vFanin1, const int * vPOs,\n                         const int nNodes, const int nPIs, const int nPOs) {\n    printf(\"-------AIG-------\\n\");\n    printf(\"id\\tfanin0\\tfanin1\\n\");\n    for (int i = 0; i <= nPIs; i++) {\n        printf(\"%d\\n\", i);\n    }\n    for (int i = nPIs + 1; i < nNodes + nPIs + 1; i++) {\n        int lit1 = vFanin0[i];\n        int lit2 = vFanin1[i];\n\n        printf(\"%d\\t\", i);\n        printf(\"%s%d\\t\", (lit1 & 1) ? \"!\" : \"\", lit1 >> 1);\n        printf(\"%s%d\\n\", (lit2 & 1) ? \"!\" : \"\", lit2 >> 1);\n    }\n    printf(\"---POs---\\n\");\n    for (int i = 0; i < nPOs; i++) {\n        int lit = vPOs[i];\n        int id = lit >> 1;\n        printf(\"%s%d\\n\", (lit & 1) ? \"!\" : \"\", id);\n    }\n    printf(\"#nodes = %d\\n\", nNodes);\n    printf(\"-----------------\\n\");\n}",
            "__global__ void printMffc(int * vCutTable, int * vCutSizes, int * vConeSizes,\n                          const int * pFanin0, const int * pFanin1, \n                          int nNodes, int nPIs, int nPOs) {\n    int smallConeCount = 0, largeCount = 0;\n    for (int i = 0; i < nNodes; i++) {\n        int id = i + nPIs + 1;\n        // printf(\"node: %d, saved size: %d | \", id, vConeSizes[id]);\n        // for (int j = 0; j < vCutSizes[id]; j++) {\n        //     printf(\"%d \", vCutTable[id * CUT_TABLE_SIZE + j]);\n        // }\n        // printf(\"\\n\");\n\n        if (vConeSizes[id] < 2 && vCutSizes[id] != -1)\n            smallConeCount++;\n        if (vCutSizes[id] == -1)\n            largeCount++;\n    }\n    printf(\"Too small cone: %d, too large cut: %d\\n\", smallConeCount, largeCount);\n}"
        ]
    },
    "minkowski-cuda": {
        "/Users/gbolet/hecbench-roofline/src/minkowski-cuda/main.cu": [
            "__global__ \nvoid minkowski(\n  const float *__restrict__ a,\n  const float *__restrict__ b,\n        float *__restrict__ c,\n  const float p,\n  const float one_over_p)\n{\n  int col = blockIdx.x * blockDim.x + threadIdx.x;\n  int row = blockIdx.y * blockDim.y + threadIdx.y;\n  if( col < k && row < m)\n  {\n    float sum = 0;\n    for(int i = 0; i < n; i++)\n    {\n      sum += powf(fabsf(a[row * n + i] - b[i * k + col]), p);\n    }\n    c[row * k + col] = powf(sum, one_over_p);\n  }\n}"
        ]
    },
    "knn-cuda": {
        "/Users/gbolet/hecbench-roofline/src/knn-cuda/main.cu": [
            "__global__\nvoid cuComputeDistanceGlobal(const float *__restrict__ A,\n                             int wA,\n                             const float *__restrict__ B,\n                             int wB,\n                             int dim,\n                             float *__restrict__ AB)\n{\n  // Declaration of the shared memory arrays As and Bs used to store the\n  // sub-matrix of A and B\n  __shared__ float shared_A[BLOCK_DIM][BLOCK_DIM];\n  __shared__ float shared_B[BLOCK_DIM][BLOCK_DIM];\n\n  // Sub-matrix of A (begin, step, end) and Sub-matrix of B (begin, step)\n  __shared__ int begin_A;\n  __shared__ int begin_B;\n  __shared__ int step_A;\n  __shared__ int step_B;\n  __shared__ int end_A;\n\n  // Thread index\n  int tx = threadIdx.x;\n  int ty = threadIdx.y;\n\n  // Other variables\n  float tmp;\n  float ssd = 0;\n\n  // Loop parameters\n  begin_A = BLOCK_DIM * blockIdx.y;\n  begin_B = BLOCK_DIM * blockIdx.x;\n  step_A = BLOCK_DIM * wA;\n  step_B = BLOCK_DIM * wB;\n  end_A = begin_A + (dim - 1) * wA;\n\n  // Conditions\n  int cond0 = (begin_A + tx < wA); // used to write in shared memory\n  int cond1 = (begin_B + tx < wB); // used to write in shared memory & to\n                                   // computations and to write in output matrix\n  int cond2 =\n      (begin_A + ty < wA); // used to computations and to write in output matrix\n\n  // Loop over all the sub-matrices of A and B required to compute the block\n  // sub-matrix\n  for (int a = begin_A, b = begin_B; a <= end_A; a += step_A, b += step_B) {\n    // Load the matrices from device memory to shared memory; each thread loads\n    // one element of each matrix\n    if (a / wA + ty < dim) {\n      shared_A[ty][tx] = (cond0) ? A[a + wA * ty + tx] : 0;\n      shared_B[ty][tx] = (cond1) ? B[b + wB * ty + tx] : 0;\n    } else {\n      shared_A[ty][tx] = 0;\n      shared_B[ty][tx] = 0;\n    }\n\n    // Synchronize to make sure the matrices are loaded\n    __syncthreads();\n\n    // Compute the difference between the two matrixes; each thread computes one\n    // element of the block sub-matrix\n    if (cond2 && cond1) {\n      for (int k = 0; k < BLOCK_DIM; ++k) {\n        tmp = shared_A[k][ty] - shared_B[k][tx];\n        ssd += tmp * tmp;\n      }\n    }\n\n    // Synchronize to make sure that the preceding computation is done before\n    // loading two new sub-matrices of A and B in the next iteration\n    __syncthreads();\n  }\n\n  // Write the block sub-matrix to device memory; each thread writes one element\n  if (cond2 && cond1)\n    AB[(begin_A + ty) * wB + begin_B + tx] = ssd;\n}",
            "__global__\nvoid cuInsertionSort(float *__restrict__ dist,\n                       int *__restrict__ ind,\n                       int width, int height, int k)\n{\n  // Variables\n  int l, i, j;\n  float *p_dist;\n  int *p_ind;\n  float curr_dist, max_dist;\n  int curr_row, max_row;\n  unsigned int xIndex = blockIdx.x * blockDim.x + threadIdx.x;\n\n  if (xIndex < width) {\n    // Pointer shift, initialization, and max value\n    p_dist = dist + xIndex;\n    p_ind = ind + xIndex;\n    max_dist = p_dist[0];\n    p_ind[0] = 0;\n\n    // Part 1 : sort kth firt elementZ\n    for (l = 1; l < k; l++) {\n      curr_row = l * width;\n      curr_dist = p_dist[curr_row];\n      if (curr_dist < max_dist) {\n        i = l - 1;\n        for (int a = 0; a < l - 1; a++) {\n          if (p_dist[a * width] > curr_dist) {\n            i = a;\n            break;\n          }\n        }\n        for (j = l; j > i; j--) {\n          p_dist[j * width] = p_dist[(j - 1) * width];\n          p_ind[j * width] = p_ind[(j - 1) * width];\n        }\n        p_dist[i * width] = curr_dist;\n        p_ind[i * width] = l;\n      } else {\n        p_ind[l * width] = l;\n      }\n      max_dist = p_dist[curr_row];\n    }\n\n    // Part 2 : insert element in the k-th first lines\n    max_row = (k - 1) * width;\n    for (l = k; l < height; l++) {\n      curr_dist = p_dist[l * width];\n      if (curr_dist < max_dist) {\n        i = k - 1;\n        for (int a = 0; a < k - 1; a++) {\n          if (p_dist[a * width] > curr_dist) {\n            i = a;\n            break;\n          }\n        }\n        for (j = k - 1; j > i; j--) {\n          p_dist[j * width] = p_dist[(j - 1) * width];\n          p_ind[j * width] = p_ind[(j - 1) * width];\n        }\n        p_dist[i * width] = curr_dist;\n        p_ind[i * width] = l;\n        max_dist = p_dist[max_row];\n      }\n    }\n  }\n}",
            "__global__ void cuParallelSqrt(float *dist, int width, int k) {\n  unsigned int xIndex = blockIdx.x * blockDim.x + threadIdx.x;\n  unsigned int yIndex = blockIdx.y * blockDim.y + threadIdx.y;\n  if (xIndex < width && yIndex < k)\n    dist[yIndex * width + xIndex] = sqrt(dist[yIndex * width + xIndex]);\n}"
        ]
    },
    "clenergy-cuda": {
        "/Users/gbolet/hecbench-roofline/src/clenergy-cuda/clenergy.cu": [
            "__global__ void cenergy(const int numatoms, const float gridspacing, \n                        float *energygrid, const float4 *atominfo) \n{\n  unsigned int xindex  = __umul24((unsigned)blockIdx.x, (unsigned)blockDim.x) * UNROLLX\n                         + threadIdx.x;\n  unsigned int yindex  = __umul24((unsigned)blockIdx.y, (unsigned)blockDim.y) + threadIdx.y;\n  unsigned int outaddr = (__umul24((unsigned)gridDim.x, (unsigned)blockDim.x) * UNROLLX) * yindex\n                         + xindex;\n\n  float coory = gridspacing * yindex;\n  float coorx = gridspacing * xindex;\n\n  float energyvalx1=0.0f;\n  float energyvalx2=0.0f;\n  float energyvalx3=0.0f;\n  float energyvalx4=0.0f;\n  float energyvalx5=0.0f;\n  float energyvalx6=0.0f;\n  float energyvalx7=0.0f;\n  float energyvalx8=0.0f;\n\n  float gridspacing_u = gridspacing * BLOCKSIZEX;\n\n  //\n  // XXX 59/8 FLOPS per atom\n  //\n  int atomid;\n  for (atomid=0; atomid<numatoms; atomid++) {\n    float dy = coory - atominfo[atomid].y;\n    float dyz2 = (dy * dy) + atominfo[atomid].z;\n\n    float dx1 = coorx - atominfo[atomid].x;\n    float dx2 = dx1 + gridspacing_u;\n    float dx3 = dx2 + gridspacing_u;\n    float dx4 = dx3 + gridspacing_u;\n    float dx5 = dx4 + gridspacing_u;\n    float dx6 = dx5 + gridspacing_u;\n    float dx7 = dx6 + gridspacing_u;\n    float dx8 = dx7 + gridspacing_u;\n\n    energyvalx1 += atominfo[atomid].w * rsqrtf(dx1*dx1 + dyz2);\n    energyvalx2 += atominfo[atomid].w * rsqrtf(dx2*dx2 + dyz2);\n    energyvalx3 += atominfo[atomid].w * rsqrtf(dx3*dx3 + dyz2);\n    energyvalx4 += atominfo[atomid].w * rsqrtf(dx4*dx4 + dyz2);\n    energyvalx5 += atominfo[atomid].w * rsqrtf(dx5*dx5 + dyz2);\n    energyvalx6 += atominfo[atomid].w * rsqrtf(dx6*dx6 + dyz2);\n    energyvalx7 += atominfo[atomid].w * rsqrtf(dx7*dx7 + dyz2);\n    energyvalx8 += atominfo[atomid].w * rsqrtf(dx8*dx8 + dyz2);\n  }\n\n  energygrid[outaddr             ] += energyvalx1;\n  energygrid[outaddr+1*BLOCKSIZEX] += energyvalx2;\n  energygrid[outaddr+2*BLOCKSIZEX] += energyvalx3;\n  energygrid[outaddr+3*BLOCKSIZEX] += energyvalx4;\n  energygrid[outaddr+4*BLOCKSIZEX] += energyvalx5;\n  energygrid[outaddr+5*BLOCKSIZEX] += energyvalx6;\n  energygrid[outaddr+6*BLOCKSIZEX] += energyvalx7;\n  energygrid[outaddr+7*BLOCKSIZEX] += energyvalx8;\n}"
        ]
    },
    "resize-cuda": {
        "/Users/gbolet/hecbench-roofline/src/resize-cuda/main.cu": [
            "#define T ((int)32)\n\n\ninline __host__ __device__ uint4 make_uint4(int4 a)\n{\n    return make_uint4(uint(a.x), uint(a.y), uint(a.z), uint(a.w));\n}\n\ninline __host__ __device__ uint4 min(uint4 a, uint4 b)\n{\n    return make_uint4(min(a.x,b.x), min(a.y,b.y), min(a.z,b.z), min(a.w,b.w));\n}\n\n__global__\nvoid resize (\n    T *__restrict__ output,\n    size_t output_size, int out_height, int out_width,\n    const T *__restrict__ input, int in_height, int in_width,\n    float o2i_fy, float o2i_fx, bool round, bool half_pixel_centers)\n{\n    auto in_image_size = in_height * in_width;\n    auto out_image_size = out_height * out_width;\n\n    /* think of the output and input as a collection of 2d images with the last axis\n     * representing the width and the last but one axis representing the height\n     *\n     * the remaining axis together form a collection of these images/channels\n     */\n    auto num_effective_channels = output_size / out_image_size;\n\n    /* we process multiple channels every iteration to reuse the identical computation\n     * involved with the spatial dimensions\n     *\n     * if we are processing `CHANNELS_PER_ITER` channels per iteration, we will need\n     * (num_effective_channels / CHANNELS_PER_ITER) iterations per (x, y) location\n     */\n    auto num_channel_iters_per_xy = (num_effective_channels / CHANNELS_PER_ITER);\n\n    /* we need `num_channel_iters_per_xy` iterations per (x, y) and there are `out_image_size`\n     * combinations of (x, y); hence, we'll need `num_channel_iters_per_xy * out_image_size`\n     * iterations in total to finish the resize operation\n     */\n    auto iters_required = num_channel_iters_per_xy * out_image_size;\n\n    for (int iter = blockIdx.x * blockDim.x + threadIdx.x;\n             iter < iters_required; iter += blockDim.x * gridDim.x) {\n\n        const int c_start = (iter / out_image_size) * CHANNELS_PER_ITER;\n\n        /* note here that consecutive `iter` values will often have consecutive `x` values\n         * => stores into output will be coalesced across threads\n         */\n        const int y = (iter % out_image_size) / out_width;\n        const int x = iter % out_width;\n\n        auto in_yf = half_pixel_centers ? (y + 0.5f) * o2i_fy : y * o2i_fy;\n        int in_y = round ? lroundf(in_yf) : static_cast<int>(in_yf);\n\n        auto in_xf = half_pixel_centers ? (x + 0.5f) * o2i_fx : x * o2i_fx;\n        int in_x = round ? lroundf(in_xf) : static_cast<int>(in_xf);\n\n        in_x = min(in_x, in_width - 1);\n        in_y = min(in_y, in_height - 1);\n\n        int in_idx = c_start * in_image_size + in_y * in_width + in_x;\n        int out_idx = c_start * out_image_size + y * out_width + x;\n\n        for (int i = 0; i < CHANNELS_PER_ITER; i++) {\n            output[out_idx] = input[in_idx];\n            in_idx += in_image_size;\n            out_idx += out_image_size;\n        }\n    }\n}",
            "#define T ((int)32)\n\n\ninline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\ninline __host__ __device__ float4 fmaxf(float4 a, float4 b)\n{\n    return make_float4(fmaxf(a.x,b.x), fmaxf(a.y,b.y), fmaxf(a.z,b.z), fmaxf(a.w,b.w));\n}\n\ninline __host__ __device__ uint4 make_uint4(int4 a)\n{\n    return make_uint4(uint(a.x), uint(a.y), uint(a.z), uint(a.w));\n}\n\ninline __host__ __device__ uint4 min(uint4 a, uint4 b)\n{\n    return make_uint4(min(a.x,b.x), min(a.y,b.y), min(a.z,b.z), min(a.w,b.w));\n}\n\n__global__ void resize_bilinear(\n    T *__restrict__ output,\n    size_t output_size, int out_height, int out_width,\n    const T *__restrict__ input, int in_height, int in_width,\n    float o2i_fy, float o2i_fx, bool half_pixel_centers)\n{\n    auto in_image_size = in_height * in_width;\n    auto out_image_size = out_height * out_width;\n\n    /* think of the output and input as a collection of 2d images with the last axis\n     * representing the width and the last but one axis representing the height\n     *\n     * the remaining axis together form a collection of these images/channels\n     */\n    auto num_effective_channels = output_size / out_image_size;\n\n    /* we process multiple channels every iteration to reuse the identical computation\n     * involved with the spatial dimensions\n     *\n     * if we are processing `CHANNELS_PER_ITER` channels per iteration, we will need\n     * (num_effective_channels / CHANNELS_PER_ITER) iterations per (x, y) location\n     */\n    auto num_channel_iters_per_xy = (num_effective_channels / CHANNELS_PER_ITER);\n\n    /* we need `num_channel_iters_per_xy` iterations per (x, y) and there are `out_image_size`\n     * combinations of (x, y); hence, we'll need `num_channel_iters_per_xy * out_image_size`\n     * iterations in total to finish the resize operation\n     */\n    auto iters_required = num_channel_iters_per_xy * out_image_size;\n\n    for (int iter = blockIdx.x * blockDim.x + threadIdx.x;\n             iter < iters_required; iter += blockDim.x * gridDim.x) {\n\n        const int c_start = (iter / out_image_size) * CHANNELS_PER_ITER;\n        const int c_end = c_start + CHANNELS_PER_ITER;\n\n        /* note here that consecutive `iter` values will often have consecutive `x` values\n         * => stores into output will be coalesced across threads\n         */\n        const int y = (iter % out_image_size) / out_width;\n        const int x = iter % out_width;\n\n        auto in_x = half_pixel_centers ? fmaxf((x + 0.5f) * o2i_fx - 0.5f, 0.0f) : x * o2i_fx;\n        auto in_y = half_pixel_centers ? fmaxf((y + 0.5f) * o2i_fy - 0.5f, 0.0f) : y * o2i_fy;\n\n        auto in_x0 = static_cast<int>(in_x);\n        auto in_x1 = min(in_x0 + 1, in_width - 1);\n\n        auto in_y0 = static_cast<int>(in_y);\n\n        auto in_y1 = min(in_y0, in_height - 1);\n        auto in_y2 = min(in_y0 + 1, in_height - 1);\n\n        int in_offset_r0 = c_start * in_image_size + in_y1 * in_width;\n        int in_offset_r1 = c_start * in_image_size + in_y2 * in_width;\n        int out_idx = c_start * out_image_size + y * out_width + x;\n\n        #pragma unroll 1 /* disable unrolling to reduce register pressure; not sure how but it works */\n        for (auto c = c_start; c < c_end; c++) {\n            auto v_00 = input[in_offset_r0 + in_x0],\n                 v_01 = input[in_offset_r0 + in_x1],\n                 v_10 = input[in_offset_r1 + in_x0],\n                 v_11 = input[in_offset_r1 + in_x1];\n\n            output[out_idx] =\n                v_00 +\n                T(in_y - in_y0) * T(v_10 - v_00) +\n                T(in_x - in_x0) * T(v_01 - v_00) +\n                T(in_y - in_y0) * T(in_x - in_x0) * T(v_11 - v_01 - v_10 + v_00);\n\n            in_offset_r0 += in_image_size;\n            in_offset_r1 += in_image_size;\n            out_idx += out_image_size;\n        }\n    }\n}"
        ]
    },
    "dispatch-cuda": {
        "/Users/gbolet/hecbench-roofline/src/dispatch-cuda/main.cu": [
            "__global__\nvoid EmptyKernel() { }"
        ]
    },
    "bwt-cuda": {
        "/Users/gbolet/hecbench-roofline/src/bwt-cuda/bwt.cu": [
            "__global__ void generate_table(int* table, int table_size, int n) {\n  int index = blockIdx.x * blockDim.x + threadIdx.x;\n  int stride = blockDim.x * gridDim.x;\n  for(int i = index; i < table_size; i+=stride)\n    table[i] = (i < n) ? i : -1;\n}",
            "__device__ bool compare_rotations(const int& a, const int& b, const char* genome, int n) {\n  if (a < 0) return false;\n  if (b < 0) return true;\n  for(int i = 0; i < n; i++) {\n    if (genome[(a + i) % n] != genome[(b + i) % n]) {\n      return genome[(a + i) % n] < genome[(b + i) % n];\n    }\n  }\n  return false;\n}\n\n__global__ void bitonic_sort_step(int*__restrict table, int table_size, \n                                  int j, int k, const char*__restrict genome, int n) {\n  unsigned int i = threadIdx.x + blockDim.x * blockIdx.x;\n  unsigned int ixj = i ^ j;\n  if (i < table_size && ixj > i) {\n    bool f = (i & k) == 0;\n    int t1 = table[i];\n    int t2 = table[ixj];\n    if (compare_rotations(f ? t2 : t1, f ? t1 : t2, genome, n)) {\n      table[i] = t2;\n      table[ixj] = t1;\n    }\n  }\n}",
            "__global__ void reconstruct_sequence(const int*__restrict table, const char*__restrict sequence, \n                                     char*__restrict transformed_sequence, int n) {\n  int index = blockIdx.x * blockDim.x + threadIdx.x;\n  int stride = blockDim.x * gridDim.x;\n  for(int i = index; i < n; i += stride) {\n    transformed_sequence[i] = sequence[(n + table[i] - 1) % n];\n  }\n}"
        ]
    },
    "randomAccess-cuda": {
        "/Users/gbolet/hecbench-roofline/src/randomAccess-cuda/main.cu": [
            "__global__ void initTable (u64Int* Table, const u64Int TableSize) {\n  int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < TableSize) Table[i] = i;\n}"
        ]
    },
    "michalewicz-cuda": {
        "/Users/gbolet/hecbench-roofline/src/michalewicz-cuda/main.cu": [
            "__device__ __forceinline__\nfloat atomic_min(float *addr, float value)\n{\n  unsigned ret = __float_as_uint(*addr);\n  while(value < __uint_as_float(ret))\n  {\n    unsigned old = ret;\n    if((ret = atomicCAS((unsigned *)addr, old, __float_as_uint(value))) == old)\n      break;\n  }\n  return __uint_as_float(ret);\n}\n\n__device__ __forceinline__\nfloat michalewicz(const float *xValues, const int dim) {\n  float result = 0;\n  for (int i = 0; i < dim; ++i) {\n    float a = sinf(xValues[i]);\n    float b = sinf(((i + 1) * xValues[i] * xValues[i]) / (float)M_PI);\n    float c = powf(b, 20); // m = 10\n    result += a * c;\n  }\n  return -1.0f * result;\n}\n\n__global__ void eval (const float *values, float *minima,\n                      const size_t nVectors, const int dim)\n{\n  size_t n = blockIdx.x * blockDim.x + threadIdx.x;\n  if (n < nVectors) {\n    atomic_min(minima, michalewicz(values + n * dim, dim));\n  }\n}"
        ]
    },
    "filter-cuda": {
        "/Users/gbolet/hecbench-roofline/src/filter-cuda/main.cu": [
            "__global__ \nvoid filter (int *__restrict__ dst,\n             int *__restrict__ nres,\n             const int*__restrict__ src,\n             int n)\n{\n  __shared__ int l_n;\n  int i = blockIdx.x * blockDim.x + threadIdx.x;\n\n  // zero the counter\n  if (threadIdx.x == 0)\n    l_n = 0;\n  __syncthreads();\n\n  // get the value, evaluate the predicate, and\n  // increment the counter if needed\n  int d, pos;\n\n  if(i < n) {\n    d = src[i];\n    if(d > 0)\n      pos = atomicAdd(&l_n, 1);\n  }\n  __syncthreads();\n\n  // leader increments the global counter\n  if(threadIdx.x == 0)\n    l_n = atomicAdd(nres, l_n);\n  __syncthreads();\n\n  // threads with true predicates write their elements\n  if(i < n && d > 0) {\n    pos += l_n; // increment local pos by global counter\n    dst[pos] = d;\n  }\n  __syncthreads();\n}",
            "__device__ int atomicAggInc(int* ptr) {\n  int mask;\n  unsigned tmask = 0xFFFFFFFF;\n#if __CUDA_ARCH__ >= 700\n    // return mask of threads that have same value in tmask\n    mask = __match_any_sync(tmask, (unsigned long long)ptr);\n#else\n  for (int i = 0; i < warpSize; i++){\n    unsigned long long tptr = __shfl_sync(tmask, (unsigned long long)ptr, i);\n    unsigned my_mask = __ballot_sync(tmask, (tptr == (unsigned long long)ptr));\n    if (i == (threadIdx.x & (warpSize-1))) mask = my_mask;\n  }\n#endif\n  int leader = __ffs(mask) - 1;  // select a leader\n  int res = 0;\n  unsigned lane_id = threadIdx.x % warpSize;\n  if (lane_id == leader) {                 // leader does the update\n    res = atomicAdd(ptr, __popc(mask));\n  }\n  res = __shfl_sync(mask, res, leader);    // get leader\u2019s old value\n  return res + __popc(mask & ((1 << lane_id) - 1)); //compute old value\n}\n\n__global__\nvoid filter2 (int *__restrict__ dst,\n              int *__restrict__ nres,\n              const int*__restrict__ src,\n              int n)\n{\n  int i = threadIdx.x + blockIdx.x * blockDim.x;\n  if(i < n && src[i] > 0)\n    dst[atomicAggInc(nres)] = src[i];\n}"
        ]
    },
    "hbc-cuda": {
        "/Users/gbolet/hecbench-roofline/src/hbc-cuda/kernels.cu": [
            "__device__ void bitonic_sort(int *values, const int N)\n{\n  unsigned int idx = threadIdx.x;\n\n  for (int k = 2; k <= N; k <<= 1)\n  {\n    for (int j = k >> 1; j > 0; j = j >> 1)\n    {\n      while(idx < N) \n      {\n        int ixj = idx^j;\n        if (ixj > idx) \n        {\n          if ((idx&k) == 0 && values[idx] > values[ixj]) \n          {\n            //exchange(idx, ixj);\n            int tmp = values[idx];\n            values[idx] = values[ixj];\n            values[ixj] = tmp;\n          }\n          if ((idx&k) != 0 && values[idx] < values[ixj]) \n          {\n            //exchange(idx, ixj);\n            int tmp = values[idx];\n            values[idx] = values[ixj];\n            values[ixj] = tmp;\n          }\n        }\n        idx += blockDim.x;\n      }\n      __syncthreads();\n      idx = threadIdx.x;\n    }\n  }\n}\n\n__global__ void bc_kernel(\n  float *__restrict__ bc,\n  const int *__restrict__ R,\n  const int *__restrict__ C,\n  const int *__restrict__ F,\n  const int n,\n  const int m,\n  const int *__restrict__ d,\n  const unsigned long long *__restrict__ sigma,\n  const float *__restrict__ delta,\n  const int *__restrict__ Q,\n  const int *__restrict__ Q2,\n  const int *__restrict__ S,\n  const int *__restrict__ endpoints,\n  int *__restrict__ next_source,\n  const size_t pitch_d,\n  const size_t pitch_sigma,\n  const size_t pitch_delta,\n  const size_t pitch_Q,\n  const size_t pitch_Q2,\n  const size_t pitch_S,\n  const size_t pitch_endpoints,\n  const int start,\n  const int end,\n  int *__restrict__ jia,\n  int *__restrict__ diameters,\n  const int *__restrict__ source_vertices,\n  const bool approx)\n{\n  __shared__ int ind;\n  __shared__ int i;\n  __shared__ int *Q_row;\n  __shared__ int *Q2_row;\n  __shared__ int *S_row;\n  __shared__ int *endpoints_row;\n\n  int j = threadIdx.x;\n  int *d_row = (int*)((char*)d + blockIdx.x*pitch_d);\n  unsigned long long *sigma_row = (unsigned long long*)((char*)sigma + blockIdx.x*pitch_sigma);\n  float *delta_row = (float*)((char*)delta + blockIdx.x*pitch_delta);\n  if(j == 0)\n  {\n    ind = blockIdx.x + start;\n    i = approx ? source_vertices[ind] : ind;\n    Q_row = (int*)((char*)Q + blockIdx.x*pitch_Q);\n    Q2_row = (int*)((char*)Q2 + blockIdx.x*pitch_Q2);\n    S_row = (int*)((char*)S + blockIdx.x*pitch_S);\n    endpoints_row = (int*)((char*)endpoints + blockIdx.x*pitch_endpoints);\n    *jia = 0;\n  }\n  __syncthreads();\n\n  if((ind==0) && (j < DIAMETER_SAMPLES))\n  {\n    diameters[j] = INT_MAX;\n  }\n  __syncthreads();\n\n  while(ind < end)\n  {\n    //Initialization\n    for(int k=threadIdx.x; k<n; k+=blockDim.x)\n    {\n      if(k == i) //If k is the source node...\n      {\n        d_row[k] = 0;\n        sigma_row[k] = 1;\n      }\n      else\n      {\n        d_row[k] = INT_MAX;\n        sigma_row[k] = 0;\n      }  \n      delta_row[k] = 0;\n    }\n    __syncthreads();\n\n    //Shortest Path Calculation\n    __shared__ int Q_len;\n    __shared__ int Q2_len;\n    __shared__ int S_len;\n    __shared__ int current_depth; \n    __shared__ int endpoints_len;\n    __shared__ bool sp_calc_done;\n\n    if(j == 0)\n    {\n      Q_row[0] = i;\n      Q_len = 1;\n      Q2_len = 0;\n      S_row[0] = i;\n      S_len = 1;\n      endpoints_row[0] = 0;\n      endpoints_row[1] = 1;\n      endpoints_len = 2;\n      current_depth = 0;\n      sp_calc_done = false;\n    }\n    __syncthreads();\n\n    //Do first iteration separately since we already know the edges to traverse\n    for(int r=threadIdx.x+R[i]; r<R[i+1]; r+=blockDim.x)\n    {\n      int w = C[r];\n      //No multiple/self edges - each value of w is unique, so no need for atomics\n      if(d_row[w] == INT_MAX)\n      {\n        d_row[w] = 1; \n        int t = atomicAdd(&Q2_len,1);\n        Q2_row[t] = w;\n      }\n      if(d_row[w] == (d_row[i]+1))\n      {\n        atomicAdd(&sigma_row[w],1); \n      }\n    }\n    __syncthreads();\n\n    if(Q2_len == 0)\n    {\n      sp_calc_done = true;\n    }\n    else\n    {\n      for(int kk=threadIdx.x; kk<Q2_len; kk+=blockDim.x)\n      {\n        Q_row[kk] = Q2_row[kk];\n        S_row[kk+S_len] = Q2_row[kk];\n      }\n      __syncthreads();\n      if(j == 0)\n      {\n        endpoints_row[endpoints_len] = endpoints_row[endpoints_len-1] + Q2_len;\n        endpoints_len++;\n        Q_len = Q2_len;\n        S_len += Q2_len;\n        Q2_len = 0;\n        current_depth++;\n      }\n    }\n    __syncthreads();\n\n    while(!sp_calc_done)\n    {\n      if((*jia) && (Q_len > 512))\n      {\n        for(int k=threadIdx.x; k<2*m; k+=blockDim.x)\n        {\n          int v = F[k];\n          if(d_row[v] == current_depth) \n          {\n            int w = C[k];\n            if(atomicCAS(&d_row[w],INT_MAX,d_row[v]+1) == INT_MAX)\n            {\n              int t = atomicAdd(&Q2_len,1);\n              Q2_row[t] = w;\n            }\n            if(d_row[w] == (d_row[v]+1))\n            {\n              atomicAdd(&sigma_row[w],sigma_row[v]);\n            }\n          }  \n        }\n      }\n      else\n      {\n        __shared__ int next_index;\n        if(j == 0)\n        {\n          next_index = blockDim.x;\n        }\n        __syncthreads();\n        int k = threadIdx.x; //Initial vertices\n        while(k < Q_len)\n        {\n          int v = Q_row[k];\n          for(int r=R[v]; r<R[v+1]; r++)\n          {\n            int w = C[r];\n            //Use atomicCAS to prevent duplicates\n            if(atomicCAS(&d_row[w],INT_MAX,d_row[v]+1) == INT_MAX)\n            {\n              int t = atomicAdd(&Q2_len,1);\n              Q2_row[t] = w;\n            }\n            if(d_row[w] == (d_row[v]+1))\n            {\n              atomicAdd(&sigma_row[w],sigma_row[v]);\n            }\n          }\n          k = atomicAdd(&next_index,1);\n        }\n      }\n      __syncthreads();\n\n      if(Q2_len == 0) //If there is no additional work found, we're done\n      {\n        break;\n      }\n      else //If there is additional work, transfer elements from Q2 to Q, reset lengths, and add vertices to the stack\n      {\n        for(int kk=threadIdx.x; kk<Q2_len; kk+=blockDim.x)\n        {\n          Q_row[kk] = Q2_row[kk];\n          S_row[kk+S_len] = Q2_row[kk];\n        }\n        __syncthreads();\n        if(j == 0)\n        {\n          endpoints_row[endpoints_len] = endpoints_row[endpoints_len-1] + Q2_len;\n          endpoints_len++;\n          Q_len = Q2_len;\n          S_len += Q2_len;\n          Q2_len = 0;\n          current_depth++;\n        }\n        __syncthreads();\n      }\n    }\n\n    //The elements at the end of the stack will have the largest distance from the source\n    //Using the successor method, we can start from one depth earlier\n    if(j == 0)\n    {\n      current_depth = d_row[S_row[S_len-1]] - 1;\n      if(ind<DIAMETER_SAMPLES)\n      {\n        diameters[ind] = current_depth+1;\n      }\n    }\n    __syncthreads();\n\n    //Dependency Accumulation (Madduri/Ediger successor method)\n    while(current_depth > 0)\n    {\n      int stack_iter_len = endpoints_row[current_depth+1]-endpoints_row[current_depth];\n      if((*jia) && (stack_iter_len>512))\n      {\n        for(int kk=threadIdx.x; kk<2*m; kk+=blockDim.x)\n        {\n          int w = F[kk];\n          if(d_row[w] == current_depth)\n          {\n            int v = C[kk];\n            if(d_row[v] == (d_row[w]+1))\n            {\n              float change = (sigma_row[w]/(float)sigma_row[v])*(1.0f+delta_row[v]);\n              atomicAdd(&delta_row[w],change);\n            }    \n          }\n        }\n      }\n      else \n      {\n        for(int kk=threadIdx.x+endpoints_row[current_depth]; kk<endpoints_row[current_depth+1]; kk+=blockDim.x)\n        {\n          int w = S_row[kk];\n          float dsw = 0;\n          float sw = (float)sigma_row[w];\n          for(int z=R[w]; z<R[w+1]; z++)\n          {\n            int v = C[z];\n            if(d_row[v] == (d_row[w]+1))\n            {\n              dsw += (sw/(float)sigma_row[v])*(1.0f+delta_row[v]);\n            }\n          }\n          delta_row[w] = dsw;  \n        }\n      }\n      __syncthreads();\n      if(j == 0)\n      {\n        current_depth--;\n      }\n      __syncthreads();\n    }\n\n    for(int kk=threadIdx.x; kk<n; kk+=blockDim.x)\n    {\n      atomicAdd(&bc[kk],delta_row[kk]); //Would need to check that kk != i here, but delta_row[kk] is guaranteed to be 0.\n    }\n\n    if(j == 0)\n    {\n      ind = atomicAdd(next_source,1);\n      if(approx)\n      {\n        i = source_vertices[ind];\n      }\n      else\n      {\n        i = ind;\n      }\n    }\n    __syncthreads();\n\n    if(ind == 2*DIAMETER_SAMPLES)\n    {\n      __shared__ int diameter_keys[DIAMETER_SAMPLES];\n      for(int kk = threadIdx.x; kk<DIAMETER_SAMPLES; kk+=blockDim.x)\n      {\n        diameter_keys[kk] = diameters[kk];\n      }\n      __syncthreads();\n      bitonic_sort(diameter_keys,DIAMETER_SAMPLES);\n      __syncthreads();\n      if(j == 0)\n      {\n        int log2n = 0;\n        int tempn = n;\n        while(tempn >>= 1)\n        {\n          ++log2n;\n        }\n        if(diameter_keys[DIAMETER_SAMPLES/2] < 4*log2n) //Use the median\n        {\n          *jia = 1;\n        }\n      }\n    }\n    __syncthreads();\n  }\n}"
        ]
    },
    "channelSum-cuda": {
        "/Users/gbolet/hecbench-roofline/src/channelSum-cuda/main.cu": [
            "#define T ((int)32)\n\n\n__global__\nvoid ChannelSumNCHW(\n    const int N,\n    const int C,\n    const int HxW,\n    const T* X,\n    T*__restrict__ sum,\n    T*__restrict__ sumsq)\n{\n  __shared__\n  typename BlockReduce2D<T, kBlockDimX, kBlockDimY>::TempStorage m_storage;\n\n  __shared__\n  typename BlockReduce2D<T, kBlockDimX, kBlockDimY>::TempStorage v_storage;\n\n  T m_val = 0;\n  T v_val = 0;\n\n  const int c = blockIdx.x;\n\n  // sum batches from different channels\n  for (int n = threadIdx.x; n < N; n += blockDim.x) {\n    for (int hw = threadIdx.y; hw < HxW; hw += blockDim.y) {\n      const int index = (n * C + c) * HxW + hw;\n      m_val += __ldg(X + index);\n      v_val += __ldg(X + index) * __ldg(X + index);\n    }\n  }\n  m_val = BlockReduce2D<T, kBlockDimX, kBlockDimY>(m_storage).Sum(m_val);\n  v_val = BlockReduce2D<T, kBlockDimX, kBlockDimY>(v_storage).Sum(v_val);\n\n  if (threadIdx.x == 0 && threadIdx.y == 0) {\n    sum[c] = m_val;\n    sumsq[c] = v_val;\n  }\n}",
            "#define T ((int)32)\n\n\n__global__\nvoid ChannelSumNHWC(\n    const int N,\n    const int C,\n    const int HxW,\n    const T* X,\n    T*__restrict__ sum,\n    T*__restrict__ sumsq)\n{\n  __shared__ typename BlockReduce1D<T>::TempStorage m_storage;\n  __shared__ typename BlockReduce1D<T>::TempStorage v_storage;\n  const int inner_size = N * HxW;\n  const int c = blockIdx.x;\n  T m_val = 0;\n  T v_val = 0;\n  for (int i = threadIdx.x; i < inner_size; i += blockDim.x) {\n    const int index = i * C + c;\n    m_val += __ldg(X + index);\n    v_val += __ldg(X + index) * __ldg(X + index);\n  }\n  m_val = BlockReduce1D<T>(m_storage).Sum(m_val);\n  v_val = BlockReduce1D<T>(v_storage).Sum(v_val);\n  if (threadIdx.x == 0) {\n    sum[c] = m_val;\n    sumsq[c] = v_val;\n  }\n}"
        ]
    },
    "ccsd-trpdrv-cuda": {
        "/Users/gbolet/hecbench-roofline/src/ccsd-trpdrv-cuda/ccsd_tengy.cu": [
            "__global__ \nvoid ccsd_kernel(const double * __restrict__ f1n,    const double * __restrict__ f1t,\n                 const double * __restrict__ f2n,    const double * __restrict__ f2t,\n                 const double * __restrict__ f3n,    const double * __restrict__ f3t,\n                 const double * __restrict__ f4n,    const double * __restrict__ f4t,\n                 const double * __restrict__ dintc1, const double * __restrict__ dintx1, const double * __restrict__ t1v1,\n                 const double * __restrict__ dintc2, const double * __restrict__ dintx2, const double * __restrict__ t1v2,\n                 const double * __restrict__ eorb,   const double eaijk,\n                 double * __restrict__ emp4i, double * __restrict__ emp5i,\n                 double * __restrict__ emp4k, double * __restrict__ emp5k,\n                 const int ncor, const int nocc, const int nvir)\n{\n  const int b = blockIdx.x * blockDim.x + threadIdx.x;\n  const int c = blockIdx.y * blockDim.y + threadIdx.y;\n\n  if (b < nvir && c < nvir) {\n\n    const double denom = -1.0 / (eorb[ncor+nocc+b] + eorb[ncor+nocc+c] + eaijk);\n\n    // nvir < 10000 so this should never overflow\n    const int bc = b+c*nvir;\n    const int cb = c+b*nvir;\n\n    const double f1nbc = f1n[bc];\n    const double f1tbc = f1t[bc];\n    const double f1ncb = f1n[cb];\n    const double f1tcb = f1t[cb];\n\n    const double f2nbc = f2n[bc];\n    const double f2tbc = f2t[bc];\n    const double f2ncb = f2n[cb];\n    const double f2tcb = f2t[cb];\n\n    const double f3nbc = f3n[bc];\n    const double f3tbc = f3t[bc];\n    const double f3ncb = f3n[cb];\n    const double f3tcb = f3t[cb];\n\n    const double f4nbc = f4n[bc];\n    const double f4tbc = f4t[bc];\n    const double f4ncb = f4n[cb];\n    const double f4tcb = f4t[cb];\n\n    atomicAdd(emp4i , denom * (f1tbc+f1ncb+f2tcb+f3nbc+f4ncb) * (f1tbc-f2tbc*2-f3tbc*2+f4tbc)\n                      - denom * (f1nbc+f1tcb+f2ncb+f3ncb) * (f1tbc*2-f2tbc-f3tbc+f4tbc*2)\n                      + denom * 3 * (f1nbc*(f1nbc+f3ncb+f4tcb*2) +f2nbc*f2tcb+f3nbc*f4tbc));\n\n    atomicAdd(emp4k , denom * (f1nbc+f1tcb+f2ncb+f3tbc+f4tcb) * (f1nbc-f2nbc*2-f3nbc*2+f4nbc)\n                      - denom * (f1tbc+f1ncb+f2tcb+f3tcb) * (f1nbc*2-f2nbc-f3nbc+f4nbc*2)\n                      + denom * 3 * (f1tbc*(f1tbc+f3tcb+f4ncb*2) +f2tbc*f2ncb+f3tbc*f4nbc));\n\n    const double t1v1b = t1v1[b];\n    const double t1v2b = t1v2[b];\n\n    const double dintx1c = dintx1[c];\n    const double dintx2c = dintx2[c];\n    const double dintc1c = dintc1[c];\n    const double dintc2c = dintc2[c];\n\n    atomicAdd(emp5i, denom * t1v1b * dintx1c * (f1tbc+f2nbc+f4ncb-(f3tbc+f4nbc+f2ncb+f1nbc+f2tbc+f3ncb)*2\n                     +(f3nbc+f4tbc+f1ncb)*4)\n                     + denom * t1v1b * dintc1c * (f1nbc+f4nbc+f1tcb -(f2nbc+f3nbc+f2tcb)*2));\n    atomicAdd(emp5k, denom * t1v2b * dintx2c * (f1nbc+f2tbc+f4tcb -(f3nbc+f4tbc+f2tcb +f1tbc+f2nbc+f3tcb)*2\n                     +(f3tbc+f4nbc+f1tcb)*4)\n                     + denom * t1v2b * dintc2c * (f1tbc+f4tbc+f1ncb -(f2tbc+f3tbc+f2ncb)*2));\n  }\n}"
        ]
    },
    "logic-resim-cuda": {
        "/Users/gbolet/hecbench-roofline/src/logic-resim-cuda/Simulation/src/sim/Simulator.cu": [
            "__global__ void simulateParallel(bool *overflow, const gpu_GATE *gateMgr,\n                                 Event **oHisArr, const size_t *oHisMaxSize,\n                                 size_t *oHisSize, const char *eTableMgr,\n                                 size_t SimLimit, tUnit dumpOff) {\n  size_t gid = blockIdx.x * blockDim.x + threadIdx.x;\n  if (gid > SimLimit - 1)\n    return;\n\n  // Gate info\n  size_t maxSize = oHisMaxSize[gid], currSize;\n  const char *eTable = eTableMgr + gateMgr[gid].funcSer * TRUTH_SIZE;\n  int8_t inLen = *(eTable + TRUTH_SIZE - 1);\n\n  // Simulation cache\n  Event *iPort[MAX_INPUT_PORT];\n  Event *oPort = oHisArr[gid];\n\n  tUnit currTime = -1, iTnext[MAX_INPUT_PORT] = {0};\n  bool isInit = true;\n\n  dUnit delay = -1, d;\n\n  int16_t iVcurr = 0, iVprev = 0;\n\n  int8_t oVcurr = 0, oVprev, i, iFrom, iTo, delayIdx;\n\n  overflow[gid] = false;\n  // Init first input pattern and first trans time\n  for (i = 0; i < inLen; i++) {\n    iPort[i] = gateMgr[gid].iPort[i];\n    iVprev = iVprev << 2;\n    iVprev |= iPort[i]->v;\n    iTnext[i] = (iPort[i] + 1)->t;\n\n    isInit = (iPort[i]->t > 0) ? false : isInit;\n    currTime = (iTnext[i] < currTime) ? iTnext[i] : currTime;\n  }\n\n  // Init not simulated gate\n  if (isInit) {\n    currSize = 1;\n    oPort[0] = {0, valueX};\n    oVprev = valueX;\n  } else {\n    currSize = oHisSize[gid];\n    oVprev = oPort[currSize - 1].v;\n  }\n\n  // Drop from full simulated gate\n  if (currTime == (tUnit)-1 || currTime > dumpOff) {\n    if (oPort[currSize - 1].t != (tUnit)-1) {\n      oPort[currSize] = {(tUnit)-1, valueZ};\n      oHisSize[gid] = currSize + 1;\n    }\n    return;\n  }\n\n  // Simulation loop\n  while (currTime != (tUnit)-1 && currTime <= dumpOff) {\n    // Update newly coming input pattern and corresponding output\n    for (i = 0; i < inLen; ++i) {\n      if (iTnext[i] == currTime) {\n        ++iPort[i];\n        iTnext[i] = (iPort[i] + 1)->t;\n      }\n      iVcurr = iVcurr << 2;\n      iVcurr |= iPort[i]->v;\n    }\n\n    // Get new output value\n    oVcurr = (eTable[iVcurr >> 2] >> ((iVcurr & 3) * 2)) & 3;\n\n    // Trigger edge at output\n    if (oVcurr != oVprev) {\n      // Get minimum delay and next output trans time\n      switch (oVprev << 2 | oVcurr) {\n      case 1:\n        delayIdx = 0;\n        break; // 01\n      case 4:\n        delayIdx = 1;\n        break; // 10\n      case 2:\n        delayIdx = 2;\n        break; // 0X\n      case 9:\n        delayIdx = 3;\n        break; // X1\n      case 6:\n        delayIdx = 4;\n        break; // 1X\n      case 8:\n        delayIdx = 5;\n        break; // X0\n      }\n\n      for (i = 0; i < inLen; ++i) {\n        iTo = (iVcurr >> ((inLen - i - 1) << 1)) & 3,\n        iFrom = (iVprev >> ((inLen - i - 1) << 1)) & 3;\n        if (iTo != iFrom) {\n          d = (iFrom == 1 || iTo == 0)\n                  ? gateMgr[gid].dTable[MAX_DTUPLE * i + delayIdx + 6]\n                  : gateMgr[gid].dTable[MAX_DTUPLE * i + delayIdx];\n\n          delay = (d < delay) ? d : delay;\n        }\n      }\n\n      currTime += delay;\n\n      // Pop out late transtion\n      while (currSize && oPort[currSize - 1].t >= currTime) {\n        --currSize;\n      }\n\n      // Push back new output transtion\n      if (!currSize || oPort[currSize - 1].v != oVcurr) {\n        oPort[currSize] = {currTime, oVcurr};\n        ++currSize;\n\n        // Overflow\n        if (currSize == maxSize) {\n          overflow[gid] = true;\n          oHisSize[gid] = currSize;\n          return;\n        }\n      }\n    }\n\n    // Re-init cache\n    iVprev = iVcurr;\n    oVprev = oVcurr;\n    iVcurr = 0;\n    oVcurr = 0;\n    currTime = -1;\n    delay = -1;\n\n    // Find next trans time\n    for (i = 0; i < inLen; ++i) {\n      currTime = (iTnext[i] < currTime) ? iTnext[i] : currTime;\n    }\n  }\n  oPort[currSize] = {(tUnit)-1, valueZ};\n  oHisSize[gid] = currSize + 1;\n}"
        ]
    },
    "permutate-cuda": {
        "/Users/gbolet/hecbench-roofline/src/permutate-cuda/kernel_functions.cuh": [
            "__device__\nunsigned int LCG_random(unsigned int * seed) {\n  const unsigned int m = 2147483648;\n  const unsigned int a = 26757677;\n  const unsigned int c = 1;\n  *seed = (a * (*seed) + c) % m;\n  return *seed;\n}\n\n__global__\nvoid shuffling_kernel(uint8_t *Ndata, const uint8_t *data,\n                      const uint32_t len, const uint32_t N)\n{\n  uint64_t tid = threadIdx.x + blockIdx.x * blockDim.x;\n  uint64_t i = 0, j = len - 1, random = 0;\n  uint8_t tmp = 0;\n  uint64_t idx = 0, idx2 = 0;\n\n  uint64_t seed = 0;\n\n  for (i = 0; i < len; i++) {\n    idx= i * N + tid;\n    Ndata[idx] = data[i];\n    seed += Ndata[idx];\n  }\n  seed = seed ^ tid;\n\n  while (j > 0) {\n    random = LCG_random(&seed) % j;\n    idx = random * N + tid;\n    idx2 = j * N + tid;\n    tmp = Ndata[idx];\n    Ndata[idx] = Ndata[idx2];\n    Ndata[idx2] = tmp;\n    j--;\n  }\n}",
            "inline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\ninline __host__ __device__ float4 fabs(float4 v)\n{\n    return make_float4(fabs(v.x), fabs(v.y), fabs(v.z), fabs(v.w));\n}\n\n__device__ void dev_test1(double *out_max, const double mean, const uint8_t *data, const uint32_t len,\n    const uint32_t N, uint32_t tid)\n{\n  double max = 0, temp = 0, sum = 0;\n  uint64_t i = 0;\n  uint64_t idx = 0;\n  for (i = 0; i < len; i++) {\n    idx = i * N + tid;\n    sum += data[idx];\n    temp = fabs(sum - ((i + 1)*mean));\n    if (max < temp)\n      max = temp;\n  }\n  *out_max = max;\n}\n\n__device__ void dev_test2_6(double *out1, double *out2, double *out3, double *out4, double *out5,\n    const double median, const uint8_t *data, const uint32_t len, const uint32_t N, uint32_t tid)\n{\n  uint64_t i;\n  *out1 = 1; *out4 = 1;\n  uint32_t run1 = 1, run2 = 1;\n  uint32_t pos = 0;\n  bool f1 = 0, f2 = 0;\n  bool fm1 = 0, fm2 = 0;\n  uint64_t idx1 = 0, idx2 = 0;\n\n  idx1 = tid;\n  idx2 = N + tid;\n  if (data[idx1] <= data[idx2])\n    f1 = 1;\n\n  if (data[idx1] >= median)\n    fm1 = 1;\n\n  for (i = 1; i < (len - 1); i++) {\n    pos += f1;\n    f2 = 0;\n    fm2 = 0;\n\n    idx1 = i * N + tid;\n    idx2 = (i + 1)*N + tid;\n    if (data[idx1] <= data[idx2])\n      f2 = 1;\n\n    if (data[idx1] >= median)\n      fm2 = 1;\n\n    if (f1 == f2)\n      run1++;\n    else {\n      *out1 += 1;\n      if (run1 > * out2)\n        *out2 = run1;\n      run1 = 1;\n    }\n\n    if (fm1 == fm2)\n      run2++;\n    else {\n      *out4 += 1;\n      if (run2 > * out5)\n        *out5 = run2;\n      run2 = 1;\n    }\n    f1 = f2;\n    fm1 = fm2;\n  }\n  pos += f1;\n\n  idx1 = (len - 1)*N + tid;\n  if (data[idx1] >= median)\n    fm2 = 1;\n  else\n    fm2 = 0;\n\n  if (fm1 == fm2)\n    run2++;\n  else {\n    *out4 += 1;\n    if (run2 > * out5)\n      *out5 = run2;\n    run2 = 1;\n  }\n\n  if (pos > (len - pos))\n    *out3 = pos;\n  else\n    *out3 = (len - pos);\n}\n\n__device__ void dev_test7_8(double *out_average, double *out_max, const uint8_t *data, const uint32_t size,\n    const uint32_t len, const uint32_t N, uint32_t tid)\n{\n  uint64_t i = 0, j = 0, k = 0;\n  bool dups[256] = { 0, };\n  uint32_t cnt = 0;\n  uint32_t max = 0;\n  double avg = 0;\n  uint64_t idx = 0;\n  while (i + j < len) {\n    for (k = 0; k < (uint32_t)(1 << size); k++) dups[k] = false;\n\n    while (i + j < len) {\n      idx = (i + j)*N + tid;\n      if (dups[data[idx]]) {\n        avg += j;\n        if (j > max)\n          max = j;\n        cnt++;\n        i += j;\n        j = 0;\n        break;\n      }\n      else {\n        dups[data[idx]] = true;\n        ++j;\n      }\n    }\n    ++i;\n  }\n\n  *out_average = avg / (double)cnt;\n  *out_max = (double)max;\n}\n\n__device__ void dev_test9_and_14(double *out_num, double *out_strength, const uint8_t *data, const uint32_t len,\n    const uint32_t N, uint32_t tid, const uint32_t lag)\n{\n  double temp1 = 0, temp2 = 0;\n  uint64_t i = 0;\n  uint64_t idx1 = 0, idx2 = 0;\n  for (i = 0; i < len - lag; i++) {\n    idx1 = i * N + tid;\n    idx2 = (i + lag)*N + tid;\n\n    if (data[idx1] == data[idx2])\n      temp1++;\n    temp2 += (data[idx1] * data[idx2]);\n  }\n  *out_num = temp1;\n  *out_strength = temp2;\n}\n\n__global__\nvoid statistical_tests_kernel(\n  uint32_t *counts, const double *results, const double mean, const double median,\n  const uint8_t *Ndata, const uint32_t size, const uint32_t len, const uint32_t N,\n  const uint32_t num_block)\n{\n  uint32_t tid = threadIdx.x + (blockIdx.x % num_block) * blockDim.x;\n\n  if ((blockIdx.x / num_block) == 0) {\n    double result1 = 0, result2 = 0;\n    dev_test7_8(&result1, &result2, Ndata, size, len, N, tid);\n    if (result1 > results[6])      atomicAdd(&counts[18], 1);\n    else if (result1 == results[6])    atomicAdd(&counts[19], 1);\n    else                atomicAdd(&counts[20], 1);\n    if (result2 > results[7])      atomicAdd(&counts[21], 1);\n    else if (result2 == results[7])    atomicAdd(&counts[22], 1);\n    else                atomicAdd(&counts[23], 1);\n  }\n  else if ((blockIdx.x / num_block) == 1) {\n    double result1 = 0, result2 = 0, result3 = 0, result4 = 0, result5 = 0;\n\n    dev_test1(&result1, mean, Ndata, len, N, tid);\n    if ((float)result1 > (float)results[0])      atomicAdd(&counts[0], 1);\n    else if ((float)result1 == (float)results[0])  atomicAdd(&counts[1], 1);\n    else                      atomicAdd(&counts[2], 1);\n\n    dev_test2_6(&result1, &result2, &result3, &result4, &result5, median, Ndata, len, N, tid);\n    if (result1 > results[1])      atomicAdd(&counts[3], 1);\n    else if (result1 == results[1])    atomicAdd(&counts[4], 1);\n    else                atomicAdd(&counts[5], 1);\n    if (result2 > results[2])      atomicAdd(&counts[6], 1);\n    else if (result2 == results[2])    atomicAdd(&counts[7], 1);\n    else                atomicAdd(&counts[8], 1);\n    if (result3 > results[3])      atomicAdd(&counts[9], 1);\n    else if (result3 == results[3])    atomicAdd(&counts[10], 1);\n    else                atomicAdd(&counts[11], 1);\n    if (result4 > results[4])      atomicAdd(&counts[12], 1);\n    else if (result4 == results[4])    atomicAdd(&counts[13], 1);\n    else                atomicAdd(&counts[14], 1);\n    if (result5 > results[5])      atomicAdd(&counts[15], 1);\n    else if (result5 == results[5])    atomicAdd(&counts[16], 1);\n    else                atomicAdd(&counts[17], 1);\n\n    dev_test9_and_14(&result1, &result2, Ndata, len, N, tid, 1);\n    if (result1 > results[8])      atomicAdd(&counts[24], 1);\n    else if (result1 == results[8])    atomicAdd(&counts[25], 1);\n    else                atomicAdd(&counts[26], 1);\n    if (result2 > results[13])      atomicAdd(&counts[39], 1);\n    else if (result2 == results[13])  atomicAdd(&counts[40], 1);\n    else                atomicAdd(&counts[41], 1);\n    dev_test9_and_14(&result1, &result2, Ndata, len, N, tid, 2);\n    if (result1 > results[9])      atomicAdd(&counts[27], 1);\n    else if (result1 == results[9])    atomicAdd(&counts[28], 1);\n    else                atomicAdd(&counts[29], 1);\n    if (result2 > results[14])      atomicAdd(&counts[42], 1);\n    else if (result2 == results[14])  atomicAdd(&counts[43], 1);\n    else                atomicAdd(&counts[44], 1);\n    dev_test9_and_14(&result1, &result2, Ndata, len, N, tid, 8);\n    if (result1 > results[10])      atomicAdd(&counts[30], 1);\n    else if (result1 == results[10])  atomicAdd(&counts[31], 1);\n    else                atomicAdd(&counts[32], 1);\n    if (result2 > results[15])      atomicAdd(&counts[45], 1);\n    else if (result2 == results[15])  atomicAdd(&counts[46], 1);\n    else                atomicAdd(&counts[47], 1);\n    dev_test9_and_14(&result1, &result2, Ndata, len, N, tid, 16);\n    if (result1 > results[11])      atomicAdd(&counts[33], 1);\n    else if (result1 == results[11])  atomicAdd(&counts[34], 1);\n    else                atomicAdd(&counts[35], 1);\n    if (result2 > results[16])      atomicAdd(&counts[48], 1);\n    else if (result2 == results[16])  atomicAdd(&counts[49], 1);\n    else                atomicAdd(&counts[50], 1);\n    dev_test9_and_14(&result1, &result2, Ndata, len, N, tid, 32);\n    if (result1 > results[12])      atomicAdd(&counts[36], 1);\n    else if (result1 == results[12])  atomicAdd(&counts[37], 1);\n    else                atomicAdd(&counts[38], 1);\n    if (result2 > results[17])      atomicAdd(&counts[51], 1);\n    else if (result2 == results[17])  atomicAdd(&counts[52], 1);\n    else                atomicAdd(&counts[53], 1);\n  }\n}",
            "__device__\nunsigned int LCG_random(unsigned int * seed) {\n  const unsigned int m = 2147483648;\n  const unsigned int a = 26757677;\n  const unsigned int c = 1;\n  *seed = (a * (*seed) + c) % m;\n  return *seed;\n}\n\n__global__\nvoid binary_shuffling_kernel(\n    uint8_t *Ndata, uint8_t *bNdata, const uint8_t *data,\n    const uint32_t len, const uint32_t blen, const uint32_t N)\n{\n  uint32_t tid = threadIdx.x + blockIdx.x * blockDim.x;\n  uint32_t i = 0, j = len - 1, random = 0;\n  uint8_t tmp = 0;\n\n  uint64_t seed = 0;\n\n  for (i = 0; i < len; i++) {\n    Ndata[i * N + tid] = data[i];\n    seed += data[i]; \n  }\n  seed = seed ^ tid;\n\n  while (j > 0) {\n    random = LCG_random(&seed) % j;\n    tmp = Ndata[random * N + tid];\n    Ndata[random * N + tid] = Ndata[j * N + tid];\n    Ndata[j * N + tid] = tmp;\n    j--;\n  }\n\n  for (i = 0; i < blen; i++) {\n    tmp = (Ndata[8 * i * N + tid] & 0x1) << 7;\n    tmp ^= (Ndata[(8 * i + 1) * N + tid] & 0x1) << 6;\n    tmp ^= (Ndata[(8 * i + 2) * N + tid] & 0x1) << 5;\n    tmp ^= (Ndata[(8 * i + 3) * N + tid] & 0x1) << 4;\n    tmp ^= (Ndata[(8 * i + 4) * N + tid] & 0x1) << 3;\n    tmp ^= (Ndata[(8 * i + 5) * N + tid] & 0x1) << 2;\n    tmp ^= (Ndata[(8 * i + 6) * N + tid] & 0x1) << 1;\n    tmp ^= (Ndata[(8 * i + 7) * N + tid] & 0x1);\n    bNdata[i * N + tid] = tmp;\n  }\n}",
            "inline __host__ __device__ uint4 make_uint4(int4 a)\n{\n    return make_uint4(uint(a.x), uint(a.y), uint(a.z), uint(a.w));\n}\n\ninline __host__ __device__ uint4 max(uint4 a, uint4 b)\n{\n    return make_uint4(max(a.x,b.x), max(a.y,b.y), max(a.z,b.z), max(a.w,b.w));\n}\n\n__device__ uint8_t dev_hammingweight(uint8_t data) {\n  uint8_t tmp = 0;\n  tmp = (data >> 7) & 0x1;\n  tmp += (data >> 6) & 0x1;\n  tmp += (data >> 5) & 0x1;\n  tmp += (data >> 4) & 0x1;\n  tmp += (data >> 3) & 0x1;\n  tmp += (data >> 2) & 0x1;\n  tmp += (data >> 1) & 0x1;\n  tmp += data & 0x1;\n  return tmp;\n}\n\ninline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\ninline __host__ __device__ float4 fabs(float4 v)\n{\n    return make_float4(fabs(v.x), fabs(v.y), fabs(v.z), fabs(v.w));\n}\n\n__device__ void dev_binary_test2_4(double *out_num, double *out_len, double *out_max, const uint8_t *data,\n    const uint32_t len, const uint32_t N, uint32_t tid)\n{\n  uint32_t num_runs = 1, len_runs = 1, max_len_runs = 0, pos = 0;\n  bool bflag1 = 0, bflag2 = 0;\n  uint32_t i = 0;\n  if (dev_hammingweight(data[tid]) <= dev_hammingweight(data[N + tid]))\n    bflag1 = 1;\n\n  for (i = 1; i < len - 1; i++) {\n    pos += bflag1;\n    bflag2 = 0;\n\n    if (dev_hammingweight(data[i*N + tid]) <= dev_hammingweight(data[(i + 1)*N + tid]))\n      bflag2 = 1;\n    if (bflag1 == bflag2)\n      len_runs++;\n    else {\n      num_runs++;\n      if (len_runs > max_len_runs)\n        max_len_runs = len_runs;\n      len_runs = 1;\n    }\n    bflag1 = bflag2;\n  }\n  pos += bflag1;\n  *out_num = (double)num_runs;\n  *out_len = (double)max_len_runs;\n  *out_max = (double)max(pos, len - pos);\n}\n\n__device__ void dev_binary_test9_and_14(double *out_num, double *out_strength, const uint8_t *data, const uint32_t len,\n    const uint32_t N, uint32_t tid, const uint32_t lag)\n{\n  double temp1 = 0, temp2 = 0;\n  uint32_t i = 0;\n  for (i = 0; i < len - lag; i++) {\n    if (dev_hammingweight(data[i*N + tid]) == dev_hammingweight(data[(i + lag)*N + tid]))\n      temp1++;\n    temp2 += (dev_hammingweight(data[i*N + tid]) * dev_hammingweight(data[(i + lag)*N + tid]));\n  }\n  *out_num = temp1;\n  *out_strength = temp2;\n}\n\n__device__ void dev_test1(double *out_max, const double mean, const uint8_t *data, const uint32_t len,\n    const uint32_t N, uint32_t tid)\n{\n  double max = 0, temp = 0, sum = 0;\n  uint64_t i = 0;\n  uint64_t idx = 0;\n  for (i = 0; i < len; i++) {\n    idx = i * N + tid;\n    sum += data[idx];\n    temp = fabs(sum - ((i + 1)*mean));\n    if (max < temp)\n      max = temp;\n  }\n  *out_max = max;\n}\n\n__device__ void dev_test5_6(double *out_num, double *out_len, const double median, const uint8_t *data, const uint32_t len,\n    const uint32_t N, uint32_t tid)\n{\n  uint32_t num_runs = 1, len_runs = 1, max_len_runs = 0;\n  bool bflag1 = 0, bflag2 = 0;\n  uint32_t i = 0;\n\n  if (data[tid] >= median)\n    bflag1 = 1;\n\n  for (i = 1; i < len; i++) {\n    bflag2 = 0;\n\n    if (data[i*N + tid] >= median)\n      bflag2 = 1;\n    if (bflag1 == bflag2)\n      len_runs++;\n    else {\n      num_runs++;\n      if (len_runs > max_len_runs)\n        max_len_runs = len_runs;\n      len_runs = 1;\n    }\n    bflag1 = bflag2;\n  }\n\n  *out_num = (double)num_runs;\n  *out_len = (double)max_len_runs;\n}\n\n__device__ void dev_test7_8(double *out_average, double *out_max, const uint8_t *data, const uint32_t size,\n    const uint32_t len, const uint32_t N, uint32_t tid)\n{\n  uint64_t i = 0, j = 0, k = 0;\n  bool dups[256] = { 0, };\n  uint32_t cnt = 0;\n  uint32_t max = 0;\n  double avg = 0;\n  uint64_t idx = 0;\n  while (i + j < len) {\n    for (k = 0; k < (uint32_t)(1 << size); k++) dups[k] = false;\n\n    while (i + j < len) {\n      idx = (i + j)*N + tid;\n      if (dups[data[idx]]) {\n        avg += j;\n        if (j > max)\n          max = j;\n        cnt++;\n        i += j;\n        j = 0;\n        break;\n      }\n      else {\n        dups[data[idx]] = true;\n        ++j;\n      }\n    }\n    ++i;\n  }\n\n  *out_average = avg / (double)cnt;\n  *out_max = (double)max;\n}\n\n__global__\nvoid binary_statistical_tests_kernel(\n    uint32_t *counts, const double *results, const double mean, const double median,\n    const uint8_t *Ndata, const uint8_t *bNdata, \n    const uint32_t size, const uint32_t len, const uint32_t blen,\n    const uint32_t N, const uint32_t num_block)\n{\n  double result1 = 0, result2 = 0, result3 = 0;\n  uint32_t tid = threadIdx.x + (blockIdx.x % num_block)*blockDim.x;\n\n  if ((blockIdx.x / num_block) == 0) {\n    dev_test1(&result1, mean, Ndata, len, N, tid);\n    if ((float)result1 > (float)results[0])      atomicAdd(&counts[0], 1);\n    else if ((float)result1 == (float)results[0])  atomicAdd(&counts[1], 1);\n    else                      atomicAdd(&counts[2], 1);\n  }\n  else if ((blockIdx.x / num_block) == 1) {\n    dev_test5_6(&result1, &result2, median, Ndata, len, N, tid);\n    if (result1 > results[4])      atomicAdd(&counts[12], 1);\n    else if (result1 == results[4])    atomicAdd(&counts[13], 1);\n    else                atomicAdd(&counts[14], 1);\n    if (result2 > results[5])      atomicAdd(&counts[15], 1);\n    else if (result2 == results[5])    atomicAdd(&counts[16], 1);\n    else                atomicAdd(&counts[17], 1);\n\n    dev_binary_test2_4(&result1, &result2, &result3, bNdata, blen, N, tid);\n    if (result1 > results[1])      atomicAdd(&counts[3], 1);\n    else if (result1 == results[1])    atomicAdd(&counts[4], 1);\n    else                atomicAdd(&counts[5], 1);\n    if (result2 > results[2])      atomicAdd(&counts[6], 1);\n    else if (result2 == results[2])    atomicAdd(&counts[7], 1);\n    else                atomicAdd(&counts[8], 1);\n    if (result3 > results[3])      atomicAdd(&counts[9], 1);\n    else if (result3 == results[3])    atomicAdd(&counts[10], 1);\n    else                atomicAdd(&counts[11], 1);\n  }\n  else if ((blockIdx.x / num_block) == 2) {\n    dev_test7_8(&result1, &result2, bNdata, 8, blen, N, tid);\n    if (result1 > results[6])      atomicAdd(&counts[18], 1);\n    else if (result1 == results[6])    atomicAdd(&counts[19], 1);\n    else                atomicAdd(&counts[20], 1);\n    if (result2 > results[7])      atomicAdd(&counts[21], 1);\n    else if (result2 == results[7])    atomicAdd(&counts[22], 1);\n    else                atomicAdd(&counts[23], 1);\n  }\n  else if ((blockIdx.x / num_block) == 3) {\n    dev_binary_test9_and_14(&result1, &result2, bNdata, blen, N, tid, 1);\n    if (result1 > results[8])      atomicAdd(&counts[24], 1);\n    else if (result1 == results[8])    atomicAdd(&counts[25], 1);\n    else                atomicAdd(&counts[26], 1);\n    if (result2 > results[13])      atomicAdd(&counts[39], 1);\n    else if (result2 == results[13])  atomicAdd(&counts[40], 1);\n    else                atomicAdd(&counts[41], 1);\n\n    dev_binary_test9_and_14(&result1, &result2, bNdata, blen, N, tid, 2);\n    if (result1 > results[9])      atomicAdd(&counts[27], 1);\n    else if (result1 == results[9])    atomicAdd(&counts[28], 1);\n    else                atomicAdd(&counts[29], 1);\n    if (result2 > results[14])      atomicAdd(&counts[42], 1);\n    else if (result2 == results[14])  atomicAdd(&counts[43], 1);\n    else                atomicAdd(&counts[44], 1);\n\n    dev_binary_test9_and_14(&result1, &result2, bNdata, blen, N, tid, 8);\n    if (result1 > results[10])      atomicAdd(&counts[30], 1);\n    else if (result1 == results[10])  atomicAdd(&counts[31], 1);\n    else                atomicAdd(&counts[32], 1);\n    if (result2 > results[15])      atomicAdd(&counts[45], 1);\n    else if (result2 == results[15])  atomicAdd(&counts[46], 1);\n    else                atomicAdd(&counts[47], 1);\n\n    dev_binary_test9_and_14(&result1, &result2, bNdata, blen, N, tid, 16);\n    if (result1 > results[11])      atomicAdd(&counts[33], 1);\n    else if (result1 == results[11])  atomicAdd(&counts[34], 1);\n    else                atomicAdd(&counts[35], 1);\n    if (result2 > results[16])      atomicAdd(&counts[48], 1);\n    else if (result2 == results[16])  atomicAdd(&counts[49], 1);\n    else                atomicAdd(&counts[50], 1);\n\n    dev_binary_test9_and_14(&result1, &result2, bNdata, blen, N, tid, 32);\n    if (result1 > results[12])      atomicAdd(&counts[36], 1);\n    else if (result1 == results[12])  atomicAdd(&counts[37], 1);\n    else                atomicAdd(&counts[38], 1);\n    if (result2 > results[17])      atomicAdd(&counts[51], 1);\n    else if (result2 == results[17])  atomicAdd(&counts[52], 1);\n    else                atomicAdd(&counts[53], 1);\n  }\n}"
        ]
    },
    "aop-cuda": {
        "/Users/gbolet/hecbench-roofline/src/aop-cuda/main.cu": [
            "__global__ __launch_bounds__(NUM_THREADS_PER_BLOCK)\nvoid generate_paths_kernel(int num_timesteps, \n                           int num_paths, \n                           Payoff payoff,\n                           double dt, \n                           double S0, \n                           double r, \n                           double sigma, \n                           const double *__restrict samples, \n                           double *__restrict paths)\n{\n  // The path generated by this thread.\n  int path = blockIdx.x*NUM_THREADS_PER_BLOCK + threadIdx.x;\n\n  // Early exit.\n  if( path >= num_paths ) return;\n  \n  // Compute (r - sigma^2 / 2).\n  const double r_min_half_sigma_sq_dt = (r - 0.5*sigma*sigma)*dt;\n  // Compute sigma*sqrt(dt).\n  const double sigma_sqrt_dt = sigma*sqrt(dt);\n\n  // Keep the previous price.\n  double S = S0;\n\n  // The offset.\n  int offset = path;\n  \n  // Each thread generates several timesteps. \n  for( int timestep = 0 ; timestep < num_timesteps-1 ; ++timestep, offset += num_paths )\n  {\n    S = S * exp(r_min_half_sigma_sq_dt + sigma_sqrt_dt*samples[offset]);\n    paths[offset] = S;\n  }\n\n  // The asset price.\n  S = S * exp(r_min_half_sigma_sq_dt + sigma_sqrt_dt*samples[offset]);\n\n  // Store the payoff at expiry.\n  paths[offset] = payoff(S);\n}",
            "#define T ((int)32)\n\n\ninline __host__ __device__ int4 make_int4(float4 a)\n{\n    return make_int4(int(a.x), int(a.y), int(a.z), int(a.w));\n}\n\ninline __host__ __device__ int4 abs(int4 v)\n{\n    return make_int4(abs(v.x), abs(v.y), abs(v.z), abs(v.w));\n}\n\nstatic __device__ __forceinline__ void assemble_R(int m, double4 &sums, double *smem_svds)\n{\n  // Assemble R.\n\n  double x0 = smem_svds[0];\n  double x1 = smem_svds[1];\n  double x2 = smem_svds[2];\n\n  double x0_sq = x0 * x0;\n\n  double sum1 = sums.x - x0;\n  double sum2 = sums.y - x0_sq;\n  double sum3 = sums.z - x0_sq*x0;\n  double sum4 = sums.w - x0_sq*x0_sq;\n\n  double m_as_dbl = (double) m;\n  double sigma = m_as_dbl - 1.0;\n  double mu = sqrt(m_as_dbl);\n  double v0 = -sigma / (1.0 + mu);\n  double v0_sq = v0*v0;\n  double beta = 2.0 * v0_sq / (sigma + v0_sq);\n  \n  double inv_v0 = 1.0 / v0;\n  double one_min_beta = 1.0 - beta;\n  double beta_div_v0  = beta * inv_v0;\n  \n  smem_svds[0] = mu;\n  smem_svds[1] = one_min_beta*x0 - beta_div_v0*sum1;\n  smem_svds[2] = one_min_beta*x0_sq - beta_div_v0*sum2;\n  \n  // Rank update coefficients.\n  \n  double beta_div_v0_sq = beta_div_v0 * inv_v0;\n  \n  double c1 = beta_div_v0_sq*sum1 + beta_div_v0*x0;\n  double c2 = beta_div_v0_sq*sum2 + beta_div_v0*x0_sq;\n\n  // 2nd step of QR.\n  \n  double x1_sq = x1*x1;\n\n  sum1 -= x1;\n  sum2 -= x1_sq;\n  sum3 -= x1_sq*x1;\n  sum4 -= x1_sq*x1_sq;\n  \n  x0 = x1-c1;\n  x0_sq = x0*x0;\n  sigma = sum2 - 2.0*c1*sum1 + (m_as_dbl-2.0)*c1*c1;\n  if( abs(sigma) < 1.0e-16 )\n    beta = 0.0;\n  else\n  {\n    mu = sqrt(x0_sq + sigma);\n    if( x0 <= 0.0 )\n      v0 = x0 - mu;\n    else\n      v0 = -sigma / (x0 + mu);\n    v0_sq = v0*v0;\n    beta = 2.0*v0_sq / (sigma + v0_sq);\n  }\n  \n  inv_v0 = 1.0 / v0;\n  beta_div_v0 = beta * inv_v0;\n  \n  // The coefficient to perform the rank update.\n  double c3 = (sum3 - c1*sum2 - c2*sum1 + (m_as_dbl-2.0)*c1*c2)*beta_div_v0;\n  double c4 = (x1_sq-c2)*beta_div_v0 + c3*inv_v0;\n  double c5 = c1*c4 - c2;\n  \n  one_min_beta = 1.0 - beta;\n  \n  // Update R. \n  smem_svds[3] = one_min_beta*x0 - beta_div_v0*sigma;\n  smem_svds[4] = one_min_beta*(x1_sq-c2) - c3;\n  \n  // 3rd step of QR.\n  \n  double x2_sq = x2*x2;\n\n  sum1 -= x2;\n  sum2 -= x2_sq;\n  sum3 -= x2_sq*x2;\n  sum4 -= x2_sq*x2_sq;\n  \n  x0 = x2_sq-c4*x2+c5;\n  sigma = sum4 - 2.0*c4*sum3 + (c4*c4 + 2.0*c5)*sum2 - 2.0*c4*c5*sum1 + (m_as_dbl-3.0)*c5*c5;\n  if( abs(sigma) < 1.0e-12 )\n    beta = 0.0;\n  else\n  {\n    mu = sqrt(x0*x0 + sigma);\n    if( x0 <= 0.0 )\n      v0 = x0 - mu;\n    else\n      v0 = -sigma / (x0 + mu);\n    v0_sq = v0*v0;\n    beta = 2.0*v0_sq / (sigma + v0_sq);\n  }\n  \n  // Update R.\n  smem_svds[5] = (1.0-beta)*x0 - (beta/v0)*sigma;\n}\n\nstatic __device__ double off_diag_norm(double A01, double A02, double A12)\n{\n  return sqrt(2.0 * (A01*A01 + A02*A02 + A12*A12));\n}\n\n__device__ inline void swap(T &a, T &b) {\n  T tmp = a;\n  a = b;\n  b = tmp;\n}\n\nstatic __device__ __forceinline__ void svd_3x3(int m, double4 &sums, double *smem_svds)\n{\n  // Assemble the R matrix.\n  assemble_R(m, sums, smem_svds);\n\n  // The matrix R.\n  double R00 = smem_svds[0];\n  double R01 = smem_svds[1];\n  double R02 = smem_svds[2];\n  double R11 = smem_svds[3];\n  double R12 = smem_svds[4];\n  double R22 = smem_svds[5];\n\n  // We compute the eigenvalues/eigenvectors of A = R^T R.\n  \n  double A00 = R00*R00;\n  double A01 = R00*R01;\n  double A02 = R00*R02;\n  double A11 = R01*R01 + R11*R11;\n  double A12 = R01*R02 + R11*R12;\n  double A22 = R02*R02 + R12*R12 + R22*R22;\n  \n  // We keep track of V since A = Sigma^2 V. Each thread stores a row of V.\n  \n  double V00 = 1.0, V01 = 0.0, V02 = 0.0;\n  double V10 = 0.0, V11 = 1.0, V12 = 0.0;\n  double V20 = 0.0, V21 = 0.0, V22 = 1.0;\n  \n  // The Jacobi algorithm is iterative. We fix the max number of iter and the minimum tolerance.\n  \n  const int max_iters = 16;\n  const double tolerance = 1.0e-12;\n  \n  // Iterate until we reach the max number of iters or the tolerance.\n \n  for( int iter = 0 ; off_diag_norm(A01, A02, A12) >= tolerance && iter < max_iters ; ++iter )\n  {\n    double c, s, B00, B01, B02, B10, B11, B12, B20, B21, B22;\n    \n    // Compute the Jacobi matrix for p=0 and q=1.\n    \n    c = 1.0, s = 0.0;\n    if( A01 != 0.0 )\n    {\n      double tau = (A11 - A00) / (2.0 * A01);\n      double sgn = tau < 0.0 ? -1.0 : 1.0;\n      double t   = sgn / (sgn*tau + sqrt(1.0 + tau*tau));\n      \n      c = 1.0 / sqrt(1.0 + t*t);\n      s = t*c;\n    }\n    \n    // Update A = J^T A J and V = V J.\n    \n    B00 = c*A00 - s*A01;\n    B01 = s*A00 + c*A01;\n    B10 = c*A01 - s*A11;\n    B11 = s*A01 + c*A11;\n    B02 = A02;\n    \n    A00 = c*B00 - s*B10;\n    A01 = c*B01 - s*B11;\n    A11 = s*B01 + c*B11;\n    A02 = c*B02 - s*A12;\n    A12 = s*B02 + c*A12;\n    \n    B00 = c*V00 - s*V01;\n    V01 = s*V00 + c*V01;\n    V00 = B00;\n    \n    B10 = c*V10 - s*V11;\n    V11 = s*V10 + c*V11;\n    V10 = B10;\n    \n    B20 = c*V20 - s*V21;\n    V21 = s*V20 + c*V21;\n    V20 = B20;\n    \n    // Compute the Jacobi matrix for p=0 and q=2.\n    \n    c = 1.0, s = 0.0;\n    if( A02 != 0.0 )\n    {\n      double tau = (A22 - A00) / (2.0 * A02);\n      double sgn = tau < 0.0 ? -1.0 : 1.0;\n      double t   = sgn / (sgn*tau + sqrt(1.0 + tau*tau));\n      \n      c = 1.0 / sqrt(1.0 + t*t);\n      s = t*c;\n    }\n    \n    // Update A = J^T A J and V = V J.\n    \n    B00 = c*A00 - s*A02;\n    B01 = c*A01 - s*A12;\n    B02 = s*A00 + c*A02;\n    B20 = c*A02 - s*A22;\n    B22 = s*A02 + c*A22;\n    \n    A00 = c*B00 - s*B20;\n    A12 = s*A01 + c*A12;\n    A02 = c*B02 - s*B22;\n    A22 = s*B02 + c*B22;\n    A01 = B01;\n    \n    B00 = c*V00 - s*V02;\n    V02 = s*V00 + c*V02;\n    V00 = B00;\n    \n    B10 = c*V10 - s*V12;\n    V12 = s*V10 + c*V12;\n    V10 = B10;\n    \n    B20 = c*V20 - s*V22;\n    V22 = s*V20 + c*V22;\n    V20 = B20;\n    \n    // Compute the Jacobi matrix for p=1 and q=2.\n    \n    c = 1.0, s = 0.0;\n    if( A12 != 0.0 )\n    {\n      double tau = (A22 - A11) / (2.0 * A12);\n      double sgn = tau < 0.0 ? -1.0 : 1.0;\n      double t   = sgn / (sgn*tau + sqrt(1.0 + tau*tau));\n      \n      c = 1.0 / sqrt(1.0 + t*t);\n      s = t*c;\n    }\n    \n    // Update A = J^T A J and V = V J.\n    \n    B02 = s*A01 + c*A02;\n    B11 = c*A11 - s*A12;\n    B12 = s*A11 + c*A12;\n    B21 = c*A12 - s*A22;\n    B22 = s*A12 + c*A22;\n    \n    A01 = c*A01 - s*A02;\n    A02 = B02;\n    A11 = c*B11 - s*B21;\n    A12 = c*B12 - s*B22;\n    A22 = s*B12 + c*B22;\n    \n    B01 = c*V01 - s*V02;\n    V02 = s*V01 + c*V02;\n    V01 = B01;\n    \n    B11 = c*V11 - s*V12;\n    V12 = s*V11 + c*V12;\n    V11 = B11;\n    \n    B21 = c*V21 - s*V22;\n    V22 = s*V21 + c*V22;\n    V21 = B21;\n  }\n\n  // Swap the columns to have S[0] >= S[1] >= S[2].\n  if( A00 < A11 )\n  {\n    swap(A00, A11);\n    swap(V00, V01);\n    swap(V10, V11);\n    swap(V20, V21);\n  }\n  if( A00 < A22 )\n  {\n    swap(A00, A22);\n    swap(V00, V02);\n    swap(V10, V12);\n    swap(V20, V22);\n  }\n  if( A11 < A22 )\n  {\n    swap(A11, A22);\n    swap(V01, V02);\n    swap(V11, V12);\n    swap(V21, V22);\n  }\n\n  //printf(\"timestep=%3d, svd0=%.8lf svd1=%.8lf svd2=%.8lf\\n\", blockIdx.x, sqrt(A00), sqrt(A11), sqrt(A22));\n  \n  // Invert the diagonal terms and compute V*S^-1.\n  \n  double inv_S0 = abs(A00) < 1.0e-12 ? 0.0 : 1.0 / A00;\n  double inv_S1 = abs(A11) < 1.0e-12 ? 0.0 : 1.0 / A11;\n  double inv_S2 = abs(A22) < 1.0e-12 ? 0.0 : 1.0 / A22;\n\n  // printf(\"SVD: timestep=%3d %12.8lf %12.8lf %12.8lf\\n\", blockIdx.x, sqrt(A00), sqrt(A11), sqrt(A22));\n  \n  double U00 = V00 * inv_S0; \n  double U01 = V01 * inv_S1; \n  double U02 = V02 * inv_S2;\n  double U10 = V10 * inv_S0; \n  double U11 = V11 * inv_S1; \n  double U12 = V12 * inv_S2;\n  double U20 = V20 * inv_S0; \n  double U21 = V21 * inv_S1; \n  double U22 = V22 * inv_S2;\n  \n  // Compute V*S^-1*V^T*R^T.\n  \n#ifdef WITH_FULL_W_MATRIX\n  double B00 = U00*V00 + U01*V01 + U02*V02;\n  double B01 = U00*V10 + U01*V11 + U02*V12;\n  double B02 = U00*V20 + U01*V21 + U02*V22;\n  double B10 = U10*V00 + U11*V01 + U12*V02;\n  double B11 = U10*V10 + U11*V11 + U12*V12;\n  double B12 = U10*V20 + U11*V21 + U12*V22;\n  double B20 = U20*V00 + U21*V01 + U22*V02;\n  double B21 = U20*V10 + U21*V11 + U22*V12;\n  double B22 = U20*V20 + U21*V21 + U22*V22;\n  \n  smem_svds[ 6] = B00*R00 + B01*R01 + B02*R02;\n  smem_svds[ 7] =           B01*R11 + B02*R12;\n  smem_svds[ 8] =                     B02*R22;\n  smem_svds[ 9] = B10*R00 + B11*R01 + B12*R02;\n  smem_svds[10] =           B11*R11 + B12*R12;\n  smem_svds[11] =                     B12*R22;\n  smem_svds[12] = B20*R00 + B21*R01 + B22*R02;\n  smem_svds[13] =           B21*R11 + B22*R12;\n  smem_svds[14] =                     B22*R22;\n#else\n  double B00 = U00*V00 + U01*V01 + U02*V02;\n  double B01 = U00*V10 + U01*V11 + U02*V12;\n  double B02 = U00*V20 + U01*V21 + U02*V22;\n  double B11 = U10*V10 + U11*V11 + U12*V12;\n  double B12 = U10*V20 + U11*V21 + U12*V22;\n  double B22 = U20*V20 + U21*V21 + U22*V22;\n  \n  smem_svds[ 6] = B00*R00 + B01*R01 + B02*R02;\n  smem_svds[ 7] =           B01*R11 + B02*R12;\n  smem_svds[ 8] =                     B02*R22;\n  smem_svds[ 9] =           B11*R11 + B12*R12;\n  smem_svds[10] =                     B12*R22;\n  smem_svds[11] =                     B22*R22;\n#endif\n}\n\n__global__ __launch_bounds__(NUM_THREADS_PER_BLOCK, 4)\nvoid prepare_svd_kernel(int num_paths, \n                        int min_in_the_money, \n                        Payoff payoff, \n                        const double *__restrict paths, \n                                 int *__restrict all_out_of_the_money, \n                              double *__restrict svds)\n{\n  // We need to perform a scan to find the first 3 stocks pay off.\n  __shared__ int scan_input[NUM_THREADS_PER_BLOCK];\n  __shared__ int scan_output[1+NUM_THREADS_PER_BLOCK];\n\n  // sum reduction\n  __shared__ double4 lsums;\n  __shared__ int lsum;\n\n  // Shared buffer for the ouput.\n  __shared__ double smem_svds[R_W_MATRICES_SMEM_SLOTS];\n\n  // Each block works on a single timestep. \n  const int timestep = blockIdx.x;\n  // The timestep offset.\n  const int offset = timestep * num_paths;\n\n  // Sums.\n  int m = 0;\n  double4 sums = make_double4(0,0,0,0);\n\n  // Initialize the shared memory. DBL_MAX is a marker to specify that the value is invalid.\n  if( threadIdx.x < R_W_MATRICES_SMEM_SLOTS )\n    smem_svds[threadIdx.x] = 0.0;\n  __syncthreads();\n\n  // Have we already found our 3 first paths which pay off.\n  int found_paths = 0;\n\n  // Iterate over the paths.\n  for( int path = threadIdx.x ; path < num_paths ; path += NUM_THREADS_PER_BLOCK )\n  {\n    // Load the asset price to determine if it pays off.\n    double S = paths[offset + path];\n\n    // Check if it pays off.\n    const int in_the_money = payoff.is_in_the_money(S);\n\n    // Try to check if we have found the 3 first stocks.\n    scan_input[threadIdx.x] = in_the_money;\n    __syncthreads();\n    if (threadIdx.x == 0) {\n      scan_output[0] = 0;\n      for (int i = 1; i <= NUM_THREADS_PER_BLOCK; i++) \n        scan_output[i] = scan_output[i-1]+scan_input[i-1];\n    }\n    __syncthreads();\n    const int partial_sum = scan_output[threadIdx.x];\n    const int total_sum = scan_output[NUM_THREADS_PER_BLOCK];\n\n    if( found_paths < 3 )\n    {\n      if( in_the_money && found_paths + partial_sum < 3 )\n        smem_svds[found_paths + partial_sum] = S;\n      __syncthreads();\n      found_paths += total_sum;\n    }\n\n    // Early continue if no item pays off.\n    if (threadIdx.x == 0) lsum = 0;\n    __syncthreads();\n    atomicOr(&lsum, in_the_money);\n    __syncthreads();\n    if (lsum == 0) continue;\n    \n    // Update the number of payoff items.\n    m += in_the_money;\n\n    // The \"normalized\" value.\n    double x = 0.0, x_sq = 0.0;\n    if( in_the_money )\n    {\n      x = S;\n      x_sq = S*S;\n    }\n\n    // Compute the 4 sums.\n    sums.x += x;\n    sums.y += x_sq;\n    sums.z += x_sq*x;\n    sums.w += x_sq*x_sq;\n  }\n\n  // Compute the final reductions.\n  if (threadIdx.x == 0) lsum = 0;\n  __syncthreads();\n\n  atomicAdd(&lsum, m);\n\n  __syncthreads();\n\n  int not_enough_paths = 0;\n  // Do we all exit?\n  if (threadIdx.x == 0 && lsum < min_in_the_money)\n    not_enough_paths = 1;\n  \n  // Early exit if no path is in the money.\n  if( not_enough_paths )\n  {\n    if( threadIdx.x == 0 )\n      all_out_of_the_money[blockIdx.x] = 1;\n  } \n  else\n  {\n    // Compute the final reductions.\n    if (threadIdx.x == 0)\n      lsums = make_double4(0,0,0,0);\n    __syncthreads();\n\n    atomicAdd(&lsums.x, sums.x);\n    atomicAdd(&lsums.y, sums.y);\n    atomicAdd(&lsums.z, sums.z);\n    atomicAdd(&lsums.w, sums.w);\n    \n    __syncthreads();\n    \n    // The 1st thread has everything he needs to build R from the QR decomposition.\n    if( threadIdx.x == 0 )\n      svd_3x3(lsum, lsums, smem_svds);\n\n    __syncthreads();\n\n    // Store the final results.\n    if( threadIdx.x < R_W_MATRICES_SMEM_SLOTS )\n      svds[16*blockIdx.x + threadIdx.x] = smem_svds[threadIdx.x];\n  }\n}",
            "__global__ __launch_bounds__(NUM_THREADS_PER_BLOCK, 8)\nvoid compute_partial_beta_kernel(int num_paths,\n                                 Payoff payoff,\n                                 const double *__restrict svd,\n                                 const double *__restrict paths,\n                                 const double *__restrict cashflows,\n                                 const int *__restrict all_out_of_the_money,\n                                 double *__restrict partial_sums)\n{\n  // The shared memory storage.\n  __shared__ double3 lsums;\n  \n  // The shared memory to store the SVD.\n  __shared__ double shared_svd[R_W_MATRICES_SMEM_SLOTS];\n    \n  // Early exit if needed.\n  if( *all_out_of_the_money ) return;\n\n  // The number of threads per grid.\n  const int NUM_THREADS_PER_GRID = NUM_THREADS_PER_BLOCK * gridDim.x;\n\n  // The 1st threads loads the matrices SVD and R.\n  if( threadIdx.x < R_W_MATRICES_SMEM_SLOTS )\n    shared_svd[threadIdx.x] = svd[threadIdx.x];\n  __syncthreads();\n\n  // Load the terms of R.\n  const double R00 = shared_svd[ 0];\n  const double R01 = shared_svd[ 1];\n  const double R02 = shared_svd[ 2];\n  const double R11 = shared_svd[ 3];\n  const double R12 = shared_svd[ 4];\n  const double R22 = shared_svd[ 5];\n\n  // Load the elements of W.\n#ifdef WITH_FULL_W_MATRIX\n  const double W00 = shared_svd[ 6];\n  const double W01 = shared_svd[ 7];\n  const double W02 = shared_svd[ 8];\n  const double W10 = shared_svd[ 9];\n  const double W11 = shared_svd[10];\n  const double W12 = shared_svd[11];\n  const double W20 = shared_svd[12];\n  const double W21 = shared_svd[13];\n  const double W22 = shared_svd[14];\n#else\n  const double W00 = shared_svd[ 6];\n  const double W01 = shared_svd[ 7];\n  const double W02 = shared_svd[ 8];\n  const double W11 = shared_svd[ 9];\n  const double W12 = shared_svd[10];\n  const double W22 = shared_svd[11];\n#endif\n\n  // Invert the diagonal of R.\n  const double inv_R00 = R00 != 0.0 ? __drcp_rn(R00) : 0.0;\n  const double inv_R11 = R11 != 0.0 ? __drcp_rn(R11) : 0.0;\n  const double inv_R22 = R22 != 0.0 ? __drcp_rn(R22) : 0.0;\n\n  // Precompute the R terms.\n  const double inv_R01 = inv_R00*inv_R11*R01;\n  const double inv_R02 = inv_R00*inv_R22*R02;\n  const double inv_R12 =         inv_R22*R12;\n  \n  // Precompute W00/R00.\n#ifdef WITH_FULL_W_MATRIX\n  const double inv_W00 = W00*inv_R00;\n  const double inv_W10 = W10*inv_R00;\n  const double inv_W20 = W20*inv_R00;\n#else\n  const double inv_W00 = W00*inv_R00;\n#endif\n\n  // Each thread has 3 numbers to sum.\n  double beta0 = 0.0, beta1 = 0.0, beta2 = 0.0;\n\n  // Iterate over the paths.\n  for( int path = blockIdx.x*NUM_THREADS_PER_BLOCK + threadIdx.x ; path < num_paths ; path += NUM_THREADS_PER_GRID )\n  {\n    // Threads load the asset price to rebuild Q from the QR decomposition.\n    double S = paths[path];\n\n    // Is the path in the money?\n    const int in_the_money = payoff.is_in_the_money(S);\n\n    // Compute Qis. The elements of the Q matrix in the QR decomposition.\n    double Q1i = inv_R11*S - inv_R01;\n    double Q2i = inv_R22*S*S - inv_R02 - Q1i*inv_R12;\n\n    // Compute the ith row of the pseudo-inverse of [1 X X^2].\n#ifdef WITH_FULL_W_MATRIX\n    const double WI0 = inv_W00 + W01 * Q1i + W02 * Q2i;\n    const double WI1 = inv_W10 + W11 * Q1i + W12 * Q2i;\n    const double WI2 = inv_W20 + W21 * Q1i + W22 * Q2i;\n#else\n    const double WI0 = inv_W00 + W01 * Q1i + W02 * Q2i;\n    const double WI1 =           W11 * Q1i + W12 * Q2i;\n    const double WI2 =                       W22 * Q2i;\n#endif\n\n    // Each thread loads its element from the Y vector.\n    double cashflow = in_the_money ? cashflows[path] : 0.0;\n  \n    // Update beta.\n    beta0 += WI0*cashflow;\n    beta1 += WI1*cashflow;\n    beta2 += WI2*cashflow;\n  }\n\n  // Compute the sum of the elements in the block. \n  if( threadIdx.x == 0 )\n    lsums = make_double3(0,0,0);\n  __syncthreads();\n\n  atomicAdd(&lsums.x, beta0);\n  atomicAdd(&lsums.y, beta1);\n  atomicAdd(&lsums.z, beta2);\n \n  __syncthreads();\n  \n  // The 1st thread stores the result to GMEM.\n  if( threadIdx.x == 0 )\n  {\n    partial_sums[0*NUM_THREADS_PER_BLOCK + blockIdx.x] = lsums.x;\n    partial_sums[1*NUM_THREADS_PER_BLOCK + blockIdx.x] = lsums.y;\n    partial_sums[2*NUM_THREADS_PER_BLOCK + blockIdx.x] = lsums.z;\n  }\n}",
            "__global__ __launch_bounds__(NUM_THREADS_PER_BLOCK)\nvoid compute_final_beta_kernel(const int *__restrict all_out_of_the_money, double *__restrict beta)\n{\n  // The shared memory for the reduction.\n  __shared__ double3 lsums;\n\n  // Early exit if needed.\n  if( *all_out_of_the_money )\n  {\n    if( threadIdx.x < 3 )\n      beta[threadIdx.x] = 0.0;\n    return;\n  }\n\n  // The final sums.\n  double3 sums;\n  \n  // We load the elements.\n  sums.x = beta[0*NUM_THREADS_PER_BLOCK + threadIdx.x];\n  sums.y = beta[1*NUM_THREADS_PER_BLOCK + threadIdx.x];\n  sums.z = beta[2*NUM_THREADS_PER_BLOCK + threadIdx.x];\n  \n  // Compute the sums.\n  if( threadIdx.x == 0 )\n    lsums = make_double3(0,0,0);\n  __syncthreads();\n\n  atomicAdd(&lsums.x, sums.x);\n  atomicAdd(&lsums.y, sums.y);\n  atomicAdd(&lsums.z, sums.z);\n \n  __syncthreads();\n  \n  // Store beta.\n  if( threadIdx.x == 0 )\n  {\n    //printf(\"beta0=%.8lf beta1=%.8lf beta2=%.8lf\\n\", sums.x, sums.y, sums.z);\n    beta[0] = lsums.x; \n    beta[1] = lsums.y;\n    beta[2] = lsums.z;\n  }\n}",
            "__global__ __launch_bounds__(NUM_THREADS_PER_BLOCK)\nvoid update_cashflow_kernel(int num_paths,\n                            Payoff payoff_object,\n                            double exp_min_r_dt,\n                            const double *__restrict beta,\n                            const double *__restrict paths,\n                            const int *__restrict all_out_of_the_money,\n                            double *__restrict cashflows)\n{\n  const int NUM_THREADS_PER_GRID = gridDim.x * NUM_THREADS_PER_BLOCK;\n\n  // Are we going to skip the computations.\n  const int skip_computations = *all_out_of_the_money;\n\n  // Load the beta coefficients for the linear regression.\n  const double beta0 = beta[0];\n  const double beta1 = beta[1];\n  const double beta2 = beta[2];\n\n  // Iterate over the paths.\n  int path = blockIdx.x*NUM_THREADS_PER_BLOCK + threadIdx.x;\n  for( ; path < num_paths ; path += NUM_THREADS_PER_GRID )\n  {\n    // The cashflow.\n    const double old_cashflow = exp_min_r_dt*cashflows[path];\n    if( skip_computations )\n    {\n      cashflows[path] = old_cashflow;\n      continue;\n    }\n  \n    // Load the asset price.\n    double S  = paths[path];\n    double S2 = S*S;\n\n    // The payoff.\n    double payoff = payoff_object(S);\n\n    // Compute the estimated payoff from continuing.\n    double estimated_payoff = beta0 + beta1*S + beta2*S2;\n\n    // Discount the payoff because we did not take it into account for beta.\n    estimated_payoff *= exp_min_r_dt;\n\n    // Update the payoff.\n    if( payoff <= 1.0e-8 || payoff <= estimated_payoff )\n      payoff = old_cashflow;\n    \n    // Store the updated cashflow.\n    cashflows[path] = payoff;\n  }\n}",
            "__global__ __launch_bounds__(NUM_THREADS_PER_BLOCK)\nvoid compute_partial_sums_kernel(int num_paths, const double *__restrict cashflows, double *__restrict sums)\n{\n  // Shared memory to compute the final sum.\n  __shared__ double lsum;\n\n  // Each thread works on a single path.\n  const int path = blockIdx.x * NUM_THREADS_PER_BLOCK + threadIdx.x;\n\n  // Load the final sum.\n  double sum = 0.0;\n  if( path < num_paths )\n    sum = cashflows[path];\n\n  // Compute the sum over the block.\n  if (threadIdx.x == 0)\n    lsum = 0;\n  __syncthreads();\n  \n  atomicAdd(&lsum, sum);\n  __syncthreads();\n\n  // The block leader writes the sum to GMEM.\n  if( threadIdx.x == 0 )\n    sums[blockIdx.x] = lsum;\n}",
            "__global__ __launch_bounds__(NUM_THREADS_PER_BLOCK)\nvoid compute_final_sum_kernel(int num_paths, int num_blocks, double exp_min_r_dt, double *__restrict sums)\n{\n  // Shared memory to compute the final sum.\n  __shared__ double lsum;\n\n  // The sum.\n  double sum = 0.0;\n  for( int item = threadIdx.x ; item < num_blocks ; item += NUM_THREADS_PER_BLOCK )\n    sum += sums[item];\n\n  // Compute the sum over the block.\n  if (threadIdx.x == 0) lsum = 0;\n  __syncthreads();\n  \n  atomicAdd(&lsum, sum);\n  __syncthreads();\n\n  // The block leader writes the sum to GMEM.\n  if( threadIdx.x == 0 )\n  {\n    sums[0] = exp_min_r_dt * lsum / (double) num_paths;\n  }\n}"
        ]
    },
    "zeropoint-cuda": {
        "/Users/gbolet/hecbench-roofline/src/zeropoint-cuda/main.cu": [
            "inline __host__ __device__ int4 make_int4(float4 a)\n{\n    return make_int4(int(a.x), int(a.y), int(a.z), int(a.w));\n}\n\ninline __host__ __device__ int4 abs(int4 v)\n{\n    return make_int4(abs(v.x), abs(v.y), abs(v.z), abs(v.w));\n}\n\ninline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\ninline __host__ __device__ float4 fabs(float4 v)\n{\n    return make_float4(fabs(v.x), fabs(v.y), fabs(v.z), fabs(v.w));\n}\n\ninline __host__ __device__ float4 fmaxf(float4 a, float4 b)\n{\n    return make_float4(fmaxf(a.x,b.x), fmaxf(a.y,b.y), fmaxf(a.z,b.z), fmaxf(a.w,b.w));\n}\n\ninline  __host__ __device__ float4 fminf(float4 a, float4 b)\n{\n    return make_float4(fminf(a.x,b.x), fminf(a.y,b.y), fminf(a.z,b.z), fminf(a.w,b.w));\n}\n\n__global__ void zero_point (\n    const float* x_min,\n    const float* x_max,\n    int32_t qmin,\n    int32_t qmax,\n    int size,\n    bool preserve_sparsity,\n    float* scale,\n    int32_t* zero_point)\n{\n  int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < size) {\n    float min_val = x_min[i];\n    float max_val = x_max[i];\n\n    if (min_val < 0 && max_val > 0 && preserve_sparsity) {\n      int symmetric_qmin = -((qmax - qmin) / 2 + 1);\n      int symmetric_qmax = (qmax - qmin) / 2;\n      double max_scale = fmax(\n          fabs(min_val / symmetric_qmin), fabs(max_val / symmetric_qmax));\n      min_val = max_scale * symmetric_qmin;\n      max_val = max_scale * symmetric_qmax;\n    }\n\n    // We extend the [min, max] interval to ensure that it contains 0.\n    // Otherwise, we would not meet the requirement that 0 be an exactly\n    // representable value.\n    min_val = fminf(min_val, 0.f);\n    max_val = fmaxf(max_val, 0.f);\n    scale[i] = (static_cast<double>(max_val) - min_val) / (qmax - qmin);\n\n    // Moving this check outside this function would result in extra Device to\n    // Host copy of the min and max val which would result in a perf hit.\n    if (scale[i] == 0.0f || isinf(1.0f / scale[i])) {\n      scale[i] = 0.1;\n    }\n\n    double zero_point_from_min = qmin - min_val / static_cast<double>(scale[i]);\n    double zero_point_from_max = qmax - max_val / static_cast<double>(scale[i]);\n    double zero_point_from_min_error = abs(qmin) + abs(min_val / static_cast<double>(scale[i]));\n    double zero_point_from_max_error = abs(qmax) + abs(max_val / static_cast<double>(scale[i]));\n    double initial_zero_point = zero_point_from_min_error < zero_point_from_max_error\n                                ? zero_point_from_min\n                                : zero_point_from_max;\n\n    // Note: preserve_sparsity here means symmetric quantization.\n    // for symmetric quantization, we force zero_point\n    // to be a middle value between qmin and qmax.\n    // If either min or max is 0, then we just use 0 as zero_point.\n    if (min_val < 0 && max_val > 0 && preserve_sparsity) {\n      initial_zero_point = static_cast<double>(qmin + qmax) / 2;\n    }\n    // Now we need to nudge the zero point to be an integer\n    // (our zero points are integer, and this is motivated by the\n    // requirement to be able to represent the real value \"0\" exactly as a\n    // quantized value, which is required in multiple places, for example in\n    // Im2col with zero padding).\n    int32_t nudged_zero_point = 0;\n    if (initial_zero_point < qmin) {\n      nudged_zero_point = qmin;\n    } else if (initial_zero_point > qmax) {\n      nudged_zero_point = qmax;\n    } else {\n      nudged_zero_point = nearbyint(initial_zero_point);\n    }\n    zero_point[i] = nudged_zero_point;\n  }\n}"
        ]
    },
    "nqueen-cuda": {
        "/Users/gbolet/hecbench-roofline/src/nqueen-cuda/main.cu": [
            "__device__  bool queens_stillLegal(const char *board, const int r)\n{\n  bool safe = true;\n  // Check vertical\n  for (int i = 0; i < r; ++i)\n    if (board[i] == board[r]) safe = false;\n  // Check diagonals\n  int ld = board[r];  //left diagonal columns\n  int rd = board[r];  // right diagonal columns\n  for (int i = r-1; i >= 0; --i) {\n    --ld; ++rd;\n    if (board[i] == ld || board[i] == rd) safe = false;\n  }\n  return safe;\n}\n\n__global__ void BP_queens_root_dfs(\n  int N, unsigned int nPreFixos, int depthPreFixos,\n  const QueenRoot *__restrict__ root_prefixes,\n  unsigned long long *__restrict__ vector_of_tree_size,\n  unsigned long long *__restrict__ sols)\n{\n  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  if (idx < nPreFixos) {\n     unsigned int flag = 0;\n     unsigned int bit_test = 0;\n     char vertice[20];\n     int N_l = N;\n     int i, depth; \n     unsigned long long  qtd_solutions_thread = 0ULL;\n     int depthGlobal = depthPreFixos;\n     unsigned long long tree_size = 0ULL;\n\n#pragma unroll 2\n    for (i = 0; i < N_l; ++i) {\n      vertice[i] = _EMPTY_;\n    }\n\n    flag = root_prefixes[idx].control;\n\n#pragma unroll 2\n    for (i = 0; i < depthGlobal; ++i)\n      vertice[i] = root_prefixes[idx].board[i];\n\n    depth = depthGlobal;\n\n    do {\n      vertice[depth]++;\n      bit_test = 0;\n      bit_test |= (1<<vertice[depth]);\n      if(vertice[depth] == N_l){\n        vertice[depth] = _EMPTY_;\n      } else if (!(flag & bit_test ) && queens_stillLegal(vertice, depth)){\n        ++tree_size;\n        flag |= (1ULL<<vertice[depth]);\n        depth++;\n        if (depth == N_l) { //sol\n          ++qtd_solutions_thread; \n        } else continue;\n      } else continue;\n      depth--;\n      flag &= ~(1ULL<<vertice[depth]);\n    } while(depth >= depthGlobal);\n\n    sols[idx] = qtd_solutions_thread;\n    vector_of_tree_size[idx] = tree_size;\n  }//if\n}"
        ]
    },
    "scan2-cuda": {
        "/Users/gbolet/hecbench-roofline/src/scan2-cuda/main.cu": [
            "__global__\nvoid blockAddition(const float*__restrict__ input,\n                         float*__restrict__ output)\n{  \n  int bid = blockIdx.x;\n  int tid = threadIdx.x;\n  int gid = bid * blockDim.x + tid;\n\n  __shared__ float value;\n\n  /* Only 1 thread of a group will read from global buffer */\n  if(tid == 0)\n  {\n    value = input[bid];\n  }\n  __syncthreads();\n\n  output[gid] += value;\n}",
            "__global__\nvoid ScanLargeArrays(float *__restrict__ output,\n                     const float *__restrict__ input,\n                     const unsigned int block_size,   // size of block\n                     float *__restrict__ sumBuffer)  // sum of blocks\n{\n  extern __shared__ float block[];   // Size : block_size\n  int tid = threadIdx.x;\n  int bid = blockIdx.x;\n  int gid = bid * blockDim.x + tid;\n\n  /* Cache the computational window in shared memory */\n  block[2*tid]     = input[2*gid];\n  block[2*tid + 1] = input[2*gid + 1];\n  __syncthreads();\n\n  float cache0 = block[0];\n  float cache1 = cache0 + block[1];\n\n  /* build the sum in place up the tree */\n  for(int stride = 1; stride < block_size; stride *=2)\n  {\n    if(2*tid>=stride)\n    {\n      cache0 = block[2*tid-stride]+block[2*tid];\n      cache1 = block[2*tid+1-stride]+block[2*tid+1];\n    }\n    __syncthreads();\n\n    block[2*tid] = cache0;\n    block[2*tid+1] = cache1;\n\n    __syncthreads();\n  }\n\n  /* store the value in sum buffer before making it to 0 */   \n  sumBuffer[bid] = block[block_size-1];\n\n  /*write the results back to global memory */\n  if(tid==0)\n  {\n    output[2*gid]     = 0;\n    output[2*gid+1]   = block[2*tid];\n  }\n  else\n  {\n    output[2*gid]     = block[2*tid-1];\n    output[2*gid + 1] = block[2*tid];\n  }\n}",
            "__global__ \nvoid prefixSum(float *__restrict__ output, \n               const float *__restrict__ input,\n               const unsigned int block_size)\n{\n  int tid = threadIdx.x;\n  int bid = blockIdx.x;\n  int gid = bid * blockDim.x + tid;\n\n  extern __shared__ float block[];\n\n  /* Cache the computational window in shared memory */\n  block[2*tid]     = input[2*gid];\n  block[2*tid + 1] = input[2*gid + 1];\n  __syncthreads();\n\n  float cache0 = block[0];\n  float cache1 = cache0 + block[1];\n\n  /* build the sum in place up the tree */\n  for(int stride = 1; stride < block_size; stride *=2)\n  {\n\n    if(2*tid>=stride)\n    {\n      cache0 = block[2*tid-stride]+block[2*tid];\n      cache1 = block[2*tid+1-stride]+block[2*tid+1];\n    }\n    __syncthreads();\n\n    block[2*tid] = cache0;\n    block[2*tid+1] = cache1;\n\n    __syncthreads();\n  }\n\n  /*write the results back to global memory */\n  if(tid==0)\n  {\n    output[2*gid]     = 0;\n    output[2*gid+1]   = block[2*tid];\n  }\n  else\n  {\n    output[2*gid]     = block[2*tid-1];\n    output[2*gid + 1] = block[2*tid];\n  }\n}"
        ]
    },
    "qkv-cuda": {
        "/Users/gbolet/hecbench-roofline/src/qkv-cuda/main.cu": [
            "__global__ void matmul_forward_kernel1(float* out,\n                                       const float* inp, const float* weight, const float* bias,\n                                       int BT, int C, int OC) {\n    // out is (B,T,OC). OC is short for \"output channels\", e.g. OC = 4 * C\n    // inp is (B,T,C), weight is (OC, C), bias is (OC)\n    // in the naive kernel, every thread handles one element of out\n    int bt = blockIdx.x * blockDim.x + threadIdx.x;\n    int oc = blockIdx.y * blockDim.y + threadIdx.y;\n    if (bt < BT && oc < OC) {\n        float val = (bias != NULL) ? bias[oc] : 0.0f;\n        const float* wrow = weight + oc * C;\n        const float* inp_bt = inp + bt * C;\n        for (int i = 0; i < C; i++) {\n            val += inp_bt[i] * wrow[i];\n        }\n        out[bt * OC + oc] = val;\n    }\n}",
            "__global__ void add_bias(float* out, const float* bias, int B, int T, int OC) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    int stride = blockDim.x * gridDim.x;\n    for (int i = idx; i < B * T * OC; i += stride) {\n        int col = i % OC;\n        out[i] += bias[col];\n    }\n}",
            "__device__ float4 ld_vec(const float* address) {\n    return *reinterpret_cast<const float4*>(address);\n}\n\n__device__ void st_vec(float* address, float4 val) {\n    *reinterpret_cast<float4*>(address) = val;\n}\n\n__global__ void __launch_bounds__(16*16) \nmatmul_forward_kernel4(float* out, const float* inp,\n                       const float* weight, const float* bias, int C, int OC) {\n    // out is (B,T,OC). OC is short for \"output channels\", e.g. OC = 4 * C\n    // inp is (B,T,C), weight is (OC, C), bias is (OC)\n    // each thread handles 8x8 elements; each block 128 by 128 elements.\n    int oc = 8*(blockIdx.y * blockDim.y + threadIdx.y);\n\n    // buffers to cache chunks of the input matrices\n    __shared__ float lhs_s[128][32];\n    __shared__ float rhs_s[128][32];\n\n    // adjust our pointers for the current block\n    inp += 128 * blockIdx.x * C;\n    weight += 128 * blockIdx.y * C;\n    out += 128 * blockIdx.x * OC + 128 * blockIdx.y;\n\n    float vals[8][8] = {};\n    if(bias != NULL) {\n        for (int i = 0; i < 8; i++) {\n            for (int j = 0; j < 8; j += 4) {\n                float4 b = ld_vec(bias + oc + j);\n                vals[i][j+0] = b.x;\n                vals[i][j+1] = b.y;\n                vals[i][j+2] = b.z;\n                vals[i][j+3] = b.w;\n            }\n        }\n    }\n\n    int si_start = 4*(16 * threadIdx.y + threadIdx.x);\n    for (int so = 0; so < C; so += 32) {\n        __syncthreads();\n        int xmod8 = threadIdx.x % 8;\n        int xby8 = threadIdx.x / 8;\n        int xo = 4 * xmod8;\n        for(int y = 2 * threadIdx.y + xby8; y < 128; y += 32) {\n            st_vec(&lhs_s[y][xo], ld_vec(inp + y * C + so + xo));\n            st_vec(&rhs_s[y][xo], ld_vec(weight + y * C + so + xo));\n        }\n        __syncthreads();\n\n        for (int si = si_start; si < si_start + 32; si += 4) {\n            float4 rhs[8];\n            for (int u = 0; u < 8; ++u) {\n                rhs[u] = ld_vec(&rhs_s[u + 8 * threadIdx.y][si % 32]);\n            }\n\n            for (int ii = 0; ii < 8; ++ii) {\n                float4 lhs = ld_vec(&lhs_s[ii + 8 * threadIdx.x][si % 32]);\n                for (int ji = 0; ji < 8; ++ji) {\n                    vals[ii][ji] += lhs.x * rhs[ji].x;\n                    vals[ii][ji] += lhs.y * rhs[ji].y;\n                    vals[ii][ji] += lhs.z * rhs[ji].z;\n                    vals[ii][ji] += lhs.w * rhs[ji].w;\n                }\n            }\n        }\n    }\n\n    for (int i = 0; i < 8; ++i) {\n        for (int j = 0; j < 8; j += 4) {\n            float4 result;\n            result.x = vals[i][j + 0];\n            result.y = vals[i][j + 1];\n            result.z = vals[i][j + 2];\n            result.w = vals[i][j + 3];\n            st_vec(out + (8*threadIdx.x+i) * OC + 8*threadIdx.y + j, result);\n        }\n    }\n}"
        ]
    },
    "hausdorff-cuda": {
        "/Users/gbolet/hecbench-roofline/src/hausdorff-cuda/main.cu": [
            "__device__ __forceinline__\nvoid atomic_max(float *address, float val)\n{\n  unsigned int ret = __float_as_uint(*address);\n  while(val > __uint_as_float(ret))\n  {\n    unsigned int old = ret;\n    if((ret = atomicCAS((unsigned int *)address, old, __float_as_uint(val))) == old)\n      break;\n  }\n}\n\n__host__ __device__\ninline float hd (const float2 ap, const float2 bp)\n{\n  return (ap.x - bp.x) * (ap.x - bp.x)\n       + (ap.y - bp.y) * (ap.y - bp.y);\n}\n\n__global__\nvoid computeDistance(const float2* __restrict__ Apoints,\n                     const float2* __restrict__ Bpoints,\n                           float*  __restrict__ distance,\n                     const int numA, const int numB)\n{\n  int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i >= numA) return;\n\n  float d = std::numeric_limits<float>::max();\n  float2 p = Apoints[i];\n  for (int j = 0; j < numB; j++)\n  {\n    float t = hd(p, Bpoints[j]);\n    d = std::min(t, d);\n  }\n  \n  atomic_max(distance, d);\n}"
        ]
    },
    "tqs-cuda": {
        "/Users/gbolet/hecbench-roofline/src/tqs-cuda/kernel.cu": [
            "__global__\nvoid TaskQueue_gpu(const task_t *__restrict__ queue,\n                   int *__restrict__ data,\n                   int *__restrict__ consumed,\n                   const int iterations,\n                   const int offset,\n                   const int gpuQueueSize)\n{\n  extern __shared__ int l_mem[];\n  int* next = l_mem;\n  task_t* t = (task_t*)&next[1];\n\n  const int tid       = threadIdx.x;\n  const int tile_size = blockDim.x;\n\n  // Fetch task\n  if(tid == 0) {\n    *next = atomicAdd(consumed, 1);\n    t->id = queue[*next].id;\n    t->op = queue[*next].op;\n  }\n  __syncthreads();\n  while(*next < gpuQueueSize) {\n    // Compute task\n    if(t->op == SIGNAL_WORK_KERNEL) {\n      for(int i = 0; i < iterations; i++) {\n        data[(t->id - offset) * tile_size + tid] += tile_size;\n      }\n\n      data[(t->id - offset) * tile_size + tid] += t->id;\n    }\n    if(t->op == SIGNAL_NOTWORK_KERNEL) {\n      for(int i = 0; i < 1; i++) {\n        data[(t->id - offset) * tile_size + tid] += tile_size;\n      }\n\n      data[(t->id - offset) * tile_size + tid] += t->id;\n    }\n    if(tid == 0) {\n      *next = atomicAdd(consumed, 1);\n      // Fetch task\n      t->id = queue[*next].id;\n      t->op = queue[*next].op;\n    }\n    __syncthreads();\n  }\n}"
        ]
    },
    "remap-cuda": {
        "/Users/gbolet/hecbench-roofline/src/remap-cuda/main.cu": [
            "__global__\nvoid remap_kernel(\n  thrust::device_ptr<const int> second_order,\n  thrust::device_ptr<const int> first_order,\n  int *output,\n  const int N,\n  const int K)\n{\n  int i = blockDim.x * blockIdx.x + threadIdx.x;\n  if (i >= K) return;\n  int idx = second_order[i];\n  output[first_order[idx]] = i;\n  for (idx++; idx < N && (i == K - 1 || idx != second_order[i + 1]); idx++) {\n    output[first_order[idx]] = i;\n  }\n}"
        ]
    },
    "laplace3d-cuda": {
        "/Users/gbolet/hecbench-roofline/src/laplace3d-cuda/kernel.h": [
            "__global__ void laplace3d(int NX, int NY, int NZ, int pitch, \n                          const float *__restrict__ d_u1,\n                                float *__restrict__ d_u2)\n{\n  int   indg, indg_h, indg0;\n  int   i, j, k, ind, ind_h, halo, active;\n  float u2, sixth=1.0f/6.0f;\n\n  int NXM1 = NX-1;\n  int NYM1 = NY-1;\n  int NZM1 = NZ-1;\n\n  //\n  // define local array offsets\n  //\n\n#define IOFF  1\n#define JOFF (BLOCK_X+2)\n#define KOFF (BLOCK_X+2)*(BLOCK_Y+2)\n  __shared__ float u1[3*KOFF];\n\n  //\n  // first set up indices for halos\n  //\n\n  k    = threadIdx.x + threadIdx.y*BLOCK_X;\n  halo = k < 2*(BLOCK_X+BLOCK_Y+2);\n\n  if (halo) {\n    if (threadIdx.y<2) {               // y-halos (coalesced)\n      i = threadIdx.x;\n      j = threadIdx.y*(BLOCK_Y+1) - 1;\n    }\n    else {                             // x-halos (not coalesced)\n      i = (k%2)*(BLOCK_X+1) - 1;\n      j =  k/2 - BLOCK_X - 1;\n    }\n\n    ind_h  = INDEX(i+1,j+1,JOFF) + KOFF;\n\n    i      = INDEX(i,blockIdx.x,BLOCK_X);   // global indices\n    j      = INDEX(j,blockIdx.y,BLOCK_Y);\n    indg_h = INDEX(i,j,pitch);\n\n    halo   =  (i>=0) && (i<NX) && (j>=0) && (j<NY);\n  }\n\n  //\n  // then set up indices for main block\n  //\n\n  i    = threadIdx.x;\n  j    = threadIdx.y;\n  ind  = INDEX(i+1,j+1,JOFF) + KOFF;\n\n  i    = INDEX(i,blockIdx.x,BLOCK_X);     // global indices\n  j    = INDEX(j,blockIdx.y,BLOCK_Y);\n  indg = INDEX(i,j,pitch);\n\n  active = (i<NX) && (j<NY);\n\n  //\n  // read initial plane of u1 array\n  //\n\n  if (active) u1[ind+KOFF] = d_u1[indg];\n  if (halo) u1[ind_h+KOFF] = d_u1[indg_h];\n\n  //\n  // loop over k-planes\n  //\n\n  for (k=0; k<NZ; k++) {\n\n    // move two planes down and read in new plane k+1\n\n    if (active) {\n      indg0 = indg;\n      indg  = INDEX(indg,NY,pitch);\n      u1[ind-KOFF] = u1[ind];\n      u1[ind]      = u1[ind+KOFF];\n      if (k<NZM1)\n        u1[ind+KOFF] = d_u1[indg];\n    }\n\n    if (halo) {\n      indg_h = INDEX(indg_h,NY,pitch);\n      u1[ind_h-KOFF] = u1[ind_h];\n      u1[ind_h]      = u1[ind_h+KOFF];\n      if (k<NZM1)\n        u1[ind_h+KOFF] = d_u1[indg_h];\n    }\n\n    __syncthreads();\n\n  //\n  // perform Jacobi iteration to set values in u2\n  //\n\n    if (active) {\n      if (i==0 || i==NXM1 || j==0 || j==NYM1 || k==0 || k==NZM1) {\n        u2 = u1[ind];          // Dirichlet b.c.'s\n      }\n      else {\n        u2 = ( u1[ind-IOFF] + u1[ind+IOFF]\n             + u1[ind-JOFF] + u1[ind+JOFF]\n             + u1[ind-KOFF] + u1[ind+KOFF] ) * sixth;\n      }\n      d_u2[indg0] = u2;\n    }\n\n    __syncthreads();\n\n  }\n}"
        ]
    },
    "vol2col-cuda": {
        "/Users/gbolet/hecbench-roofline/src/vol2col-cuda/main.cu": [
            "#define T ((int)32)\n\n\n__global__ void vol2col_kernel(\n    const uint64_t range,\n    const T* data_vol,\n    const int depth,\n    const int height,\n    const int width,\n    const int ksize_t,\n    const int ksize_h,\n    const int ksize_w,\n    const int pad_t,\n    const int pad_h,\n    const int pad_w,\n    const int stride_t,\n    const int stride_h,\n    const int stride_w,\n    const int dilation_t,\n    const int dilation_h,\n    const int dilation_w,\n    const int depth_col,\n    const int height_col,\n    const int width_col,\n    T* data_col)\n{\n  for (int64_t n = blockDim.x * blockIdx.x + threadIdx.x;\n               n < range; n += blockDim.x * gridDim.x) {\n\n    int w_out = n % width_col;\n    int64_t index = n / width_col;\n    int h_out = index % height_col;\n    index = index / height_col;\n    int t_out = index % depth_col;\n    int channel_in = index / depth_col;\n    int channel_out = channel_in * ksize_t * ksize_h * ksize_w;\n    int t_in = t_out * stride_t - pad_t;\n    int h_in = h_out * stride_h - pad_h;\n    int w_in = w_out * stride_w - pad_w;\n    data_vol += ((channel_in * depth + t_in) * height + h_in) * width + w_in;\n    data_col += ((channel_out * depth_col + t_out) * height_col + h_out) * width_col + w_out;\n\n    for (int i = 0; i < ksize_t; ++i) {\n      for (int j = 0; j < ksize_h; ++j) {\n        for (int k = 0; k < ksize_w; ++k) {\n          int t = t_in + i * dilation_t;\n          int h = h_in + j * dilation_h;\n          int w = w_in + k * dilation_w;\n          *data_col = (t >= 0 && h >= 0 && w >= 0 && t < depth && h < height && w < width)\n              ? data_vol[i * dilation_t * height * width +\n                         j * dilation_h * width + k * dilation_w]\n              : static_cast<T>(0);\n          data_col += depth_col * height_col * width_col;\n        }\n      }\n    }\n  }\n}",
            "#define T ((int)32)\n\n\n__global__ void col2vol_kernel(\n    const uint64_t n,\n    const T* data_col,\n    const unsigned depth,\n    const unsigned height,\n    const unsigned width,\n    const unsigned kernel_t,\n    const unsigned kernel_h,\n    const unsigned kernel_w,\n    const unsigned pad_t,\n    const unsigned pad_h,\n    const unsigned pad_w,\n    const unsigned stride_t,\n    const unsigned stride_h,\n    const unsigned stride_w,\n    const unsigned dilation_t,\n    const unsigned dilation_h,\n    const unsigned dilation_w,\n    const unsigned depth_col,\n    const unsigned height_col,\n    const unsigned width_col,\n    T* data_vol)\n{\n  for (uint64_t index = blockDim.x * blockIdx.x + threadIdx.x;\n                index < n; index += blockDim.x * gridDim.x) {\n    accT val = static_cast<accT>(0);\n    const unsigned w_im = index % width + pad_w;\n    const unsigned h_im = (index / width) % height + pad_h;\n    const unsigned t_im = (index / width / height) % depth + pad_t;\n    const unsigned c_im = index / (width * height * depth);\n    auto kernel_extent_w = (kernel_w - 1) * dilation_w + 1;\n    auto kernel_extent_h = (kernel_h - 1) * dilation_h + 1;\n    auto kernel_extent_t = (kernel_t - 1) * dilation_t + 1;\n    // compute the start and end of the output\n    const auto w_col_start =\n        (w_im < kernel_extent_w) ? 0 : (w_im - kernel_extent_w) / stride_w + 1;\n    const auto w_col_end = std::min(w_im / stride_w + 1, width_col);\n    const auto h_col_start =\n        (h_im < kernel_extent_h) ? 0 : (h_im - kernel_extent_h) / stride_h + 1;\n    const auto h_col_end = std::min(h_im / stride_h + 1, height_col);\n    const auto t_col_start =\n        (t_im < kernel_extent_t) ? 0 : (t_im - kernel_extent_t) / stride_t + 1;\n    const auto t_col_end = std::min(t_im / stride_t + 1, depth_col);\n    // TODO: use LCM of stride and dilation to avoid unnecessary loops\n    for (unsigned t_col = t_col_start; t_col < t_col_end; t_col += 1) {\n      for (unsigned h_col = h_col_start; h_col < h_col_end; h_col += 1) {\n        for (unsigned w_col = w_col_start; w_col < w_col_end; w_col += 1) {\n          uint64_t t_k = (t_im - t_col * stride_t);\n          uint64_t h_k = (h_im - h_col * stride_h);\n          uint64_t w_k = (w_im - w_col * stride_w);\n          if (t_k % dilation_t == 0 && h_k % dilation_h == 0 &&\n              w_k % dilation_w == 0) {\n            t_k /= dilation_t;\n            h_k /= dilation_h;\n            w_k /= dilation_w;\n            const uint64_t idx_k =\n                ((c_im * kernel_t + t_k) * kernel_h + h_k) * kernel_w + w_k;\n            const uint64_t data_col_index =\n                ((idx_k * depth_col + t_col) *\n                    height_col + h_col) *\n                  width_col + w_col;\n            val += data_col[data_col_index];\n          }\n        }\n      }\n    }\n    data_vol[index] = static_cast<T>(val);\n  }\n}"
        ]
    },
    "leukocyte-cuda": {
        "/Users/gbolet/hecbench-roofline/src/leukocyte-cuda/kernel_dilated.h": [
            "__global__ \nvoid kernel_dilated (\n    const float*__restrict__ c_strel,\n    const float*__restrict__ img,\n    float*__restrict__ dilated,\n    const int strel_m,\n    const int strel_n,\n    const int max_gicov_m,\n    const int max_gicov_n)\n{\n  // Find the center of the structuring element\n  int el_center_i = strel_m / 2;\n  int el_center_j = strel_n / 2;\n\n  // Determine this thread's location in the matrix\n  int thread_id = blockIdx.x*blockDim.x+threadIdx.x;\n  int i = thread_id % max_gicov_m;\n  int j = thread_id / max_gicov_m;\n\n  if(j >= max_gicov_n) return;\n\n  // Initialize the maximum GICOV score seen so far to zero\n  float max = 0.0f;\n\n  // Iterate across the structuring element in one dimension\n  int el_i, el_j, x, y;\n\n  for (el_i = 0; el_i < strel_m; el_i++) {\n    y = i - el_center_i + el_i;\n    // Make sure we have not gone off the edge of the matrix\n    if ( (y >= 0) && (y < max_gicov_m) ) {\n      // Iterate across the structuring element in the other dimension\n      for (el_j = 0; el_j < strel_n; el_j++) {\n        x = j - el_center_j + el_j;\n        // Make sure we have not gone off the edge of the matrix\n        //  and that the current structuring element value is not zero\n        if ( (x >= 0) &&\n            (x < max_gicov_n) &&\n            (c_strel[(el_i * strel_n) + el_j] != 0) ) {\n          // Determine if this is the maximal value seen so far\n          int addr = (x * max_gicov_m) + y;\n          float temp = img[addr];\n          if (temp > max) max = temp;\n        }\n      }\n    }\n  }\n  // Store the maximum value found\n  dilated[(i * max_gicov_n) + j] = max;\n}"
        ],
        "/Users/gbolet/hecbench-roofline/src/leukocyte-cuda/kernel_GICOV.h": [
            "__global__\nvoid kernel_GICOV (\n    const float*__restrict__ grad_x,\n    const float*__restrict__ grad_y,\n    const float*__restrict__ sin_angle,\n    const float*__restrict__ cos_angle,\n    const int*__restrict__ tX,\n    const int*__restrict__ tY,\n    float*__restrict__ gicov,\n    const int local_work_size,\n    const int num_work_groups,\n    const int grad_m)\n{\n  int i, j, k, n, x, y;\n  int gid = blockIdx.x*blockDim.x+threadIdx.x;\n\n  if (gid >= local_work_size*num_work_groups) return;\n\n  // Determine this thread's pixel\n  i = gid/local_work_size + MAX_RAD + 2;\n  j = gid%local_work_size + MAX_RAD + 2;\n\n  // Initialize the maximal GICOV score to 0\n  float max_GICOV = 0.f;\n\n  // Iterate across each stencil\n  for (k = 0; k < NCIRCLES; k++) {\n    // Variables used to compute the mean and variance\n    //  of the gradients along the current stencil\n    float sum = 0.f, M2 = 0.f, mean = 0.f;    \n\n    // Iterate across each sample point in the current stencil\n    for (n = 0; n < NPOINTS; n++) {\n      // Determine the x- and y-coordinates of the current sample point\n      y = j + tY[(k * NPOINTS) + n];\n      x = i + tX[(k * NPOINTS) + n];\n\n      // Compute the combined gradient value at the current sample point\n      int addr = x * grad_m + y;\n      float p = grad_x[addr] * cos_angle[n] + \n        grad_y[addr] * sin_angle[n];\n\n      // Update the running total\n      sum += p;\n\n      // Partially compute the variance\n      float delta = p - mean;\n      mean = mean + (delta / (float) (n + 1));\n      M2 = M2 + (delta * (p - mean));\n    }\n\n    // Finish computing the mean\n    mean = sum / ((float) NPOINTS);\n\n    // Finish computing the variance\n    float var = M2 / ((float) (NPOINTS - 1));\n\n    // Keep track of the maximal GICOV value seen so far\n    if (((mean * mean) / var) > max_GICOV) max_GICOV = (mean * mean) / var;\n  }\n\n  // Store the maximal GICOV value\n  gicov[(i * grad_m) + j] = max_GICOV;\n}"
        ],
        "/Users/gbolet/hecbench-roofline/src/leukocyte-cuda/kernel_IMGVF.h": [
            "#define FP_TYPE float\n\n\ninline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\ninline __host__ __device__ float4 fabs(float4 v)\n{\n    return make_float4(fabs(v.x), fabs(v.y), fabs(v.z), fabs(v.w));\n}\n\n__device__\nFP_TYPE heaviside(FP_TYPE x) {\n  return atan(x) * ONE_OVER_PI + FP_CONST(0.5);\n}\n\n__global__\nvoid kernel_IMGVF(\n          float*__restrict__ IMGVF_array,\n    const float*__restrict__ I_array,\n    const int*__restrict__ I_offsets,\n    const int*__restrict__ m_array,\n    const int*__restrict__ n_array,\n    const float vx_float,\n    const float vy_float,\n    const float e_float,\n    const float cutoff_float,\n    const int max_iterations )\n{\n  __shared__ float IMGVF [41*81];\n  __shared__ float IMGVF_buffer [LOCAL_WORK_SIZE];\n  __shared__ int cell_converged;\n\n  // Figure out which cell this thread block is working on\n  int cell_num = blockIdx.x;\n\n  // Get pointers to current cell's input image and inital matrix\n  int I_offset = I_offsets[cell_num];\n  float* IMGVF_global = &(IMGVF_array[I_offset]);\n  //auto I = &(I_array[I_offset]);\n\n  // Get current cell's matrix dimensions\n  int m = m_array[cell_num];\n  int n = n_array[cell_num];\n\n  // Compute the number of virtual thread blocks\n  int IMGVF_Size = m * n;\n  int tb_count = (m * n + LOCAL_WORK_SIZE - 1) / LOCAL_WORK_SIZE;\n\n  // Load the initial IMGVF matrix into shared memory\n  int thread_id = threadIdx.x;\n  int thread_block, i, j;\n  for (thread_block = 0; thread_block < tb_count; thread_block++) {\n    int offset = thread_block * LOCAL_WORK_SIZE;\n    i = (thread_id + offset) / n;\n    j = (thread_id + offset) % n;\n    if (i < m) IMGVF[(i * n) + j] = IMGVF_global[(i * n) + j];\n  }\n  __syncthreads();\n\n  // Set the converged flag to false\n  if (thread_id == 0) cell_converged = 0;\n  __syncthreads();\n\n  // Constants used to iterate through virtual thread blocks\n  const float one_nth = 1.0f / (float) n;\n  const int tid_mod = thread_id % n;\n  const int tbsize_mod = LOCAL_WORK_SIZE % n;\n\n  // Constant used in the computation of Heaviside values\n  FP_TYPE one_over_e = FP_CONST(1.0) / e_float;\n\n  // Iteratively compute the IMGVF matrix until the computation has\n  //  converged or we have reached the tb_countimum number of iterations\n  int iterations = 0;\n  while ((! cell_converged) && (iterations < max_iterations)) {\n\n    // The total change to this thread's matrix elements in the current iteration\n    FP_TYPE total_diff = FP_CONST(0.0);\n\n    int old_i = 0, old_j = 0;\n    j = tid_mod - tbsize_mod;\n\n    // Iterate over virtual thread blocks\n    for (thread_block = 0; thread_block < tb_count; thread_block++) {\n      // Store the index of this thread's previous matrix element\n      //  (used in the buffering scheme below)\n      old_i = i;\n      old_j = j;\n\n      // Determine the index of this thread's current matrix element \n      int offset = thread_block * LOCAL_WORK_SIZE;\n      i = (thread_id + offset) * one_nth;\n      j += tbsize_mod;\n      if (j >= n) j -= n;\n\n      FP_TYPE new_val = FP_CONST(0.0);\n      FP_TYPE old_val = FP_CONST(0.0);\n\n      // Make sure the thread has not gone off the end of the matrix\n      if (i < m) {\n        // Compute neighboring matrix element indices\n        int rowU = (i == 0) ? 0 : i - 1;\n        int rowD = (i == m - 1) ? m - 1 : i + 1;\n        int colL = (j == 0) ? 0 : j - 1;\n        int colR = (j == n - 1) ? n - 1 : j + 1;\n\n        // Compute the difference between the matrix element and its eight neighbors\n        old_val    = IMGVF[(i * n) + j];\n        FP_TYPE U  = IMGVF[(rowU * n) + j   ] - old_val;\n        FP_TYPE D  = IMGVF[(rowD * n) + j   ] - old_val;\n        FP_TYPE L  = IMGVF[(i    * n) + colL] - old_val;\n        FP_TYPE R  = IMGVF[(i    * n) + colR] - old_val;\n        FP_TYPE UR = IMGVF[(rowU * n) + colR] - old_val;\n        FP_TYPE DR = IMGVF[(rowD * n) + colR] - old_val;\n        FP_TYPE UL = IMGVF[(rowU * n) + colL] - old_val;\n        FP_TYPE DL = IMGVF[(rowD * n) + colL] - old_val;\n\n        // Compute the regularized heaviside value for these differences\n        FP_TYPE UHe  = heaviside((U  *       -vy_float)  * one_over_e);\n        FP_TYPE DHe  = heaviside((D  *        vy_float)  * one_over_e);\n        FP_TYPE LHe  = heaviside((L  *  -vx_float     )  * one_over_e);\n        FP_TYPE RHe  = heaviside((R  *   vx_float     )  * one_over_e);\n        FP_TYPE URHe = heaviside((UR * ( vx_float - vy_float)) * one_over_e);\n        FP_TYPE DRHe = heaviside((DR * ( vx_float + vy_float)) * one_over_e);\n        FP_TYPE ULHe = heaviside((UL * (-vx_float - vy_float)) * one_over_e);\n        FP_TYPE DLHe = heaviside((DL * (-vx_float + vy_float)) * one_over_e);\n\n        // Update the IMGVF value in two steps:\n        // 1) Compute IMGVF += (mu / lambda)(UHe .*U  + DHe .*D  + LHe .*L  + RHe .*R +\n        //                                   URHe.*UR + DRHe.*DR + ULHe.*UL + DLHe.*DL);\n        new_val = old_val + (MU / LAMBDA) * (UHe  * U  + DHe  * D  + LHe  * L  + RHe  * R +\n            URHe * UR + DRHe * DR + ULHe * UL + DLHe * DL);\n        // 2) Compute IMGVF -= (1 / lambda)(I .* (IMGVF - I))\n        //FP_TYPE vI = I[(i * n) + j];\n        FP_TYPE vI = I_array[I_offset+ (i * n) + j];\n        new_val -= ((1.0 / LAMBDA) * vI * (new_val - vI));\n\n      }\n      // Save the previous virtual thread block's value (if it exists)\n      if (thread_block > 0) {\n        offset = (thread_block - 1) * LOCAL_WORK_SIZE;\n        if (old_i < m) IMGVF[(old_i * n) + old_j] = IMGVF_buffer[thread_id];\n      }\n      if (thread_block < tb_count - 1) {\n        // Write the new value to the IMGVF_buffer\n        IMGVF_buffer[thread_id] = new_val;\n      } else {\n        // We've reached the final virtual thread block,\n        //  so write directly to the matrix\n        if (i < m) IMGVF[(i * n) + j] = new_val;\n      }\n\n      // Keep track of the total change of this thread's matrix elements\n      total_diff += fabs(new_val - old_val);\n\n      // We need to synchronize between virtual thread blocks to prevent\n      //  threads from writing the values from the buffer to the actual\n      //  IMGVF matrix too early\n      __syncthreads();\n    }\n\n    // We need to compute the overall sum of the change at each matrix element\n    //  by performing a tree reduction across the whole threadblock\n    IMGVF_buffer[thread_id] = total_diff;\n    __syncthreads();\n\n    // Account for thread block sizes that are not a power of 2\n    if (thread_id >= NEXT_LOWEST_POWER_OF_TWO) {\n      IMGVF_buffer[thread_id - NEXT_LOWEST_POWER_OF_TWO] += IMGVF_buffer[thread_id];\n    }\n    __syncthreads();\n\n    // Perform the tree reduction\n    int th;\n    for (th = NEXT_LOWEST_POWER_OF_TWO / 2; th > 0; th /= 2) {\n      if (thread_id < th) {\n        IMGVF_buffer[thread_id] += IMGVF_buffer[thread_id + th];\n      }\n      __syncthreads();\n    }\n\n    // Figure out if we have converged\n    if(thread_id == 0) {\n      FP_TYPE mean = IMGVF_buffer[thread_id] / (FP_TYPE) (m * n);\n      if (mean < cutoff_float) {\n        // We have converged, so set the appropriate flag\n        cell_converged = 1;\n      }\n    }\n\n    // We need to synchronize to ensure that all threads\n    //  read the correct value of the convergence flag\n    __syncthreads();\n\n    // Keep track of the number of iterations we have performed\n    iterations++;\n  }\n\n  // Save the final IMGVF matrix to global memory\n  for (thread_block = 0; thread_block < tb_count; thread_block++) {\n    int offset = thread_block * LOCAL_WORK_SIZE + thread_id;\n    if (offset < IMGVF_Size)\n      IMGVF_global[offset] = IMGVF[offset];\n  }\n}"
        ]
    },
    "lfib4-cuda": {
        "/Users/gbolet/hecbench-roofline/src/lfib4-cuda/main.cu": [
            "__global__ void firstColGPU(uint32_t *x, int s) {\n  __shared__ uint32_t cx[2 * P4];\n\n  uint32_t *px = &cx[P4];\n  int myid = threadIdx.x;\n  cx[myid] = x[myid];\n  __syncthreads();\n\n  for (int k = 1; k < s / P4; k++) {\n\n    for (int i = 0; i < P4; i += LWDR) {\n      if (myid < LWDR) {\n        px[i + myid] = px[i + myid - P1] + px[i + myid - P2]\n                     + px[i + myid - P3] + px[i + myid - P4];\n      }\n      __syncthreads();\n    }\n\n    x[k * P4 + myid] = cx[myid] = px[myid];\n    __syncthreads();\n  }\n}",
            "__global__ void colYGPU(uint32_t *y, int s) {\n  __shared__ uint32_t cy[3 * P4];\n\n  uint32_t *ay = &cy[P4 * 2];\n  int myid = threadIdx.x;\n  ay[myid] = y[2 * P4 + myid];\n  __syncthreads();\n\n  for (int k = 0; k < s / P4; k++) {\n\n    cy[myid] = cy[myid + P4];\n    cy[myid + P4] = ay[myid];\n    __syncthreads();\n\n    for (int i = 0; i < P4; i += LWDR) {\n      if (myid < LWDR) {\n        ay[i + myid] = ay[i + myid - P1] + ay[i + myid - P2]\n          + ay[i + myid - P3] + ay[i + myid - P4];\n      }\n      __syncthreads();\n    }\n  }\n\n  y[2 * P4 + myid] = cy[2 * P4 + myid];\n  y[P4 + myid] = cy[P4 + myid];\n  y[myid] = cy[myid];\n}",
            "__global__ void lastEntGPU(uint32_t *__restrict__ x, uint32_t *__restrict__ y, int s, int r) {\n\n  __shared__ uint32_t a0[3 * P4];\n  __shared__ uint32_t b0[2 * P4];\n  __shared__ uint32_t c0[2 * P4];\n  __shared__ uint32_t d0[2 * P4];\n\n  uint32_t *a = a0 + P4;\n  uint32_t *b = b0 + P4;\n  uint32_t *c = c0 + P4;\n  uint32_t *d = d0 + P4;\n\n  int myid = threadIdx.x;\n\n  a0[myid] = y[myid];\n  __syncthreads();\n\n  if (myid < P4)\n    a0[myid + P4 * 2] = y[myid + P4 * 2];\n  __syncthreads();\n\n  d0[myid] = c0[myid] = b0[myid] = a[myid];\n  __syncthreads();\n\n  b[myid - P4] += a[-(P4 - P3) + myid];\n  __syncthreads();\n\n  c[myid - P4] += (a[-(P3 - P2) + myid] + a[-(P4 - P2) + myid]);\n  __syncthreads();\n\n  d[myid - P4] += (a[-(P2 - P1) + myid] + a[-(P3 - P1) + myid]\n      + a[-(P4 - P1) + myid]);\n  __syncthreads();\n\n  a += P4;\n\n  for (int i = 1; i < r; i++) {\n\n    uint32_t *xc = &x[i * s];\n    uint32_t tmp = 0;\n\n    if (myid < P4) {\n\n      for (int k = 0; k < P4 - P3; k++)\n        tmp += xc[-P4 + k] * a[myid - k];\n\n      for (int k = 0; k < P3 - P2; k++)\n        tmp += xc[-P3 + k] * b[myid - k];\n\n      for (int k = 0; k < P2 - P1; k++)\n        tmp += xc[-P2 + k] * c[myid - k];\n\n      for (int k = 0; k < P1; k++)\n        tmp += xc[-P1 + k] * d[myid - k];\n\n      xc[s - P4 + myid] = tmp;\n\n    }\n    __syncthreads();\n  }\n}",
            "__global__ void colsGPU(uint32_t *x, int s, int r) {\n  int k0 = blockIdx.x * LKNB;     // \n  int k1 = threadIdx.x / LWDR;    // \n  int k2 = threadIdx.x % LWDR;    // \n\n  __shared__ uint32_t cx[LKNB][2 * P4];\n\n  int fcol = (blockIdx.x == 0) ? 1 : 0;\n  int ecol = (blockIdx.x == gridDim.x - 1 && r % LKNB) ? r % LKNB : LKNB;\n\n  for (int i = fcol; i < ecol; i++)\n    cx[i][threadIdx.x] = x[(k0 + i) * s - P4 + threadIdx.x];\n\n  __syncthreads();\n\n  uint32_t *pcx = &cx[k1][P4];\n\n  for (int k = 0; k < s / P4 - 1; k++) {\n\n    for (int i = 0; i < P4; i += LWDR)\n    {\n      if (!(blockIdx.x == 0 && threadIdx.x == 0)\n          && !(blockIdx.x == gridDim.x - 1 && k1 >= ecol))\n        pcx[i + k2] = pcx[i + k2 - P1] + pcx[i + k2 - P2]\n          + pcx[i + k2 - P3] + pcx[i + k2 - P4];\n\n      __syncthreads();\n\n    }\n\n    for (int i = fcol; i < ecol; i++)\n      x[(k0 + i) * s + k * P4 + threadIdx.x] = cx[i][threadIdx.x] =\n        cx[i][P4 + threadIdx.x];\n\n    __syncthreads();\n  }\n}"
        ]
    },
    "deredundancy-cuda": {
        "/Users/gbolet/hecbench-roofline/src/deredundancy-cuda/kernels.cu": [
            "__global__ void kernel_baseToNumber(char *reads, const long length)\n{\n  long index = blockIdx.x * blockDim.x + threadIdx.x;\n  while (index < length) {\n    switch (reads[index]) {\n      case 'A':\n        reads[index] = 0;\n        break;\n      case 'a':\n        reads[index] = 0;\n        break;\n      case 'C':\n        reads[index] = 1;\n        break;\n      case 'c':\n        reads[index] = 1;\n        break;\n      case 'G':\n        reads[index] = 2;\n        break;\n      case 'g':\n        reads[index] = 2;\n        break;\n      case 'T':\n        reads[index] = 3;\n        break;\n      case 't':\n        reads[index] = 3;\n        break;\n      case 'U':\n        reads[index] = 3;\n        break;\n      case 'u':\n        reads[index] = 3;\n        break;\n      default:\n        reads[index] = 4;\n        break;\n    }\n    index += 128*128;\n  }\n}",
            "__global__ void kernel_compressData(\n    const int *lengths, \n    const long *offsets, \n    const char *reads,\n    unsigned int *compressed, \n    int *gaps, \n    const int readsCount)\n{\n  int index = blockIdx.x * blockDim.x + threadIdx.x;\n  if (index >= readsCount) return;  // out of range\n  long mark = offsets[index]/16;  // compressed data offset\n  int round = 0;  // write when round is 16\n  int gapCount = 0;  // gap count\n  unsigned int compressedTemp = 0;  // compressed data\n  long start = offsets[index];\n  long end = start + lengths[index];\n  for (long i=start; i<end; i++) {\n    unsigned char base = reads[i];  // read a base\n    if (base < 4) {\n      compressedTemp += base << (15-round)*2;\n      round++;\n      if (round == 16) {\n        compressed[mark] = compressedTemp;\n        compressedTemp = 0;\n        round = 0;\n        mark++;\n      }\n    } else {  // gap\n      gapCount++;\n    }\n  }\n  compressed[mark] = compressedTemp;\n  gaps[index] = gapCount;\n}",
            "inline __host__ __device__ uint4 make_uint4(int4 a)\n{\n    return make_uint4(uint(a.x), uint(a.y), uint(a.z), uint(a.w));\n}\n\ninline __host__ __device__ uint4 max(uint4 a, uint4 b)\n{\n    return make_uint4(max(a.x,b.x), max(a.y,b.y), max(a.z,b.z), max(a.w,b.w));\n}\n\n__global__ void kernel_createIndex4(\n    const char *reads, \n    const int *lengths, \n    const long *offsets,\n    unsigned short *indexs, \n    unsigned short *orders, \n    long *words, \n    int *magicBase,\n    const int readsCount)\n{\n  int index = blockIdx.x * blockDim.x + threadIdx.x;\n  if (index >= readsCount) return;  // out of range\n  int start = offsets[index];\n  int end = start + lengths[index];\n  int magic0=0, magic1=0, magic2=0, magic3=0;  // magic base\n  char bases[4];\n  for(int i=0; i<4; i++) {  // default is gap\n    bases[i] = 4;\n  }\n  int wordCount = 0;\n  for (int i=start; i<end; i++) {\n    for(int j=0; j<3; j++) {  // copy base to array\n      bases[j] = bases[j+1];\n    }\n    bases[3] = reads[i];\n    switch (bases[3]) {  // update magic\n      case 0:\n        magic0++;\n        break;\n      case 1:\n        magic1++;\n        break;\n      case 2:\n        magic2++;\n        break;\n      case 3:\n        magic3++;\n        break;\n    }\n    unsigned short indexValue = 0;\n    int flag = 0;  // if has gap\n    for (int j=0; j<4; j++) {\n      indexValue += (bases[j]&3)<<(3-j)*2;\n      flag += max((int)(bases[j] - 3), 0);\n    }\n    indexs[i] = flag?65535:indexValue;  // gap value is 65535\n    wordCount += flag?0:1;\n  }\n  words[index] = wordCount;  // index length\n  magicBase[index*4+0] = magic0;  // update magicBase\n  magicBase[index*4+1] = magic1;\n  magicBase[index*4+2] = magic2;\n  magicBase[index*4+3] = magic3;\n}",
            "inline __host__ __device__ uint4 make_uint4(int4 a)\n{\n    return make_uint4(uint(a.x), uint(a.y), uint(a.z), uint(a.w));\n}\n\ninline __host__ __device__ uint4 max(uint4 a, uint4 b)\n{\n    return make_uint4(max(a.x,b.x), max(a.y,b.y), max(a.z,b.z), max(a.w,b.w));\n}\n\n__global__ void kernel_createIndex5(\n    const char *reads, \n    const int *lengths, \n    const long *offsets,\n    unsigned short *indexs, \n    unsigned short *orders, \n    long *words, \n    int *magicBase,\n    const int readsCount)\n{\n  int index = blockIdx.x * blockDim.x + threadIdx.x;\n  if (index >= readsCount) return;\n  int start = offsets[index];\n  int end = start + lengths[index];\n  int magic0=0, magic1=0, magic2=0, magic3=0;\n  char bases[5];\n  for(int i=0; i<5; i++) {\n    bases[i] = 4;\n  }\n  int wordCount = 0;\n  for (int i=start; i<end; i++) {\n    for(int j=0; j<4; j++) {\n      bases[j] = bases[j+1];\n    }\n    bases[4] = reads[i];\n    switch (bases[4]) {\n      case 0:\n        magic0++;\n        break;\n      case 1:\n        magic1++;\n        break;\n      case 2:\n        magic2++;\n        break;\n      case 3:\n        magic3++;\n        break;\n    }\n    unsigned short indexValue = 0;\n    int flag = 0;\n    for (int j=0; j<5; j++) {\n      indexValue += (bases[j]&3)<<(4-j)*2;\n      flag += max((int)(bases[j] - 3), 0);\n    }\n    indexs[i] = flag?65535:indexValue;\n    wordCount += flag?0:1;\n  }\n  words[index] = wordCount;\n  magicBase[index*4+0] = magic0;\n  magicBase[index*4+1] = magic1;\n  magicBase[index*4+2] = magic2;\n  magicBase[index*4+3] = magic3;\n}",
            "inline __host__ __device__ uint4 make_uint4(int4 a)\n{\n    return make_uint4(uint(a.x), uint(a.y), uint(a.z), uint(a.w));\n}\n\ninline __host__ __device__ uint4 max(uint4 a, uint4 b)\n{\n    return make_uint4(max(a.x,b.x), max(a.y,b.y), max(a.z,b.z), max(a.w,b.w));\n}\n\n__global__ void kernel_createIndex6(\n    const char *reads, \n    const int *lengths, \n    const long *offsets,\n    unsigned short *indexs, \n    unsigned short *orders, \n    long *words, \n    int *magicBase,\n    const int readsCount)\n{\n  int index = blockIdx.x * blockDim.x + threadIdx.x;\n  if (index >= readsCount) return;\n  int start = offsets[index];\n  int end = start + lengths[index];\n  int magic0=0, magic1=0, magic2=0, magic3=0;\n  char bases[6];\n  for(int i=0; i<6; i++) {\n    bases[i] = 4;\n  }\n  int wordCount = 0;\n  for (int i=start; i<end; i++) {\n    for(int j=0; j<5; j++) {\n      bases[j] = bases[j+1];\n    }\n    bases[5] = reads[i];\n    switch (bases[5]) {\n      case 0:\n        magic0++;\n        break;\n      case 1:\n        magic1++;\n        break;\n      case 2:\n        magic2++;\n        break;\n      case 3:\n        magic3++;\n        break;\n    }\n    unsigned short indexValue = 0;\n    int flag = 0;\n    for (int j=0; j<6; j++) {\n      indexValue += (bases[j]&3)<<(5-j)*2;\n      flag += max((int)(bases[j] - 3), 0);\n    }\n    indexs[i] = flag?65535:indexValue;\n    wordCount += flag?0:1;\n  }\n  words[index] = wordCount;\n  magicBase[index*4+0] = magic0;\n  magicBase[index*4+1] = magic1;\n  magicBase[index*4+2] = magic2;\n  magicBase[index*4+3] = magic3;\n}",
            "inline __host__ __device__ uint4 make_uint4(int4 a)\n{\n    return make_uint4(uint(a.x), uint(a.y), uint(a.z), uint(a.w));\n}\n\ninline __host__ __device__ uint4 max(uint4 a, uint4 b)\n{\n    return make_uint4(max(a.x,b.x), max(a.y,b.y), max(a.z,b.z), max(a.w,b.w));\n}\n\n__global__ void kernel_createIndex7(\n    const char *reads, \n    const int *lengths, \n    const long *offsets,\n    unsigned short *indexs, \n    unsigned short *orders, \n    long *words, \n    int *magicBase,\n    const int readsCount)\n{\n  int index = blockIdx.x * blockDim.x + threadIdx.x;\n  if (index >= readsCount) return;\n  int start = offsets[index];\n  int end = start + lengths[index];\n  int magic0=0, magic1=0, magic2=0, magic3=0;\n  char bases[7];\n  for(int i=0; i<7; i++) {\n    bases[i] = 4;\n  }\n  int wordCount = 0;\n  for (int i=start; i<end; i++) {\n    for(int j=0; j<6; j++) {\n      bases[j] = bases[j+1];\n    }\n    bases[6] = reads[i];\n    switch (bases[6]) {\n      case 0:\n        magic0++;\n        break;\n      case 1:\n        magic1++;\n        break;\n      case 2:\n        magic2++;\n        break;\n      case 3:\n        magic3++;\n        break;\n    }\n    unsigned short indexValue = 0;\n    int flag = 0;\n    for (int j=0; j<7; j++) {\n      indexValue += (bases[j]&3)<<(6-j)*2;\n      flag += max((int)(bases[j] - 3), 0);\n    }\n    indexs[i] = flag?65535:indexValue;\n    wordCount += flag?0:1;\n  }\n  words[index] = wordCount;\n  magicBase[index*4+0] = magic0;\n  magicBase[index*4+1] = magic1;\n  magicBase[index*4+2] = magic2;\n  magicBase[index*4+3] = magic3;\n}",
            "#define T ((int)32)\n\n\n__host__ __device__ __forceinline__ constexpr T ceil(T a, T b) {\n  return (a + b - 1) / b;\n}\n\n__global__ void kernel_createCutoff(\n    float threshold, \n    int wordLength,\n    const int *lengths, \n    long *words, \n    int *wordCutoff, \n    const int readsCount)\n{\n  int index = blockIdx.x * blockDim.x + threadIdx.x;\n  if (index >= readsCount) return;  // out of range\n  int length = lengths[index];\n  int required = length - wordLength + 1;\n  int cutoff = ceil((float)length * (1.f - threshold) * (float)wordLength);\n  required -= cutoff;\n  wordCutoff[index] = required;\n}",
            "__global__ void kernel_mergeIndex(\n    const long *offsets, \n    const unsigned short *indexs,\n    unsigned short *orders, \n    const long *words, \n    const int readsCount)\n{\n  int index = blockIdx.x * blockDim.x + threadIdx.x;\n  if (index >= readsCount) return;  // out of range\n  int start = offsets[index];\n  int end = start + words[index];\n  unsigned short basePrevious = indexs[start];\n  unsigned short baseNow;\n  int count = 1;\n  for (int i=start+1; i<end; i++) {  // merge same index orders is count\n    baseNow = indexs[i];\n    if (baseNow == basePrevious) {\n      count++;\n      orders[i-1] = 0;\n    } else {\n      basePrevious = baseNow;\n      orders[i-1] = count;\n      count = 1;\n    }\n  }\n  orders[end-1] = count;\n}",
            "__global__ void kernel_updateRepresentative(\n    int *cluster, \n    int *representative, \n    const int readsCount) \n{\n  int r = *representative;\n  r++;\n  while (r < readsCount) {\n    if (cluster[r] < 0) {  // is representative\n      cluster[r] = r;\n      break;\n    }\n    r++;\n  }\n  *representative = r;\n}",
            "__global__ void kernel_makeTable(\n    const long *offsets,\n    const unsigned short *indexs,\n    const unsigned short *orders,\n    const long *words,\n    unsigned short *table,\n    int representative)\n{\n  int index = blockIdx.x * blockDim.x + threadIdx.x;\n  int start = offsets[representative];\n  int end = start + words[representative];\n  for (int i=index+start; i<end; i+=128*128) {\n    unsigned short order = orders[i];\n    if (order == 0) continue;\n    table[indexs[i]] = order;\n  }\n}",
            "__global__ void kernel_cleanTable(\n    const long *offsets, \n    const unsigned short *indexs,\n    const unsigned short *orders,  \n    const long *words,\n    unsigned short *table,\n    const int representative)\n{\n  int index = blockIdx.x * blockDim.x + threadIdx.x;\n  int start = offsets[representative];\n  int end = start + words[representative];\n  for (int i=index+start; i<end; i+=128*128) {\n    if (orders[i] == 0) continue;\n    table[indexs[i]] = 0;\n  }\n}",
            "#define T ((int)32)\n\n\ninline __host__ __device__ uint4 make_uint4(int4 a)\n{\n    return make_uint4(uint(a.x), uint(a.y), uint(a.z), uint(a.w));\n}\n\ninline __host__ __device__ uint4 min(uint4 a, uint4 b)\n{\n    return make_uint4(min(a.x,b.x), min(a.y,b.y), min(a.z,b.z), min(a.w,b.w));\n}\n\n__host__ __device__ __forceinline__ constexpr T ceil(T a, T b) {\n  return (a + b - 1) / b;\n}\n\n__global__ void kernel_magic(float threshold, \n    const int *lengths, \n    const int *magicBase,\n    int *cluster, \n    const int representative, \n    const int readsCount)\n{\n  int index = blockIdx.x * blockDim.x + threadIdx.x;\n  if (index >= readsCount) return;  // out of range\n  if (cluster[index] >= 0) return;  // is clustered\n  int offsetOne = representative*4;  // representative magic offset\n  int offsetTwo = index*4;  // query magic offset\n  int magic = min(magicBase[offsetOne + 0], magicBase[offsetTwo + 0]) +\n    min(magicBase[offsetOne + 1], magicBase[offsetTwo + 1]) +\n    min(magicBase[offsetOne + 2], magicBase[offsetTwo + 2]) +\n    min(magicBase[offsetOne + 3], magicBase[offsetTwo + 3]);\n  int length = lengths[index];\n  int minLength = ceil((float)length*threshold);\n  if (magic > minLength) {  // pass filter\n    cluster[index] = -2;\n  }\n}",
            "inline __host__ __device__ uint4 make_uint4(int4 a)\n{\n    return make_uint4(uint(a.x), uint(a.y), uint(a.z), uint(a.w));\n}\n\ninline __host__ __device__ uint4 min(uint4 a, uint4 b)\n{\n    return make_uint4(min(a.x,b.x), min(a.y,b.y), min(a.z,b.z), min(a.w,b.w));\n}\n\n__global__ void kernel_filter(\n    const float threshold, \n    const int wordLength, \n    const int *lengths,\n    const long *offsets, \n    const unsigned short *indexs, \n    const unsigned short *orders, \n    const long *words,\n    const int *wordCutoff, \n    int *cluster, \n    const unsigned short *table, \n    const int readsCount)\n{\n  __shared__ int result[128];\n\n  int gid = blockIdx.x;\n  int lid = threadIdx.x;\n\n  if (gid >= readsCount) return;  // out of range\n  if (cluster[gid] != -2) return; // out of filter\n  result[lid] = 0;             // result in thread\n  int start = offsets[gid];\n  int end = start + words[gid];\n  for (int i = lid + start; i < end; i += 128) {\n    result[lid] += min(table[indexs[i]], orders[i]);\n  }\n  __syncthreads();\n\n  if (lid == 0) {\n    for (int i=1; i<128; i++) {\n      result[0] += result[i];\n    }\n    if (result[0] > wordCutoff[gid]) { // pass filter\n      cluster[gid] = -3;\n    } else {\n      cluster[gid] = -1; // not pass filter\n    }\n  }\n}",
            "#define T ((int)32)\n\n\ninline __host__ __device__ uint4 make_uint4(int4 a)\n{\n    return make_uint4(uint(a.x), uint(a.y), uint(a.z), uint(a.w));\n}\n\ninline __host__ __device__ uint4 max(uint4 a, uint4 b)\n{\n    return make_uint4(max(a.x,b.x), max(a.y,b.y), max(a.z,b.z), max(a.w,b.w));\n}\n\ninline __host__ __device__ uint4 min(uint4 a, uint4 b)\n{\n    return make_uint4(min(a.x,b.x), min(a.y,b.y), min(a.z,b.z), min(a.w,b.w));\n}\n\n__host__ __device__ __forceinline__ constexpr T ceil(T a, T b) {\n  return (a + b - 1) / b;\n}\n\n__global__ void kernel_align(\n    const float threshold,\n    const int *lengths,\n    const long *offsets, \n    const unsigned int *compressed,\n    const int *gaps,\n    const int representative,\n    int *cluster,\n    const int readsCount)\n{\n  // variables\n  int index = blockIdx.x * blockDim.x + threadIdx.x;\n  if (index >= readsCount) return;  // out of range\n  if (cluster[index] != -3) return;  // not pass filter\n  int target = representative;  // representative read\n  int query = index;  // query read\n  int minLength = ceil((float)lengths[index] * threshold);\n  int targetLength = lengths[target] - gaps[target];  // representative base count\n  int queryLength = lengths[query] - gaps[query];  // query base count\n  int target32Length = targetLength/16+1;  // compressed target length\n  int query32Length  = queryLength/16+1;  // compressed query length\n  int targetOffset = offsets[target]/16;  // representative offset\n  int queryOffset = offsets[query]/16;  // query offset\n  short rowNow[3000] = {0};  // dynamic matrix row\n  short rowPrevious[3000] = {0};  // dynamic matrix row\n  int columnPrevious[17] = {0};  // dynamic matrix column\n  int columnNow[17] = {0};  // dynamic matrix column\n  int shift = ceil((float)targetLength - (float)queryLength*threshold);\n  shift = ceil((float)shift / 16.f); // shift form diagonal\n  // compute\n  for (int i = 0; i < query32Length; i++) {  // query is column\n    // first big loop\n    for (int j=0; j<17; j++) {\n      columnPrevious[j] = 0;\n      columnNow[j] = 0;\n    }\n    int targetIndex = 0;  // target position\n    unsigned int queryPack = compressed[queryOffset+i];  // get 16 query bases\n    int jstart = i-shift;\n    jstart = max(jstart, 0);\n    int jend = i+shift;\n    jend = min(jend, target32Length);\n    for (int j=0; j<target32Length; j++) {  // target is row\n      columnPrevious[0] = rowPrevious[targetIndex];\n      unsigned int targetPack = compressed[targetOffset+j];  // get 16 target bases\n      //---16*16core----//\n      for (int k=30; k>=0; k-=2) {  // 16 loops get target bases\n        // first small loop\n        int targetBase = (targetPack>>k)&3;  // get base from target\n        int m=0;\n        columnNow[m] = rowPrevious[targetIndex+1];\n        for (int l=30; l>=0; l-=2) {  // 16 loops get query bases\n          m++;\n          int queryBase = (queryPack>>l)&3;  // get base from query\n          int diffScore = queryBase == targetBase;\n          columnNow[m] = columnPrevious[m-1] + diffScore;\n          columnNow[m] = max(columnNow[m], columnNow[m - 1]);\n          columnNow[m] = max(columnNow[m], columnPrevious[m]);\n        }\n        targetIndex++;\n        rowNow[targetIndex] = columnNow[16];\n        if (targetIndex == targetLength) {  // last column of dynamic matirx\n          if(i == query32Length-1) {  // complete\n            int score = columnNow[queryLength%16];\n            if (score >= minLength) {\n              cluster[index] = target;\n            } else {\n              cluster[index] = -1;\n            }\n            return;\n          }\n          break;\n        }\n        // secode small loop exchange columnPrevious and columnNow\n        k-=2;\n        targetBase = (targetPack>>k)&3;\n        m=0;\n        columnPrevious[m] = rowPrevious[targetIndex+1];\n        for (int l=30; l>=0; l-=2) {\n          m++;\n          int queryBase = (queryPack>>l)&3;\n          int diffScore = queryBase == targetBase;\n          columnPrevious[m] = columnNow[m-1] + diffScore;\n          columnPrevious[m] =\n            max(columnPrevious[m], columnPrevious[m - 1]);\n          columnPrevious[m] =\n            max(columnPrevious[m], columnNow[m]);\n        }\n        targetIndex++;\n        rowNow[targetIndex] = columnPrevious[16];\n        if (targetIndex == targetLength) {\n          if(i == query32Length-1) {\n            int score = columnPrevious[queryLength%16];\n            if (score >= minLength) {\n              cluster[index] = target;\n            } else {\n              cluster[index] = -1;\n            }\n            return;\n          }\n          break;\n        }\n      }\n    }\n    // secode big loop exchage rowPrevious and rowNow\n    i++;\n    for (int j=0; j<17; j++) {\n      columnPrevious[j] = 0;\n      columnNow[j] = 0;\n    }\n    targetIndex = 0;\n    queryPack = compressed[queryOffset+i];\n    jstart = i-shift;\n    jstart = max(jstart, 0);\n    jend = i+shift;\n    jend = min(jend, target32Length);\n    for (int j=0; j<target32Length; j++) {\n      unsigned int targetPack = compressed[targetOffset+j];\n      //---16*16 core----//\n      for (int k=30; k>=0; k-=2) {\n        // first small loop\n        int targetBase = (targetPack>>k)&3;\n        int m=0;\n        columnNow[m] = rowNow[targetIndex+1];\n        for (int l=30; l>=0; l-=2) {\n          m++;\n          int queryBase = (queryPack>>l)&3;\n          int diffScore = queryBase == targetBase;\n          columnNow[m] = columnPrevious[m-1] + diffScore;\n          columnNow[m] = max(columnNow[m], columnNow[m - 1]);\n          columnNow[m] = max(columnNow[m], columnPrevious[m]);\n        }\n        targetIndex++;\n        rowPrevious[targetIndex] = columnNow[16];\n        if (targetIndex == targetLength) {\n          if(i == query32Length-1) {\n            int score = columnNow[queryLength%16];\n            if (score >= minLength) {\n              cluster[index] = target;\n            } else {\n              cluster[index] = -1;\n            }\n            return;\n          }\n          break;\n        }\n        // second small loop\n        k-=2;\n        targetBase = (targetPack>>k)&3;\n        m=0;\n        columnPrevious[m] = rowNow[targetIndex+1];\n        for (int l=30; l>=0; l-=2) {\n          m++;\n          int queryBase = (queryPack>>l)&3;\n          int diffScore = queryBase == targetBase;\n          columnPrevious[m] = columnNow[m-1] + diffScore;\n          columnPrevious[m] =\n            max(columnPrevious[m], columnPrevious[m - 1]);\n          columnPrevious[m] =\n            max(columnPrevious[m], columnNow[m]);\n        }\n        targetIndex++;\n        rowPrevious[targetIndex] = columnPrevious[16];\n        if (targetIndex == targetLength) {\n          if(i == query32Length-1) {\n            int score = columnPrevious[queryLength%16];\n            if (score >= minLength) {\n              cluster[index] = target;\n            } else {\n              cluster[index] = -1;\n            }\n            return;\n          }\n          break;\n        }\n      }\n    }\n  }\n}"
        ]
    },
    "fwt-cuda": {
        "/Users/gbolet/hecbench-roofline/src/fwt-cuda/kernels.cu": [
            "__global__ \nvoid fwtBatch1Kernel(      float *__restrict__ d_Output, \n                     const float *__restrict__ d_Input,\n                           int log2N)\n{\n    // Handle to thread block group\n    const int    N = 1 << log2N;\n    const int base = blockIdx.x << log2N;\n\n    //(2 ** 11) * 4 bytes == 8KB -- maximum s_data[] size for G80\n    extern __shared__ float s_data[];\n    const float *d_Src = d_Input  + base;\n    float *d_Dst = d_Output + base;\n\n    for (int pos = threadIdx.x; pos < N; pos += blockDim.x)\n    {\n        s_data[pos] = d_Src[pos];\n    }\n\n    //Main radix-4 stages\n    const int pos = threadIdx.x;\n\n    for (int stride = N >> 2; stride > 0; stride >>= 2)\n    {\n        int lo = pos & (stride - 1);\n        int i0 = ((pos - lo) << 2) + lo;\n        int i1 = i0 + stride;\n        int i2 = i1 + stride;\n        int i3 = i2 + stride;\n\n        __syncthreads();\n        float D0 = s_data[i0];\n        float D1 = s_data[i1];\n        float D2 = s_data[i2];\n        float D3 = s_data[i3];\n\n        float T;\n        T = D0;\n        D0         = D0 + D2;\n        D2         = T - D2;\n        T = D1;\n        D1         = D1 + D3;\n        D3         = T - D3;\n        T = D0;\n        s_data[i0] = D0 + D1;\n        s_data[i1] = T - D1;\n        T = D2;\n        s_data[i2] = D2 + D3;\n        s_data[i3] = T - D3;\n    }\n\n    //Do single radix-2 stage for odd power of two\n    if (log2N & 1)\n    {\n        __syncthreads();\n\n        for (int pos = threadIdx.x; pos < N / 2; pos += blockDim.x)\n        {\n            int i0 = pos << 1;\n            int i1 = i0 + 1;\n\n            float D0 = s_data[i0];\n            float D1 = s_data[i1];\n            s_data[i0] = D0 + D1;\n            s_data[i1] = D0 - D1;\n        }\n    }\n\n    __syncthreads();\n\n    for (int pos = threadIdx.x; pos < N; pos += blockDim.x)\n    {\n        d_Dst[pos] = s_data[pos];\n    }\n}",
            "__global__\nvoid fwtBatch2Kernel(\n          float *__restrict__ d_Output,\n    const float *__restrict__ d_Input,\n    int stride)\n{\n    const int pos = blockIdx.x * blockDim.x + threadIdx.x;\n    const int   N = blockDim.x *  gridDim.x * 4;\n\n    const float *d_Src = d_Input  + blockIdx.y * N;\n    float *d_Dst = d_Output + blockIdx.y * N;\n\n    int lo = pos & (stride - 1);\n    int i0 = ((pos - lo) << 2) + lo;\n    int i1 = i0 + stride;\n    int i2 = i1 + stride;\n    int i3 = i2 + stride;\n\n    float D0 = d_Src[i0];\n    float D1 = d_Src[i1];\n    float D2 = d_Src[i2];\n    float D3 = d_Src[i3];\n\n    float T;\n    T = D0;\n    D0        = D0 + D2;\n    D2        = T - D2;\n    T = D1;\n    D1        = D1 + D3;\n    D3        = T - D3;\n    T = D0;\n    d_Dst[i0] = D0 + D1;\n    d_Dst[i1] = T - D1;\n    T = D2;\n    d_Dst[i2] = D2 + D3;\n    d_Dst[i3] = T - D3;\n}",
            "__global__ \nvoid modulateKernel(      float *__restrict__ d_A, \n                    const float *__restrict__ d_B, \n                          int N)\n{\n    int        tid = blockIdx.x * blockDim.x + threadIdx.x;\n    int numThreads = blockDim.x * gridDim.x;\n    float     rcpN = 1.0f / (float)N;\n\n    for (int pos = tid; pos < N; pos += numThreads)\n    {\n        d_A[pos] *= d_B[pos] * rcpN;\n    }\n}"
        ]
    },
    "amgmk-cuda": {
        "/Users/gbolet/hecbench-roofline/src/amgmk-cuda/main.c": [
            "__global__ void\nrelax (const double *A_diag_data, const int *A_diag_i, const int *A_diag_j, \n             double *u_data, const double *f_data, const int n)\n{\n    int i = blockDim.x * blockIdx.x + threadIdx.x;\n    if (i >= n) return;\n\n    /*-----------------------------------------------------------\n     * If diagonal is nonzero, relax point i; otherwise, skip it.\n     *-----------------------------------------------------------*/\n\n    if ( A_diag_data[A_diag_i[i]] != 0.0)\n    {\n      double res = f_data[i];\n      for (int jj = A_diag_i[i]+1; jj < A_diag_i[i+1]; jj++)\n      {\n        int ii = A_diag_j[jj];\n        res -= A_diag_data[jj] * u_data[ii];\n      }\n      u_data[i] = res / A_diag_data[A_diag_i[i]];\n    }\n}"
        ]
    },
    "daphne-cuda": {
        "/Users/gbolet/hecbench-roofline/src/daphne-cuda/src/points2image/kernel.cu": [
            "__device__ __forceinline__ float atomicFloatMin(float * addr, float value) {\n  return  __int_as_float(atomicMin((int *)addr, __float_as_int(value)));\n}\n\n__device__ __forceinline__\ndouble atomicMax(double *address, double val)\n{\n  unsigned long long ret = __double_as_longlong(*address);\n  while(val > __longlong_as_double(ret))\n  {\n    unsigned long long old = ret;\n    if((ret = atomicCAS((unsigned long long *)address, old, __double_as_longlong(val))) == old)\n      break;\n  }\n  return __longlong_as_double(ret);\n}\n\n__global__ void compute_point_from_pointcloud(\n    const float*  __restrict__ cp, \n          float* volatile msg_distance,\n          float* volatile msg_intensity,\n          float* __restrict__ msg_min_height,\n    int width, int height, int point_step,\n    int w, int h, \n    Mat33 invR,\n    Mat13 invT,\n    Vec5 distCoeff,\n    Mat33 cameraMat,\n    int* __restrict__ min_y,\n    int* __restrict__ max_y) \n{\n\n  // determine index in cloud memory\n  int y = blockIdx.x;\n  int x = blockIdx.y * THREADS + threadIdx.x;\n  if (x >= width) return;\n\n  const float* fp = (float *)((uintptr_t)cp + (x + y*width) * point_step);\n\n  float intensity = fp[4];\n  // first step of the transformation\n  Mat13 point, point2;\n  point2.data[0] = double(fp[0]);\n  point2.data[1] = double(fp[1]);\n  point2.data[2] = double(fp[2]);\n\n  for (int row = 0; row < 3; row++) {\n    point.data[row] = invT.data[row];\n    for (int col = 0; col < 3; col++) \n      point.data[row] += point2.data[col] * invR.data[row][col];\n  }\n\n  // discard points of low depth\n  if (point.data[2] <= 2.5) return;\n\n  // second transformation step\n  double tmpx = point.data[0] / point.data[2];\n  double tmpy = point.data[1] / point.data[2];\n  double r2 = tmpx * tmpx + tmpy * tmpy;\n  double tmpdist = 1.0 + distCoeff.data[0] * r2 + distCoeff.data[1] * r2 * r2\n                   + distCoeff.data[4] * r2 * r2 * r2;\n\n  Point2d imagepoint;\n  imagepoint.x = tmpx * tmpdist + 2.0 * distCoeff.data[2] * tmpx * tmpy\n                 + distCoeff.data[3] * (r2 + 2.0 * tmpx * tmpx);\n  imagepoint.y = tmpy * tmpdist + distCoeff.data[2] * (r2 + 2.0 * tmpy * tmpy)\n                 + 2.0 * distCoeff.data[3] * tmpx * tmpy;\n\n  // apply camera intrinsics to yield a point on the image\n  imagepoint.x = cameraMat.data[0][0] * imagepoint.x + cameraMat.data[0][2];\n  imagepoint.y = cameraMat.data[1][1] * imagepoint.y + cameraMat.data[1][2];\n  int px = int(imagepoint.x + 0.5);\n  int py = int(imagepoint.y + 0.5);\n\n  float cm_point;\n  int pid;\n  // safe point characteristics in the image\n  if (0 <= px && px < w && 0 <= py && py < h)\n  {\n    pid = py * w + px;\n    cm_point = point.data[2] * 100.0;  // double precision multiply\n    atomicCAS((int*)&msg_distance[pid], 0, __float_as_int(cm_point));\n    atomicFloatMin(&msg_distance[pid], cm_point);\n  }\n  // synchronize required for deterministic intensity in the image\n  __syncthreads();\n\n  if (0 <= px && px < w && 0 <= py && py < h)\n  {\n    float newvalue = msg_distance[pid];\n\n    // update intensity, height and image extends\n    if ( newvalue>= cm_point)\n    {\n      msg_intensity[pid] = intensity;\n      atomicMax(max_y, py);\n      atomicMin(min_y, py);\n    }\n    msg_min_height[pid] = -1.25f;\n  }\n}"
        ]
    },
    "intrinsics-cast-cuda": {
        "/Users/gbolet/hecbench-roofline/src/intrinsics-cast-cuda/main.cu": [
            "__global__\nvoid cast1_intrinsics(const int n,\n                      const double* input,\n                            long long int* output)\n{\n  int i = threadIdx.x + blockIdx.x * blockDim.x;\n  if (i >= n) return;\n\n  int r1 = 0;\n  unsigned int r2 = 0;\n  long long int r3 = 0;\n  unsigned long long int r4 = 0;\n  \n  double x = input[i];\n\n  r1 ^= __double2hiint(x);\n  r1 ^= __double2loint(x);\n\n  r1 ^= __double2int_rd(x);\n  r1 ^= __double2int_rn(x);\n  r1 ^= __double2int_ru(x);\n  r1 ^= __double2int_rz(x);\n\n  r1 ^= __float2int_rd(x);\n  r1 ^= __float2int_rn(x);\n  r1 ^= __float2int_ru(x);\n  r1 ^= __float2int_rz(x);\n\n  r1 ^= __float_as_int(x);\n\n  r2 ^= __double2uint_rd(x);\n  r2 ^= __double2uint_rn(x);\n  r2 ^= __double2uint_ru(x);\n  r2 ^= __double2uint_rz(x);\n\n  r2 ^= __float2uint_rd(x);\n  r2 ^= __float2uint_rn(x);\n  r2 ^= __float2uint_ru(x);\n  r2 ^= __float2uint_rz(x);\n  \n  r2 ^= __float_as_uint(x);\n\n  r3 ^= __double2ll_rd(x);\n  r3 ^= __double2ll_rn(x);\n  r3 ^= __double2ll_ru(x);\n  r3 ^= __double2ll_rz(x);\n\n  r3 ^= __float2ll_rd(x);\n  r3 ^= __float2ll_rn(x);\n  r3 ^= __float2ll_ru(x);\n  r3 ^= __float2ll_rz(x);\n\n  r3 ^= __double_as_longlong(x);\n\n  r4 ^= __double2ull_rd(x);\n  r4 ^= __double2ull_rn(x);\n  r4 ^= __double2ull_ru(x);\n  r4 ^= __double2ull_rz(x);\n\n  r4 ^= __float2ull_rd(x);\n  r4 ^= __float2ull_rn(x);\n  r4 ^= __float2ull_ru(x);\n  r4 ^= __float2ull_rz(x);\n\n  output[i] = (r1 + r2) + (r3 + r4);\n}",
            "__global__\nvoid cast2_intrinsics(const int n,\n                      const long long int* input,\n                            long long int* output)\n{\n  int i = threadIdx.x + blockIdx.x * blockDim.x;\n  if (i >= n) return;\n\n  float r1 = 0;\n  double r2 = 0;\n  \n  long long int x = input[i];\n\n  r1 += __hiloint2double(x >> 32, x);\n\n  r1 += __int2float_rd(x);\n  r1 += __int2float_rn(x);\n  r1 += __int2float_ru(x);\n  r1 += __int2float_rz(x);\n\n  r1 += __uint2float_rd(x);\n  r1 += __uint2float_rn(x);\n  r1 += __uint2float_ru(x);\n  r1 += __uint2float_rz(x);\n\n  r1 += __int_as_float(x);\n  r1 += __uint_as_float(x);\n\n  r1 += __ll2float_rd(x);\n  r1 += __ll2float_rn(x);\n  r1 += __ll2float_ru(x);\n  r1 += __ll2float_rz(x);\n\n  r1 += __ull2float_rd(x);\n  r1 += __ull2float_rn(x);\n  r1 += __ull2float_ru(x);\n  r1 += __ull2float_rz(x);\n\n  r2 += __int2double_rn(x);\n  r2 += __uint2double_rn(x);\n\n  r2 += __ll2double_rd(x);\n  r2 += __ll2double_rn(x);\n  r2 += __ll2double_ru(x);\n  r2 += __ll2double_rz(x);\n\n  r2 += __ull2double_rd(x);\n  r2 += __ull2double_rn(x);\n  r2 += __ull2double_ru(x);\n  r2 += __ull2double_rz(x);\n\n  r2 += __longlong_as_double(x);\n\n  output[i] = __double_as_longlong(r1+r2);\n}"
        ]
    },
    "bn-cuda": {
        "/Users/gbolet/hecbench-roofline/src/bn-cuda/kernels.cu": [
            "__device__ int D_C(int n, int a){\n  int i,res=1,atmp=a;\n\n  for(i=0;i<atmp;i++){\n    res*=n;\n    n--;\n  }\n\n  for(i=0;i<atmp;i++){\n    res/=a;\n    a--;\n  }\n\n  return res;\n}\n\n__device__ void Dincr(int *bit,int n){\n\n  while(n<=NODE_N){\n    bit[n]++;\n    if(bit[n]>=2)\n    {\n      bit[n]=0;\n      n++;\n    }\n    else{\n      break;\n    }\n  }\n\n  return;\n}\n\n__device__ void DincrS(int *bit,int n){\n\n  bit[n]++;\n  if(bit[n]>=STATE_N)\n  {\n    bit[n]=0;\n    Dincr(bit,n+1);\n  }\n\n  return;\n}\n\n__device__ void D_findComb(int* comb, int l, int n)\n{\n  const int len = 4;\n  if (l == 0)\n  {\n    for (int i = 0; i < len; i++)\n      comb[i] = -1;\n    return;\n  }\n  int sum = 0;\n  int k = 1;\n\n  while (sum < l)\n    sum += D_C(n,k++);\n  l -= sum - D_C(n,--k);\n  int low = 0;\n  int pos = 0;\n  while (k > 1)\n  {\n    sum = 0;\n    int s = 1;\n    while (sum < l)\n      sum += D_C(n-s++,k-1);\n    l -= sum - D_C(n-(--s),--k);\n    low += s;\n    comb[pos++] = low;\n    n -= s;\n  }\n  comb[pos] = low + l;\n  for (int i = pos+1; i < 4; i++)\n    comb[i] = -1;\n}\n\n__device__ bool D_getState(int parN,int *sta,int time){\n  int i,j=1;\n\n  for(i=0;i<parN;i++){\n    j*=STATE_N;\n  }\n  j--;\n  if(time>j) return false;\n\n  if(time>=1)\n    DincrS(sta,0);\n\n  return true;\n\n}\n\n__global__ void genScoreKernel(const int sizepernode, \n                               float *D_localscore,\n                               const int *D_data,\n                               const float *D_LG)\n{\n  int id=blockIdx.x*256+threadIdx.x;\n  int node,index;\n  bool flag;\n  int parent[5]={0};\n  int pre[NODE_N]={0};\n  int state[5]={0};\n  int i,j,parN=0,tmp,t;\n  int t1=0,t2=0;\n  float ls=0;\n  int Nij[STATE_N]={0};\n\n  if(id<sizepernode){\n\n    D_findComb(parent,id,NODE_N-1);\n\n    for(i=0;i<4;i++)\n    {\n      if(parent[i]>0) parN++;\n    }\n\n    for(node=0;node<NODE_N;node++){\n\n      j=1;\n      for(i=0;i<NODE_N;i++)\n      {\n        if(i!=node)pre[j++]=i;\n\n      }\n\n      for(tmp=0;tmp<parN;tmp++)\n        state[tmp]=0;\n\n      index=sizepernode*node+id;\n\n      //priors\n      t=0;\n      while(D_getState(parN,state,t++)){   // for get state\n        //printf(\"test %u\\n\",id);\n        ls=0;\n        for(tmp=0;tmp<STATE_N;tmp++)\n          Nij[tmp]=0;\n\n        for(t1=0;t1<DATA_N;t1++){\n          flag=true;\n          for(t2=0;t2<parN;t2++){\n            if(D_data[t1*NODE_N+pre[parent[t2]]]!=state[t2]) {\n              flag=false;\n              break;\n            }\n          }\n          if(!flag) continue;\n\n          Nij[D_data[t1*NODE_N+node]]++;\n\n        }\n\n        tmp=STATE_N-1;\n\n        for(t1=0;t1<STATE_N;t1++){\n          ls+=D_LG[Nij[t1]];\n          tmp+=Nij[t1];\n        }\n\n        ls-=D_LG[tmp];\n        ls+=D_LG[STATE_N-1];\n\n        D_localscore[index]+=ls;\n\n      }\n    }\n  }\n}",
            "__device__ int D_C(int n, int a){\n  int i,res=1,atmp=a;\n\n  for(i=0;i<atmp;i++){\n    res*=n;\n    n--;\n  }\n\n  for(i=0;i<atmp;i++){\n    res/=a;\n    a--;\n  }\n\n  return res;\n}\n\n__device__ void D_findComb(int* comb, int l, int n)\n{\n  const int len = 4;\n  if (l == 0)\n  {\n    for (int i = 0; i < len; i++)\n      comb[i] = -1;\n    return;\n  }\n  int sum = 0;\n  int k = 1;\n\n  while (sum < l)\n    sum += D_C(n,k++);\n  l -= sum - D_C(n,--k);\n  int low = 0;\n  int pos = 0;\n  while (k > 1)\n  {\n    sum = 0;\n    int s = 1;\n    while (sum < l)\n      sum += D_C(n-s++,k-1);\n    l -= sum - D_C(n-(--s),--k);\n    low += s;\n    comb[pos++] = low;\n    n -= s;\n  }\n  comb[pos] = low + l;\n  for (int i = pos+1; i < 4; i++)\n    comb[i] = -1;\n}\n\n__device__ int D_findindex(int *arr, int size){  //reminder: arr[0] has to be 0 && size == array size-1 && index start from 0\n  int i,j,index=0;\n\n  for(i=1;i<size;i++){\n    index+=D_C(NODE_N-1,i);\n  }\n\n  for(i=1;i<=size-1;i++){\n    for(j=arr[i-1]+1;j<=arr[i]-1;j++){\n      index+=D_C(NODE_N-1-j,size-i);\n    }\n  }\n\n  index+=arr[size]-arr[size-1];\n\n  return index;\n}\n\n__global__ void computeKernel(const int taskperthr,\n                              const int sizepernode, \n                              const float *D_localscore, \n                              const bool *D_parent, \n                              const int node, \n                              const int total, \n                              float *D_Score,\n                              int *D_resP)\n{\n  extern __shared__ float lsinblock[];\n  const unsigned int id = blockIdx.x*256 + threadIdx.x;\n  const unsigned int tid = threadIdx.x;\n  const unsigned int bid = blockIdx.x;\n  int posN=1,i,index,t,tmp;\n  int pre[NODE_N]={0};\n  int parN=0;\n  int bestparent[4]={0},parent[5]={-1};\n  float bestls=-999999999999999.f,ls;\n\n  for(i=0;i<NODE_N;i++){\n    if(D_parent[i]==1){pre[posN++]=i;}\n  }\n\n  for(i=0;i<taskperthr&&((id*taskperthr+i)<total);i++){\n\n    D_findComb(parent,id*taskperthr+i,posN);\n\n    for(parN=0;parN<4;parN++){\n      if(parent[parN]<0) break;\n      if(pre[parent[parN]]>node) parent[parN]=pre[parent[parN]];\n      else                       parent[parN]=pre[parent[parN]]+1;\n    }\n\n    for(tmp=parN;tmp>0;tmp--){\n      parent[tmp]=parent[tmp-1];\n    }\n    parent[0]=0;\n\n    index=D_findindex(parent,parN);\n    index+=sizepernode*node;\n\n    ls=D_localscore[index];\n\n    if(ls>bestls){\n      bestls=ls;\n      for(tmp=0;tmp<4;tmp++)\n        bestparent[tmp]=parent[tmp+1];\n    }\n  }\n\n  lsinblock[tid]=bestls;\n\n  __syncthreads();\n\n  for(i=128;i>=1;i/=2){\n\n    if(tid<i){\n      if(lsinblock[tid+i]>lsinblock[tid]&&lsinblock[tid+i]<0){\n        lsinblock[tid]=lsinblock[tid+i];\n        lsinblock[tid+i]=(float)(tid+i);\n      }\n      else if(lsinblock[tid+i]<lsinblock[tid]&&lsinblock[tid]<0){\n        lsinblock[tid+i]=(float)tid;\n      }\n      else if(lsinblock[tid]>0&&lsinblock[tid+i]<0){\n        lsinblock[tid]=lsinblock[tid+i];\n        lsinblock[tid+i]=(float)(tid+i);\n      }\n      else if(lsinblock[tid]<0&&lsinblock[tid+i]>0){\n        lsinblock[tid+i]=(float)tid;\n      }\n\n    }\n    __syncthreads();\n  }\n\n  if(tid==0){\n\n    D_Score[bid]=lsinblock[0];\n    t=0;\n    for(i=0;i<7&&t<128&&t>=0;i++){\n      t=(int)lsinblock[(int)powf(2.0,i)+t];\n    }\n\n    lsinblock[0]=(float)t;\n  }\n\n  __syncthreads();\n\n  if(tid==(int)lsinblock[0]){\n    for(i=0;i<4;i++){\n      D_resP[bid*4+i]=bestparent[i];\n    }\n  }\n}"
        ]
    },
    "idivide-cuda": {
        "/Users/gbolet/hecbench-roofline/src/idivide-cuda/kernels.h": [
            "__global__ void throughput_test(\n    divisor_type d1,\n    divisor_type d2,\n    divisor_type d3,\n    int dummy,\n    int * buf)\n{\n  int x = blockIdx.x * blockDim.x + threadIdx.x;\n  int x1 = x / d1;\n  int x2 = x / d2;\n  int x3 = x / d3;\n  int aggregate = x1 + x2 + x3;  \n  if (aggregate & dummy == 1) buf[0] = aggregate;\n}",
            "__global__ void latency_test(\n    divisor_type d1,\n    divisor_type d2,\n    divisor_type d3,\n    divisor_type d4,\n    divisor_type d5,\n    divisor_type d6,\n    divisor_type d7,\n    divisor_type d8,\n    divisor_type d9,\n    divisor_type d10,\n    int dummy,\n    int * buf)\n{\n  int x = blockIdx.x * blockDim.x + threadIdx.x;\n  x /= d1;\n  x /= d2;\n  x /= d3;\n  x /= d4;\n  x /= d5;\n  x /= d6;\n  x /= d7;\n  x /= d8;\n  x /= d9;\n  x /= d10;\n  if (x & dummy == 1) buf[0] = x;\n}",
            "__global__ void check(int_fastdiv divisor, int * results)\n{\n  int divident = blockIdx.x * blockDim.x + threadIdx.x;\n\n  int quotient = divident / (int)divisor;\n  int fast_quotient = divident / divisor;\n\n  if (quotient != fast_quotient)\n  {\n    int error_id = atomicAdd(&results[0], 1);\n    if (error_id == 0)\n    {\n      results[1] = divident;\n      results[2] = quotient;\n      results[3] = fast_quotient;\n    }\n  }\n\n  divident = -divident;\n  quotient = divident / (int)divisor;\n  fast_quotient = divident / divisor;\n\n  if (quotient != fast_quotient)\n  {\n    int error_id = atomicAdd(&results[0], 1);\n    if (error_id == 0)\n    {\n      results[1] = divident;\n      results[2] = quotient;\n      results[3] = fast_quotient;\n    }\n  }\n}"
        ]
    },
    "hexciton-cuda": {
        "/Users/gbolet/hecbench-roofline/src/hexciton-cuda/kernels.cu": [
            "__global__ \nvoid comm_empty(\n    real_2_t *__restrict__ sigma_in,\n    real_2_t *__restrict__ sigma_out, \n    real_2_t *__restrict__ hamiltonian)\n{ \n}",
            "__global__ \nvoid comm_init (\n    const real_2_t *__restrict__ sigma_in,\n          real_2_t *__restrict__ sigma_out, \n    const real_2_t *__restrict__ hamiltonian,\n    const int dim)\n{\n\n  int gid = blockIdx.x * blockDim.x + threadIdx.x;\n  int sigma_id = gid * dim * dim;\n  // compute commutator: -i * dt/hbar * (hamiltonian * sigma_in[sigma_id] - sigma_in[sigma_id] * hamiltonian)\n  for (int i = 0; i < dim; ++i) {\n    for (int j = 0; j < dim; ++j) {\n      real_2_t tmp;\n      tmp.x = 0.0;\n      tmp.y = 0.0;\n      for (int k = 0; k < dim; ++k) {\n        // z=(x,y), w=(u,v)  z*w = (xu-yv, xv+yu)\n        tmp.x += (hamiltonian[i * dim + k].x * sigma_in[sigma_id + k * dim + j].x - \n            sigma_in[sigma_id + i * dim + k].x * hamiltonian[k * dim + j].x);\n        tmp.x -= (hamiltonian[i * dim + k].y * sigma_in[sigma_id + k * dim + j].y - \n            sigma_in[sigma_id + i * dim + k].y * hamiltonian[k * dim + j].y);\n        tmp.y += (hamiltonian[i * dim + k].x * sigma_in[sigma_id + k * dim + j].y - \n            sigma_in[sigma_id + i * dim + k].x * hamiltonian[k * dim + j].y);\n        tmp.y += (hamiltonian[i * dim + k].y * sigma_in[sigma_id + k * dim + j].x -\n            sigma_in[sigma_id + i * dim + k].y * hamiltonian[k * dim + j].x);\n      }\n      // multiply with -i * dt / hbar\n      sigma_out[sigma_id + i * dim + j].x += hdt * tmp.y;\n      sigma_out[sigma_id + i * dim + j].y -= hdt * tmp.x;\n    }\n  }\n}",
            "__global__\nvoid comm_refactor(\n    const real_2_t *__restrict__ sigma2_in,\n          real_2_t *__restrict__ sigma2_out, \n    const real_2_t *__restrict__ hamiltonian2,\n    const int dim)\n{\n#define sigma_real(i, j) (sigma_id + 2 * ((i) * dim + (j)))\n#define sigma_imag(i, j) (sigma_id + 2 * ((i) * dim + (j)) + 1)\n\n#define ham_real(i, j) (2 * ((i) * dim + (j)))\n#define ham_imag(i, j) (2 * ((i) * dim + (k)) + 1)\n  real_t *__restrict__ sigma_in = (real_t*) sigma2_in;\n  real_t *__restrict__ sigma_out = (real_t*) sigma2_out;\n  real_t *__restrict__ hamiltonian = (real_t*) hamiltonian2;\n\n  int gid = blockIdx.x * blockDim.x + threadIdx.x;\n  int sigma_id = gid * dim * dim * 2;\n\n  for (int i = 0; i < dim; ++i) {\n    for (int j = 0; j < dim; ++j) {\n      real_t tmp_real = 0.0;\n      real_t tmp_imag = 0.0;\n      for (int k = 0; k < dim; ++k) {\n        tmp_real += hamiltonian[ham_real(i, k)] * sigma_in[sigma_real(k, j)];\n        tmp_real -= sigma_in[sigma_real(i, k)] * hamiltonian[ham_real(k, j)];\n        tmp_real -= hamiltonian[ham_imag(i, k)] * sigma_in[sigma_imag(k, j)];\n        tmp_real += sigma_in[sigma_imag(i, k)] * hamiltonian[ham_imag(k, j)];\n        tmp_imag += hamiltonian[ham_real(i, k)] * sigma_in[sigma_imag(k, j)];\n        tmp_imag -= sigma_in[sigma_real(i, k)] * hamiltonian[ham_imag(k, j)];\n        tmp_imag += hamiltonian[ham_imag(i, k)] * sigma_in[sigma_real(k, j)]; \n        tmp_imag -= sigma_in[sigma_imag(i, k)] * hamiltonian[ham_real(k, j)];\n      }\n      // multiply with -i dt/hbar\n      sigma_out[sigma_real(i, j)] += hdt * tmp_imag;\n      sigma_out[sigma_imag(i, j)] -= hdt * tmp_real;\n    }\n  }\n}",
            "__global__\nvoid comm_refactor_direct_store(\n    const real_2_t *__restrict__ sigma2_in,\n          real_2_t *__restrict__ sigma2_out, \n    const real_2_t *__restrict__ hamiltonian2,\n    const int dim)\n{\n#define sigma_real(i, j) (sigma_id + 2 * ((i) * dim + (j)))\n#define sigma_imag(i, j) (sigma_id + 2 * ((i) * dim + (j)) + 1)\n#define ham_real(i, j) (2 * ((i) * dim + (j)))\n#define ham_imag(i, j) (2 * ((i) * dim + (k)) + 1)\n\n  real_t *__restrict__ sigma_in = (real_t*) sigma2_in;\n  real_t *__restrict__ sigma_out = (real_t*) sigma2_out;\n  real_t *__restrict__ hamiltonian = (real_t*) hamiltonian2;\n\n  int gid = blockIdx.x * blockDim.x + threadIdx.x;\n  int sigma_id = gid * dim * dim * 2;\n\n  for (int i = 0; i < dim; ++i) {\n    for (int j = 0; j < dim; ++j) {\n      for (int k = 0; k < dim; ++k) {\n        sigma_out[sigma_real(i, j)] += hamiltonian[ham_real(i, k)] * sigma_in[sigma_imag(k, j)];\n        sigma_out[sigma_real(i, j)] -= sigma_in[sigma_real(i, k)] * hamiltonian[ham_imag(k, j)];\n        sigma_out[sigma_real(i, j)] += hamiltonian[ham_imag(i, k)] * sigma_in[sigma_real(k, j)];\n        sigma_out[sigma_real(i, j)] -= sigma_in[sigma_imag(i, k)] * hamiltonian[ham_real(k, j)];\n        sigma_out[sigma_imag(i, j)] -= hamiltonian[ham_real(i, k)] * sigma_in[sigma_real(k, j)];\n        sigma_out[sigma_imag(i, j)] += sigma_in[sigma_real(i, k)] * hamiltonian[ham_real(k, j)];\n        sigma_out[sigma_imag(i, j)] += hamiltonian[ham_imag(i, k)] * sigma_in[sigma_imag(k, j)];\n        sigma_out[sigma_imag(i, j)] -= sigma_in[sigma_imag(i, k)] * hamiltonian[ham_imag(k, j)];\n      }\n    }\n  }\n}",
            "__global__ \nvoid comm_aosoa_naive(\n    const real_2_t *__restrict__ sigma2_in,\n          real_2_t *__restrict__ sigma2_out, \n    const real_2_t *__restrict__ hamiltonian2,\n    const int dim)\n{\n  real_t *__restrict__ sigma_in = (real_t*) sigma2_in;\n  real_t *__restrict__ sigma_out = (real_t*) sigma2_out;\n  real_t *__restrict__ hamiltonian = (real_t*) hamiltonian2;\n\n  int gid = blockIdx.x * blockDim.x + threadIdx.x;\n\n#define package_id ((gid / VEC_LENGTH_AUTO) * VEC_LENGTH_AUTO * 2 * dim * dim)\n#define sigma_id (gid % VEC_LENGTH_AUTO)\n\n#define sigma_real(i, j) (package_id + 2 * VEC_LENGTH_AUTO * (dim * (i) + (j)) + (sigma_id))\n#define sigma_imag(i, j) (package_id + 2 * VEC_LENGTH_AUTO * (dim * (i) + (j)) + VEC_LENGTH_AUTO + (sigma_id))\n\n#define ham_real(i, j) ((i) * dim + (j))\n#define ham_imag(i, j) (dim * dim + (i) * dim + (j))\n\n  for (int i = 0; i < dim; ++i) {\n    for (int j = 0; j < dim; ++j) {\n      real_t tmp_real = 0.0;\n      real_t tmp_imag = 0.0;\n      for (int k = 0; k < dim; ++k) {\n        tmp_imag -= hamiltonian[ham_real(i, k)] * sigma_in[sigma_real(k, j)];\n        tmp_imag += sigma_in[sigma_real(i, k)] * hamiltonian[ham_real(k, j)];\n        tmp_imag += hamiltonian[ham_imag(i, k)] * sigma_in[sigma_imag(k, j)];\n        tmp_imag -= sigma_in[sigma_imag(i, k)] * hamiltonian[ham_imag(k, j)];\n        tmp_real += hamiltonian[ham_real(i, k)] * sigma_in[sigma_imag(k, j)];\n        tmp_real -= sigma_in[sigma_real(i, k)] * hamiltonian[ham_imag(k, j)];\n        tmp_real += hamiltonian[ham_imag(i, k)] * sigma_in[sigma_real(k, j)];\n        tmp_real -= sigma_in[sigma_imag(i, k)] * hamiltonian[ham_real(k, j)];\n      }\n      sigma_out[sigma_real(i, j)] += tmp_real;\n      sigma_out[sigma_imag(i, j)] += tmp_imag;\n    }\n  }\n}",
            "__global__ \nvoid comm_aosoa_naive_constants (\n    const real_2_t *__restrict__ sigma2_in,\n          real_2_t *__restrict__ sigma2_out, \n    const real_2_t *__restrict__ hamiltonian2,\n    const int dim)\n{\n  real_t *__restrict__ sigma_in = (real_t*) sigma2_in;\n  real_t *__restrict__ sigma_out = (real_t*) sigma2_out;\n  real_t *__restrict__ hamiltonian = (real_t*) hamiltonian2;\n\n  int gid = blockIdx.x * blockDim.x + threadIdx.x;\n\n#define package_id ((gid / VEC_LENGTH_AUTO) * VEC_LENGTH_AUTO * 2 * DIM * DIM)\n#define sigma_id (gid % VEC_LENGTH_AUTO)\n\n#define sigma_real(i, j) (package_id + 2 * VEC_LENGTH_AUTO * (DIM * (i) + (j)) + (sigma_id))\n#define sigma_imag(i, j) (package_id + 2 * VEC_LENGTH_AUTO * (DIM * (i) + (j)) + VEC_LENGTH_AUTO + (sigma_id))\n\n#define ham_real(i, j) ((i) * DIM + (j))\n#define ham_imag(i, j) (DIM * DIM + (i) * DIM + (j))\n\n  for (int i = 0; i < DIM; ++i) {\n    for (int j = 0; j < DIM; ++j) {\n      real_t tmp_real = 0.0;\n      real_t tmp_imag = 0.0;\n      for (int k = 0; k < DIM; ++k) {\n        tmp_imag -= hamiltonian[ham_real(i, k)] * sigma_in[sigma_real(k, j)];\n        tmp_imag += sigma_in[sigma_real(i, k)] * hamiltonian[ham_real(k, j)];\n        tmp_imag += hamiltonian[ham_imag(i, k)] * sigma_in[sigma_imag(k, j)];\n        tmp_imag -= sigma_in[sigma_imag(i, k)] * hamiltonian[ham_imag(k, j)];\n        tmp_real += hamiltonian[ham_real(i, k)] * sigma_in[sigma_imag(k, j)];\n        tmp_real -= sigma_in[sigma_real(i, k)] * hamiltonian[ham_imag(k, j)];\n        tmp_real += hamiltonian[ham_imag(i, k)] * sigma_in[sigma_real(k, j)];\n        tmp_real -= sigma_in[sigma_imag(i, k)] * hamiltonian[ham_real(k, j)];\n      }\n      sigma_out[sigma_real(i, j)] += tmp_real;\n      sigma_out[sigma_imag(i, j)] += tmp_imag;\n    }\n  }\n}",
            "__global__ \nvoid comm_aosoa_naive_constants_perm (\n    const real_2_t *__restrict__ sigma2_in,\n          real_2_t *__restrict__ sigma2_out, \n    const real_2_t *__restrict__ hamiltonian2)\n{\n  real_t *__restrict__ sigma_in = (real_t*) sigma2_in;\n  real_t *__restrict__ sigma_out = (real_t*) sigma2_out;\n  real_t *__restrict__ hamiltonian = (real_t*) hamiltonian2;\n\n  int gid = blockIdx.x * blockDim.x + threadIdx.x;\n#define package_id ((gid / VEC_LENGTH_AUTO) * VEC_LENGTH_AUTO * 2 * DIM * DIM)\n#define sigma_id (gid % VEC_LENGTH_AUTO)\n\n#define sigma_real(i, j) (package_id + 2 * VEC_LENGTH_AUTO * (DIM * (i) + (j)) + (sigma_id))\n#define sigma_imag(i, j) (package_id + 2 * VEC_LENGTH_AUTO * (DIM * (i) + (j)) + VEC_LENGTH_AUTO + (sigma_id))\n\n#define ham_real(i, j) ((i) * DIM + (j))\n#define ham_imag(i, j) (DIM * DIM + (i) * DIM + (j))\n\n  // compute commutator: (hamiltonian * sigma_in[sigma_id] - sigma_in[sigma_id] * hamiltonian)\n  for (int i = 0; i < DIM; ++i) {\n    for (int k = 0; k < DIM; ++k) {\n      real_t ham_real_tmp = hamiltonian[ham_real(i, k)];\n      real_t ham_imag_tmp = hamiltonian[ham_imag(i, k)];\n      real_t sigma_real_tmp = sigma_in[sigma_real(i, k)];\n      real_t sigma_imag_tmp = sigma_in[sigma_imag(i, k)];\n      for (int j = 0; j < DIM; ++j) {\n#ifdef USE_INITZERO\n        real_t tmp_real = 0.0;\n        real_t tmp_imag = 0.0;\n#else\n        real_t tmp_real = sigma_out[sigma_real(i, j)];\n        real_t tmp_imag = sigma_out[sigma_imag(i, j)];\n#endif\n        tmp_imag -= ham_real_tmp * sigma_in[sigma_real(k, j)];\n        tmp_imag += sigma_real_tmp * hamiltonian[ham_real(k, j)];\n        tmp_imag += ham_imag_tmp * sigma_in[sigma_imag(k, j)];\n        tmp_imag -= sigma_imag_tmp * hamiltonian[ham_imag(k, j)];\n        tmp_real += ham_real_tmp * sigma_in[sigma_imag(k, j)];\n        tmp_real -= sigma_real_tmp * hamiltonian[ham_imag(k, j)];\n        tmp_real += ham_imag_tmp * sigma_in[sigma_real(k, j)];\n        tmp_real -= sigma_imag_tmp * hamiltonian[ham_real(k, j)];\n#ifdef USE_INITZERO\n        sigma_out[sigma_real(i, j)] += tmp_real;\n        sigma_out[sigma_imag(i, j)] += tmp_imag;\n#else\n        sigma_out[sigma_real(i, j)] = tmp_real;\n        sigma_out[sigma_imag(i, j)] = tmp_imag;\n#endif\n      }\n    }\n  }\n}",
            "__global__ \nvoid comm_aosoa_naive_direct (\n    const real_2_t *__restrict__ sigma2_in,\n          real_2_t *__restrict__ sigma2_out, \n    const real_2_t *__restrict__ hamiltonian2,\n    const int dim)\n{\n  real_t *__restrict__ sigma_in = (real_t*) sigma2_in;\n  real_t *__restrict__ sigma_out = (real_t*) sigma2_out;\n  real_t *__restrict__ hamiltonian = (real_t*) hamiltonian2;\n\n  int gid = blockIdx.x * blockDim.x + threadIdx.x;\n\n#define package_id ((gid / VEC_LENGTH_AUTO) * VEC_LENGTH_AUTO * 2 * dim * dim)\n#define sigma_id (gid % VEC_LENGTH_AUTO)\n\n#define sigma_real(i, j) (package_id + 2 * VEC_LENGTH_AUTO * (dim * (i) + (j)) + (sigma_id))\n#define sigma_imag(i, j) (package_id + 2 * VEC_LENGTH_AUTO * (dim * (i) + (j)) + VEC_LENGTH_AUTO + (sigma_id))\n\n#define ham_real(i, j) ((i) * dim + (j))\n#define ham_imag(i, j) (dim * dim + (i) * dim + (j))\n\n  // compute commutator: (hamiltonian * sigma_in[sigma_id] - sigma_in[sigma_id] * hamiltonian)\n  for (int i = 0; i < dim; ++i) {\n    for (int j = 0; j < dim; ++j) {\n      for (int k = 0; k < dim; ++k) {\n        sigma_out[sigma_imag(i, j)] -= hamiltonian[ham_real(i, k)] * sigma_in[sigma_real(k, j)];\n        sigma_out[sigma_imag(i, j)] += sigma_in[sigma_real(i, k)] * hamiltonian[ham_real(k, j)];\n        sigma_out[sigma_imag(i, j)] += hamiltonian[ham_imag(i, k)] * sigma_in[sigma_imag(k, j)];\n        sigma_out[sigma_imag(i, j)] -= sigma_in[sigma_imag(i, k)] * hamiltonian[ham_imag(k, j)];\n        sigma_out[sigma_real(i, j)] += hamiltonian[ham_real(i, k)] * sigma_in[sigma_imag(k, j)];\n        sigma_out[sigma_real(i, j)] -= sigma_in[sigma_real(i, k)] * hamiltonian[ham_imag(k, j)];\n        sigma_out[sigma_real(i, j)] += hamiltonian[ham_imag(i, k)] * sigma_in[sigma_real(k, j)];\n        sigma_out[sigma_real(i, j)] -= sigma_in[sigma_imag(i, k)] * hamiltonian[ham_real(k, j)];\n      }\n    }\n  }\n}",
            "__global__ \nvoid comm_aosoa_naive_constants_direct (\n    const real_2_t *__restrict__ sigma2_in,\n          real_2_t *__restrict__ sigma2_out, \n    const real_2_t *__restrict__ hamiltonian2)\n{\n  real_t *__restrict__ sigma_in = (real_t*) sigma2_in;\n  real_t *__restrict__ sigma_out = (real_t*) sigma2_out;\n  real_t *__restrict__ hamiltonian = (real_t*) hamiltonian2;\n\n  int gid = blockIdx.x * blockDim.x + threadIdx.x;\n#define package_id ((gid / VEC_LENGTH_AUTO) * VEC_LENGTH_AUTO * 2 * DIM * DIM)\n#define sigma_id (gid % VEC_LENGTH_AUTO)\n\n#define sigma_real(i, j) (package_id + 2 * VEC_LENGTH_AUTO * (DIM * (i) + (j)) + (sigma_id))\n#define sigma_imag(i, j) (package_id + 2 * VEC_LENGTH_AUTO * (DIM * (i) + (j)) + VEC_LENGTH_AUTO + (sigma_id))\n\n#define ham_real(i, j) ((i) * DIM + (j))\n#define ham_imag(i, j) (DIM * DIM + (i) * DIM + (j))\n\n  // compute commutator: (hamiltonian * sigma_in[sigma_id] - sigma_in[sigma_id] * hamiltonian)\n  for (int i = 0; i < DIM; ++i) {\n    for (int j = 0; j < DIM; ++j) {\n      for (int k = 0; k < DIM; ++k) {\n        sigma_out[sigma_imag(i, j)] -= hamiltonian[ham_real(i, k)] * sigma_in[sigma_real(k, j)];\n        sigma_out[sigma_imag(i, j)] += sigma_in[sigma_real(i, k)] * hamiltonian[ham_real(k, j)];\n        sigma_out[sigma_imag(i, j)] += hamiltonian[ham_imag(i, k)] * sigma_in[sigma_imag(k, j)];\n        sigma_out[sigma_imag(i, j)] -= sigma_in[sigma_imag(i, k)] * hamiltonian[ham_imag(k, j)];\n        sigma_out[sigma_real(i, j)] += hamiltonian[ham_real(i, k)] * sigma_in[sigma_imag(k, j)];\n        sigma_out[sigma_real(i, j)] -= sigma_in[sigma_real(i, k)] * hamiltonian[ham_imag(k, j)];\n        sigma_out[sigma_real(i, j)] += hamiltonian[ham_imag(i, k)] * sigma_in[sigma_real(k, j)];\n        sigma_out[sigma_real(i, j)] -= sigma_in[sigma_imag(i, k)] * hamiltonian[ham_real(k, j)];\n      }\n    }\n  }\n}",
            "__global__ \nvoid comm_aosoa_naive_constants_direct_perm (\n    const real_2_t *__restrict__ sigma2_in,\n          real_2_t *__restrict__ sigma2_out, \n    const real_2_t *__restrict__ hamiltonian2)\n{\n  real_t *__restrict__ sigma_in = (real_t*) sigma2_in;\n  real_t *__restrict__ sigma_out = (real_t*) sigma2_out;\n  real_t *__restrict__ hamiltonian = (real_t*) hamiltonian2;\n\n  int gid = blockIdx.x * blockDim.x + threadIdx.x;\n#define package_id ((gid / VEC_LENGTH_AUTO) * VEC_LENGTH_AUTO * 2 * DIM * DIM)\n#define sigma_id (gid % VEC_LENGTH_AUTO)\n\n#define sigma_real(i, j) (package_id + 2 * VEC_LENGTH_AUTO * (DIM * (i) + (j)) + (sigma_id))\n#define sigma_imag(i, j) (package_id + 2 * VEC_LENGTH_AUTO * (DIM * (i) + (j)) + VEC_LENGTH_AUTO + (sigma_id))\n\n#define ham_real(i, j) ((i) * DIM + (j))\n#define ham_imag(i, j) (DIM * DIM + (i) * DIM + (j))\n\n  // compute commutator: (hamiltonian * sigma_in[sigma_id] - sigma_in[sigma_id] * hamiltonian)\n  for (int i = 0; i < DIM; ++i) {\n    for (int k = 0; k < DIM; ++k) {\n      real_t ham_real_tmp = hamiltonian[ham_real(i, k)];\n      real_t ham_imag_tmp = hamiltonian[ham_imag(i, k)];\n      real_t sigma_real_tmp = sigma_in[sigma_real(i, k)];\n      real_t sigma_imag_tmp = sigma_in[sigma_imag(i, k)];\n      for (int j = 0; j < DIM; ++j) {\n        sigma_out[sigma_imag(i, j)] -= ham_real_tmp * sigma_in[sigma_real(k, j)];\n        sigma_out[sigma_imag(i, j)] += sigma_real_tmp * hamiltonian[ham_real(k, j)];\n        sigma_out[sigma_imag(i, j)] += ham_imag_tmp * sigma_in[sigma_imag(k, j)];\n        sigma_out[sigma_imag(i, j)] -= sigma_imag_tmp * hamiltonian[ham_imag(k, j)];\n        sigma_out[sigma_real(i, j)] += ham_real_tmp * sigma_in[sigma_imag(k, j)];\n        sigma_out[sigma_real(i, j)] -= sigma_real_tmp * hamiltonian[ham_imag(k, j)];\n        sigma_out[sigma_real(i, j)] += ham_imag_tmp * sigma_in[sigma_real(k, j)];\n        sigma_out[sigma_real(i, j)] -= sigma_imag_tmp * hamiltonian[ham_real(k, j)];\n      }\n    }\n  }\n}",
            "__global__ \nvoid comm_aosoa (\n    const real_2_t *__restrict__ sigma2_in,\n          real_2_t *__restrict__ sigma2_out, \n    const real_2_t *__restrict__ hamiltonian2,\n    const int dim)\n{\n  real_t *__restrict__ sigma_in = (real_t*) sigma2_in;\n  real_t *__restrict__ sigma_out = (real_t*) sigma2_out;\n  real_t *__restrict__ hamiltonian = (real_t*) hamiltonian2;\n\n#define package_id ((PACKAGES_PER_WG * blockIdx.y + threadIdx.y) * (VEC_LENGTH_AUTO * 2 * dim * dim))\n#define sigma_id threadIdx.x\n\n#define sigma_real(i, j) (package_id + 2 * VEC_LENGTH_AUTO * (dim * (i) + (j)) + sigma_id)\n#define sigma_imag(i, j) (package_id + 2 * VEC_LENGTH_AUTO * (dim * (i) + (j)) + VEC_LENGTH_AUTO + sigma_id)\n\n#define ham_real(i, j) ((i) * dim + (j))\n#define ham_imag(i, j) (dim * dim + (i) * dim + (j))\n\n  for (int i = 0; i < dim; ++i) {\n    for (int j = 0; j < dim; ++j) {\n      real_t tmp_real = 0.0;\n      real_t tmp_imag = 0.0;\n      for (int k = 0; k < dim; ++k) {\n        tmp_imag -= hamiltonian[ham_real(i, k)] * sigma_in[sigma_real(k, j)];\n        tmp_imag += sigma_in[sigma_real(i, k)] * hamiltonian[ham_real(k, j)];\n        tmp_imag += hamiltonian[ham_imag(i, k)] * sigma_in[sigma_imag(k, j)];\n        tmp_imag -= sigma_in[sigma_imag(i, k)] * hamiltonian[ham_imag(k, j)];\n        tmp_real += hamiltonian[ham_real(i, k)] * sigma_in[sigma_imag(k, j)];\n        tmp_real -= sigma_in[sigma_real(i, k)] * hamiltonian[ham_imag(k, j)];\n        tmp_real += hamiltonian[ham_imag(i, k)] * sigma_in[sigma_real(k, j)];\n        tmp_real -= sigma_in[sigma_imag(i, k)] * hamiltonian[ham_real(k, j)];\n      }\n      sigma_out[sigma_real(i, j)] += tmp_real;\n      sigma_out[sigma_imag(i, j)] += tmp_imag;\n    }\n  }\n}",
            "__global__ \nvoid comm_aosoa_constants (\n    const real_2_t *__restrict__ sigma2_in,\n          real_2_t *__restrict__ sigma2_out, \n    const real_2_t *__restrict__ hamiltonian2)\n{\n  real_t *__restrict__ sigma_in = (real_t*) sigma2_in;\n  real_t *__restrict__ sigma_out = (real_t*) sigma2_out;\n  real_t *__restrict__ hamiltonian = (real_t*) hamiltonian2;\n\n#define package_id ((PACKAGES_PER_WG * blockIdx.y + threadIdx.y) * (VEC_LENGTH_AUTO * 2 * DIM * DIM))\n#define sigma_id threadIdx.x\n\n#define sigma_real(i, j) (package_id + 2 * VEC_LENGTH_AUTO * (DIM * (i) + (j)) + sigma_id)\n#define sigma_imag(i, j) (package_id + 2 * VEC_LENGTH_AUTO * (DIM * (i) + (j)) + VEC_LENGTH_AUTO + sigma_id)\n\n#define ham_real(i, j) ((i) * DIM + (j))\n#define ham_imag(i, j) (DIM * DIM + (i) * DIM + (j))\n\n  for (int i = 0; i < DIM; ++i) {\n    for (int j = 0; j < DIM; ++j) {\n      real_t tmp_real = 0.0;\n      real_t tmp_imag = 0.0;\n      for (int k = 0; k < DIM; ++k) {\n        tmp_imag -= hamiltonian[ham_real(i, k)] * sigma_in[sigma_real(k, j)];\n        tmp_imag += sigma_in[sigma_real(i, k)] * hamiltonian[ham_real(k, j)];\n        tmp_imag += hamiltonian[ham_imag(i, k)] * sigma_in[sigma_imag(k, j)];\n        tmp_imag -= sigma_in[sigma_imag(i, k)] * hamiltonian[ham_imag(k, j)];\n        tmp_real += hamiltonian[ham_real(i, k)] * sigma_in[sigma_imag(k, j)];\n        tmp_real -= sigma_in[sigma_real(i, k)] * hamiltonian[ham_imag(k, j)];\n        tmp_real += hamiltonian[ham_imag(i, k)] * sigma_in[sigma_real(k, j)];\n        tmp_real -= sigma_in[sigma_imag(i, k)] * hamiltonian[ham_real(k, j)];\n      }\n      sigma_out[sigma_real(i, j)] += tmp_real;\n      sigma_out[sigma_imag(i, j)] += tmp_imag;\n    }\n  }\n}",
            "__global__ \nvoid comm_aosoa_constants_perm (\n    const real_2_t *__restrict__ sigma2_in,\n          real_2_t *__restrict__ sigma2_out, \n    const real_2_t *__restrict__ hamiltonian2)\n{\n  real_t *__restrict__ sigma_in = (real_t*) sigma2_in;\n  real_t *__restrict__ sigma_out = (real_t*) sigma2_out;\n  real_t *__restrict__ hamiltonian = (real_t*) hamiltonian2;\n\n#define package_id ((PACKAGES_PER_WG * blockIdx.y + threadIdx.y) * (VEC_LENGTH_AUTO * 2 * DIM * DIM))\n#define sigma_id threadIdx.x\n\n#define sigma_real(i, j) (package_id + 2 * VEC_LENGTH_AUTO * (DIM * (i) + (j)) + sigma_id)\n#define sigma_imag(i, j) (package_id + 2 * VEC_LENGTH_AUTO * (DIM * (i) + (j)) + VEC_LENGTH_AUTO + sigma_id)\n\n#define ham_real(i, j) ((i) * DIM + (j))\n#define ham_imag(i, j) (DIM * DIM + (i) * DIM + (j))\n\n  for (int i = 0; i < DIM; ++i) {\n    for (int k = 0; k < DIM; ++k) {\n      real_t ham_real_tmp = hamiltonian[ham_real(i, k)];\n      real_t ham_imag_tmp = hamiltonian[ham_imag(i, k)];\n      real_t sigma_real_tmp = sigma_in[sigma_real(i, k)];\n      real_t sigma_imag_tmp = sigma_in[sigma_imag(i, k)];\n      for (int j = 0; j < DIM; ++j) {\n#ifdef USE_INITZERO\n        real_t tmp_real = 0.0;\n        real_t tmp_imag = 0.0;\n#else\n        real_t tmp_real = sigma_out[sigma_real(i, j)];\n        real_t tmp_imag = sigma_out[sigma_imag(i, j)];\n#endif\n        tmp_imag -= ham_real_tmp * sigma_in[sigma_real(k, j)];\n        tmp_imag += sigma_real_tmp * hamiltonian[ham_real(k, j)];\n        tmp_imag += ham_imag_tmp * sigma_in[sigma_imag(k, j)];\n        tmp_imag -= sigma_imag_tmp * hamiltonian[ham_imag(k, j)];\n        tmp_real += ham_real_tmp * sigma_in[sigma_imag(k, j)];\n        tmp_real -= sigma_real_tmp * hamiltonian[ham_imag(k, j)];\n        tmp_real += ham_imag_tmp * sigma_in[sigma_real(k, j)];\n        tmp_real -= sigma_imag_tmp * hamiltonian[ham_real(k, j)];\n#ifdef USE_INITZERO\n        sigma_out[sigma_real(i, j)] += tmp_real;\n        sigma_out[sigma_imag(i, j)] += tmp_imag;\n#else\n        sigma_out[sigma_real(i, j)] = tmp_real;\n        sigma_out[sigma_imag(i, j)] = tmp_imag;\n#endif\n      }\n    }\n  }\n}",
            "__global__ \nvoid comm_aosoa_direct (\n    const real_2_t *__restrict__ sigma2_in,\n          real_2_t *__restrict__ sigma2_out, \n    const real_2_t *__restrict__ hamiltonian2,\n    const int dim)\n{\n  real_t *__restrict__ sigma_in = (real_t*) sigma2_in;\n  real_t *__restrict__ sigma_out = (real_t*) sigma2_out;\n  real_t *__restrict__ hamiltonian = (real_t*) hamiltonian2;\n#define package_id ((PACKAGES_PER_WG * blockIdx.y + threadIdx.y) * (VEC_LENGTH_AUTO * 2 * dim * dim))\n#define sigma_id threadIdx.x\n\n#define sigma_real(i, j) (package_id + 2 * VEC_LENGTH_AUTO * (dim * (i) + (j)) + sigma_id)\n#define sigma_imag(i, j) (package_id + 2 * VEC_LENGTH_AUTO * (dim * (i) + (j)) + VEC_LENGTH_AUTO + sigma_id)\n\n#define ham_real(i, j) ((i) * dim + (j))\n#define ham_imag(i, j) (dim * dim + (i) * dim + (j))\n\n  for (int i = 0; i < dim; ++i) {\n    for (int j = 0; j < dim; ++j) {\n      for (int k = 0; k < dim; ++k) {\n        sigma_out[sigma_imag(i, j)] -= hamiltonian[ham_real(i, k)] * sigma_in[sigma_real(k, j)];\n        sigma_out[sigma_imag(i, j)] += sigma_in[sigma_real(i, k)] * hamiltonian[ham_real(k, j)];\n        sigma_out[sigma_imag(i, j)] += hamiltonian[ham_imag(i, k)] * sigma_in[sigma_imag(k, j)];\n        sigma_out[sigma_imag(i, j)] -= sigma_in[sigma_imag(i, k)] * hamiltonian[ham_imag(k, j)];\n        sigma_out[sigma_real(i, j)] += hamiltonian[ham_real(i, k)] * sigma_in[sigma_imag(k, j)];\n        sigma_out[sigma_real(i, j)] -= sigma_in[sigma_real(i, k)] * hamiltonian[ham_imag(k, j)];\n        sigma_out[sigma_real(i, j)] += hamiltonian[ham_imag(i, k)] * sigma_in[sigma_real(k, j)];\n        sigma_out[sigma_real(i, j)] -= sigma_in[sigma_imag(i, k)] * hamiltonian[ham_real(k, j)];\n      }\n    }\n  }\n}",
            "__global__ \nvoid comm_aosoa_constants_direct (\n    const real_2_t *__restrict__ sigma2_in,\n          real_2_t *__restrict__ sigma2_out, \n    const real_2_t *__restrict__ hamiltonian2)\n{\n  real_t *__restrict__ sigma_in = (real_t*) sigma2_in;\n  real_t *__restrict__ sigma_out = (real_t*) sigma2_out;\n  real_t *__restrict__ hamiltonian = (real_t*) hamiltonian2;\n\n#define package_id ((PACKAGES_PER_WG * blockIdx.y + threadIdx.y) * (VEC_LENGTH_AUTO * 2 * DIM * DIM))\n#define sigma_id threadIdx.x\n\n#define sigma_real(i, j) (package_id + 2 * VEC_LENGTH_AUTO * (DIM * (i) + (j)) + sigma_id)\n#define sigma_imag(i, j) (package_id + 2 * VEC_LENGTH_AUTO * (DIM * (i) + (j)) + VEC_LENGTH_AUTO + sigma_id)\n#define ham_real(i, j) ((i) * DIM + (j))\n#define ham_imag(i, j) (DIM * DIM + (i) * DIM + (j))\n\n  for (int i = 0; i < DIM; ++i) {\n    for (int j = 0; j < DIM; ++j) {\n      for (int k = 0; k < DIM; ++k) {\n        sigma_out[sigma_imag(i, j)] -= hamiltonian[ham_real(i, k)] * sigma_in[sigma_real(k, j)];\n        sigma_out[sigma_imag(i, j)] += sigma_in[sigma_real(i, k)] * hamiltonian[ham_real(k, j)];\n        sigma_out[sigma_imag(i, j)] += hamiltonian[ham_imag(i, k)] * sigma_in[sigma_imag(k, j)];\n        sigma_out[sigma_imag(i, j)] -= sigma_in[sigma_imag(i, k)] * hamiltonian[ham_imag(k, j)];\n        sigma_out[sigma_real(i, j)] += hamiltonian[ham_real(i, k)] * sigma_in[sigma_imag(k, j)];\n        sigma_out[sigma_real(i, j)] -= sigma_in[sigma_real(i, k)] * hamiltonian[ham_imag(k, j)];\n        sigma_out[sigma_real(i, j)] += hamiltonian[ham_imag(i, k)] * sigma_in[sigma_real(k, j)];\n        sigma_out[sigma_real(i, j)] -= sigma_in[sigma_imag(i, k)] * hamiltonian[ham_real(k, j)];\n      }\n    }\n  }\n}",
            "__global__ \nvoid comm_aosoa_constants_direct_perm (\n    const real_2_t *__restrict__ sigma2_in,\n          real_2_t *__restrict__ sigma2_out, \n    const real_2_t *__restrict__ hamiltonian2)\n{\n  real_t *__restrict__ sigma_in = (real_t*) sigma2_in;\n  real_t *__restrict__ sigma_out = (real_t*) sigma2_out;\n  real_t *__restrict__ hamiltonian = (real_t*) hamiltonian2;\n\n#define package_id ((PACKAGES_PER_WG * blockIdx.y + threadIdx.y) * (VEC_LENGTH_AUTO * 2 * DIM * DIM))\n#define sigma_id threadIdx.x\n\n#define sigma_real(i, j) (package_id + 2 * VEC_LENGTH_AUTO * (DIM * (i) + (j)) + sigma_id)\n#define sigma_imag(i, j) (package_id + 2 * VEC_LENGTH_AUTO * (DIM * (i) + (j)) + VEC_LENGTH_AUTO + sigma_id)\n\n#define ham_real(i, j) ((i) * DIM + (j))\n#define ham_imag(i, j) (DIM * DIM + (i) * DIM + (j))\n\n  for (int i = 0; i < DIM; ++i) {\n    for (int k = 0; k < DIM; ++k) {\n      real_t ham_real_tmp = hamiltonian[ham_real(i, k)];\n      real_t ham_imag_tmp = hamiltonian[ham_imag(i, k)];\n      real_t sigma_real_tmp = sigma_in[sigma_real(i, k)];\n      real_t sigma_imag_tmp = sigma_in[sigma_imag(i, k)];\n      for (int j = 0; j < DIM; ++j) {\n        sigma_out[sigma_imag(i, j)] -= ham_real_tmp * sigma_in[sigma_real(k, j)];\n        sigma_out[sigma_imag(i, j)] += sigma_real_tmp * hamiltonian[ham_real(k, j)];\n        sigma_out[sigma_imag(i, j)] += ham_imag_tmp * sigma_in[sigma_imag(k, j)];\n        sigma_out[sigma_imag(i, j)] -= sigma_imag_tmp * hamiltonian[ham_imag(k, j)];\n        sigma_out[sigma_real(i, j)] += ham_real_tmp * sigma_in[sigma_imag(k, j)];\n        sigma_out[sigma_real(i, j)] -= sigma_real_tmp * hamiltonian[ham_imag(k, j)];\n        sigma_out[sigma_real(i, j)] += ham_imag_tmp * sigma_in[sigma_real(k, j)];\n        sigma_out[sigma_real(i, j)] -= sigma_imag_tmp * hamiltonian[ham_real(k, j)];\n      }\n    }\n  }\n}",
            "#define T ((int)32)\n\n\n__device__ void push(T const &v) {\n    buf[free_index] = v;\n    free_index += THREADS;\n  }\n\n__global__ \nvoid comm_manual_aosoa (\n    const real_2_t *__restrict__ sigma2_in,\n          real_2_t *__restrict__ sigma2_out, \n    const real_2_t *__restrict__ hamiltonian2,\n    const int dim)\n{\n  real_vec_t *__restrict__ sigma_in = (real_vec_t*) sigma2_in;\n  real_vec_t *__restrict__ sigma_out = (real_vec_t*) sigma2_out;\n  real_t *__restrict__ hamiltonian = (real_t*) hamiltonian2;\n\n  int gid = blockIdx.x * blockDim.x + threadIdx.x;\n\n  // number of package to process == get_global_id(0)\n#define package_id (gid * dim * dim * 2)\n\n#define sigma_real(i, j) (package_id + 2 * (dim * (i) + (j)))\n#define sigma_imag(i, j) (package_id + 2 * (dim * (i) + (j)) + 1)\n\n#define ham_real(i, j) ((i) * dim + (j))\n#define ham_imag(i, j) (dim * dim + (i) * dim + (j))\n\n  // compute commutator: (hamiltonian * sigma_in[sigma_id] - sigma_in[sigma_id] * hamiltonian)\n  for (int i = 0; i < dim; ++i) {\n    for (int j = 0; j < dim; ++j) {\n      real_vec_t tmp_real = v(0.0);\n      real_vec_t tmp_imag = v(0.0);\n      for (int k = 0; k < dim; ++k) {\n        tmp_imag -= hamiltonian[ham_real(i, k)] * sigma_in[sigma_real(k, j)];\n        tmp_imag += sigma_in[sigma_real(i, k)] * hamiltonian[ham_real(k, j)];\n        tmp_imag += hamiltonian[ham_imag(i, k)] * sigma_in[sigma_imag(k, j)];\n        tmp_imag -= sigma_in[sigma_imag(i, k)] * hamiltonian[ham_imag(k, j)];\n        tmp_real += hamiltonian[ham_real(i, k)] * sigma_in[sigma_imag(k, j)];\n        tmp_real -= sigma_in[sigma_real(i, k)] * hamiltonian[ham_imag(k, j)];\n        tmp_real += hamiltonian[ham_imag(i, k)] * sigma_in[sigma_real(k, j)];\n        tmp_real -= sigma_in[sigma_imag(i, k)] * hamiltonian[ham_real(k, j)];\n      }\n      sigma_out[sigma_real(i, j)] += tmp_real;\n      sigma_out[sigma_imag(i, j)] += tmp_imag;\n    }\n  }\n}",
            "#define T ((int)32)\n\n\n__device__ void push(T const &v) {\n    buf[free_index] = v;\n    free_index += THREADS;\n  }\n\n__global__ \nvoid comm_manual_aosoa_constants (\n    const real_2_t *__restrict__ sigma2_in,\n          real_2_t *__restrict__ sigma2_out, \n    const real_2_t *__restrict__ hamiltonian2)\n{\n  real_vec_t *__restrict__ sigma_in = (real_vec_t*) sigma2_in;\n  real_vec_t *__restrict__ sigma_out = (real_vec_t*) sigma2_out;\n  real_t *__restrict__ hamiltonian = (real_t*) hamiltonian2;\n\n  int gid = blockIdx.x * blockDim.x + threadIdx.x;\n  #define package_id (gid * DIM * DIM * 2)\n  \n  #define sigma_real(i, j) (package_id + 2 * (DIM * (i) + (j)))\n  #define sigma_imag(i, j) (package_id + 2 * (DIM * (i) + (j)) + 1)\n  \n  #define ham_real(i, j) ((i) * DIM + (j))\n  #define ham_imag(i, j) (DIM * DIM + (i) * DIM + (j))\n  \n  // compute commutator: (hamiltonian * sigma_in[sigma_id] - sigma_in[sigma_id] * hamiltonian)\n  for (int i = 0; i < DIM; ++i) {\n    for (int j = 0; j < DIM; ++j) {\n      real_vec_t tmp_real = v(0.0);\n      real_vec_t tmp_imag = v(0.0);\n      for (int k = 0; k < DIM; ++k) {\n        tmp_imag -= hamiltonian[ham_real(i, k)] * sigma_in[sigma_real(k, j)];\n        tmp_imag += sigma_in[sigma_real(i, k)] * hamiltonian[ham_real(k, j)];\n        tmp_imag += hamiltonian[ham_imag(i, k)] * sigma_in[sigma_imag(k, j)];\n        tmp_imag -= sigma_in[sigma_imag(i, k)] * hamiltonian[ham_imag(k, j)];\n        tmp_real += hamiltonian[ham_real(i, k)] * sigma_in[sigma_imag(k, j)];\n        tmp_real -= sigma_in[sigma_real(i, k)] * hamiltonian[ham_imag(k, j)];\n        tmp_real += hamiltonian[ham_imag(i, k)] * sigma_in[sigma_real(k, j)];\n        tmp_real -= sigma_in[sigma_imag(i, k)] * hamiltonian[ham_real(k, j)];\n      }\n      sigma_out[sigma_real(i, j)] += tmp_real;\n      sigma_out[sigma_imag(i, j)] += tmp_imag;\n    }\n  }\n}",
            "#define T ((int)32)\n\n\n__device__ void push(T const &v) {\n    buf[free_index] = v;\n    free_index += THREADS;\n  }\n\n__global__ \nvoid comm_manual_aosoa_constants_perm (\n    const real_2_t *__restrict__ sigma2_in,\n          real_2_t *__restrict__ sigma2_out, \n    const real_2_t *__restrict__ hamiltonian2)\n{\n  real_vec_t *__restrict__ sigma_in = (real_vec_t*) sigma2_in;\n  real_vec_t *__restrict__ sigma_out = (real_vec_t*) sigma2_out;\n  real_t *__restrict__ hamiltonian = (real_t*) hamiltonian2;\n\n  int gid = blockIdx.x * blockDim.x + threadIdx.x;\n  #define package_id (gid * DIM * DIM * 2)\n  \n  #define sigma_real(i, j) (package_id + 2 * (DIM * (i) + (j)))\n  #define sigma_imag(i, j) (package_id + 2 * (DIM * (i) + (j)) + 1)\n  \n  #define ham_real(i, j) ((i) * DIM + (j))\n  #define ham_imag(i, j) (DIM * DIM + (i) * DIM + (j))\n  \n  // compute commutator: (hamiltonian * sigma_in[sigma_id] - sigma_in[sigma_id] * hamiltonian)\n  for (int i = 0; i < DIM; ++i) {\n    for (int k = 0; k < DIM; ++k) {\n      real_vec_t ham_real_tmp = v(hamiltonian[ham_real(i, k)]);\n      real_vec_t ham_imag_tmp = v(hamiltonian[ham_imag(i, k)]);\n      real_vec_t sigma_real_tmp = sigma_in[sigma_real(i, k)];\n      real_vec_t sigma_imag_tmp = sigma_in[sigma_imag(i, k)];\n      for (int j = 0; j < DIM; ++j) {\n        #ifdef USE_INITZERO\n        real_vec_t tmp_real = v(0.0);\n        real_vec_t tmp_imag = v(0.0);\n        #else\n        real_vec_t tmp_real = sigma_out[sigma_real(i, j)];\n        real_vec_t tmp_imag = sigma_out[sigma_imag(i, j)];\n        #endif\n        tmp_imag -= ham_real_tmp * sigma_in[sigma_real(k, j)];\n        tmp_imag += sigma_real_tmp * hamiltonian[ham_real(k, j)];\n        tmp_imag += ham_imag_tmp * sigma_in[sigma_imag(k, j)];\n        tmp_imag -= sigma_imag_tmp * hamiltonian[ham_imag(k, j)];\n        tmp_real += ham_real_tmp * sigma_in[sigma_imag(k, j)];\n        tmp_real -= sigma_real_tmp * hamiltonian[ham_imag(k, j)];\n        tmp_real += ham_imag_tmp * sigma_in[sigma_real(k, j)];\n        tmp_real -= sigma_imag_tmp * hamiltonian[ham_real(k, j)];\n        #ifdef USE_INITZERO\n        sigma_out[sigma_real(i, j)] += tmp_real;\n        sigma_out[sigma_imag(i, j)] += tmp_imag;\n        #else\n        sigma_out[sigma_real(i, j)] = tmp_real;\n        sigma_out[sigma_imag(i, j)] = tmp_imag;\n        #endif\n      }\n    }\n  }\n}",
            "#define T ((int)32)\n\n\n__device__ void push(T const &v) {\n    buf[free_index] = v;\n    free_index += THREADS;\n  }\n\n__global__ \nvoid comm_manual_aosoa_constants_perm_prefetch (\n    const real_2_t *__restrict__ sigma2_in,\n          real_2_t *__restrict__ sigma2_out, \n    const real_2_t *__restrict__ hamiltonian2)\n{\n  real_vec_t *__restrict__ sigma_in = (real_vec_t*) sigma2_in;\n  real_vec_t *__restrict__ sigma_out = (real_vec_t*) sigma2_out;\n  real_t *__restrict__ hamiltonian = (real_t*) hamiltonian2;\n\n  int gid = blockIdx.x * blockDim.x + threadIdx.x;\n  #define package_id (gid * DIM * DIM * 2)\n  \n  #define sigma_real(i, j) (package_id + 2 * (DIM * (i) + (j)))\n  #define sigma_imag(i, j) (package_id + 2 * (DIM * (i) + (j)) + 1)\n  \n  #define ham_real(i, j) ((i) * DIM + (j))\n  #define ham_imag(i, j) (DIM * DIM + (i) * DIM + (j))\n\n  // compute commutator: (hamiltonian * sigma_in[sigma_id] - sigma_in[sigma_id] * hamiltonian)\n  for (int i = 0; i < DIM; ++i) {\n    int j = 0;\n    //(sigma_out.get_pointer() + sigma_real(i, j)).prefetch(2 * DIM);\n    for (j = 0; j < DIM; ++j) {\n      real_vec_t tmp_real = v(0.0);\n      real_vec_t tmp_imag = v(0.0);\n      for (int k = 0; k < DIM; ++k) {\n        tmp_imag -= hamiltonian[ham_real(i, k)] * sigma_in[sigma_real(k, j)];\n        tmp_imag += sigma_in[sigma_real(i, k)] * hamiltonian[ham_real(k, j)];\n        tmp_imag += hamiltonian[ham_imag(i, k)] * sigma_in[sigma_imag(k, j)];\n        tmp_imag -= sigma_in[sigma_imag(i, k)] * hamiltonian[ham_imag(k, j)];\n        tmp_real += hamiltonian[ham_real(i, k)] * sigma_in[sigma_imag(k, j)];\n        tmp_real -= sigma_in[sigma_real(i, k)] * hamiltonian[ham_imag(k, j)];\n        tmp_real += hamiltonian[ham_imag(i, k)] * sigma_in[sigma_real(k, j)];\n        tmp_real -= sigma_in[sigma_imag(i, k)] * hamiltonian[ham_real(k, j)];\n      }\n      sigma_out[sigma_real(i, j)] += tmp_real;\n      sigma_out[sigma_imag(i, j)] += tmp_imag;\n    }\n  }\n}",
            "__global__ \nvoid comm_manual_aosoa_direct (\n    const real_2_t *__restrict__ sigma2_in,\n          real_2_t *__restrict__ sigma2_out, \n    const real_2_t *__restrict__ hamiltonian2,\n    const int dim)\n{\n  real_vec_t *__restrict__ sigma_in = (real_vec_t*) sigma2_in;\n  real_vec_t *__restrict__ sigma_out = (real_vec_t*) sigma2_out;\n  real_t *__restrict__ hamiltonian = (real_t*) hamiltonian2;\n\n  int gid = blockIdx.x * blockDim.x + threadIdx.x;\n\n  #define package_id (gid * dim * dim * 2)\n  \n  #define sigma_real(i, j) (package_id + 2 * (dim * (i) + (j)))\n  #define sigma_imag(i, j) (package_id + 2 * (dim * (i) + (j)) + 1)\n  \n  #define ham_real(i, j) ((i) * dim + (j))\n  #define ham_imag(i, j) (dim * dim + (i) * dim + (j))\n  \n  for (int i = 0; i < dim; ++i) {\n    for (int j = 0; j < dim; ++j) {\n      for (int k = 0; k < dim; ++k) {\n        sigma_out[sigma_imag(i, j)] -= hamiltonian[ham_real(i, k)] * sigma_in[sigma_real(k, j)];\n        sigma_out[sigma_imag(i, j)] += sigma_in[sigma_real(i, k)] * hamiltonian[ham_real(k, j)];\n        sigma_out[sigma_imag(i, j)] += hamiltonian[ham_imag(i, k)] * sigma_in[sigma_imag(k, j)];\n        sigma_out[sigma_imag(i, j)] -= sigma_in[sigma_imag(i, k)] * hamiltonian[ham_imag(k, j)];\n        sigma_out[sigma_real(i, j)] += hamiltonian[ham_real(i, k)] * sigma_in[sigma_imag(k, j)];\n        sigma_out[sigma_real(i, j)] -= sigma_in[sigma_real(i, k)] * hamiltonian[ham_imag(k, j)];\n        sigma_out[sigma_real(i, j)] += hamiltonian[ham_imag(i, k)] * sigma_in[sigma_real(k, j)];\n        sigma_out[sigma_real(i, j)] -= sigma_in[sigma_imag(i, k)] * hamiltonian[ham_real(k, j)];\n      }\n    }\n  }\n}",
            "__global__ \nvoid comm_manual_aosoa_constants_direct (\n    const real_2_t *__restrict__ sigma2_in,\n          real_2_t *__restrict__ sigma2_out, \n    const real_2_t *__restrict__ hamiltonian2)\n{\n  real_vec_t *__restrict__ sigma_in = (real_vec_t*) sigma2_in;\n  real_vec_t *__restrict__ sigma_out = (real_vec_t*) sigma2_out;\n  real_t *__restrict__ hamiltonian = (real_t*) hamiltonian2;\n\n  int gid = blockIdx.x * blockDim.x + threadIdx.x;\n  #define package_id (gid * DIM * DIM * 2)\n  \n  #define sigma_real(i, j) (package_id + 2 * (DIM * (i) + (j)))\n  #define sigma_imag(i, j) (package_id + 2 * (DIM * (i) + (j)) + 1)\n  \n  #define ham_real(i, j) ((i) * DIM + (j))\n  #define ham_imag(i, j) (DIM * DIM + (i) * DIM + (j))\n  \n  for (int i = 0; i < DIM; ++i) {\n    for (int j = 0; j < DIM; ++j) {\n      for (int k = 0; k < DIM; ++k) {\n        sigma_out[sigma_imag(i, j)] -= hamiltonian[ham_real(i, k)] * sigma_in[sigma_real(k, j)];\n        sigma_out[sigma_imag(i, j)] += sigma_in[sigma_real(i, k)] * hamiltonian[ham_real(k, j)];\n        sigma_out[sigma_imag(i, j)] += hamiltonian[ham_imag(i, k)] * sigma_in[sigma_imag(k, j)];\n        sigma_out[sigma_imag(i, j)] -= sigma_in[sigma_imag(i, k)] * hamiltonian[ham_imag(k, j)];\n        sigma_out[sigma_real(i, j)] += hamiltonian[ham_real(i, k)] * sigma_in[sigma_imag(k, j)];\n        sigma_out[sigma_real(i, j)] -= sigma_in[sigma_real(i, k)] * hamiltonian[ham_imag(k, j)];\n        sigma_out[sigma_real(i, j)] += hamiltonian[ham_imag(i, k)] * sigma_in[sigma_real(k, j)];\n        sigma_out[sigma_real(i, j)] -= sigma_in[sigma_imag(i, k)] * hamiltonian[ham_real(k, j)];\n      }\n    }\n  }\n}",
            "__global__ \nvoid comm_manual_aosoa_constants_direct_prefetch (\n    const real_2_t *__restrict__ sigma2_in,\n          real_2_t *__restrict__ sigma2_out, \n    const real_2_t *__restrict__ hamiltonian2)\n{\n  real_vec_t *__restrict__ sigma_in = (real_vec_t*) sigma2_in;\n  real_vec_t *__restrict__ sigma_out = (real_vec_t*) sigma2_out;\n  real_t *__restrict__ hamiltonian = (real_t*) hamiltonian2;\n\n  int gid = blockIdx.x * blockDim.x + threadIdx.x;\n  #define package_id (gid * DIM * DIM * 2)\n  #define sigma_real(i, j) (package_id + 2 * (DIM * (i) + (j)))\n  #define sigma_imag(i, j) (package_id + 2 * (DIM * (i) + (j)) + 1)\n  \n  #define ham_real(i, j) ((i) * DIM + (j))\n  #define ham_imag(i, j) (DIM * DIM + (i) * DIM + (j))\n  \n  // compute commutator: (hamiltonian * sigma_in[sigma_id] - sigma_in[sigma_id] * hamiltonian)\n  for (int i = 0; i < DIM; ++i) {\n    // prefetch result memory for the next inner loops \n    int j = 0;\n    //prefetch(&sigma_out[sigma_real(i, j)], 2 * DIM);\n    //(sigma_out.get_pointer() + sigma_real(i, j)).prefetch(2 * DIM);\n    for (j = 0; j < DIM; ++j) {\n      for (int k = 0; k < DIM; ++k)\n      {\n        sigma_out[sigma_imag(i, j)] -= hamiltonian[ham_real(i, k)] * sigma_in[sigma_real(k, j)];\n        sigma_out[sigma_imag(i, j)] += sigma_in[sigma_real(i, k)] * hamiltonian[ham_real(k, j)];\n        sigma_out[sigma_imag(i, j)] += hamiltonian[ham_imag(i, k)] * sigma_in[sigma_imag(k, j)];\n        sigma_out[sigma_imag(i, j)] -= sigma_in[sigma_imag(i, k)] * hamiltonian[ham_imag(k, j)];\n        sigma_out[sigma_real(i, j)] += hamiltonian[ham_real(i, k)] * sigma_in[sigma_imag(k, j)];\n        sigma_out[sigma_real(i, j)] -= sigma_in[sigma_real(i, k)] * hamiltonian[ham_imag(k, j)];\n        sigma_out[sigma_real(i, j)] += hamiltonian[ham_imag(i, k)] * sigma_in[sigma_real(k, j)];\n        sigma_out[sigma_real(i, j)] -= sigma_in[sigma_imag(i, k)] * hamiltonian[ham_real(k, j)];\n      }\n    }\n  }\n}",
            "#define T ((int)32)\n\n\n__device__ void push(T const &v) {\n    buf[free_index] = v;\n    free_index += THREADS;\n  }\n\n__global__ \nvoid comm_manual_aosoa_constants_direct_perm (\n    const real_2_t *__restrict__ sigma2_in,\n          real_2_t *__restrict__ sigma2_out, \n    const real_2_t *__restrict__ hamiltonian2)\n{\n  real_vec_t *__restrict__ sigma_in = (real_vec_t*) sigma2_in;\n  real_vec_t *__restrict__ sigma_out = (real_vec_t*) sigma2_out;\n  real_t *__restrict__ hamiltonian = (real_t*) hamiltonian2;\n\n  int gid = blockIdx.x * blockDim.x + threadIdx.x;\n  #define package_id (gid * DIM * DIM * 2)\n  \n  #define sigma_real(i, j) (package_id + 2 * (DIM * (i) + (j)))\n  #define sigma_imag(i, j) (package_id + 2 * (DIM * (i) + (j)) + 1)\n  \n  #define ham_real(i, j) ((i) * DIM + (j))\n  #define ham_imag(i, j) (DIM * DIM + (i) * DIM + (j))\n  \n  // compute commutator: (hamiltonian * sigma_in[sigma_id] - sigma_in[sigma_id] * hamiltonian)\n  for (int i = 0; i < DIM; ++i) {\n    for (int k = 0; k < DIM; ++k) {\n      real_vec_t ham_real_tmp = v(hamiltonian[ham_real(i, k)]);\n      real_vec_t ham_imag_tmp = v(hamiltonian[ham_imag(i, k)]);\n      real_vec_t sigma_real_tmp = sigma_in[sigma_real(i, k)];\n      real_vec_t sigma_imag_tmp = sigma_in[sigma_imag(i, k)];\n      for (int j = 0; j < DIM; ++j) {\n        sigma_out[sigma_imag(i, j)] -= ham_real_tmp * sigma_in[sigma_real(k, j)];\n        sigma_out[sigma_imag(i, j)] += sigma_real_tmp * hamiltonian[ham_real(k, j)];\n        sigma_out[sigma_imag(i, j)] += ham_imag_tmp * sigma_in[sigma_imag(k, j)];\n        sigma_out[sigma_imag(i, j)] -= sigma_imag_tmp * hamiltonian[ham_imag(k, j)];\n        sigma_out[sigma_real(i, j)] += ham_real_tmp * sigma_in[sigma_imag(k, j)];\n        sigma_out[sigma_real(i, j)] -= sigma_real_tmp * hamiltonian[ham_imag(k, j)];\n        sigma_out[sigma_real(i, j)] += ham_imag_tmp * sigma_in[sigma_real(k, j)];\n        sigma_out[sigma_real(i, j)] -= sigma_imag_tmp * hamiltonian[ham_real(k, j)];\n      }\n    }\n  }\n}",
            "inline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\ninline  __host__ __device__ float4 fminf(float4 a, float4 b)\n{\n    return make_float4(fminf(a.x,b.x), fminf(a.y,b.y), fminf(a.z,b.z), fminf(a.w,b.w));\n}\n\n__device__ __forceinline__ float MIN<float>(float in, float in2)\n{\n    return fminf(in, in2);\n}\n\n__global__ \nvoid final_gpu_kernel (\n    const real_2_t *__restrict__ sigma_in,\n          real_2_t *__restrict__ sigma_out, \n    const real_2_t *__restrict__ hamiltonian,\n    const int num)\n{\n  #define id_2d_to_1d(i,j) ((i) * DIM + (j))\n  #define sigma_id(i,j,m) ((m) * DIM * DIM + ((i) * DIM + (j)))\n  #define MIN(X,Y) ((X) < (Y) ? (X) : (Y))\n  // Local memory: shared between all work items in the same work group\n  // 2-way shared memory bank conflicts will occur for real_t = double\n  // real parts and imaginary parts are stored separately to avoid 4-way bank conflicts in case of real_2_t = double2\n  // Input sigma matrix: real part (2 matrices are processed at once)\n  // Input sigma matrix: imag part (2 matrices are processed at once)\n  __shared__ real_t ham_local_real[DIM*DIM];\n  __shared__ real_t ham_local_imag[DIM*DIM];\n  __shared__ real_t sigma_local_real[2][NUM_SUB_GROUPS][DIM*DIM];\n  __shared__ real_t sigma_local_imag[2][NUM_SUB_GROUPS][DIM*DIM];\n\n  // Determine matrix index (i,j) this work item is responsible for\n  int ij = threadIdx.x;\n  int i = ij / DIM; // Matrix index 'i' to be processed by this work item in any of 'start -> stop' matrices\n  int j = ij % DIM; // Matrix index 'j' to be processed by this work item in any of 'start -> stop' matrices\n\n  // Determine working set : Each work item participates in processing CHUNK_SIZE matrices : 'start -> stop'\n  int sub_group_id = threadIdx.y; // Local matrix ID within work group\n  int start = blockIdx.x * NUM_SUB_GROUPS * CHUNK_SIZE + sub_group_id * CHUNK_SIZE; // Global matrix ID : start\n  int stop = MIN(num, start + CHUNK_SIZE); // Global matrix ID : stop\n\n  // Local variables\n  real_2_t snew1_ij, snew2_ij;\n  real_2_t s1, s2;\n\n  // Load Hamiltonian into local memory: only the first sub-group participates\n  if (ij < (DIM * DIM) && sub_group_id == 0)\n  {\n    const real_2_t h = hamiltonian[ij];\n    ham_local_real[ij] = h.x;\n    ham_local_imag[ij] = h.y;\n  }\n\n  // Process all CHUNK_SIZE matrices: two matrices are processed at once (therefore increment 2)\n  for (int m = start; m < stop; m += 2)\n  {\n    __syncthreads();\n    if (ij < (DIM * DIM)) \n    { // Load input sigma matrix into local memory: only threads with valid IDs participate\n      s1 = sigma_in[sigma_id(i, j, m)]; // Real and imaginary part of matrix 'm', element (i,j)\n      sigma_local_real[0][sub_group_id][ij] = s1.x;\n      sigma_local_imag[0][sub_group_id][ij] = s1.y;\n\n      s2 = sigma_in[sigma_id(i, j, m + 1)]; // Real and imaginary part of matrix 'm+1', element (i,j)\n      sigma_local_real[1][sub_group_id][ij] = s2.x;\n      sigma_local_imag[1][sub_group_id][ij] = s2.y;\n\n      s1 = sigma_out[sigma_id(i, j, m)]; // Prefetch real and imaginary part of output sigma matrix 'm', element (i,j)\n      snew1_ij.x = s1.x;\n      snew2_ij.x = s1.y;\n\n      s2 = sigma_out[sigma_id(i, j, m + 1)]; // Prefetch real and imaginary part of output sigma matrix 'm+1', element (i,j)\n      snew1_ij.y = s2.x;\n      snew2_ij.y = s2.y;\n    }\n    __syncthreads();\n\n    if (ij < (DIM * DIM))\n    {\n      // Compute commutator: [H,sigma] = H * sigma - sigma * H <=> [H,sigma]_ij = \\sum_k ( H_ik * sigma_kj - sigma_ik * H_kj )\n      for (int k = 0; k < DIM; ++k)\n      {\n        const int ik = id_2d_to_1d(i, k);\n        const int kj = id_2d_to_1d(k, j);\n\n        // Reassemble real_2_t elements from local memory: 'vector processing' gives better performance here\n        s1 = {sigma_local_real[0][sub_group_id][kj], sigma_local_real[1][sub_group_id][kj]};\n        s2 = {sigma_local_imag[0][sub_group_id][kj], sigma_local_imag[1][sub_group_id][kj]};\n        snew1_ij += ham_local_real[ik] * s2;\n        snew1_ij += ham_local_imag[ik] * s1;\n        snew2_ij -= ham_local_real[ik] * s1;\n        snew2_ij += ham_local_imag[ik] * s2;\n\n        // Reassemble real_2_t elements from local memory: 'vector processing' gives better performance here\n        s1 = {sigma_local_real[0][sub_group_id][ik], sigma_local_real[1][sub_group_id][ik]};\n        s2 = {sigma_local_imag[0][sub_group_id][ik], sigma_local_imag[1][sub_group_id][ik]};\n        snew1_ij -= ham_local_real[kj] * s2;\n        snew1_ij += ham_local_imag[kj] * s1;\n        snew2_ij += ham_local_real[kj] * s1;\n        snew2_ij -= ham_local_imag[kj] * s2;\n      }\n\n      // Write output sigma matrices 'm' and 'm+1', element (i,j)\n      sigma_out[sigma_id(i, j, m)] = {snew1_ij.x, snew2_ij.x};\n      sigma_out[sigma_id(i, j, m + 1)] = {snew1_ij.y, snew2_ij.y};\n    }\n  }\n}"
        ]
    },
    "mcmd-cuda": {
        "/Users/gbolet/hecbench-roofline/src/mcmd-cuda/force_kernel.cu": [
            "__global__\nvoid calculateForceKernel(\n  d_atom *__restrict__ atom_list, \n  const int N,\n  const double cutoffD,\n  const double *__restrict__ basis,\n  const double *__restrict__ reciprocal_basis,\n  const int pformD,\n  const double ewald_alpha,\n  const int kmax,\n  const int kspace,\n  const double polar_damp)\n{\n  // only run for real atoms (no ghost threads)\n  int i = threadIdx.x + blockDim.x * blockIdx.x;\n  if(i < N) {\n    const d_atom anchoratom = atom_list[i];\n    const int pform = pformD;\n    const double damp = polar_damp;\n    const double alpha = ewald_alpha;\n    const double cutoff = cutoffD;\n    double rimg, rsq;\n    const double sqrtPI=sqrt(M_PI);\n    double d[3], di[3], img[3], dimg[3],r,r2,ri,ri2;\n    int q,j,n;\n    double sig,eps,r6,s6,u[3]= {0,0,0};\n    double af[3] = {0,0,0}; // accumulated forces for anchoratom\n    double holder,chargeprod; // for ES force\n\n    // if LJ\n    if (pform == 0 || pform == 1 || pform == 2) {\n      for (j=i+1; j<N; j++) {\n\n        if (anchoratom.molid == atom_list[j].molid) continue; // skip same molecule\n        if (anchoratom.frozen && atom_list[j].frozen) continue; // skip frozens\n\n        // LB mixing\n        sig = anchoratom.sig;\n        if (sig != atom_list[j].sig) sig = 0.5*(sig+atom_list[j].sig);\n        eps = anchoratom.eps;\n        if (eps != atom_list[j].eps) eps = sqrt(eps * atom_list[j].eps);\n        if (sig == 0 || eps == 0) continue;\n\n        // get R (nearest image)\n        for (n=0; n<3; n++) d[n] = anchoratom.pos[n] - atom_list[j].pos[n];\n        for (n=0; n<3; n++) {\n          img[n]=0;\n          for (q=0; q<3; q++) {\n            img[n] += reciprocal_basis[n*3+q]*d[q];\n          }\n          img[n] = rint(img[n]);\n        }\n        for (n=0; n<3; n++) {\n          di[n] = 0;\n          for (q=0; q<3; q++) {\n            di[n] += basis[n*3+q]*img[q];\n          }\n          di[n] = d[n] - di[n];\n        }\n\n        r2=0;\n        ri2=0;\n        for (n=0; n<3; n++) {\n          r2 += d[n]*d[n];\n          ri2 += di[n]*di[n];\n        }\n        r = sqrt(r2);\n        ri = sqrt(ri2);\n        if (ri != ri) {\n          rimg=r;\n          rsq=r2;\n          for (n=0; n<3; n++) dimg[n] = d[n];\n        } else {\n          rimg=ri;\n          rsq=ri2;\n          for (n=0; n<3; n++) dimg[n] = di[n];\n        }\n        // distance is now rimg\n\n        if (rimg <= cutoff) {\n          r6 = rsq*rsq*rsq;\n          s6 = sig*sig;\n          s6 *= s6 * s6;\n\n          for (n=0; n<3; n++) {\n            holder = 24.0*dimg[n]*eps*(2*(s6*s6)/(r6*r6*rsq) - s6/(r6*rsq));\n            atomicAdd(&(atom_list[j].f[n]), -holder);\n            af[n] += holder;\n          }\n        }\n      } // end pair j\n\n      // finally add the accumulated forces (stored on register) to the anchor atom\n      for (n=0; n<3; n++)\n        atomicAdd(&(atom_list[i].f[n]), af[n]);\n\n    } // end if LJ\n\n    // ==============================================================================\n    // Now handle electrostatics\n    // ==============================================================================\n    if (pform == 1 || pform == 2) {\n      for (n=0; n<3; n++) af[n]=0; // reset register-stored force for anchoratom.\n      double invV;\n      int l[3], p, q;\n      double k[3], k_sq, fourPI = 4.0*M_PI;\n      invV =  basis[0] * (basis[4]*basis[8] - basis[7]*basis[5] );\n      invV += basis[3] * (basis[7]*basis[2] - basis[1]*basis[8] );\n      invV += basis[6] * (basis[1]*basis[5] - basis[5]*basis[2] );\n      invV = 1.0/invV;\n\n      for (j=0; j<N; j++) {\n        if (anchoratom.frozen && atom_list[j].frozen) continue; // don't do frozen pairs\n        if (anchoratom.charge == 0 || atom_list[j].charge == 0) continue; // skip 0-force\n        if (i==j) continue; // don't do atom with itself\n\n        // get R (nearest image)\n        for (n=0; n<3; n++) d[n] = anchoratom.pos[n] - atom_list[j].pos[n];\n        for (n=0; n<3; n++) {\n          img[n]=0;\n          for (q=0; q<3; q++) {\n            img[n] += reciprocal_basis[n*3+q]*d[q];\n          }\n          img[n] = rint(img[n]);\n        }\n        for (n=0; n<3; n++) {\n          di[n] = 0;\n          for (q=0; q<3; q++) {\n            di[n] += basis[n*3+q]*img[q];\n          }\n        }\n        for (n=0; n<3; n++) di[n] = d[n] - di[n];\n        r2=0;\n        ri2=0;\n        for (n=0; n<3; n++) {\n          r2 += d[n]*d[n];\n          ri2 += di[n]*di[n];\n        }\n        r = sqrt(r2);\n        ri = sqrt(ri2);\n        if (ri != ri) {\n          rimg=r;\n          rsq=r2;\n          for (n=0; n<3; n++) dimg[n] = d[n];\n        } else {\n          rimg=ri;\n          rsq=ri2;\n          for (n=0; n<3; n++) dimg[n] = di[n];\n        }\n\n        // real-space\n        if (rimg <= cutoff && (anchoratom.molid < atom_list[j].molid)) { // non-duplicated pairs, not intramolecular, not beyond cutoff\n          chargeprod = anchoratom.charge * atom_list[j].charge;\n          for (n=0; n<3; n++) u[n] = dimg[n]/rimg;\n          for (n=0; n<3; n++) {\n            holder = -((-2.0*chargeprod*alpha*exp(-alpha*alpha*rsq))/(sqrtPI*rimg) - (chargeprod*erfc(alpha*rimg)/rsq))*u[n];\n            af[n] += holder;\n            atomicAdd(&(atom_list[j].f[n]), -holder);\n          }\n        }\n        // k-space\n        if (kspace && (anchoratom.molid < atom_list[j].molid)) {\n          chargeprod = anchoratom.charge * atom_list[j].charge;\n\n          for (n=0; n<3; n++) {\n            for (l[0] = 0; l[0] <= kmax; l[0]++) {\n              for (l[1] = (!l[0] ? 0 : -kmax); l[1] <= kmax; l[1]++) {\n                for (l[2] = ((!l[0] && !l[1]) ? 1 : -kmax); l[2] <= kmax; l[2]++) {\n                  // skip if norm is out of sphere\n                  if (l[0]*l[0] + l[1]*l[1] + l[2]*l[2] > kmax*kmax) continue;\n                  /* get reciprocal lattice vectors */\n                  for (p=0; p<3; p++) {\n                    for (q=0, k[p] = 0; q < 3; q++) {\n                      k[p] += 2.0*M_PI*reciprocal_basis[3*q+p] * l[q];\n                    }\n                  }\n                  k_sq = k[0]*k[0] + k[1]*k[1] + k[2]*k[2];\n\n                  holder = chargeprod * invV * fourPI * k[n] *\n                    exp(-k_sq/(4*alpha*alpha))*\n                    sin(k[0]*dimg[0] + k[1]*dimg[1] + k[2]*dimg[2])/k_sq * 2; // times 2 b/c half-Ewald sphere\n\n                  af[n] += holder;\n                  atomicAdd(&(atom_list[j].f[n]), -holder);\n\n                } // end for l[2], n\n              } // end for l[1], m\n            } // end for l[0], l\n          } // end 3d\n        }\n\n      } // end pair loop j\n\n      // finally add ES contribution to anchor-atom\n      for (n=0; n<3; n++) atomicAdd(&(atom_list[i].f[n]), af[n]);\n    } // end ES component\n\n    // ============================================================\n    // Polarization\n    // ============================================================\n    if (pform == 2) {\n      double common_factor, r, rinv, r2, r2inv, r3, r3inv, r5inv, r7inv;\n      double x2,y2,z2,x,y,z;\n      double udotu, ujdotr, uidotr;\n      const double cc2inv = 1.0/(cutoff*cutoff);\n      double t1,t2,t3,p1,p2,p3,p4,p5;\n      const double u_i[3] = {anchoratom.u[0], anchoratom.u[1], anchoratom.u[2]};\n      double u_j[3];\n      // loop all pair atoms\n      for (int j=i+1; j<N; j++) {\n        for (n=0; n<3; n++) af[n] = 0; // reset local force for this pair.\n        if (anchoratom.molid == atom_list[j].molid) continue; // no same-molecule\n        // get R (nearest image)\n        \n        for (n=0; n<3; n++) d[n] = anchoratom.pos[n] - atom_list[j].pos[n];\n        for (n=0; n<3; n++) {\n          img[n]=0;\n          for (q=0; q<3; q++) {\n            img[n] += reciprocal_basis[n*3+q]*d[q];\n          }\n          img[n] = rint(img[n]);\n        }\n        for (n=0; n<3; n++) {\n          di[n] = 0;\n          for (q=0; q<3; q++) {\n            di[n] += basis[n*3+q]*img[q];\n          }\n        }\n        for (n=0; n<3; n++) di[n] = d[n] - di[n];\n        r2=0;\n        ri2=0;\n        for (n=0; n<3; n++) {\n          r2 += d[n]*d[n];\n          ri2 += di[n]*di[n];\n        }\n        r = sqrt(r2);\n        ri = sqrt(ri2);\n        if (ri != ri) {\n          rimg=r;\n          rsq=r2;\n          for (n=0; n<3; n++) dimg[n] = d[n];\n        } else {\n          rimg=ri;\n          rsq=ri2;\n          for (n=0; n<3; n++) dimg[n] = di[n];\n        }\n        // got pair displacements\n\n        if (rimg > cutoff) continue; // skip outside cutoff\n        r = rimg;\n        x = dimg[0];\n        y = dimg[1];\n        z = dimg[2];\n        x2 = x*x;\n        y2 = y*y;\n        z2 = z*z;\n        r2 = r*r;\n        r3 = r2*r;\n        rinv = 1./r;\n        r2inv = rinv*rinv;\n        r3inv = r2inv*rinv;\n        for (n=0; n<3; n++) u_j[n] = atom_list[j].u[n];\n\n        // (1) u_i -- q_j\n        if (atom_list[j].charge != 0 && anchoratom.polar != 0) {\n          common_factor = atom_list[j].charge * r3inv;\n\n          af[0] += common_factor*((u_i[0]*(r2inv*(-2*x2 + y2 + z2) - cc2inv*(y2 + z2))) + (u_i[1]*(r2inv*(-3*x*y) + cc2inv*x*y)) + (u_i[2]*(r2inv*(-3*x*z) + cc2inv*x*z)));\n\n          af[1] += common_factor*(u_i[0]*(r2inv*(-3*x*y) + cc2inv*x*y) + u_i[1]*(r2inv*(-2*y2 + x2 + z2) - cc2inv*(x2 + z2)) + u_i[2]*(r2inv*(-3*y*z) + cc2inv*y*z));\n\n          af[2] += common_factor*(u_i[0]*(r2inv*(-3*x*z) + cc2inv*x*z) + u_i[1]*(r2inv*(-3*y*z) + cc2inv*y*z) + u_i[2]*(r2inv*(-2*z2 + x2 + y2) - cc2inv*(x2 + y2)));\n\n        }\n\n        // (2) u_j -- q_i\n        if (anchoratom.charge != 0 && atom_list[j].polar != 0) {\n          common_factor = anchoratom.charge * r3inv;\n\n          af[0] -= common_factor*((u_j[0]*(r2inv*(-2*x2 + y2 + z2) - cc2inv*(y2 + z2))) + (u_j[1]*(r2inv*(-3*x*y) + cc2inv*x*y)) + (u_j[2]*(r2inv*(-3*x*z) + cc2inv*x*z)));\n\n          af[1] -= common_factor*(u_j[0]*(r2inv*(-3*x*y) + cc2inv*x*y) + u_j[1]*(r2inv*(-2*y2 + x2 + z2) - cc2inv*(x2 + z2)) + u_j[2]*(r2inv*(-3*y*z) + cc2inv*y*z));\n\n          af[2] -= common_factor*(u_j[0]*(r2inv*(-3*x*z) + cc2inv*x*z) + u_j[1]*(r2inv*(-3*y*z) + cc2inv*y*z) + u_j[2]*(r2inv*(-2*z2 + x2 + y2) - cc2inv*(x2 + y2)));\n        }\n\n        // (3) u_i -- u_j\n        if (anchoratom.polar != 0 && atom_list[j].polar != 0) {\n          r5inv = r2inv*r3inv;\n          r7inv = r5inv*r2inv;\n          udotu = u_i[0]*u_j[0] + u_i[1]*u_j[1] + u_i[2]*u_j[2];\n          uidotr = u_i[0]*dimg[0] + u_i[1]*dimg[1] + u_i[2]*dimg[2];\n          ujdotr = u_j[0]*dimg[0] + u_j[1]*dimg[1] + u_j[2]*dimg[2];\n\n          t1 = exp(-damp*r);\n          t2 = 1. + damp*r + 0.5*damp*damp*r2;\n          t3 = t2 + damp*damp*damp*r3/6.;\n          p1 = 3*r5inv*udotu*(1. - t1*t2) - r7inv*15.*uidotr*ujdotr*(1. - t1*t3);\n          p2 = 3*r5inv*ujdotr*(1. - t1*t3);\n          p3 = 3*r5inv*uidotr*(1. - t1*t3);\n          p4 = -udotu*r3inv*(-t1*(damp*rinv + damp*damp) + rinv*t1*damp*t2);\n          p5 = 3*r5inv*uidotr*ujdotr*(-t1*(rinv*damp + damp*damp + 0.5*r*damp*damp*damp) + rinv*t1*damp*t3);\n\n          af[0] += p1*x + p2*u_i[0] + p3*u_j[0] + p4*x + p5*x;\n          af[1] += p1*y + p2*u_i[1] + p3*u_j[1] + p4*y + p5*y;\n          af[2] += p1*z + p2*u_i[2] + p3*u_j[2] + p4*z + p5*z;\n        }\n\n        // apply Newton for pair.\n        for (n=0; n<3; n++) {\n          atomicAdd(&(atom_list[i].f[n]), af[n]);\n          atomicAdd(&(atom_list[j].f[n]), -af[n]);\n        }\n      } // end pair loop with atoms j\n    } // end polarization forces\n  } // end if i<n (all threads)\n}"
        ]
    },
    "wedford-cuda": {
        "/Users/gbolet/hecbench-roofline/src/wedford-cuda/main.cu": [
            "#define C 12345\n\n\n#define T ((int)32)\n\n\ninline __host__ __device__ uint4 make_uint4(int4 a)\n{\n    return make_uint4(uint(a.x), uint(a.y), uint(a.z), uint(a.w));\n}\n\ninline __host__ __device__ uint4 max(uint4 a, uint4 b)\n{\n    return make_uint4(max(a.x,b.x), max(a.y,b.y), max(a.z,b.z), max(a.w,b.w));\n}\n\n__device__ __forceinline__\nvoid welford_merge_element(C& count,\n                           T& mean,\n                           T& m2n,\n                           const C& num_new,\n                           const T& mean_new,\n                           const T& m2n_new) {\n  T factor = T(1.0) / max(1, (count + num_new));\n  T delta0 = mean - mean_new;\n  mean = (mean_new * num_new + mean * count) * factor;\n  m2n += m2n_new + delta0 * delta0 * num_new * count * factor;\n  count += num_new;\n}\n\n__device__ __forceinline__\nvoid warp_reduce_mean_m2n(T &mean, T &m2n, int &num)\n{\n  #pragma unroll\n  for(int i = WARP_SIZE/2; i > 0; i >>= 1) {\n    auto num_new = __shfl_down_sync(0xffffffff, num, i);\n    auto mean_new = __shfl_down_sync(0xffffffff, mean, i);\n    auto m2n_new = __shfl_down_sync(0xffffffff, m2n, i);\n    welford_merge_element(num, mean, m2n, num_new, mean_new, m2n_new);\n  }\n}\n\n__device__ void welford_reduce_mean_m2n(\n      T* __restrict__ x,\n      int* __restrict__ count,\n      T &mean,\n      T &m2n,\n      int &num,\n      int block_size,\n      int thread_id)\n{\n  int lane = thread_id % WARP_SIZE;\n  int wid = thread_id / WARP_SIZE;\n\n  if (block_size > 32) {\n    warp_reduce_mean_m2n(mean, m2n, num);\n    if (lane == 0) {\n      x[wid*2] = mean;\n      x[wid*2+1] = m2n;\n      count[wid] = num;\n    }\n    __syncthreads();\n\n    if (wid == 0) {\n      mean = (thread_id < block_size / WARP_SIZE)? x[lane*2] : T(0);\n      m2n = (thread_id < block_size / WARP_SIZE)? x[lane*2+1] : T(0);\n      num = (thread_id < block_size / WARP_SIZE)? count[lane] : int(0);\n    }\n  }\n\n  if (wid==0) warp_reduce_mean_m2n(mean, m2n, num);\n}\n\n__global__ void welford_kernel(\n      const scalar_t* __restrict__ input,\n      outscalar_t* __restrict__ out_mean,\n      outscalar_t* __restrict__ out_var_biased,\n      const int bs,\n      const int fs,\n      const int ss)\n{\n  int block_size = blockDim.x * blockDim.y;\n  int count = 0;\n  accscalar_t x_mean = accscalar_t(0);\n  accscalar_t m_2_n = accscalar_t(0);\n\n  int thread_id = threadIdx.y*blockDim.x + threadIdx.x;\n\n  for (int batch_id = threadIdx.y; batch_id < bs; batch_id += blockDim.y) {\n    int input_base = blockIdx.x*ss + batch_id*ss*fs;\n    for (int offset = threadIdx.x; offset < ss ; offset += blockDim.x) {\n      count++;\n      auto x_n = static_cast<accscalar_t>(input[offset+input_base]);\n      // sequential welford\n      auto d = x_n - x_mean;\n      x_mean += d / count;\n      m_2_n += d * (x_n - x_mean);\n    }\n  }\n\n  static __shared__ int s_mem[160];\n  accscalar_t* s_mem_ac = (accscalar_t*) &s_mem[32];\n\n  welford_reduce_mean_m2n<accscalar_t>(s_mem_ac, s_mem, x_mean, m_2_n, count, block_size, thread_id);\n\n  if (thread_id == 0) {\n    out_mean[blockIdx.x] = static_cast<outscalar_t>(x_mean);\n    out_var_biased[blockIdx.x] = static_cast<outscalar_t>(m_2_n/count);\n  }\n}"
        ]
    },
    "medianfilter-cuda": {
        "/Users/gbolet/hecbench-roofline/src/medianfilter-cuda/MedianFilter.cu": [
            "__global__ void ckMedian(\n    const uchar4* uc4Source,\n    unsigned int* uiDest,\n    const int iLocalPixPitch,\n    const int iImageWidth,\n    const int iDevImageHeight)\n{\n  // Get parent image x and y pixel coordinates from global ID, and compute offset into parent GMEM data\n  int iLocalIdX = threadIdx.x;\n  int iLocalIdY = threadIdx.y;\n  int iGroupIdX = blockIdx.x; \n  int iBlockX = blockDim.x;\n  int iBlockY = blockDim.y;\n  int iImagePosX = blockIdx.x * iBlockX + iLocalIdX; \n  int iDevYPrime = blockIdx.y * iBlockY + iLocalIdY - 1;  // Shift offset up 1 radius (1 row) for reads\n  int iImageX = gridDim.x * blockDim.x; \n\n  extern __shared__ uchar4 uc4LocalData[];\n\n  int iDevGMEMOffset = __mul24(iDevYPrime, iImageX) + iImagePosX; \n\n  // Compute initial offset of current pixel within work group LMEM block\n  int iLocalPixOffset = __mul24(iLocalIdY, iLocalPixPitch) + iLocalIdX + 1;\n\n  // Main read of GMEM data into LMEM\n  if((iDevYPrime > -1) && (iDevYPrime < iDevImageHeight) && (iImagePosX < iImageWidth))\n  {\n    uc4LocalData[iLocalPixOffset] = uc4Source[iDevGMEMOffset];\n  }\n  else \n  {\n    uc4LocalData[iLocalPixOffset] = make_uchar4(0, 0, 0, 0); \n  }\n\n  // Work items with y ID < 2 read bottom 2 rows of LMEM \n  if (iLocalIdY < 2)\n  {\n    // Increase local offset by 1 workgroup LMEM block height\n    // to read in top rows from the next block region down\n    iLocalPixOffset += __mul24(iBlockY, iLocalPixPitch);\n\n    // If source offset is within the image boundaries\n    if (((iDevYPrime + iBlockY) < iDevImageHeight) && (iImagePosX < iImageWidth))\n    {\n      // Read in top rows from the next block region down\n      uc4LocalData[iLocalPixOffset] = uc4Source[iDevGMEMOffset + \n                                      __mul24(iBlockY, iImageX)];\n    }\n    else \n    {\n      uc4LocalData[iLocalPixOffset] = make_uchar4(0, 0, 0, 0); \n    }\n  }\n\n  // Work items with x ID at right workgroup edge will read Left apron pixel\n  if (iLocalIdX == (iBlockX - 1))\n  {\n    // set local offset to read data from the next region over\n    iLocalPixOffset = __mul24(iLocalIdY, iLocalPixPitch);\n\n    // If source offset is within the image boundaries and not at the leftmost workgroup\n    if ((iDevYPrime > -1) && (iDevYPrime < iDevImageHeight) && (iGroupIdX > 0))\n    {\n      // Read data into the LMEM apron from the GMEM at the left edge of the next block region over\n      uc4LocalData[iLocalPixOffset] = uc4Source[__mul24(iDevYPrime, \n      iImageX) + __mul24(iGroupIdX, iBlockX) - 1];\n    }\n    else \n    {\n      uc4LocalData[iLocalPixOffset] = make_uchar4(0, 0, 0, 0); \n    }\n\n    // If in the bottom 2 rows of workgroup block \n    if (iLocalIdY < 2)\n    {\n      // Increase local offset by 1 workgroup LMEM block height\n      // to read in top rows from the next block region down\n      iLocalPixOffset += __mul24(iBlockY, iLocalPixPitch);\n\n      // If source offset in the next block down isn't off the image and not at the leftmost workgroup\n      if (((iDevYPrime + iBlockY) < iDevImageHeight) && (iGroupIdX > 0))\n      {\n        // read in from GMEM (reaching down 1 workgroup LMEM block height and left 1 pixel)\n        uc4LocalData[iLocalPixOffset] = uc4Source[__mul24((iDevYPrime + \n        iBlockY), iImageX) + __mul24(iGroupIdX, iBlockX) - 1];\n      }\n      else \n      {\n        uc4LocalData[iLocalPixOffset] = make_uchar4(0, 0, 0, 0); \n      }\n    }\n  } \n  else if (iLocalIdX == 0) // Work items with x ID at left workgroup edge will read right apron pixel\n  {\n    // set local offset \n    iLocalPixOffset = __mul24((iLocalIdY + 1), iLocalPixPitch) - 1;\n\n    if ((iDevYPrime > -1) && (iDevYPrime < iDevImageHeight) && \n        (__mul24((iGroupIdX + 1), iBlockX) < iImageWidth))\n    {\n      // read in from GMEM (reaching left 1 pixel) if source offset is within image boundaries\n      uc4LocalData[iLocalPixOffset] = uc4Source[__mul24(iDevYPrime, \n      iImageX) + __mul24((iGroupIdX + 1), iBlockX)];\n    }\n    else \n    {\n      uc4LocalData[iLocalPixOffset] = make_uchar4(0, 0, 0, 0); \n    }\n\n    // Read bottom 2 rows of workgroup LMEM block\n    if (iLocalIdY < 2)\n    {\n      // increase local offset by 1 workgroup LMEM block height\n      iLocalPixOffset += (__mul24(iBlockY, iLocalPixPitch));\n\n      if (((iDevYPrime + iBlockY) < iDevImageHeight) && \n          (__mul24((iGroupIdX + 1), iBlockX) < iImageWidth) )\n      {\n        // read in from GMEM (reaching down 1 workgroup LMEM block height and left 1 pixel) if source offset is within image boundaries\n        uc4LocalData[iLocalPixOffset] = uc4Source[__mul24((iDevYPrime + \n        iBlockY), iImageX) + __mul24((iGroupIdX + 1), iBlockX)];\n      }\n      else \n      {\n        uc4LocalData[iLocalPixOffset] = make_uchar4(0, 0, 0, 0); \n      }\n    }\n  }\n\n  // Synchronize the read into LMEM\n  __syncthreads();\n\n  // Compute \n  // reset accumulators  \n  float fMedianEstimate[3] = {128.0f, 128.0f, 128.0f};\n  float fMinBound[3] = {0.0f, 0.0f, 0.0f};\n  float fMaxBound[3] = {255.0f, 255.0f, 255.0f};\n\n  // now find the median using a binary search - Divide and Conquer 256 gv levels for 8 bit plane\n  for(int iSearch = 0; iSearch < 8; iSearch++)  // for 8 bit data, use 0..8.  For 16 bit data, 0..16. More iterations for more bits.\n  {\n    unsigned int uiHighCount [3] = {0, 0, 0};\n\n    // set local offset and kernel offset\n    iLocalPixOffset = __mul24(iLocalIdY, iLocalPixPitch) + iLocalIdX;\n\n    // Row1 Left Pix (RGB)\n    uiHighCount[0] += (fMedianEstimate[0] < uc4LocalData[iLocalPixOffset].x);          \n    uiHighCount[1] += (fMedianEstimate[1] < uc4LocalData[iLocalPixOffset].y);          \n    uiHighCount[2] += (fMedianEstimate[2] < uc4LocalData[iLocalPixOffset++].z);          \n\n    // Row1 Middle Pix (RGB)\n    uiHighCount[0] += (fMedianEstimate[0] < uc4LocalData[iLocalPixOffset].x);          \n    uiHighCount[1] += (fMedianEstimate[1] < uc4LocalData[iLocalPixOffset].y);          \n    uiHighCount[2] += (fMedianEstimate[2] < uc4LocalData[iLocalPixOffset++].z);          \n\n    // Row1 Right Pix (RGB)\n    uiHighCount[0] += (fMedianEstimate[0] < uc4LocalData[iLocalPixOffset].x);          \n    uiHighCount[1] += (fMedianEstimate[1] < uc4LocalData[iLocalPixOffset].y);          \n    uiHighCount[2] += (fMedianEstimate[2] < uc4LocalData[iLocalPixOffset].z);          \n\n    // set the offset into SMEM for next row\n    iLocalPixOffset += (iLocalPixPitch - 2);  \n\n    // Row2 Left Pix (RGB)\n    uiHighCount[0] += (fMedianEstimate[0] < uc4LocalData[iLocalPixOffset].x);          \n    uiHighCount[1] += (fMedianEstimate[1] < uc4LocalData[iLocalPixOffset].y);          \n    uiHighCount[2] += (fMedianEstimate[2] < uc4LocalData[iLocalPixOffset++].z);          \n\n    // Row2 Middle Pix (RGB)\n    uiHighCount[0] += (fMedianEstimate[0] < uc4LocalData[iLocalPixOffset].x);          \n    uiHighCount[1] += (fMedianEstimate[1] < uc4LocalData[iLocalPixOffset].y);          \n    uiHighCount[2] += (fMedianEstimate[2] < uc4LocalData[iLocalPixOffset++].z);          \n\n    // Row2 Right Pix (RGB)\n    uiHighCount[0] += (fMedianEstimate[0] < uc4LocalData[iLocalPixOffset].x);          \n    uiHighCount[1] += (fMedianEstimate[1] < uc4LocalData[iLocalPixOffset].y);          \n    uiHighCount[2] += (fMedianEstimate[2] < uc4LocalData[iLocalPixOffset].z);          \n\n    // set the offset into SMEM for next row\n    iLocalPixOffset += (iLocalPixPitch - 2);  \n\n    // Row3 Left Pix (RGB)\n    uiHighCount[0] += (fMedianEstimate[0] < uc4LocalData[iLocalPixOffset].x);          \n    uiHighCount[1] += (fMedianEstimate[1] < uc4LocalData[iLocalPixOffset].y);          \n    uiHighCount[2] += (fMedianEstimate[2] < uc4LocalData[iLocalPixOffset++].z);          \n\n    // Row3 Middle Pix (RGB)\n    uiHighCount[0] += (fMedianEstimate[0] < uc4LocalData[iLocalPixOffset].x);          \n    uiHighCount[1] += (fMedianEstimate[1] < uc4LocalData[iLocalPixOffset].y);          \n    uiHighCount[2] += (fMedianEstimate[2] < uc4LocalData[iLocalPixOffset++].z);          \n\n    // Row3 Right Pix (RGB)\n    uiHighCount[0] += (fMedianEstimate[0] < uc4LocalData[iLocalPixOffset].x);          \n    uiHighCount[1] += (fMedianEstimate[1] < uc4LocalData[iLocalPixOffset].y);          \n    uiHighCount[2] += (fMedianEstimate[2] < uc4LocalData[iLocalPixOffset].z);          \n\n    //********************************\n    // reset the appropriate bound, depending upon counter\n    if(uiHighCount[0] > 4)\n    {\n      fMinBound[0] = fMedianEstimate[0];        \n    }\n    else\n    {\n      fMaxBound[0] = fMedianEstimate[0];        \n    }\n\n    if(uiHighCount[1] > 4)\n    {\n      fMinBound[1] = fMedianEstimate[1];        \n    }\n    else\n    {\n      fMaxBound[1] = fMedianEstimate[1];        \n    }\n\n    if(uiHighCount[2] > 4)\n    {\n      fMinBound[2] = fMedianEstimate[2];        \n    }\n    else\n    {\n      fMaxBound[2] = fMedianEstimate[2];        \n    }\n\n    // refine the estimate\n    fMedianEstimate[0] = 0.5f * (fMaxBound[0] + fMinBound[0]);\n    fMedianEstimate[1] = 0.5f * (fMaxBound[1] + fMinBound[1]);\n    fMedianEstimate[2] = 0.5f * (fMaxBound[2] + fMinBound[2]);\n  }\n\n  // pack into a monochrome unsigned int \n  unsigned int uiPackedPix = 0x000000FF & (unsigned int)(fMedianEstimate[0] + 0.5f);\n  uiPackedPix |= 0x0000FF00 & (((unsigned int)(fMedianEstimate[1] + 0.5f)) << 8);\n  uiPackedPix |= 0x00FF0000 & (((unsigned int)(fMedianEstimate[2] + 0.5f)) << 16);\n\n  // Write out to GMEM with restored offset\n  if((iDevYPrime < iDevImageHeight) && (iImagePosX < iImageWidth))\n  {\n    uiDest[iDevGMEMOffset + iImageX] = uiPackedPix;\n  }\n}"
        ]
    },
    "dropout-cuda": {
        "/Users/gbolet/hecbench-roofline/src/dropout-cuda/main.cu": [
            "__global__ void\nfused_dropout_kernel(\n  const scalar_t *__restrict__ a,\n        scalar_t *__restrict__ b,\n         mask_t *__restrict__ c,\n  IndexType totalElements,\n  accscalar_t p,\n  std::pair<uint64_t, uint64_t> seeds) \n{\n  accscalar_t scale = accscalar_t(1)/p;\n  IndexType idx = blockIdx.x * blockDim.x + threadIdx.x;\n\n  curandStatePhilox4_32_10_t state;\n  curand_init(seeds.first, idx, seeds.second, &state);\n\n  IndexType rounded_size = ((totalElements - 1)/(blockDim.x * gridDim.x * UNROLL)+1) *\n                           blockDim.x * gridDim.x * UNROLL;\n\n  for (IndexType linearIndex = idx;\n       linearIndex < rounded_size;\n       linearIndex += gridDim.x * blockDim.x * UNROLL) {\n\n    float4 rand = curand_uniform4(&state);\n    scalar_t src[UNROLL];\n    rand.x = rand.x < p;\n    rand.y = rand.y < p;\n    rand.z = rand.z < p;\n    rand.w = rand.w < p;\n\n    #pragma unroll\n    for (int ii = 0; ii < UNROLL; ii++) {\n      IndexType li = linearIndex + blockDim.x * gridDim.x * ii;\n      if (li < totalElements) {\n        const IndexType aOffset = li;\n        src[ii] = a[aOffset];\n      }\n    }\n\n    #pragma unroll\n    for (int ii = 0; ii < UNROLL; ii++) {\n      IndexType li = linearIndex + blockDim.x * gridDim.x * ii;\n      if (li < totalElements) {\n        const IndexType bOffset = li;\n        b[bOffset] = src[ii]*(&rand.x)[ii]*scale;\n        c[bOffset] = (mask_t)(&rand.x)[ii];\n      }\n    }\n    __syncthreads();\n  }\n}",
            "__global__ void\nfused_dropout_kernel_vec(\n  const scalar_t *__restrict__ a,\n        scalar_t *__restrict__ b,\n         mask_t *__restrict__ c,\n  IndexType totalElements,\n  accscalar_t p,\n  std::pair<uint64_t, uint64_t> seeds) \n{\n  using LoadT = aligned_vector<scalar_t, VEC>;\n  using MaskLoadT = aligned_vector<mask_t, VEC>;\n\n  // Helps align the total number of times curand_uniform4 is called by each thread for the same totalElements\n  bool gridxvec_loop_state = 0;\n\n  accscalar_t scale = accscalar_t(1)/p;\n  IndexType idx = blockIdx.x * blockDim.x + threadIdx.x;\n\n  curandStatePhilox4_32_10_t state;\n  curand_init(seeds.first, idx, seeds.second, &state);\n\n  // Note: Vectorized loads means we'll stride each thread by an additional VEC factor, as we'll load VEC elements at a time\n  for (IndexType linearIndex = idx * VEC;\n       linearIndex < totalElements;\n       linearIndex += gridDim.x * blockDim.x * VEC) {\n\n    scalar_t src[VEC];\n    LoadT *value = reinterpret_cast<LoadT*>(&src);\n\n    float4 rand;\n    if ((VEC == 4) || (gridxvec_loop_state == 0)) {\n      rand = curand_uniform4(&state);\n    } else {\n      // sets up the last two values we generated last iteration to be used this iteration.\n      rand.x = rand.z;\n      rand.y = rand.w;\n      gridxvec_loop_state ^= 1;\n    }\n    rand.x = rand.x < p;\n    rand.y = rand.y < p;\n    if (VEC == 4) {\n      rand.z = rand.z < p;\n      rand.w = rand.w < p;\n    }\n\n    *value = *reinterpret_cast<const LoadT*>(&a[linearIndex]);\n\n    scalar_t r[VEC];\n    mask_t mask[VEC];\n\n    // Perform the actual computation\n    #pragma unroll\n    for (int ii = 0; ii < VEC; ii++) {\n      r[ii] = src[ii]*(&rand.x)[ii]*scale;\n      mask[ii] = (mask_t)(&rand.x)[ii];\n    }\n    // Vectorized writes for both mask & result\n    *(reinterpret_cast<LoadT*>(&b[linearIndex])) = *reinterpret_cast<LoadT*>(&r[0]);\n    *(reinterpret_cast<MaskLoadT*>(&c[linearIndex])) = *reinterpret_cast<MaskLoadT*>(&mask[0]);\n\n    __syncthreads();\n  }\n}"
        ]
    },
    "softmax-fused-cuda": {
        "/Users/gbolet/hecbench-roofline/src/softmax-fused-cuda/main.cu": [
            "#define T ((int)32)\n\n\n#define WARP_SIZE 32\n\n\n__device__ __forceinline__ T\nWARP_SHFL_XOR_NATIVE(T value, int laneMask, int width = warpSize,\n                     unsigned int mask = 0xffffffff) {\n  return __shfl_xor_sync(mask, value, laneMask, width);\n}\n\n__device__ __inline__ void copy_vector<uint8_t, 4>(uint8_t *dst,\n                                                   const uint8_t *src) {\n  *((uchar4 *)dst) = *((uchar4 *)src);\n}\n\n__device__ __forceinline__ void warp_reduce(acc_t *sum) {\n  ReduceOp<acc_t> r;\n#pragma unroll\n  for (int offset = WARP_SIZE / 2; offset > 0; offset /= 2) {\n#pragma unroll\n    for (int i = 0; i < WARP_BATCH; ++i) {\n      acc_t b = WARP_SHFL_XOR_NATIVE(sum[i], offset, WARP_SIZE);\n      sum[i] = r(sum[i], b);\n    }\n  }\n}\n\n__global__ void scaled_masked_softmax_warp_forward(\n    output_t *dst, const input_t *src, const uint8_t *mask, const acc_t scale,\n    int micro_batch_size, int element_count, int pad_batches) {\n  constexpr int next_power_of_two = 1 << log2_elements;\n  constexpr int WARP_SIZE =\n      (next_power_of_two < C10_WARP_SIZE) ? next_power_of_two : C10_WARP_SIZE;\n  constexpr int WARP_ITERATIONS = next_power_of_two / WARP_SIZE;\n  constexpr int WARP_BATCH = (next_power_of_two <= 128) ? 2 : 1;\n  constexpr int ELEMENTS_PER_LDG_STG = (WARP_ITERATIONS < 4) ? 1 : 4;\n\n  // blockDim/threadIdx = (WARP_SIZE, WARPS_PER_BLOCK, 1)\n  // gridDim/blockIdx = (seq_len, attn_heads, batches)\n  int first_batch =\n      (blockDim.y *\n           (blockIdx.x + gridDim.x * (blockIdx.y + gridDim.y * blockIdx.z)) +\n       threadIdx.y) *\n      WARP_BATCH;\n  int pad_first_batch;\n  if (pad_batches != 1) { // bert style\n    pad_first_batch =\n        (blockDim.y * (blockIdx.x + gridDim.x * blockIdx.z) + threadIdx.y) *\n        WARP_BATCH;\n  } else { // gpt2 style\n    pad_first_batch = (blockDim.y * blockIdx.x + threadIdx.y) * WARP_BATCH;\n  }\n\n  // micro_batch_size might not be a multiple of WARP_BATCH. Check how\n  // many batches have to computed within this WARP.\n  int local_batches = micro_batch_size - first_batch;\n  if (local_batches > WARP_BATCH)\n    local_batches = WARP_BATCH;\n\n  // there might be multiple batches per warp. compute the index within the\n  // batch\n  int local_idx = threadIdx.x;\n\n  src += first_batch * element_count + ELEMENTS_PER_LDG_STG * local_idx;\n  dst += first_batch * element_count + ELEMENTS_PER_LDG_STG * local_idx;\n  mask += pad_first_batch * element_count + ELEMENTS_PER_LDG_STG * local_idx;\n\n  // load data from global memory\n  acc_t elements[WARP_BATCH][WARP_ITERATIONS];\n  input_t temp_data[ELEMENTS_PER_LDG_STG];\n  uint8_t temp_mask[ELEMENTS_PER_LDG_STG];\n#pragma unroll\n  for (int i = 0; i < local_batches; ++i) {\n#pragma unroll\n    for (int it = 0; it < WARP_ITERATIONS; it += ELEMENTS_PER_LDG_STG) {\n      int element_index = ELEMENTS_PER_LDG_STG * local_idx + it * WARP_SIZE;\n      if (element_index < element_count) {\n\n        int itr_idx = i * element_count + it * WARP_SIZE;\n        copy_vector<input_t, ELEMENTS_PER_LDG_STG>(temp_data, src + itr_idx);\n        copy_vector<uint8_t, ELEMENTS_PER_LDG_STG>(temp_mask, mask + itr_idx);\n\n#pragma unroll\n        for (int element = 0; element < ELEMENTS_PER_LDG_STG; ++element) {\n          if (temp_mask[element] != 1) {\n            elements[i][it + element] = (acc_t)temp_data[element] * scale;\n          } else {\n            elements[i][it + element] = (acc_t)-10000.0;\n          }\n        }\n      } else {\n#pragma unroll\n        for (int element = 0; element < ELEMENTS_PER_LDG_STG; ++element) {\n          elements[i][it + element] = -std::numeric_limits<acc_t>::infinity();\n        }\n      }\n    }\n  }\n\n  // compute max_value\n  acc_t max_value[WARP_BATCH];\n#pragma unroll\n  for (int i = 0; i < WARP_BATCH; ++i) {\n    max_value[i] = elements[i][0];\n#pragma unroll\n    for (int it = 1; it < WARP_ITERATIONS; ++it) {\n      max_value[i] =\n          (max_value[i] > elements[i][it]) ? max_value[i] : elements[i][it];\n    }\n  }\n  warp_reduce<acc_t, WARP_BATCH, WARP_SIZE, Max>(max_value);\n\n  // compute scale value to account for full mask\n  acc_t mask_value[WARP_BATCH];\n#pragma unroll\n  for (int i = 0; i < WARP_BATCH; ++i) {\n    mask_value[i] = (max_value[i] == (acc_t)-10000.0) ? (acc_t)0.0 : (acc_t)1.0;\n  }\n\n  acc_t sum[WARP_BATCH]{0.0f};\n#pragma unroll\n  for (int i = 0; i < WARP_BATCH; ++i) {\n#pragma unroll\n    for (int it = 0; it < WARP_ITERATIONS; ++it) {\n      elements[i][it] = std::exp((elements[i][it] - max_value[i]));\n      sum[i] += elements[i][it];\n    }\n  }\n  warp_reduce<acc_t, WARP_BATCH, WARP_SIZE, Add>(sum);\n\n  // store result\n  output_t out[ELEMENTS_PER_LDG_STG];\n#pragma unroll\n  for (int i = 0; i < local_batches; ++i) {\n#pragma unroll\n    for (int it = 0; it < WARP_ITERATIONS; it += ELEMENTS_PER_LDG_STG) {\n      int element_index = ELEMENTS_PER_LDG_STG * local_idx + it * WARP_SIZE;\n      if (element_index < element_count) {\n#pragma unroll\n        for (int element = 0; element < ELEMENTS_PER_LDG_STG; ++element) {\n          if (mask_value[i])\n            out[element] = elements[i][it + element] / sum[i];\n          else\n            out[element] = (output_t)0;\n        }\n        copy_vector<output_t, ELEMENTS_PER_LDG_STG>(\n            dst + i * element_count + it * WARP_SIZE, out);\n      } else {\n        break;\n      }\n    }\n  }\n}"
        ]
    },
    "convolutionSeparable-cuda": {
        "/Users/gbolet/hecbench-roofline/src/convolutionSeparable-cuda/conv.cu": [
            "__global__ void conv_rows(\n    float *__restrict__ dst,\n    const float *__restrict__ src,\n    const float *__restrict__ kernel,\n    const int imageW,\n    const int imageH,\n    const int pitch)\n{\n  __shared__ float l_Data[ROWS_BLOCKDIM_Y][(ROWS_RESULT_STEPS + 2 * ROWS_HALO_STEPS) * ROWS_BLOCKDIM_X];\n\n  int gidX = blockIdx.x;\n  int gidY = blockIdx.y;\n  int lidX = threadIdx.x;\n  int lidY = threadIdx.y;\n  //Offset to the left halo edge\n  const int baseX = (gidX * ROWS_RESULT_STEPS - ROWS_HALO_STEPS) * ROWS_BLOCKDIM_X + lidX;\n  const int baseY = gidY * ROWS_BLOCKDIM_Y + lidY;\n\n  src += baseY * pitch + baseX;\n  dst += baseY * pitch + baseX;\n\n  //Load main data\n  #pragma unroll\n  for(int i = ROWS_HALO_STEPS; i < ROWS_HALO_STEPS + ROWS_RESULT_STEPS; i++)\n    l_Data[lidY][lidX + i * ROWS_BLOCKDIM_X] = src[i * ROWS_BLOCKDIM_X];\n\n  //Load left halo\n  #pragma unroll\n  for(int i = 0; i < ROWS_HALO_STEPS; i++)\n    l_Data[lidY][lidX + i * ROWS_BLOCKDIM_X] = (baseX + i * ROWS_BLOCKDIM_X >= 0) ? src[i * ROWS_BLOCKDIM_X] : 0;\n\n  //Load right halo\n  #pragma unroll\n  for(int i = ROWS_HALO_STEPS + ROWS_RESULT_STEPS; i < ROWS_HALO_STEPS + ROWS_RESULT_STEPS + ROWS_HALO_STEPS; i++)\n    l_Data[lidY][lidX + i * ROWS_BLOCKDIM_X] = (baseX + i * ROWS_BLOCKDIM_X < imageW) ? src[i * ROWS_BLOCKDIM_X] : 0;\n\n  //Compute and store results\n  __syncthreads();\n\n  #pragma unroll\n  for(int i = ROWS_HALO_STEPS; i < ROWS_HALO_STEPS + ROWS_RESULT_STEPS; i++) {\n    float sum = 0;\n\n    #pragma unroll\n    for(int j = -KERNEL_RADIUS; j <= KERNEL_RADIUS; j++)\n      sum += kernel[KERNEL_RADIUS - j] * l_Data[lidY][lidX + i * ROWS_BLOCKDIM_X + j];\n\n    dst[i * ROWS_BLOCKDIM_X] = sum;\n  }\n}",
            "__global__ void conv_cols(\n    float *__restrict__ dst,\n    const float *__restrict__ src,\n    const float *__restrict__ kernel,\n    const int imageW,\n    const int imageH,\n    const int pitch)\n{\n  __shared__ float l_Data[COLUMNS_BLOCKDIM_X][(COLUMNS_RESULT_STEPS + 2 * COLUMNS_HALO_STEPS) * COLUMNS_BLOCKDIM_Y + 1];\n\n  int gidX = blockIdx.x;\n  int gidY = blockIdx.y;\n  int lidX = threadIdx.x;\n  int lidY = threadIdx.y;\n\n  //Offset to the upper halo edge\n  const int baseX = gidX * COLUMNS_BLOCKDIM_X + lidX;\n  const int baseY = (gidY * COLUMNS_RESULT_STEPS - COLUMNS_HALO_STEPS) * COLUMNS_BLOCKDIM_Y + lidY;\n  src += baseY * pitch + baseX;\n  dst += baseY * pitch + baseX;\n\n  //Load main data\n  #pragma unroll\n  for(int i = COLUMNS_HALO_STEPS; i < COLUMNS_HALO_STEPS + COLUMNS_RESULT_STEPS; i++)\n    l_Data[lidX][lidY + i * COLUMNS_BLOCKDIM_Y] = src[i * COLUMNS_BLOCKDIM_Y * pitch];\n\n  //Load upper halo\n  #pragma unroll\n  for(int i = 0; i < COLUMNS_HALO_STEPS; i++)\n    l_Data[lidX][lidY + i * COLUMNS_BLOCKDIM_Y] = (baseY + i * COLUMNS_BLOCKDIM_Y >= 0) ? src[i * COLUMNS_BLOCKDIM_Y * pitch] : 0;\n\n  //Load lower halo\n  #pragma unroll\n  for(int i = COLUMNS_HALO_STEPS + COLUMNS_RESULT_STEPS; i < COLUMNS_HALO_STEPS + COLUMNS_RESULT_STEPS + COLUMNS_HALO_STEPS; i++)\n    l_Data[lidX][lidY + i * COLUMNS_BLOCKDIM_Y] = (baseY + i * COLUMNS_BLOCKDIM_Y < imageH) ? src[i * COLUMNS_BLOCKDIM_Y * pitch] : 0;\n\n  //Compute and store results\n  __syncthreads();\n\n  #pragma unroll\n  for(int i = COLUMNS_HALO_STEPS; i < COLUMNS_HALO_STEPS + COLUMNS_RESULT_STEPS; i++) {\n    float sum = 0;\n\n    #pragma unroll\n    for(int j = -KERNEL_RADIUS; j <= KERNEL_RADIUS; j++)\n      sum += kernel[KERNEL_RADIUS - j] * l_Data[lidX][lidY + i * COLUMNS_BLOCKDIM_Y + j];\n\n    dst[i * COLUMNS_BLOCKDIM_Y * pitch] = sum;\n  }\n}"
        ]
    },
    "mr-cuda": {
        "/Users/gbolet/hecbench-roofline/src/mr-cuda/kernels.h": [
            "__global__ void mr32_sf(\n  const uint32_t *__restrict__ bases,\n  const uint32_t *__restrict__ n32,\n  int *__restrict__ val,\n  int iter)\n{\n  int j = blockIdx.x * blockDim.x + threadIdx.x;\n  if (j < iter) {\n    int n = n32[j];\n    for (int cnt = 1; cnt <= BASES_CNT32; cnt++) {\n      atomicAdd(val, straightforward_mr32(bases, cnt, n));\n    }\n  }\n}",
            "__global__ void mr32_eff(\n  const uint32_t *__restrict__ bases,\n  const uint32_t *__restrict__ n32,\n  int *__restrict__ val,\n  int iter)\n{\n  int j = blockIdx.x * blockDim.x + threadIdx.x;\n  if (j < iter) {\n    int n = n32[j];\n    for (int cnt = 1; cnt <= BASES_CNT32; cnt++) {\n      atomicAdd(val, efficient_mr32(bases, cnt, n));\n    }\n  }\n}"
        ]
    },
    "lanczos-cuda": {
        "/Users/gbolet/hecbench-roofline/src/lanczos-cuda/lanczos.cu": [
            "#define T ((int)32)\n\n\n__global__ void dot_product_kernel(const int N, const T *x, const T *y,\n    T *z) {\n\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    __shared__ T result[THREADS_PER_BLOCK];\n\n    if (index < N) {\n        result[threadIdx.x] = x[index] * y[index];\n    } else {\n        result[threadIdx.x] = 0;\n    }\n    __syncthreads();\n\n    int half = THREADS_PER_BLOCK / 2;\n    while (half > 0) {\n        if (threadIdx.x < half) {\n            result[threadIdx.x] += result[threadIdx.x + half];\n        }\n        __syncthreads();\n        half /= 2;\n    }\n\n    if (threadIdx.x == 0) {\n        z[blockIdx.x] = result[0];\n    }\n}",
            "#define T ((int)32)\n\n\n__global__ void multiply_inplace_kernel(const int N, T *x, const T k) {\n\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (index < N) {\n        x[index] = x[index] * k;\n    }\n}",
            "#define T ((int)32)\n\n\n__global__ void saxpy_inplace_kernel(const int N, T *y, const T *x,\n    const T a) {\n\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (index < N) {\n        y[index] += a * x[index];\n    }\n}",
            "#define T ((int)32)\n\n\n__global__ void warp_multiply_kernel(const int group_size, const int rows,\n    const int begin_row, const int *row_ptr, const int *col_ind,\n    const T *values, const T *x, T *y) {\n\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    int r = index / group_size + begin_row;\n    int lane = index % group_size;\n    __shared__ volatile T result[THREADS_PER_BLOCK];\n\n    result[threadIdx.x] = 0;\n    if (r < rows) {\n        int start = row_ptr[r];\n        int end = row_ptr[r + 1];\n        for (int i = start + lane; i < end; i+= group_size) {\n            result[threadIdx.x] += values[i] * x[col_ind[i]];\n        }\n        // Threads in a warp are synchronized, so we can do this\n        int half = group_size / 2;\n        while (half > 0) {\n            if (lane < half) {\n                result[threadIdx.x] += result[threadIdx.x + half];\n            }\n            half /= 2;\n        }\n        if (lane == 0) {\n            y[r] = result[threadIdx.x];\n        }\n    }\n}"
        ]
    },
    "clock-cuda": {
        "/Users/gbolet/hecbench-roofline/src/clock-cuda/main.cu": [
            "__global__ static void timedReduction(const float *input, float *output, clock_t *timer)\n{\n    // __shared__ float shared[2 * blockDim.x];\n    extern __shared__ float shared[];\n\n    const int tid = threadIdx.x;\n    const int bid = blockIdx.x;\n\n    if (tid == 0) timer[bid] = clock();\n\n    // Copy input.\n    shared[tid] = input[tid];\n    shared[tid + blockDim.x] = input[tid + blockDim.x];\n\n    // Perform reduction to find minimum.\n    for (int d = blockDim.x; d > 0; d /= 2)\n    {\n        __syncthreads();\n\n        if (tid < d)\n        {\n            float f0 = shared[tid];\n            float f1 = shared[tid + d];\n\n            if (f1 < f0)\n            {\n                shared[tid] = f1;\n            }\n        }\n    }\n\n    // Write result.\n    if (tid == 0) output[bid] = shared[0];\n\n    __syncthreads();\n\n    if (tid == 0) timer[bid+gridDim.x] = clock();\n}"
        ]
    },
    "complex-cuda": {
        "/Users/gbolet/hecbench-roofline/src/complex-cuda/kernels.h": [
            "inline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\ninline __host__ __device__ float4 sqrtf(float4 v)\n{\n    return make_float4(sqrtf(v.x), sqrtf(v.y), sqrtf(v.z), sqrtf(v.w));\n}\n\n__host__ __device__\nstatic __inline__ float Cimagf (FloatComplex x) \n{ \n    return x.y; \n}\n\n__host__ __device__\nstatic __inline__ float Crealf (FloatComplex x) \n{ \n    return x.x; \n}\n\n__host__ __device__\nstatic __inline__ FloatComplex make_FloatComplex (float r, float i)\n{\n    FloatComplex res;\n    res.x = r;\n    res.y = i;\n    return res;\n}\n\n__host__ __device__\nstatic __inline__ float Cabsf (FloatComplex x)\n{\n    float a = Crealf(x);\n    float b = Cimagf(x);\n    float v, w, t;\n    a = fabsf(a);\n    b = fabsf(b);\n    if (a > b) {\n        v = a;\n        w = b; \n    } else {\n        v = b;\n        w = a;\n    }\n    t = w / v;\n    t = 1.0f + t * t;\n    t = v * sqrtf(t);\n    if ((v == 0.0f) || (v > 3.402823466e38f) || (w > 3.402823466e38f)) {\n        t = v + w;\n    }\n    return t;\n}\n\n__host__ __device__\nstatic __inline__ FloatComplex Caddf (FloatComplex x, FloatComplex y)\n{\n    return make_FloatComplex (Crealf(x) + Crealf(y), \n                                Cimagf(x) + Cimagf(y));\n}\n\n__host__ __device__\nstatic __inline__ FloatComplex Cdivf (FloatComplex x, FloatComplex y)\n{\n    FloatComplex quot;\n    float s = fabsf(Crealf(y)) + fabsf(Cimagf(y));\n    float oos = 1.0f / s;\n    float ars = Crealf(x) * oos;\n    float ais = Cimagf(x) * oos;\n    float brs = Crealf(y) * oos;\n    float bis = Cimagf(y) * oos;\n    s = (brs * brs) + (bis * bis);\n    oos = 1.0f / s;\n    quot = make_FloatComplex (((ars * brs) + (ais * bis)) * oos,\n                                ((ais * brs) - (ars * bis)) * oos);\n    return quot;\n}\n\n__host__ __device__\nstatic __inline__ FloatComplex Cmulf (FloatComplex x, FloatComplex y)\n{\n    FloatComplex prod;\n    prod = make_FloatComplex  ((Crealf(x) * Crealf(y)) - \n                                 (Cimagf(x) * Cimagf(y)),\n                                 (Crealf(x) * Cimagf(y)) + \n                                 (Cimagf(x) * Crealf(y)));\n    return prod;\n}\n\n__host__ __device__\nstatic __inline__ FloatComplex Conjf (FloatComplex x)\n{\n    return make_FloatComplex (Crealf(x), -Cimagf(x));\n}\n\n__host__ __device__\nstatic __inline__ FloatComplex Csubf (FloatComplex x, FloatComplex y)\n{\n    return make_FloatComplex (Crealf(x) - Crealf(y), \n                                    Cimagf(x) - Cimagf(y));\n}\n\n__device__\ndouble LCG_random_double(uint64_t * seed)\n{\n  const uint64_t m = 9223372036854775808ULL; // 2^63\n  const uint64_t a = 2806196910506780709ULL;\n  const uint64_t c = 1ULL;\n  *seed = (a * (*seed) + c) % m;\n  return (double) (*seed) / (double) m;\n}\n\n__device__ uint64_t fast_forward_LCG(uint64_t seed, uint64_t n)\n{\n  const uint64_t m = 9223372036854775808ULL; // 2^63\n  uint64_t a = 2806196910506780709ULL;\n  uint64_t c = 1ULL;\n\n  n = n % m;\n\n  uint64_t a_new = 1;\n  uint64_t c_new = 0;\n\n  while(n > 0) \n  {\n    if(n & 1)\n    {\n      a_new *= a;\n      c_new = c_new * a + c;\n    }\n    c *= (a + 1);\n    a *= a;\n\n    n >>= 1;\n  }\n\n  return (a_new * seed + c_new) % m;\n}\n\n__global__ void complex_float (char* checkSum, int n)\n{\n  int i = threadIdx.x + blockIdx.x * blockDim.x;\n  if (i >= n) return; \n  uint64_t seed = 1ULL;\n  seed = fast_forward_LCG(seed, i);\n  float r1 = LCG_random_double(&seed);\n  float r2 = LCG_random_double(&seed); \n  float r3 = LCG_random_double(&seed); \n  float r4 = LCG_random_double(&seed); \n\n  FloatComplex z1 = make_FloatComplex(r1, r2);\n  FloatComplex z2 = make_FloatComplex(r3, r4);\n\n  char s = fabsf(Cabsf(Cmulf(z1, z2)) - Cabsf(z1) * Cabsf(z2)) < 1e-3f;\n\n  s += fabsf(Cabsf(Caddf(z1, z2)) * Cabsf(Caddf(z1 , z2)) -\n             Crealf(Cmulf(Caddf(z1, z2) , Caddf(Conjf(z1), Conjf(z2))))) < 1e-3f; \n\n  s += fabsf(Cabsf(Csubf(z1, z2)) * Cabsf(Csubf(z1 , z2)) -\n             Crealf(Cmulf(Csubf(z1, z2) , Csubf(Conjf(z1), Conjf(z2))))) < 1e-3f;\n\n  s += fabsf(Crealf(Caddf(Cmulf(z1, Conjf(z2)) , Cmulf(z2, Conjf(z1)))) -\n             2.0f * (Crealf(z1) * Crealf(z2) + Cimagf(z1) * Cimagf(z2))) < 1e-3f;\n\n  s += fabsf(Cabsf(Cdivf(Conjf(z1), z2)) -\n             Cabsf(Cdivf(Conjf(z1), Conjf(z2)))) < 1e-3f;\n\n  checkSum[i] = s;\n}",
            "inline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\ninline __host__ __device__ float4 fabs(float4 v)\n{\n    return make_float4(fabs(v.x), fabs(v.y), fabs(v.z), fabs(v.w));\n}\n\n__host__ __device__\nstatic __inline__ double Cimag (DoubleComplex x) \n{ \n    return x.y; \n}\n\n__host__ __device__\nstatic __inline__ double Creal (DoubleComplex x) \n{ \n    return x.x; \n}\n\n__host__ __device__\nstatic __inline__ DoubleComplex make_DoubleComplex (double r, double i)\n{\n    DoubleComplex res;\n    res.x = r;\n    res.y = i;\n    return res;\n}\n\n__host__ __device__\nstatic __inline__ double Cabs (DoubleComplex x)\n{\n    double a = Creal(x);\n    double b = Cimag(x);\n    double v, w, t;\n    a = fabs(a);\n    b = fabs(b);\n    if (a > b) {\n        v = a;\n        w = b; \n    } else {\n        v = b;\n        w = a;\n    }\n    t = w / v;\n    t = 1.0 + t * t;\n    t = v * sqrt(t);\n    if ((v == 0.0) || \n        (v > 1.79769313486231570e+308) || (w > 1.79769313486231570e+308)) {\n        t = v + w;\n    }\n    return t;\n}\n\n__host__ __device__\nstatic __inline__ DoubleComplex Cadd(DoubleComplex x, DoubleComplex y)\n{\n    return make_DoubleComplex (Creal(x) + Creal(y), \n                                 Cimag(x) + Cimag(y));\n}\n\n__host__ __device__\nstatic __inline__ DoubleComplex Cdiv(DoubleComplex x, DoubleComplex y)\n{\n    DoubleComplex quot;\n    double s = (fabs(Creal(y))) + (fabs(Cimag(y)));\n    double oos = 1.0 / s;\n    double ars = Creal(x) * oos;\n    double ais = Cimag(x) * oos;\n    double brs = Creal(y) * oos;\n    double bis = Cimag(y) * oos;\n    s = (brs * brs) + (bis * bis);\n    oos = 1.0 / s;\n    quot = make_DoubleComplex (((ars * brs) + (ais * bis)) * oos,\n                                 ((ais * brs) - (ars * bis)) * oos);\n    return quot;\n}\n\n__host__ __device__\nstatic __inline__ DoubleComplex Cmul(DoubleComplex x, DoubleComplex y)\n{\n    DoubleComplex prod;\n    prod = make_DoubleComplex ((Creal(x) * Creal(y)) - \n                                 (Cimag(x) * Cimag(y)),\n                                 (Creal(x) * Cimag(y)) + \n                                 (Cimag(x) * Creal(y)));\n    return prod;\n}\n\n__host__ __device__\nstatic __inline__ DoubleComplex Conj(DoubleComplex x)\n{\n    return make_DoubleComplex (Creal(x), -Cimag(x));\n}\n\n__host__ __device__\nstatic __inline__ DoubleComplex Csub(DoubleComplex x, DoubleComplex y)\n{\n    return make_DoubleComplex (Creal(x) - Creal(y), \n                                 Cimag(x) - Cimag(y));\n}\n\n__device__\ndouble LCG_random_double(uint64_t * seed)\n{\n  const uint64_t m = 9223372036854775808ULL; // 2^63\n  const uint64_t a = 2806196910506780709ULL;\n  const uint64_t c = 1ULL;\n  *seed = (a * (*seed) + c) % m;\n  return (double) (*seed) / (double) m;\n}\n\n__device__ uint64_t fast_forward_LCG(uint64_t seed, uint64_t n)\n{\n  const uint64_t m = 9223372036854775808ULL; // 2^63\n  uint64_t a = 2806196910506780709ULL;\n  uint64_t c = 1ULL;\n\n  n = n % m;\n\n  uint64_t a_new = 1;\n  uint64_t c_new = 0;\n\n  while(n > 0) \n  {\n    if(n & 1)\n    {\n      a_new *= a;\n      c_new = c_new * a + c;\n    }\n    c *= (a + 1);\n    a *= a;\n\n    n >>= 1;\n  }\n\n  return (a_new * seed + c_new) % m;\n}\n\n__global__ void complex_double (char* checkSum, int n)\n{\n  int i = threadIdx.x + blockIdx.x * blockDim.x;\n  if (i >= n) return; \n  uint64_t seed = 1ULL;\n  seed = fast_forward_LCG(seed, i);\n  double r1 = LCG_random_double(&seed);\n  double r2 = LCG_random_double(&seed); \n  double r3 = LCG_random_double(&seed); \n  double r4 = LCG_random_double(&seed); \n\n  DoubleComplex z1 = make_DoubleComplex(r1, r2);\n  DoubleComplex z2 = make_DoubleComplex(r3, r4);\n\n  char s = fabs(Cabs(Cmul(z1, z2)) - Cabs(z1) * Cabs(z2)) < 1e-3;\n\n  s += fabs(Cabs(Cadd(z1, z2)) * Cabs(Cadd(z1 , z2)) -\n            Creal(Cmul(Cadd(z1, z2) , Cadd(Conj(z1), Conj(z2))))) < 1e-3; \n\n  s += fabs(Cabs(Csub(z1, z2)) * Cabs(Csub(z1 , z2)) -\n            Creal(Cmul(Csub(z1, z2) , Csub(Conj(z1), Conj(z2))))) < 1e-3;\n\n  s += fabs(Creal(Cadd(Cmul(z1, Conj(z2)) , Cmul(z2, Conj(z1)))) -\n            2.0 * (Creal(z1) * Creal(z2) + Cimag(z1) * Cimag(z2))) < 1e-3;\n\n  s += fabs(Cabs(Cdiv(Conj(z1), z2)) -\n            Cabs(Cdiv(Conj(z1), Conj(z2)))) < 1e-3;\n\n  checkSum[i] = s;\n}"
        ],
        "/Users/gbolet/hecbench-roofline/src/complex-cuda/reference.h": [
            "__host__ __device__\nstatic __inline__ float Cimagf (FloatComplex x) \n{ \n    return x.y; \n}\n\n__host__ __device__\nstatic __inline__ float Crealf (FloatComplex x) \n{ \n    return x.x; \n}\n\n__host__ __device__\nstatic __inline__ FloatComplex make_FloatComplex (float r, float i)\n{\n    FloatComplex res;\n    res.x = r;\n    res.y = i;\n    return res;\n}\n\n__host__ __device__\nstatic __inline__ FloatComplex Cmulf (FloatComplex x, FloatComplex y)\n{\n    FloatComplex prod;\n    prod = make_FloatComplex  ((Crealf(x) * Crealf(y)) - \n                                 (Cimagf(x) * Cimagf(y)),\n                                 (Crealf(x) * Cimagf(y)) + \n                                 (Cimagf(x) * Crealf(y)));\n    return prod;\n}\n\n__device__\ndouble LCG_random_double(uint64_t * seed)\n{\n  const uint64_t m = 9223372036854775808ULL; // 2^63\n  const uint64_t a = 2806196910506780709ULL;\n  const uint64_t c = 1ULL;\n  *seed = (a * (*seed) + c) % m;\n  return (double) (*seed) / (double) m;\n}\n\n__device__ uint64_t fast_forward_LCG(uint64_t seed, uint64_t n)\n{\n  const uint64_t m = 9223372036854775808ULL; // 2^63\n  uint64_t a = 2806196910506780709ULL;\n  uint64_t c = 1ULL;\n\n  n = n % m;\n\n  uint64_t a_new = 1;\n  uint64_t c_new = 0;\n\n  while(n > 0) \n  {\n    if(n & 1)\n    {\n      a_new *= a;\n      c_new = c_new * a + c;\n    }\n    c *= (a + 1);\n    a *= a;\n\n    n >>= 1;\n  }\n\n  return (a_new * seed + c_new) % m;\n}\n\n__global__ void ref_complex_float (char* checkSum, int n)\n{\n  int i = threadIdx.x + blockIdx.x * blockDim.x;\n  if (i >= n) return; \n  uint64_t seed = 1ULL;\n  seed = fast_forward_LCG(seed, i);\n  float r1 = LCG_random_double(&seed);\n  float r2 = LCG_random_double(&seed); \n  float r3 = LCG_random_double(&seed); \n  float r4 = LCG_random_double(&seed); \n\n  cuFloatComplex z1 = make_cuFloatComplex(r1, r2);\n  cuFloatComplex z2 = make_cuFloatComplex(r3, r4);\n\n  char s = fabsf(cuCabsf(Cmulf(z1, z2)) - cuCabsf(z1) * cuCabsf(z2)) < 1e-3f;\n\n  s += fabsf(cuCabsf(cuCaddf(z1, z2)) * cuCabsf(cuCaddf(z1 , z2)) -\n             cuCrealf(cuCmulf(cuCaddf(z1, z2) , cuCaddf(cuConjf(z1), cuConjf(z2))))) < 1e-3f; \n\n  s += fabsf(cuCabsf(cuCsubf(z1, z2)) * cuCabsf(cuCsubf(z1 , z2)) -\n             cuCrealf(cuCmulf(cuCsubf(z1, z2) , cuCsubf(cuConjf(z1), cuConjf(z2))))) < 1e-3f;\n\n  s += fabsf(cuCrealf(cuCaddf(cuCmulf(z1, cuConjf(z2)) , cuCmulf(z2, cuConjf(z1)))) -\n             2.0f * (cuCrealf(z1) * cuCrealf(z2) + cuCimagf(z1) * cuCimagf(z2))) < 1e-3f;\n\n  s += fabsf(cuCabsf(cuCdivf(cuConjf(z1), z2)) -\n             cuCabsf(cuCdivf(cuConjf(z1), cuConjf(z2)))) < 1e-3f;\n\n  checkSum[i] = s;\n}",
            "inline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\ninline __host__ __device__ float4 fabs(float4 v)\n{\n    return make_float4(fabs(v.x), fabs(v.y), fabs(v.z), fabs(v.w));\n}\n\n__device__\ndouble LCG_random_double(uint64_t * seed)\n{\n  const uint64_t m = 9223372036854775808ULL; // 2^63\n  const uint64_t a = 2806196910506780709ULL;\n  const uint64_t c = 1ULL;\n  *seed = (a * (*seed) + c) % m;\n  return (double) (*seed) / (double) m;\n}\n\n__device__ uint64_t fast_forward_LCG(uint64_t seed, uint64_t n)\n{\n  const uint64_t m = 9223372036854775808ULL; // 2^63\n  uint64_t a = 2806196910506780709ULL;\n  uint64_t c = 1ULL;\n\n  n = n % m;\n\n  uint64_t a_new = 1;\n  uint64_t c_new = 0;\n\n  while(n > 0) \n  {\n    if(n & 1)\n    {\n      a_new *= a;\n      c_new = c_new * a + c;\n    }\n    c *= (a + 1);\n    a *= a;\n\n    n >>= 1;\n  }\n\n  return (a_new * seed + c_new) % m;\n}\n\n__global__ void ref_complex_double (char* checkSum, int n)\n{\n  int i = threadIdx.x + blockIdx.x * blockDim.x;\n  if (i >= n) return; \n  uint64_t seed = 1ULL;\n  seed = fast_forward_LCG(seed, i);\n  double r1 = LCG_random_double(&seed);\n  double r2 = LCG_random_double(&seed); \n  double r3 = LCG_random_double(&seed); \n  double r4 = LCG_random_double(&seed); \n\n  cuDoubleComplex z1 = make_cuDoubleComplex(r1, r2);\n  cuDoubleComplex z2 = make_cuDoubleComplex(r3, r4);\n\n  char s = fabs(cuCabs(cuCmul(z1, z2)) - cuCabs(z1) * cuCabs(z2)) < 1e-3;\n\n  s += fabs(cuCabs(cuCadd(z1, z2)) * cuCabs(cuCadd(z1 , z2)) -\n            cuCreal(cuCmul(cuCadd(z1, z2) , cuCadd(cuConj(z1), cuConj(z2))))) < 1e-3; \n\n  s += fabs(cuCabs(cuCsub(z1, z2)) * cuCabs(cuCsub(z1 , z2)) -\n            cuCreal(cuCmul(cuCsub(z1, z2) , cuCsub(cuConj(z1), cuConj(z2))))) < 1e-3;\n\n  s += fabs(cuCreal(cuCadd(cuCmul(z1, cuConj(z2)) , cuCmul(z2, cuConj(z1)))) -\n            2.0 * (cuCreal(z1) * cuCreal(z2) + cuCimag(z1) * cuCimag(z2))) < 1e-3;\n\n  s += fabs(cuCabs(cuCdiv(cuConj(z1), z2)) -\n            cuCabs(cuCdiv(cuConj(z1), cuConj(z2)))) < 1e-3;\n\n  checkSum[i] = s;\n}"
        ]
    },
    "lzss-cuda": {
        "/Users/gbolet/hecbench-roofline/src/lzss-cuda/main.cu": [
            "#define T ((int)32)\n\n\n__global__ void compressKernelI(const T *input, uint32_t numOfBlocks,\n                                uint32_t *__restrict__ flagArrSizeGlobal,\n                                uint32_t *__restrict__ compressedDataSizeGlobal,\n                                uint8_t *__restrict__ tmpFlagArrGlobal,\n                                uint8_t *__restrict__ tmpCompressedDataGlobal,\n                                int minEncodeLength)\n{\n  // Block size in uint of datatype\n  const uint32_t blockSize = BLOCK_SIZE / sizeof(T);\n\n  // Window size in uint of datatype\n  const uint32_t threadSize = THREAD_SIZE;\n\n  // Allocate shared memory for the lookahead buffer of the whole block, the\n  // sliding window is included\n  __shared__ T buffer[blockSize];\n  __shared__ uint8_t lengthBuffer[blockSize];\n  __shared__ uint8_t offsetBuffer[blockSize];\n  __shared__ uint32_t prefixBuffer[blockSize + 1];\n\n  // initialize the tid\n  int tid = 0;\n\n  // Copy the memeory from global to shared\n  for (int i = 0; i < blockSize / threadSize; i++)\n  {\n    buffer[threadIdx.x + threadSize * i] =\n        input[blockIdx.x * blockSize + threadIdx.x + threadSize * i];\n  }\n\n  // Synchronize all threads to ensure that the buffer is fully loaded\n  __syncthreads();\n\n  // find match for every data point\n  for (int iteration = 0; iteration < (int)(blockSize / threadSize);\n       iteration++)\n  {\n    // Initialize the lookahead buffer and the sliding window pointers\n    tid = threadIdx.x + iteration * threadSize;\n    int bufferStart = tid;\n    int bufferPointer = bufferStart;\n    int windowStart =\n        bufferStart - int(WINDOW_SIZE) < 0 ? 0 : bufferStart - WINDOW_SIZE;\n    int windowPointer = windowStart;\n\n    uint8_t maxLen = 0;\n    uint8_t maxOffset = 0;\n    uint8_t len = 0;\n    uint8_t offset = 0;\n\n    while (windowPointer < bufferStart && bufferPointer < blockSize)\n    {\n      if (buffer[bufferPointer] == buffer[windowPointer])\n      {\n        if (offset == 0)\n        {\n          offset = bufferPointer - windowPointer;\n        }\n        len++;\n        bufferPointer++;\n      }\n      else\n      {\n        if (len > maxLen)\n        {\n          maxLen = len;\n          maxOffset = offset;\n        }\n        len = 0;\n        offset = 0;\n        bufferPointer = bufferStart;\n      }\n      windowPointer++;\n    }\n    if (len > maxLen)\n    {\n      maxLen = len;\n      maxOffset = offset;\n    }\n\n    lengthBuffer[threadIdx.x + iteration * threadSize] = maxLen;\n    offsetBuffer[threadIdx.x + iteration * threadSize] = maxOffset;\n\n    // initialize array as 0\n    prefixBuffer[threadIdx.x + iteration * threadSize] = 0;\n  }\n  __syncthreads();\n\n  // find encode information\n  uint32_t flagCount = 0;\n  __shared__ uint8_t byteFlagArr[(blockSize / 8)];\n\n  if (threadIdx.x == 0)\n  {\n    uint8_t flagPosition = 0x01;\n    uint8_t byteFlag = 0;\n\n    int encodeIndex = 0;\n\n    while (encodeIndex < blockSize)\n    {\n      // if length < minEncodeLength, no match is found\n      if (lengthBuffer[encodeIndex] < minEncodeLength)\n      {\n        prefixBuffer[encodeIndex] = sizeof(T);\n        encodeIndex++;\n      }\n      // if length > minEncodeLength, match is found\n      else\n      {\n        prefixBuffer[encodeIndex] = 2;\n        encodeIndex += lengthBuffer[encodeIndex];\n        byteFlag |= flagPosition;\n      }\n      // store the flag if there are 8 bits already\n      if (flagPosition == 0x80)\n      {\n        byteFlagArr[flagCount] = byteFlag;\n        flagCount++;\n        flagPosition = 0x01;\n        byteFlag = 0;\n        continue;\n      }\n      flagPosition <<= 1;\n    }\n    if (flagPosition != 0x01)\n    {\n      byteFlagArr[flagCount] = byteFlag;\n      flagCount++;\n    }\n  }\n  __syncthreads();\n\n  // prefix summation, up-sweep\n  int prefixSumOffset = 1;\n  for (uint32_t d = blockSize >> 1; d > 0; d = d >> 1)\n  {\n    for (int iteration = 0; iteration < (int)(blockSize / threadSize);\n         iteration++)\n    {\n      tid = threadIdx.x + iteration * threadSize;\n      if (tid < d)\n      {\n        int ai = prefixSumOffset * (2 * tid + 1) - 1;\n        int bi = prefixSumOffset * (2 * tid + 2) - 1;\n        prefixBuffer[bi] += prefixBuffer[ai];\n      }\n      __syncthreads();\n    }\n    prefixSumOffset *= 2;\n  }\n\n  // clear the last element\n  if (threadIdx.x == 0)\n  {\n    // printf(\"block size: %d flag array size: %d\\n\", prefixBuffer[blockSize - 1], flagCount);\n    compressedDataSizeGlobal[blockIdx.x] = prefixBuffer[blockSize - 1];\n    flagArrSizeGlobal[blockIdx.x] = flagCount;\n    prefixBuffer[blockSize] = prefixBuffer[blockSize - 1];\n    prefixBuffer[blockSize - 1] = 0;\n  }\n  __syncthreads();\n\n  // prefix summation, down-sweep\n  for (int d = 1; d < blockSize; d *= 2)\n  {\n    prefixSumOffset >>= 1;\n    for (int iteration = 0; iteration < (int)(blockSize / threadSize);\n         iteration++)\n    {\n      tid = threadIdx.x + iteration * threadSize;\n\n      if (tid < d)\n      {\n        int ai = prefixSumOffset * (2 * tid + 1) - 1;\n        int bi = prefixSumOffset * (2 * tid + 2) - 1;\n\n        uint32_t t = prefixBuffer[ai];\n        prefixBuffer[ai] = prefixBuffer[bi];\n        prefixBuffer[bi] += t;\n      }\n      __syncthreads();\n    }\n  }\n\n  // encoding phase one\n  int tmpCompressedDataGlobalOffset;\n  tmpCompressedDataGlobalOffset = blockSize * blockIdx.x * sizeof(T);\n  for (int iteration = 0; iteration < (int)(blockSize / threadSize); iteration++)\n  {\n    tid = threadIdx.x + iteration * threadSize;\n    if (prefixBuffer[tid + 1] != prefixBuffer[tid])\n    {\n      if (lengthBuffer[tid] < minEncodeLength)\n      {\n        uint32_t tmpOffset = prefixBuffer[tid];\n        uint8_t *bytePtr = (uint8_t *)&buffer[tid];\n        for (int tmpIndex = 0; tmpIndex < sizeof(T); tmpIndex++)\n        {\n          tmpCompressedDataGlobal[tmpCompressedDataGlobalOffset + tmpOffset + tmpIndex] = *(bytePtr + tmpIndex);\n        }\n      }\n      else\n      {\n        uint32_t tmpOffset = prefixBuffer[tid];\n        tmpCompressedDataGlobal[tmpCompressedDataGlobalOffset + tmpOffset] = lengthBuffer[tid];\n        tmpCompressedDataGlobal[tmpCompressedDataGlobalOffset + tmpOffset + 1] = offsetBuffer[tid];\n      }\n    }\n  }\n\n  // Copy the memeory back\n  if (threadIdx.x == 0)\n  {\n    for (int flagArrIndex = 0; flagArrIndex < flagCount; flagArrIndex++)\n    {\n      tmpFlagArrGlobal[blockSize / 8 * blockIdx.x + flagArrIndex] = byteFlagArr[flagArrIndex];\n    }\n  }\n}",
            "__global__ void compressKernelIII(uint32_t numOfBlocks,\n                                  const uint32_t *__restrict__ flagArrOffsetGlobal,\n                                  const uint32_t *__restrict__ compressedDataOffsetGlobal,\n                                  const uint8_t *__restrict__ tmpFlagArrGlobal,\n                                  const uint8_t *__restrict__ tmpCompressedDataGlobal,\n                                  uint8_t *__restrict__ flagArrGlobal,\n                                  uint8_t *__restrict__ compressedDataGlobal)\n{\n  // Block size in uint of bytes\n  const int blockSize = BLOCK_SIZE / sizeof(T);\n\n  // Window size in uint of bytes\n  const int threadSize = THREAD_SIZE;\n\n  // find block index\n  int blockIndex = blockIdx.x;\n\n  int flagArrOffset = flagArrOffsetGlobal[blockIndex];\n  int flagArrSize = flagArrOffsetGlobal[blockIndex + 1] - flagArrOffsetGlobal[blockIndex];\n\n  int compressedDataOffset = compressedDataOffsetGlobal[blockIndex];\n  int compressedDataSize = compressedDataOffsetGlobal[blockIndex + 1] - compressedDataOffsetGlobal[blockIndex];\n\n  int tid = threadIdx.x;\n\n  while (tid < flagArrSize)\n  {\n    flagArrGlobal[flagArrOffset + tid] = tmpFlagArrGlobal[blockSize / 8 * blockIndex + tid];\n    tid += threadSize;\n  }\n\n  tid = threadIdx.x;\n\n  while (tid < compressedDataSize)\n  {\n    compressedDataGlobal[compressedDataOffset + tid] = tmpCompressedDataGlobal[blockSize * sizeof(T) * blockIndex + tid];\n    tid += threadSize;\n  }\n}",
            "#define T ((int)32)\n\n\n__global__ void decompressKernel(T *output, uint32_t numOfBlocks,\n                                 const uint32_t *__restrict__ flagArrOffsetGlobal,\n                                 const uint32_t *__restrict__ compressedDataOffsetGlobal,\n                                 const uint8_t *__restrict__ flagArrGlobal,\n                                 const uint8_t *__restrict__ compressedDataGlobal)\n{\n  // Block size in unit of datatype\n  const uint32_t blockSize = BLOCK_SIZE / sizeof(T);\n\n  int tid = blockIdx.x * blockDim.x + threadIdx.x;\n\n  if (tid < numOfBlocks)\n  {\n    int flagArrOffset = flagArrOffsetGlobal[tid];\n    int flagArrSize = flagArrOffsetGlobal[tid + 1] - flagArrOffsetGlobal[tid];\n\n    int compressedDataOffset = compressedDataOffsetGlobal[tid];\n\n    uint32_t dataPointsIndex = 0;\n    uint32_t compressedDataIndex = 0;\n\n    uint8_t byteFlag;\n\n    for (int flagArrayIndex = 0; flagArrayIndex < flagArrSize; flagArrayIndex++)\n    {\n      byteFlag = flagArrGlobal[flagArrOffset + flagArrayIndex];\n\n      for (int bitCount = 0; bitCount < 8; bitCount++)\n      {\n        int matchFlag = (byteFlag >> bitCount) & 0x1;\n        if (matchFlag == 1)\n        {\n          int length = compressedDataGlobal[compressedDataOffset + compressedDataIndex];\n          int offset = compressedDataGlobal[compressedDataOffset + compressedDataIndex + 1];\n          compressedDataIndex += 2;\n          int dataPointsStart = dataPointsIndex;\n          for (int tmpDecompIndex = 0; tmpDecompIndex < length; tmpDecompIndex++)\n          {\n            output[tid * blockSize + dataPointsIndex] = output[tid * blockSize + dataPointsStart - offset + tmpDecompIndex];\n            dataPointsIndex++;\n          }\n        }\n        else\n        {\n          uint8_t *tmpPtr = (uint8_t *)&output[tid * blockSize + dataPointsIndex];\n          for (int tmpDecompIndex = 0; tmpDecompIndex < sizeof(T); tmpDecompIndex++)\n          {\n            *(tmpPtr + tmpDecompIndex) = compressedDataGlobal[compressedDataOffset + compressedDataIndex + tmpDecompIndex];\n          }\n\n          compressedDataIndex += sizeof(T);\n          dataPointsIndex++;\n        }\n        if (dataPointsIndex >= blockSize)\n        {\n          return;\n        }\n      }\n    }\n  }\n}"
        ]
    },
    "mf-sgd-cuda": {
        "/Users/gbolet/hecbench-roofline/src/mf-sgd-cuda/src/sgd.cu": [
            "__device__\nvoid LCG_random_init(unsigned int * seed) {\n  const unsigned int m = 2147483648;\n  const unsigned int a = 26757677;\n  const unsigned int c = 1;\n  *seed = (a * (*seed) + c) % m;\n}\n\n__global__ void init_rand_state(unsigned int seed, unsigned int *state, int size)\n{\n  int i = blockIdx.x*blockDim.x + threadIdx.x;\n  state[i] = seed ^ i;\n  if(i < size) LCG_random_init(state+i);\n}",
            "__device__\nunsigned int LCG_random(unsigned int * seed) {\n  const unsigned int m = 2147483648;\n  const unsigned int a = 26757677;\n  const unsigned int c = 1;\n  *seed = (a * (*seed) + c) % m;\n  return *seed;\n}\n\n__global__ void random_init(\n    unsigned int *__restrict__ state,\n    int state_size,\n    half *__restrict__ array,\n    long long array_size,\n    long long k, \n    float scale)\n{\n  int tid = blockIdx.x * blockDim.x + threadIdx.x;\n  int state_id = tid % state_size;\n  for(int i = 0;i < array_size;i += gridDim.x*blockDim.x)\n  {\n    int idx = i + tid;\n    if(idx >= array_size) break;\n    array[idx] = __float2half(LCG_random(state+state_id)*scale);\n  }\n}",
            "__global__ void transform_half(\n  const half *__restrict__ gpu_half_feature,\n  float *__restrict__ gpu_float_feature,\n  long long vec_size)\n{\n  int tid = blockIdx.x*blockDim.x + threadIdx.x;\n  int number_threads = gridDim.x*blockDim.x;\n\n  for(long long i = tid;i < vec_size;i += number_threads)\n  {\n    gpu_float_feature[i] = __half2float(gpu_half_feature[i]); \n  }\n}"
        ],
        "/Users/gbolet/hecbench-roofline/src/mf-sgd-cuda/src/sgd_k128_kernel_hogwild_warp32.h": [
            "__device__\nunsigned int LCG_random(unsigned int * seed) {\n  const unsigned int m = 2147483648;\n  const unsigned int a = 26757677;\n  const unsigned int c = 1;\n  *seed = (a * (*seed) + c) % m;\n  return *seed;\n}\n\n__device__ __forceinline__\nfloat4 __shfl_down(const float4 var, const uint32_t srcLane, const uint32_t width = 32) {\n  float4 output;\n  output.x = __shfl_down_sync(0xFFFFFFFF, var.x, srcLane, width);\n  output.y = __shfl_down_sync(0xFFFFFFFF, var.y, srcLane, width);\n  output.z = __shfl_down_sync(0xFFFFFFFF, var.z, srcLane, width);\n  output.w = __shfl_down_sync(0xFFFFFFFF, var.w, srcLane, width);\n  return output;\n}\n\n__global__ void sgd_k128_kernel_hogwild_warp32_lrate(\n    const mf_node *__restrict__ R,\n    long long nnz,\n    half *__restrict__ p,\n    half *__restrict__ q,\n    unsigned int *state,\n    const float *__restrict__ dynamic_rate,\n    long long u_seg,\n    long long v_seg,\n    int k,\n    int num_iters,\n    int current_iter,\n    int update_count_per_block, \n    int update_count_this_block,\n    int update_vector_size,\n    float lambda_p,\n    float lambda_q,\n    int u_grid,\n    int v_grid,\n    int u_id,\n    int v_id)\n{\n  //persistant thread\n  for(int ite = current_iter; ite < current_iter + num_iters; ite ++)\n  {\n    float tmp_lrate = __ldg(&dynamic_rate[ite]);\n\n    for(int update_ite = 0; update_ite < update_count_this_block; update_ite ++)\n    {\n\n      int lane_id = threadIdx.x%32;\n      int local_wid = threadIdx.x/32;\n      int wid = 4*blockIdx.x + local_wid;  \n\n      long long start_id = 0;\n      if(lane_id == 0)\n      {\n        long long origin = (long long)(LCG_random(state+wid)*nnz);\n        start_id = origin%nnz;\n      }\n      start_id = __shfl(start_id, 0);\n\n      for(int i = 0;i < update_vector_size;i++)\n      {\n        int offset = (start_id + i)%nnz;\n\n        float r = __ldg(&R[offset].rate);\n        int u = __ldg(&R[offset].u);\n        int v = __ldg(&R[offset].v);\n\n        //read the p & q into register file.\n        int base_p = u*k;\n        int base_q = v*k;\n\n        float tmp_p1 = __half2float(p[base_p + lane_id]);\n        float tmp_q1 = __half2float(q[base_q + lane_id]);\n\n        float tmp_p2 = __half2float(p[base_p + lane_id + 32]);\n        float tmp_q2 = __half2float(q[base_q + lane_id + 32]);\n\n        float tmp_p3 = __half2float(p[base_p + lane_id + 64]);\n        float tmp_q3 = __half2float(q[base_q + lane_id + 64]);\n\n        float tmp_p4 = __half2float(p[base_p + lane_id + 96]);\n        float tmp_q4 = __half2float(q[base_q + lane_id + 96]);\n\n        float tmp_product = tmp_p1*tmp_q1 + tmp_p2*tmp_q2 + tmp_p3*tmp_q3 + tmp_p4*tmp_q4;\n\n        //get dot product.\n        tmp_product += __shfl_down(tmp_product, 16);\n        tmp_product += __shfl_down(tmp_product, 8);\n        tmp_product += __shfl_down(tmp_product, 4);\n        tmp_product += __shfl_down(tmp_product, 2);\n        tmp_product += __shfl_down(tmp_product, 1);\n        tmp_product = __shfl(tmp_product,0);\n\n        float ruv = r - tmp_product;\n\n        //update\n        //only works for k=blockDim.x=128\n        p[base_p + lane_id +  0] = __float2half(tmp_p1 + tmp_lrate*(ruv*tmp_q1 - lambda_p*tmp_p1));\n        q[base_q + lane_id +  0] = __float2half(tmp_q1 + tmp_lrate*(ruv*tmp_p1 - lambda_q*tmp_q1));\n\n        p[base_p + lane_id + 32] = __float2half(tmp_p2 + tmp_lrate*(ruv*tmp_q2 - lambda_p*tmp_p2));\n        q[base_q + lane_id + 32] = __float2half(tmp_q2 + tmp_lrate*(ruv*tmp_p2 - lambda_q*tmp_q2));\n\n        p[base_p + lane_id + 64] = __float2half(tmp_p3 + tmp_lrate*(ruv*tmp_q3 - lambda_p*tmp_p3));\n        q[base_q + lane_id + 64] = __float2half(tmp_q3 + tmp_lrate*(ruv*tmp_p3 - lambda_q*tmp_q3));\n\n        p[base_p + lane_id + 96] = __float2half(tmp_p4 + tmp_lrate*(ruv*tmp_q4 - lambda_p*tmp_p4));\n        q[base_q + lane_id + 96] = __float2half(tmp_q4 + tmp_lrate*(ruv*tmp_p4 - lambda_q*tmp_q4));\n      }    \n    }\n  }\n}"
        ]
    },
    "car-cuda": {
        "/Users/gbolet/hecbench-roofline/src/car-cuda/main.cu": [
            "inline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\ninline __host__ __device__ float4 floorf(float4 v)\n{\n    return make_float4(floorf(v.x), floorf(v.y), floorf(v.z), floorf(v.w));\n}\n\ninline __host__ __device__ uint4 make_uint4(int4 a)\n{\n    return make_uint4(uint(a.x), uint(a.y), uint(a.z), uint(a.w));\n}\n\ninline __host__ __device__ uint4 max(uint4 a, uint4 b)\n{\n    return make_uint4(max(a.x,b.x), max(a.y,b.y), max(a.z,b.z), max(a.w,b.w));\n}\n\ninline __host__ __device__ uint4 min(uint4 a, uint4 b)\n{\n    return make_uint4(min(a.x,b.x), min(a.y,b.y), min(a.z,b.z), min(a.w,b.w));\n}\n\ninline __host__ __device__ float4 sqrtf(float4 v)\n{\n    return make_float4(sqrtf(v.x), sqrtf(v.y), sqrtf(v.z), sqrtf(v.w));\n}\n\n__device__ __forceinline__\nvoid operator+=(float4& output, const float4 value) {\n  output.x += value.x;\n  output.y += value.y;\n  output.z += value.z;\n  output.w += value.w;\n}\n\n__global__ void car (\n    const float *__restrict__ img,\n    const float *__restrict__ kernels,\n    const float *__restrict__ offsets_h,\n    const float *__restrict__ offsets_v,\n          float *__restrict__ output,\n    const params p,\n    const int offset_unit,\n    const int padding,\n    const size_t n)\n{\n  size_t global_idx = blockDim.x * blockIdx.x + threadIdx.x;\n  if(global_idx >= n) return;\n\n  const int dim_b = p.output_dim_b;\n  const int dim_c = p.output_dim_c;\n  const int dim_h = p.output_dim_h;\n  const int dim_w = p.output_dim_w;\n  const int kernels_size = p.kernel_size;\n  const int img_w = p.image_w;\n  const int img_h = p.image_h;\n\n  const size_t vol_size = (size_t)dim_c * dim_h * dim_w;\n  const size_t img_size = (size_t)dim_h * dim_w;\n\n  const int idb = (global_idx / vol_size) % dim_b;\n  const int idc = (global_idx / img_size) % dim_c;\n  const int idy = (global_idx / dim_w) % dim_h;\n  const int idx = global_idx % dim_w;\n\n  const int k_size = (int)sqrtf(float(kernels_size));\n  const int w = img_w - 2 * padding;\n  const int h = img_h - 2 * padding;\n\n  float result = 0;\n  for(int k_y = 0; k_y < k_size; ++k_y)\n  {\n    for(int k_x = 0; k_x < k_size; ++k_x)\n    {\n      const float offset_h = offsets_h(idb,k_size * k_y + k_x,idy,idx) * offset_unit;\n      const float offset_v = offsets_v(idb,k_size * k_y + k_x,idy,idx) * offset_unit;\n\n      const float p_x = static_cast<float>(idx + 0.5f) / dim_w * w + k_x + offset_h - 0.5f;\n      const float p_y = static_cast<float>(idy + 0.5f) / dim_h * h + k_y + offset_v - 0.5f;\n      const float alpha = p_x - floorf(p_x);\n      const float beta = p_y - floorf(p_y);\n\n      const int xL = max(min(int(floorf(p_x)), w + 2 * padding - 1), 0);\n      const int xR = max(min(xL + 1, w + 2 * padding - 1), 0);\n      const int yT = max(min(int(floorf(p_y)), h + 2 * padding - 1), 0);\n      const int yB = max(min(yT + 1, h + 2 * padding - 1), 0);\n\n      float val = (1.f - alpha) * (1.f - beta) * img(idb,idc,yT,xL);\n      val += alpha * (1.f - beta) * img(idb,idc,yT,xR);\n      val += (1.f - alpha) * beta * img(idb,idc,yB,xL);\n      val += alpha * beta * img(idb,idc,yB,xR);\n      result += val * kernels(idb,k_size * k_y + k_x,idy,idx);\n    }\n  }\n  output(idb,idc,idy,idx) = result;\n}"
        ]
    },
    "is-cuda": {
        "/Users/gbolet/hecbench-roofline/src/is-cuda/kernels.h": [
            "__device__ double randlc_device(double* X, const double* A)\n{\n  double T1, T2, T3, T4;\n  double A1;\n  double A2;\n  double X1;\n  double X2;\n  double Z;\n  int j;\n\n  /*\n   * --------------------------------------------------------------------\n   * break A into two parts such that A = 2^23 * A1 + A2 and set X = N.\n   * --------------------------------------------------------------------\n   */\n  T1 = R23 * *A;\n  j  = T1;\n  A1 = j;\n  A2 = *A - T23 * A1;\n\n  /*\n   * --------------------------------------------------------------------\n   * break X into two parts such that X = 2^23 * X1 + X2, compute\n   * Z = A1 * X2 + A2 * X1  (mod 2^23), and then\n   * X = 2^23 * Z + A2 * X2  (mod 2^46). \n   * --------------------------------------------------------------------\n   */\n  T1 = R23 * *X;\n  j  = T1;\n  X1 = j;\n  X2 = *X - T23 * X1;\n  T1 = A1 * X2 + A2 * X1;\n\n  j  = R23 * T1;\n  T2 = j;\n  Z = T1 - T23 * T2;\n  T3 = T23 * Z + A2 * X2;\n  j  = R46 * T3;\n  T4 = j;\n  *X = T3 - T46 * T4;\n\n  return(R46 * *X);\n}\n\n__device__ double find_my_seed_device(\n    int kn,\n    int np,\n    long nn,\n    double s,\n    double a)\n{\n  double t1,t2;\n  long mq,nq,kk,ik;\n\n  if(kn==0) return s;\n\n  mq = (nn/4 + np - 1) / np;\n  nq = mq * 4 * kn;\n\n  t1 = s;\n  t2 = a;\n  kk = nq;\n  while(kk > 1){\n    ik = kk / 2;\n    if(2*ik==kk){\n      (void)randlc_device(&t2, &t2);\n      kk = ik;\n    }else{\n      (void)randlc_device(&t1, &t2);\n      kk = kk - 1;\n    }\n  }\n  (void)randlc_device(&t1, &t2);\n\n  return(t1);\n}\n\n__global__ void create_seq_gpu_kernel(\n    int* key_array,\n    double seed,\n    double a,\n    int number_of_blocks,\n    int amount_of_work)\n{\n  double x, s;\n  double an = a;\n  int i, k;\n  int k1, k2;\n  int myid, num_procs;\n  int mq;\n\n  myid = blockIdx.x*blockDim.x+threadIdx.x;\n  num_procs = amount_of_work;\n\n  mq = (NUM_KEYS + num_procs - 1) / num_procs;\n  k1 = mq * myid;\n  k2 = k1 + mq;\n  if(k2 > NUM_KEYS) k2 = NUM_KEYS;\n\n  s = find_my_seed_device(myid, num_procs, (long)4*NUM_KEYS, seed, an);\n\n  k = MAX_KEY/4;\n\n  for(i=k1; i<k2; i++){\n    x = randlc_device(&s, &an);\n    x += randlc_device(&s, &an);\n    x += randlc_device(&s, &an);\n    x += randlc_device(&s, &an);  \n    key_array[i] = k*x;\n  }\n}",
            "__global__ void full_verify_gpu_kernel_1(\n    const int*__restrict__ key_array,\n    int*__restrict__ key_buff2,\n    int number_of_blocks,\n    int amount_of_work)\n{\n  int i = blockIdx.x*blockDim.x+threadIdx.x;\n  key_buff2[i] = key_array[i];\n}",
            "__global__ void full_verify_gpu_kernel_2(\n    const int* __restrict__ key_buff2,\n    int*__restrict__  key_buff_ptr_global,\n    int*__restrict__  key_array,\n    int number_of_blocks,\n    int amount_of_work)\n{    \n  int value = key_buff2[blockIdx.x*blockDim.x+threadIdx.x];\n  int index = atomicAdd(&key_buff_ptr_global[value], -1) - 1;\n  key_array[index] = value;\n}",
            "__global__ void full_verify_gpu_kernel_3(\n    const int*__restrict__ key_array,\n    int*__restrict__ global_aux,\n    int number_of_blocks,\n    int amount_of_work)\n{\n  extern __shared__ int shared_data[];\n\n  int i = (blockIdx.x*blockDim.x+threadIdx.x) + 1;\n\n  if(i < NUM_KEYS){\n    if(key_array[i-1] > key_array[i]) \n      shared_data[threadIdx.x]=1;\n    else\n      shared_data[threadIdx.x]=0;\n  }else\n    shared_data[threadIdx.x]=0;\n\n  __syncthreads();\n\n  for(i=blockDim.x/2; i>0; i>>=1) {\n    if(threadIdx.x<i)\n      shared_data[threadIdx.x] += shared_data[threadIdx.x+i];\n    __syncthreads();\n  }\n\n  if(threadIdx.x==0) global_aux[blockIdx.x]=shared_data[0];\n}",
            "__global__ void rank_gpu_kernel_1(\n    int*__restrict__ key_array,\n    int*__restrict__ partial_verify_vals,\n    const int*__restrict__ test_index_array,\n    int iteration,\n    int number_of_blocks,\n    int amount_of_work)\n{\n  key_array[iteration] = iteration;\n  key_array[iteration+MAX_ITERATIONS] = MAX_KEY - iteration;\n  /*\n   * --------------------------------------------------------------------\n   * determine where the partial verify test keys are, \n   * --------------------------------------------------------------------\n   * load into top of array bucket_size  \n   * --------------------------------------------------------------------\n   */\n#pragma unroll\n  for(int i=0; i<TEST_ARRAY_SIZE; i++){\n    partial_verify_vals[i] = key_array[test_index_array[i]];\n  }\n}",
            "__global__ void rank_gpu_kernel_2(\n    int* key_buff1,\n    int number_of_blocks,\n    int amount_of_work)\n{\n  key_buff1[blockIdx.x*blockDim.x+threadIdx.x] = 0;\n}",
            "__global__ void rank_gpu_kernel_3(\n    int*__restrict__ key_buff_ptr,\n    const int*__restrict__ key_buff_ptr2,\n    int number_of_blocks,\n    int amount_of_work)\n{\n  /*\n   * --------------------------------------------------------------------\n   * in this section, the keys themselves are used as their \n   * own indexes to determine how many of each there are: their\n   * individual population  \n   * --------------------------------------------------------------------\n   */\n  atomicAdd(&key_buff_ptr[key_buff_ptr2[blockIdx.x*blockDim.x+threadIdx.x]], 1);\n}",
            "__global__ void rank_gpu_kernel_4(\n    const int*__restrict__ source,\n    int*__restrict__ destiny,\n    int*__restrict__ sum,\n    int number_of_blocks,\n    int amount_of_work)\n{\n  extern __shared__ int shared_data[];\n\n  shared_data[threadIdx.x] = 0;\n  int position = blockDim.x + threadIdx.x;\n\n  int factor = MAX_KEY / number_of_blocks;\n  int start = factor * blockIdx.x;\n  int end = start + factor;\n\n  for(int i=start; i<end; i+=blockDim.x){\n    shared_data[position] = source[i + threadIdx.x];\n\n    for(uint offset=1; offset<blockDim.x; offset<<=1){\n      __syncthreads();\n      int t = shared_data[position] + shared_data[position - offset];\n      __syncthreads();\n      shared_data[position] = t;\n    }\n\n    int prv_val = (i == start) ? 0 : destiny[i - 1];\n    destiny[i + threadIdx.x] = shared_data[position] + prv_val;\n  }\n\n  __syncthreads();\n  if(threadIdx.x==0) sum[blockIdx.x] = destiny[end-1];\n}",
            "__global__ void rank_gpu_kernel_5(\n    const int*__restrict__ source,\n    int*__restrict__ destiny,\n    int number_of_blocks,\n    int amount_of_work)\n{\n  extern __shared__ int shared_data[];\n\n  shared_data[threadIdx.x] = 0;\n  int position = blockDim.x + threadIdx.x;\n  shared_data[position] = source[threadIdx.x];\n\n  for(uint offset=1; offset<blockDim.x; offset<<=1) {\n    __syncthreads();\n    int t = shared_data[position] + shared_data[position - offset];\n    __syncthreads();\n    shared_data[position] = t;\n  }\n\n  __syncthreads();\n\n  destiny[threadIdx.x] = shared_data[position - 1];\n}",
            "__global__ void rank_gpu_kernel_6(\n    const int*__restrict__ source,\n    int*__restrict__ destiny,\n    const int*__restrict__ offset,\n    int number_of_blocks,\n    int amount_of_work)\n{\n  int factor = MAX_KEY / number_of_blocks;\n  int start = factor * blockIdx.x;\n  int end = start + factor;\n  int sum = offset[blockIdx.x];\n  for(int i=start; i<end; i+=blockDim.x)\n    destiny[i + threadIdx.x] = source[i + threadIdx.x] + sum;\n}",
            "__global__ void rank_gpu_kernel_7(\n    const int*__restrict__ partial_verify_vals,\n    const int*__restrict__ key_buff_ptr,\n    const int*__restrict__ test_rank_array,\n    int*__restrict__ passed_verification_device,\n    int iteration,\n    int number_of_blocks,\n    int amount_of_work)\n{\n  /*\n   * --------------------------------------------------------------------\n   * this is the partial verify test section \n   * observe that test_rank_array vals are\n   * shifted differently for different cases\n   * --------------------------------------------------------------------\n   */\n  int i, k;\n  int passed_verification = 0;\n  for(i=0; i<TEST_ARRAY_SIZE; i++){  \n    /* test vals were put here on partial_verify_vals */                                           \n    k = partial_verify_vals[i];          \n    if(0<k && k<=NUM_KEYS-1){\n      int key_rank = key_buff_ptr[k-1];\n      int failed = 0;\n      switch(CLASS){\n        case 'S':\n          if(i<=2){\n            if(key_rank != test_rank_array[i]+iteration)\n              failed = 1;\n            else\n              passed_verification++;\n          }else{\n            if(key_rank != test_rank_array[i]-iteration)\n              failed = 1;\n            else\n              passed_verification++;\n          }\n          break;\n        case 'W':\n          if(i<2){\n            if(key_rank != test_rank_array[i]+(iteration-2))\n              failed = 1;\n            else\n              passed_verification++;\n          }else{\n            if(key_rank != test_rank_array[i]-iteration)\n              failed = 1;\n            else\n              passed_verification++;\n          }\n          break;\n        case 'A':\n          if(i<=2){\n            if(key_rank != test_rank_array[i]+(iteration-1))\n              failed = 1;\n            else\n              passed_verification++;\n          }else{\n            if(key_rank != test_rank_array[i]-(iteration-1))\n              failed = 1;\n            else\n              passed_verification++;\n          }\n          break;\n        case 'B':\n          if(i==1 || i==2 || i==4){\n            if(key_rank != test_rank_array[i]+iteration)\n              failed = 1;\n            else\n              passed_verification++;\n          }\n          else{\n            if(key_rank != test_rank_array[i]-iteration)\n              failed = 1;\n            else\n              passed_verification++;\n          }\n          break;\n        case 'C':\n          if(i<=2){\n            if(key_rank != test_rank_array[i]+iteration)\n              failed = 1;\n            else\n              passed_verification++;\n          }else{\n            if(key_rank != test_rank_array[i]-iteration)\n              failed = 1;\n            else\n              passed_verification++;\n          }\n          break;\n        case 'D':\n          if(i<2){\n            if(key_rank != test_rank_array[i]+iteration)\n              failed = 1;\n            else\n              passed_verification++;\n          }else{\n            if(key_rank != test_rank_array[i]-iteration)\n              failed = 1;\n            else\n              passed_verification++;\n          }\n          break;\n      }\n      if(failed==1){\n        printf(\"Failed partial verification: iteration %d, test key %d\\n\", iteration, (int)i);\n      }\n    }\n  }\n  *passed_verification_device += passed_verification;\n}"
        ]
    },
    "memtest-cuda": {
        "/Users/gbolet/hecbench-roofline/src/memtest-cuda/kernels.h": [
            "__global__\nvoid kernel0_write(char*__restrict__ ptr, unsigned long size)\n{\n  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  unsigned long n = size/BLOCKSIZE;\n  int total_num_threads = gridDim.x * blockDim.x;\n\n  for (int i = idx; i < n; i += total_num_threads) {\n    unsigned long * start_p = (unsigned long*)(ptr + i*BLOCKSIZE);\n    unsigned long * end_p = (unsigned long*)(ptr + (i+1)*BLOCKSIZE);\n    unsigned long * p = start_p;\n    unsigned int pattern = 1;\n    unsigned int mask = 8;\n\n    *p = pattern;\n    pattern = (pattern << 1);\n    while(p < end_p){\n      p = (unsigned long*) (((unsigned long)start_p)|mask);\n\n      if(p == start_p) {\n        mask = (mask << 1);\n        if (mask == 0) break;\n        continue;\n      }\n\n      if (p >= end_p) break;\n\n      *p = pattern;\n      pattern = pattern <<1;\n      mask = (mask << 1);\n      if (mask == 0) break;\n    }\n  }\n}",
            "__global__\nvoid kernel0_read(\n    const char*__restrict__ ptr, unsigned long size,\n    unsigned int*__restrict__ err_count,\n    unsigned long*__restrict__ err_addr,\n    unsigned long*__restrict__ err_expect,\n    unsigned long*__restrict__ err_current,\n    unsigned long*__restrict__ err_second_read)\n{\n  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  unsigned long n = size/BLOCKSIZE;\n  int total_num_threads = gridDim.x * blockDim.x;\n\n  for (int i = idx; i < n; i += total_num_threads) {\n    unsigned long * start_p= (unsigned long*)(ptr + i*BLOCKSIZE);\n    unsigned long * end_p = (unsigned long*)(ptr + (i+1)*BLOCKSIZE);\n    unsigned long * p =start_p;\n    unsigned int pattern = 1;\n    unsigned int mask = 8;\n\n    if (*p != pattern) RECORD_ERR(err_count, p, pattern, *p);\n\n    pattern = (pattern << 1);\n    while (p< end_p) {\n      p = ( unsigned long*)( ((unsigned long)start_p)|mask);\n\n      if(p == start_p) {\n        mask = (mask << 1);\n        if (mask == 0) break;\n        continue;\n      }\n\n      if (p >= end_p) break;\n\n      if (*p != pattern) RECORD_ERR(err_count, p, pattern, *p);\n\n      pattern = pattern <<1;\n      mask = (mask << 1);\n      if (mask == 0) break;\n    }\n  }\n}",
            "__global__\nvoid kernel1_write(char* ptr, unsigned long size)\n{\n  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n\n  unsigned long* buf = ( unsigned long*)ptr;\n  unsigned long n = size/sizeof(unsigned long);\n  int total_num_threads = gridDim.x * blockDim.x;\n\n  for (int i = idx; i < n; i += total_num_threads)\n    buf[i] = (unsigned long)(buf+i);\n}",
            "__global__\nvoid kernel1_read(\n    const char*__restrict__ ptr, unsigned long size,\n    unsigned int*__restrict__ err_count,\n    unsigned long*__restrict__ err_addr,\n    unsigned long*__restrict__ err_expect,\n    unsigned long*__restrict__ err_current,\n    unsigned long*__restrict__ err_second_read)\n{\n  unsigned long* buf = ( unsigned long*)ptr;\n  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  unsigned long n = size/sizeof(unsigned long);\n  int total_num_threads = gridDim.x * blockDim.x;\n\n  for (int i = idx; i < n; i += total_num_threads) {\n    if(buf[i] != (unsigned long)(buf+i))\n      RECORD_ERR(err_count, &buf[i], (buf+i), buf[i]);\n  }\n}",
            "#define TYPE unsigned long\n\n\n__global__\nvoid kernel_write(char* ptr, unsigned long size, TYPE p1)\n{\n  TYPE* buf = (TYPE*)ptr;\n  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  unsigned long n = size/sizeof(TYPE);\n  int total_num_threads = gridDim.x * blockDim.x;\n\n  for (int i = idx; i < n; i+= total_num_threads)\n    buf[i] = p1;\n}",
            "#define TYPE unsigned long\n\n\n__global__\nvoid kernel_read_write(\n    char*__restrict__ ptr, unsigned long size, TYPE p1, TYPE p2,\n    unsigned int*__restrict__ err_count,\n    unsigned long*__restrict__ err_addr,\n    unsigned long*__restrict__ err_expect,\n    unsigned long*__restrict__ err_current,\n    unsigned long*__restrict__ err_second_read)\n{\n  TYPE* buf = (TYPE*) ptr;\n  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  unsigned long n =  size/sizeof(TYPE);\n  int total_num_threads = gridDim.x * blockDim.x;\n  TYPE localp;\n\n  for (int i = idx; i < n; i += total_num_threads) {\n\n    localp = buf[i];\n\n    if (localp != p1) RECORD_ERR(err_count, &buf[i], p1, localp);\n\n    buf[i] = p2;\n  }\n}",
            "#define TYPE unsigned long\n\n\n__global__\nvoid kernel_read(\n    const char*__restrict__ ptr, unsigned long size, TYPE p1,\n    unsigned int*__restrict__ err_count,\n    unsigned long*__restrict__ err_addr,\n    unsigned long*__restrict__ err_expect,\n    unsigned long*__restrict__ err_current,\n    unsigned long*__restrict__ err_second_read)\n{\n  TYPE* buf = (TYPE*) ptr;\n  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  unsigned long n =  size/sizeof(TYPE);\n  int total_num_threads = gridDim.x * blockDim.x;\n  TYPE localp;\n\n  for (int i = idx; i < n; i += total_num_threads) {\n    localp = buf[i];\n    if (localp != p1) RECORD_ERR(err_count, &buf[i], p1, localp);\n  }\n}",
            "__global__\nvoid kernel5_init(char* ptr, unsigned long size)\n{\n  unsigned int * buf = (unsigned int*)ptr;\n  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  unsigned long n = size/64;\n  int total_num_threads = gridDim.x * blockDim.x;\n\n  unsigned int p1 = 1;\n  unsigned int p2;\n  p1 = p1 << (idx % 32);\n  p2 = ~p1;\n  for (int i = idx; i < n; i+= total_num_threads){\n    buf[i*16] = p1;\n    buf[i*16 + 1] = p1;\n    buf[i*16 + 2] = p2;\n    buf[i*16 + 3] = p2;\n    buf[i*16 + 4] = p1;\n    buf[i*16 + 5] = p1;\n    buf[i*16 + 6] = p2;\n    buf[i*16 + 7] = p2;\n    buf[i*16 + 8] = p1;\n    buf[i*16 + 9] = p1;\n    buf[i*16 + 10] = p2;\n    buf[i*16 + 11] = p2;\n    buf[i*16 + 12] = p1;\n    buf[i*16 + 13] = p1;\n    buf[i*16 + 14] = p2;\n    buf[i*16 + 15] = p2;\n  }\n}",
            "__global__\nvoid kernel5_move(char* ptr, unsigned long size)\n{\n  int i, j;\n  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  unsigned long n = size/BLOCKSIZE;\n  int total_num_threads = gridDim.x * blockDim.x;\n  unsigned int half_count = BLOCKSIZE/sizeof(unsigned int)/2;\n  for (i = idx; i < n; i+= total_num_threads){\n    unsigned int* mybuf = (unsigned int*)(ptr + i*BLOCKSIZE);\n    unsigned int* mybuf_mid = (unsigned int*)(ptr + i*BLOCKSIZE + BLOCKSIZE/2);\n    for (j = 0; j < half_count; j++)\n      mybuf_mid[j] = mybuf[j];\n\n    for (j = 0;j < half_count -8; j++)\n      mybuf[j+8] = mybuf_mid[j];\n\n    for (j = 0; j < 8; j++)\n      mybuf[j] = mybuf_mid[half_count - 8+j];\n  }\n}",
            "__global__\nvoid kernel5_check(\n    const char*__restrict__ ptr, unsigned long size,\n    unsigned int*__restrict__ err_count,\n    unsigned long*__restrict__ err_addr,\n    unsigned long*__restrict__ err_expect,\n    unsigned long*__restrict__ err_current,\n    unsigned long*__restrict__ err_second_read)\n{\n  unsigned int * buf = (unsigned int*)ptr;\n  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  unsigned long n = size/(2*sizeof(unsigned int));\n  int total_num_threads = gridDim.x * blockDim.x;\n\n  for (int i = idx; i < n; i += total_num_threads) {\n    if (buf[2*i] != buf[2*i+1])\n      RECORD_ERR(err_count, &buf[2*i], buf[2*i+1], buf[2*i]);\n  }\n}"
        ]
    },
    "inversek2j-cuda": {
        "/Users/gbolet/hecbench-roofline/src/inversek2j-cuda/main.cu": [
            "inline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\ninline __host__ __device__ float4 sqrtf(float4 v)\n{\n    return make_float4(sqrtf(v.x), sqrtf(v.y), sqrtf(v.z), sqrtf(v.w));\n}\n\n__global__ void invkin_kernel(\n  const float *__restrict__ xTarget_in,\n  const float *__restrict__ yTarget_in,\n        float *__restrict__ angles,\n  int size)\n{\n  int blockId = blockIdx.x + blockIdx.y * gridDim.x; \n  int idx = blockId * (blockDim.x * blockDim.y) + (threadIdx.y * blockDim.x) + threadIdx.x;\n\n  if(idx < size)\n  {  \n    float angle_out[NUM_JOINTS];\n    float curr_xTargetIn = xTarget_in[idx];\n    float curr_yTargetIn = yTarget_in[idx];\n\n    for(int i = 0; i < NUM_JOINTS; i++)\n    {\n      angle_out[i] = 0.0;\n    }\n\n    float angle;\n    // Initialize x and y data\n    float xData[NUM_JOINTS_P1];\n    float yData[NUM_JOINTS_P1];\n\n    for (int i = 0 ; i < NUM_JOINTS_P1; i++)\n    {\n      xData[i] = i;\n      yData[i] = 0.f;\n    }\n\n    for(int curr_loop = 0; curr_loop < MAX_LOOP; curr_loop++)\n    {\n      for (int iter = NUM_JOINTS; iter > 0; iter--) \n      {\n        float pe_x = xData[NUM_JOINTS];\n        float pe_y = yData[NUM_JOINTS];\n        float pc_x = xData[iter-1];\n        float pc_y = yData[iter-1];\n        float diff_pe_pc_x = pe_x - pc_x;\n        float diff_pe_pc_y = pe_y - pc_y;\n        float diff_tgt_pc_x = curr_xTargetIn - pc_x;\n        float diff_tgt_pc_y = curr_yTargetIn - pc_y;\n        float len_diff_pe_pc = sqrtf(diff_pe_pc_x * diff_pe_pc_x + diff_pe_pc_y * diff_pe_pc_y);\n        float len_diff_tgt_pc = sqrtf(diff_tgt_pc_x * diff_tgt_pc_x + diff_tgt_pc_y * diff_tgt_pc_y);\n        float a_x = diff_pe_pc_x / len_diff_pe_pc;\n        float a_y = diff_pe_pc_y / len_diff_pe_pc;\n        float b_x = diff_tgt_pc_x / len_diff_tgt_pc;\n        float b_y = diff_tgt_pc_y / len_diff_tgt_pc;\n        float a_dot_b = a_x * b_x + a_y * b_y;\n        if (a_dot_b > 1.f)\n          a_dot_b = 1.f;\n        else if (a_dot_b < -1.f)\n          a_dot_b = -1.f;\n        angle = acosf(a_dot_b) * (180.f / PI);\n        // Determine angle direction\n        float direction = a_x * b_y - a_y * b_x;\n        if (direction < 0.f)\n          angle = -angle;\n        // Make the result look more natural (these checks may be omitted)\n        if (angle > 30.f)\n          angle = 30.f;\n        else if (angle < -30.f)\n          angle = -30.f;\n        // Save angle\n        angle_out[iter - 1] = angle;\n        for (int i = 0; i < NUM_JOINTS; i++) \n        {\n          if(i < NUM_JOINTS - 1)\n          {\n            angle_out[i+1] += angle_out[i];\n          }\n        }\n      }\n    }\n\n    angles[idx * NUM_JOINTS + 0] = angle_out[0];\n    angles[idx * NUM_JOINTS + 1] = angle_out[1];\n    angles[idx * NUM_JOINTS + 2] = angle_out[2];\n  }\n}"
        ]
    },
    "compute-score-cuda": {
        "/Users/gbolet/hecbench-roofline/src/compute-score-cuda/main.cu": [
            "__global__ void\nreduction( const ulong* __restrict__ docInfo,\n           const uint*  __restrict__ partial_highbits_dimm1,\n           const uint*  __restrict__ partial_lowbits_dimm2,\n                 ulong* __restrict__ result)\n{\n  int gid = blockIdx.x * blockDim.x + threadIdx.x;\n  ulong info = docInfo[gid];\n  unsigned start = info >> 32;\n  unsigned end = info & 0xFFFFFFFF;\n\n  ulong total = 0;\n  #pragma unroll 2\n  for (unsigned i=start; i<=end; i++) {\n    ulong upper = partial_highbits_dimm1[i];\n    ulong lower = partial_lowbits_dimm2[i];\n    ulong sum = (upper << 32) | lower;\n    total += sum;\n  }\n  result[gid] = total;\n}"
        ]
    },
    "asta-cuda": {
        "/Users/gbolet/hecbench-roofline/src/asta-cuda/main.cu": [
            "#define T ((int)32)\n\n\n__global__ void PTTWAC_soa_asta(const int A, \n                                const int B, \n                                const int b, \n                                  T *__restrict__ input, \n                                int *__restrict__ finished, \n                                int *__restrict__ head) \n{\n  __shared__ int lmem[2];\n\n  const int tid = threadIdx.x;\n  int       m   = A * B - 1;\n\n  if(tid == 0) // Dynamic fetch\n    lmem[1] = atomicAdd(&head[0], 1);\n  __syncthreads();\n\n  while(lmem[1] < m) {\n    int next_in_cycle = (lmem[1] * A) - m * (lmem[1] / B);\n    if(next_in_cycle == lmem[1]) {\n      if(tid == 0) // Dynamic fetch\n        lmem[1] = atomicAdd(&head[0], 1);\n      __syncthreads();\n      continue;\n    }\n    T   data1, data2, data3, data4;\n    int i = tid;\n    if(i < b)\n      data1 = input[lmem[1] * b + i];\n    i += blockDim.x;\n    if(i < b)\n      data2 = input[lmem[1] * b + i];\n    i += blockDim.x;\n    if(i < b)\n      data3 = input[lmem[1] * b + i];\n    i += blockDim.x;\n    if(i < b)\n      data4 = input[lmem[1] * b + i];\n\n    if(tid == 0) {\n      //make sure the read is not cached\n      lmem[0] = atomicAdd(&finished[lmem[1]], 0);\n    }\n    __syncthreads();\n\n    for(; lmem[0] == 0; next_in_cycle = (next_in_cycle * A) - m * (next_in_cycle / B)) {\n      T backup1, backup2, backup3, backup4;\n      i = tid;\n      if(i < b)\n        backup1 = input[next_in_cycle * b + i];\n      i += blockDim.x;\n      if(i < b)\n        backup2 = input[next_in_cycle * b + i];\n      i += blockDim.x;\n      if(i < b)\n        backup3 = input[next_in_cycle * b + i];\n      i += blockDim.x;\n      if(i < b)\n        backup4 = input[next_in_cycle * b + i];\n\n      if(tid == 0) {\n        lmem[0] = atomicExch(&finished[next_in_cycle], (int)1);\n      }\n      __syncthreads();\n\n      if(!lmem[0]) {\n        i = tid;\n        if(i < b)\n          input[next_in_cycle * b + i] = data1;\n        i += blockDim.x;\n        if(i < b)\n          input[next_in_cycle * b + i] = data2;\n        i += blockDim.x;\n        if(i < b)\n          input[next_in_cycle * b + i] = data3;\n        i += blockDim.x;\n        if(i < b)\n          input[next_in_cycle * b + i] = data4;\n      }\n      i = tid;\n      if(i < b)\n        data1 = backup1;\n      i += blockDim.x;\n      if(i < b)\n        data2 = backup2;\n      i += blockDim.x;\n      if(i < b)\n        data3 = backup3;\n      i += blockDim.x;\n      if(i < b)\n        data4 = backup4;\n    }\n\n    if(tid == 0) // Dynamic fetch\n      lmem[1] = atomicAdd(&head[0], 1);\n    __syncthreads();\n  }\n}"
        ]
    },
    "bspline-vgh-cuda": {
        "/Users/gbolet/hecbench-roofline/src/bspline-vgh-cuda/kernel.h": [
            "__device__\nstatic inline void eval_UBspline_3d_s_vgh (\n    const float *__restrict__ coefs_init,\n    const intptr_t xs,\n    const intptr_t ys,\n    const intptr_t zs,\n    float *__restrict__ vals,\n    float *__restrict__ grads,\n    float *__restrict__ hess,\n    const float *a,\n    const float *b,\n    const float *c,\n    const float *da,\n    const float *db,\n    const float *dc,\n    const float *d2a,\n    const float *d2b,\n    const float *d2c,\n    const float dxInv,\n    const float dyInv,\n    const float dzInv)\n{\n  float h[9];\n  float v0 = 0.f;\n  for (int i = 0; i < 9; ++i) h[i] = 0.f;\n\n  for (int i=0; i<4; i++)\n    for (int j=0; j<4; j++){\n      float pre20 = d2a[i]*  b[j];\n      float pre10 =  da[i]*  b[j];\n      float pre00 =   a[i]*  b[j];\n      float pre11 =  da[i]* db[j];\n      float pre01 =   a[i]* db[j];\n      float pre02 =   a[i]*d2b[j];\n\n      const float *coefs = coefs_init + i*xs + j*ys;\n\n      float sum0 =   c[0] * coefs[0] +   c[1] * coefs[zs] +   c[2] * coefs[zs*2] +   c[3] * coefs[zs*3];\n      float sum1 =  dc[0] * coefs[0] +  dc[1] * coefs[zs] +  dc[2] * coefs[zs*2] +  dc[3] * coefs[zs*3];\n      float sum2 = d2c[0] * coefs[0] + d2c[1] * coefs[zs] + d2c[2] * coefs[zs*2] + d2c[3] * coefs[zs*3];\n\n      h[0]  += pre20 * sum0;\n      h[1]  += pre11 * sum0;\n      h[2]  += pre10 * sum1;\n      h[4]  += pre02 * sum0;\n      h[5]  += pre01 * sum1;\n      h[8]  += pre00 * sum2;\n      h[3]  += pre10 * sum0;\n      h[6]  += pre01 * sum0;\n      h[7]  += pre00 * sum1;\n      v0    += pre00 * sum0;\n    }\n  vals[0] = v0;\n  grads[0]  = h[3] * dxInv;\n  grads[1]  = h[6] * dyInv;\n  grads[2]  = h[7] * dzInv;\n\n  hess [0] = h[0]*dxInv*dxInv;\n  hess [1] = h[1]*dxInv*dyInv;\n  hess [2] = h[2]*dxInv*dzInv;\n  hess [3] = h[1]*dxInv*dyInv; // Copy hessian elements into lower half of 3x3 matrix\n  hess [4] = h[4]*dyInv*dyInv;\n  hess [5] = h[5]*dyInv*dzInv;\n  hess [6] = h[2]*dxInv*dzInv; // Copy hessian elements into lower half of 3x3 matrix\n  hess [7] = h[5]*dyInv*dzInv; //Copy hessian elements into lower half of 3x3 matrix\n  hess [8] = h[8]*dzInv*dzInv;\n}\n\n__global__ void bspline (\n    const float *__restrict__ spline_coefs,\n    const intptr_t xs,\n    const intptr_t ys,\n    const intptr_t zs,\n          float*__restrict__ walkers_vals, \n          float*__restrict__ walkers_grads,\n          float*__restrict__ walkers_hess,\n    const float*__restrict__ a,\n    const float*__restrict__ b,\n    const float*__restrict__ c,\n    const float*__restrict__ da,\n    const float*__restrict__ db,\n    const float*__restrict__ dc,\n    const float*__restrict__ d2a,\n    const float*__restrict__ d2b,\n    const float*__restrict__ d2c,\n    const float spline_x_grid_delta_inv, \n    const float spline_y_grid_delta_inv, \n    const float spline_z_grid_delta_inv,\n    const int spline_num_splines,\n    const int i,\n    const int ix,\n    const int iy,\n    const int iz) \n{\n  int n = blockIdx.x * blockDim.x + threadIdx.x;\n  if (n < spline_num_splines)\n    eval_UBspline_3d_s_vgh ( \n        spline_coefs+ix*xs+iy*ys+iz*zs+n,\n        xs, ys, zs, \n        walkers_vals+i*NSIZE+n,\n        walkers_grads+i*MSIZE+n*3,\n        walkers_hess+i*OSIZE+n*9,\n        a,\n        b,\n        c,\n        da,\n        db,\n        dc,\n        d2a,\n        d2b,\n        d2c,\n        spline_x_grid_delta_inv,\n        spline_y_grid_delta_inv,\n        spline_z_grid_delta_inv );\n}"
        ]
    },
    "mnist-cuda": {
        "/Users/gbolet/hecbench-roofline/src/mnist-cuda/kernels.h": [
            "inline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\ninline __host__ __device__ float4 expf(float4 v)\n{\n    return make_float4(expf(v.x), expf(v.y), expf(v.z), expf(v.w));\n}\n\n__device__ float step_function(float v)\n{\n  return 1.f / (1.f + expf(-v));\n}\n\n__global__ void apply_step_function(float *input, float *output, const int N)\n{\n  const int pos = blockIdx.x * blockDim.x + threadIdx.x;\n  const int size = blockDim.x * gridDim.x;\n\n  for (int idx = N * pos / size; idx < N * (pos+1) / size; ++idx) {\n    output[idx] = step_function(input[idx]);\n  }\n}",
            "__global__ void makeError(float *err, float *output, unsigned int Y, const int N)\n{\n  const int pos = blockIdx.x * blockDim.x + threadIdx.x;\n  const int size = blockDim.x * gridDim.x;\n\n  for (int idx = N * pos / size; idx < N * (pos+1) / size; ++idx) {\n    err[idx] = ((Y == idx ? 1.0f : 0.0f) - output[idx]);\n  }\n}",
            "__global__ void apply_grad(float *output, float *grad, const int N)\n{\n  const int pos = blockIdx.x * blockDim.x + threadIdx.x;\n  const int size = blockDim.x * gridDim.x;\n\n  for (int idx = N * pos / size; idx < N * (pos+1) / size; ++idx) {\n    output[idx] += dt * grad[idx];\n  }\n}",
            "__global__ void fp_preact_c1(float input[28][28], float preact[6][24][24], float weight[6][5][5])\n{\n  const int pos = blockIdx.x * blockDim.x + threadIdx.x;\n  const int size = blockDim.x * gridDim.x;\n  const int N = 5*5*6*24*24;\n\n  for (int n = N * pos / size; n < N * (pos+1) / size; ++n) {\n    int idx = n;\n    const int i1 = ((idx /= 1  ) % 5);\n    const int i2 = ((idx /= 5  ) % 5);\n    const int i3 = ((idx /= 5  ) % 6);\n    const int i4 = ((idx /= 6  ) % 24);\n    const int i5 = ((idx /= 24  ) % 24);\n    atomicAdd(&preact[i3][i4][i5], weight[i3][i1][i2] * input[i4 + i1][i5 + i2]);\n  }\n}",
            "__global__ void fp_bias_c1(float preact[6][24][24], float bias[6])\n{\n  const int pos = blockIdx.x * blockDim.x + threadIdx.x;\n  const int size = blockDim.x * gridDim.x;\n  const int N = 6*24*24;\n\n  for (int n = N * pos / size; n < N * (pos+1) / size; ++n) {\n    int idx = n;\n    const int i1 = ((idx /= 1  ) % 6);\n    const int i2 = ((idx /= 6  ) % 24);\n    const int i3 = ((idx /= 24  ) % 24);\n    preact[i1][i2][i3] += bias[i1];\n  }\n}",
            "__global__ void fp_preact_s1(float input[6][24][24], float preact[6][6][6], float weight[1][4][4])\n{\n  const int pos = blockIdx.x * blockDim.x + threadIdx.x;\n  const int size = blockDim.x * gridDim.x;\n  const int N = 4*4*6*6*6;\n\n  for (int n = N * pos / size; n < N * (pos+1) / size; ++n) {\n    int idx = n;\n    const int i1 = ((idx /= 1  ) % 4);\n    const int i2 = ((idx /= 4  ) % 4);\n    const int i3 = ((idx /= 4  ) % 6);\n    const int i4 = ((idx /= 6  ) % 6);\n    const int i5 = ((idx /= 6  ) % 6);\n    atomicAdd(&preact[i3][i4][i5], weight[0][i1][i2] * input[i3][i4 * 4 + i1][i5 * 4 + i2]);\n  }\n}",
            "__global__ void fp_bias_s1(float preact[6][6][6], float bias[1])\n{\n  const int pos = blockIdx.x * blockDim.x + threadIdx.x;\n  const int size = blockDim.x * gridDim.x;\n  const int N = 6*6*6;\n\n  for (int n = N * pos / size; n < N * (pos+1) / size; ++n) {\n    int idx = n;\n    const int i1 = ((idx /= 1  ) % 6);\n    const int i2 = ((idx /= 6  ) % 6);\n    const int i3 = ((idx /= 6  ) % 6);\n    preact[i1][i2][i3] += bias[0];\n  }\n}",
            "__global__ void fp_preact_f(float input[6][6][6], float preact[10], float weight[10][6][6][6])\n{\n  const int pos = blockIdx.x * blockDim.x + threadIdx.x;\n  const int size = blockDim.x * gridDim.x;\n  const int N = 10*6*6*6;\n\n  for (int n = N * pos / size; n < N * (pos+1) / size; ++n) {\n    int idx = n;\n    const int i1 = ((idx /= 1  ) % 10);\n    const int i2 = ((idx /= 10  ) % 6);\n    const int i3 = ((idx /= 6  ) % 6);\n    const int i4 = ((idx /= 6  ) % 6);\n    atomicAdd(&preact[i1], weight[i1][i2][i3][i4] * input[i2][i3][i4]);\n  }\n}",
            "__global__ void fp_bias_f(float preact[10], float bias[10])\n{\n  const int pos = blockIdx.x * blockDim.x + threadIdx.x;\n  const int size = blockDim.x * gridDim.x;\n  const int N = 10;\n  for (int idx = N * pos / size; idx < N * (pos+1) / size; ++idx) {\n    preact[idx] += bias[idx];\n  }\n}",
            "__global__ void bp_weight_f(float d_weight[10][6][6][6], float d_preact[10], float p_output[6][6][6])\n{\n  const int pos = blockIdx.x * blockDim.x + threadIdx.x;\n  const int size = blockDim.x * gridDim.x;\n  const int N = 10*6*6*6;\n\n  for (int n = N * pos / size; n < N * (pos+1) / size; ++n) {\n    int idx = n;\n    const int i1 = ((idx /= 1  ) % 10);\n    const int i2 = ((idx /= 10  ) % 6);\n    const int i3 = ((idx /= 6  ) % 6);\n    const int i4 = ((idx /= 6  ) % 6);\n    d_weight[i1][i2][i3][i4] = d_preact[i1] * p_output[i2][i3][i4];\n  }\n}",
            "__global__ void bp_bias_f(float bias[10], float d_preact[10])\n{\n  const int pos = blockIdx.x * blockDim.x + threadIdx.x;\n  const int size = blockDim.x * gridDim.x;\n  const int N = 10;\n  for (int idx = N * pos / size; idx < N * (pos+1) / size; ++idx) {\n    bias[idx] += dt * d_preact[idx];\n  }\n}",
            "__global__ void bp_output_s1(float d_output[6][6][6], float n_weight[10][6][6][6], float nd_preact[10])\n{\n  const int pos = blockIdx.x * blockDim.x + threadIdx.x;\n  const int size = blockDim.x * gridDim.x;\n  const int N = 10*6*6*6;\n\n  for (int n = N * pos / size; n < N * (pos+1) / size; ++n) {\n    int idx = n;\n    const int i1 = ((idx /= 1  ) % 10);\n    const int i2 = ((idx /= 10  ) % 6);\n    const int i3 = ((idx /= 6  ) % 6);\n    const int i4 = ((idx /= 6  ) % 6);\n    atomicAdd(&d_output[i2][i3][i4], n_weight[i1][i2][i3][i4] * nd_preact[i1]);\n  }\n}",
            "inline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\ninline __host__ __device__ float4 expf(float4 v)\n{\n    return make_float4(expf(v.x), expf(v.y), expf(v.z), expf(v.w));\n}\n\n__device__ float step_function(float v)\n{\n  return 1.f / (1.f + expf(-v));\n}\n\n__global__ void bp_preact_s1(float d_preact[6][6][6], float d_output[6][6][6], float preact[6][6][6])\n{\n  const int pos = blockIdx.x * blockDim.x + threadIdx.x;\n  const int size = blockDim.x * gridDim.x;\n  const int N = 6*6*6;\n\n  for (int n = N * pos / size; n < N * (pos+1) / size; ++n) {\n    int idx = n;\n    const int i1 = ((idx /= 1  ) % 6);\n    const int i2 = ((idx /= 6  ) % 6);\n    const int i3 = ((idx /= 6  ) % 6);\n    const float o = step_function(preact[i1][i2][i3]);\n    d_preact[i1][i2][i3] = d_output[i1][i2][i3] * o * (1 - o);\n  }\n}",
            "__global__ void bp_weight_s1(float d_weight[1][4][4], float d_preact[6][6][6], float p_output[6][24][24])\n{\n  const int pos = blockIdx.x * blockDim.x + threadIdx.x;\n  const int size = blockDim.x * gridDim.x;\n  const int N = 1*4*4*6*6*6;\n\n  for (int n = N * pos / size; n < N * (pos+1) / size; ++n) {\n    int idx = n;\n    const int i1 = ((idx /= 1  ) % 1);\n    const int i2 = ((idx /= 1  ) % 4);\n    const int i3 = ((idx /= 4  ) % 4);\n    const int i4 = ((idx /= 4  ) % 6);\n    const int i5 = ((idx /= 6  ) % 6);\n    const int i6 = ((idx /= 6  ) % 6);\n    atomicAdd(&d_weight[i1][i2][i3], d_preact[i4][i5][i6] * p_output[i4][i5 * 4 + i2][i6 * 4 + i3]);\n  }\n}",
            "__global__ void bp_bias_s1(float bias[1], float d_preact[6][6][6])\n{\n  const int pos = blockIdx.x * blockDim.x + threadIdx.x;\n  const int size = blockDim.x * gridDim.x;\n  const int N = 6*6*6;\n  const float d = 216; //pow(6.0f, 3.0f);\n\n  for (int n = N * pos / size; n < N * (pos+1) / size; ++n) {\n    int idx = n;\n    const int i1 = ((idx /= 1  ) % 6);\n    const int i2 = ((idx /= 6  ) % 6);\n    const int i3 = ((idx /= 6  ) % 6);\n    atomicAdd(&bias[0], dt * d_preact[i1][i2][i3] / d);\n  }\n}",
            "__global__ void bp_output_c1(float d_output[6][24][24], float n_weight[1][4][4], float nd_preact[6][6][6])\n{\n  const int pos = blockIdx.x * blockDim.x + threadIdx.x;\n  const int size = blockDim.x * gridDim.x;\n  const int N = 1*4*4*6*6*6;\n\n  for (int n = N * pos / size; n < N * (pos+1) / size; ++n) {\n    int idx = n;\n    const int i1 = ((idx /= 1  ) % 1);\n    const int i2 = ((idx /= 1  ) % 4);\n    const int i3 = ((idx /= 4  ) % 4);\n    const int i4 = ((idx /= 4  ) % 6);\n    const int i5 = ((idx /= 6  ) % 6);\n    const int i6 = ((idx /= 6  ) % 6);\n    atomicAdd(&d_output[i4][i5 * 4 + i2][i6 * 4 + i3], n_weight[i1][i2][i3] * nd_preact[i4][i5][i6]);\n  }\n}",
            "inline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\ninline __host__ __device__ float4 expf(float4 v)\n{\n    return make_float4(expf(v.x), expf(v.y), expf(v.z), expf(v.w));\n}\n\n__device__ float step_function(float v)\n{\n  return 1.f / (1.f + expf(-v));\n}\n\n__global__ void bp_preact_c1(float d_preact[6][24][24], float d_output[6][24][24], float preact[6][24][24])\n{\n  const int pos = blockIdx.x * blockDim.x + threadIdx.x;\n  const int size = blockDim.x * gridDim.x;\n  const int N = 6*24*24;\n\n  for (int n = N * pos / size; n < N * (pos+1) / size; ++n) {\n    int idx = n;\n    const int i1 = ((idx /= 1  ) % 6);\n    const int i2 = ((idx /= 6  ) % 24);\n    const int i3 = ((idx /= 24  ) % 24);\n    const float o = step_function(preact[i1][i2][i3]);\n    d_preact[i1][i2][i3] = d_output[i1][i2][i3] * o * (1 - o);\n  }\n}",
            "__global__ void bp_weight_c1(float d_weight[6][5][5], float d_preact[6][24][24], float p_output[28][28])\n{\n  const int pos = blockIdx.x * blockDim.x + threadIdx.x;\n  const int size = blockDim.x * gridDim.x;\n  const int N = 6*5*5*24*24;\n  const float d = 576; //pow(24.0f, 2.0f);\n\n  for (int n = N * pos / size; n < N * (pos+1) / size; ++n) {\n    int idx = n;\n    const int i1 = ((idx /= 1  ) % 6);\n    const int i2 = ((idx /= 6  ) % 5);\n    const int i3 = ((idx /= 5  ) % 5);\n    const int i4 = ((idx /= 5  ) % 24);\n    const int i5 = ((idx /= 24  ) % 24);\n    atomicAdd(&d_weight[i1][i2][i3], d_preact[i1][i4][i5] * p_output[i4 + i2][i5 + i3] / d);\n  }\n}",
            "__global__ void bp_bias_c1(float bias[6], float d_preact[6][24][24])\n{\n  const int pos = blockIdx.x * blockDim.x + threadIdx.x;\n  const int size = blockDim.x * gridDim.x;\n  const int N = 6*24*24;\n  const float d = 576; //pow(24.0f, 2.0f);\n\n  for (int n = N * pos / size; n < N * (pos+1) / size; ++n) {\n    int idx = n;\n    const int i1 = ((idx /= 1  ) % 6);\n    const int i2 = ((idx /= 6  ) % 24);\n    const int i3 = ((idx /= 24  ) % 24);\n    atomicAdd(&bias[i1], dt * d_preact[i1][i2][i3] / d);\n  }\n}"
        ]
    },
    "atan2-cuda": {
        "/Users/gbolet/hecbench-roofline/src/atan2-cuda/main.cu": [
            "__device__ __host__\nconstexpr float approx_atan2f_P<15>(float x) {\n  auto z = x * x;\n  return x * (float(-0xf.ffff4p-4) +\n              z * (float(0x5.552f9p-4 + z * (float(-0x3.30f728p-4) +\n                                             z * (float(0x2.39826p-4) +\n                                                  z * (float(-0x1.8a880cp-4) +\n                                                       z * (float(0xe.484d6p-8) +\n                                                            z * (float(-0x5.93d5p-8) + z * float(0x1.0875dcp-8)))))))));\n}\n\n__device__ __host__\nconstexpr float unsafe_atan2f_impl(float y, float x) {\n  constexpr float pi4f = 3.1415926535897932384626434 / 4;\n  constexpr float pi34f = 3.1415926535897932384626434 * 3 / 4;\n\n  auto r = (std::abs(x) - std::abs(y)) / (std::abs(x) + std::abs(y));\n  if (x < 0)\n    r = -r;\n\n  auto angle = (x >= 0) ? pi4f : pi34f;\n  angle += approx_atan2f_P<DEGREE>(r);\n\n  return ((y < 0)) ? -angle : angle;\n}\n\n__device__ __host__\nconstexpr float safe_atan2f(float y, float x) {\n  return unsafe_atan2f_impl<DEGREE>(y, ((y == 0.f) & (x == 0.f)) ? 0.2f : x);\n}\n\n__global__\nvoid compute_f (const int n,\n                const float *x,\n                const float *y,\n                      float *r)\n{\n  const int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i >= n) return;\n  const float vy = y[i];\n  const float vx = x[i];\n  r[i] = safe_atan2f< 3>(vy, vx) +\n         safe_atan2f< 5>(vy, vx) +\n         safe_atan2f< 7>(vy, vx) +\n         safe_atan2f< 9>(vy, vx) +\n         safe_atan2f<11>(vy, vx) +\n         safe_atan2f<13>(vy, vx) +\n         safe_atan2f<15>(vy, vx);\n}",
            "__device__ __host__\nconstexpr float approx_atan2s_P<9>(float x) {\n  auto z = x * x;\n  return x * ((-10428.984375f) + z * (3445.20654296875f + z * ((-1879.137939453125f) +\n                                                               z * (888.22314453125f + z * (-217.42669677734375f)))));\n}\n\n__device__ __host__\nconstexpr short unsafe_atan2s_impl(float y, float x) {\n  constexpr int maxshort = (int)(std::numeric_limits<short>::max()) + 1;\n  constexpr short pi4 = short(maxshort / 4);\n  constexpr short pi34 = short(3 * maxshort / 4);\n\n  auto r = (std::abs(x) - std::abs(y)) / (std::abs(x) + std::abs(y));\n  if (x < 0)\n    r = -r;\n\n  auto angle = (x >= 0) ? pi4 : pi34;\n  angle += short(approx_atan2s_P<DEGREE>(r));\n\n  return (y < 0) ? -angle : angle;\n}\n\n__device__ __host__\nconstexpr short unsafe_atan2s(float y, float x) {\n  return unsafe_atan2s_impl<DEGREE>(y, x);\n}\n\n__global__\nvoid compute_s (const int n,\n                const float *x,\n                const float *y,\n                      short *r)\n{\n  const int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i >= n) return;\n  const float vy = y[i];\n  const float vx = x[i];\n  r[i] = unsafe_atan2s< 3>(vy, vx) +\n         unsafe_atan2s< 5>(vy, vx) +\n         unsafe_atan2s< 7>(vy, vx) +\n         unsafe_atan2s< 9>(vy, vx);\n}",
            "__device__ __host__\nconstexpr float approx_atan2i_P<15>(float x) {\n  auto z = x * x;\n  return x * (-683562624.f +\n              z * (227746080.f +\n                   z * (-135400128.f + z * (90460848.f + z * (-54431464.f + z * (22973256.f + z * (-4657049.f)))))));\n}\n\n__device__ __host__\nconstexpr int unsafe_atan2i_impl(float y, float x) {\n  constexpr long long maxint = (long long)(std::numeric_limits<int>::max()) + 1LL;\n  constexpr int pi4 = int(maxint / 4LL);\n  constexpr int pi34 = int(3LL * maxint / 4LL);\n\n  auto r = (std::abs(x) - std::abs(y)) / (std::abs(x) + std::abs(y));\n  if (x < 0)\n    r = -r;\n\n  auto angle = (x >= 0) ? pi4 : pi34;\n  angle += int(approx_atan2i_P<DEGREE>(r));\n\n  return (y < 0) ? -angle : angle;\n}\n\n__device__ __host__\nconstexpr int unsafe_atan2i(float y, float x) {\n  return unsafe_atan2i_impl<DEGREE>(y, x);\n}\n\n__global__\nvoid compute_i (const int n,\n                const float *x,\n                const float *y,\n                      int *r)\n{\n  const int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i >= n) return;\n  const float vy = y[i];\n  const float vx = x[i];\n  r[i] = unsafe_atan2i< 3>(vy, vx) +\n         unsafe_atan2i< 5>(vy, vx) +\n         unsafe_atan2i< 7>(vy, vx) +\n         unsafe_atan2i< 9>(vy, vx) +\n         unsafe_atan2i<11>(vy, vx) +\n         unsafe_atan2i<13>(vy, vx) +\n         unsafe_atan2i<15>(vy, vx);\n}"
        ]
    },
    "atomicPerf-cuda": {
        "/Users/gbolet/hecbench-roofline/src/atomicPerf-cuda/main.cu": [
            "#define T ((int)32)\n\n\n__global__ void BlockRangeAtomicOnGlobalMem(T* data, int n)\n{\n  unsigned int tid = (blockIdx.x * blockDim.x) + threadIdx.x;\n  for ( unsigned int i = tid; i < n; i += blockDim.x*gridDim.x){\n    atomicAdd(data+threadIdx.x, (T)1);  //arbitrary number to add\n  }\n}",
            "#define T ((int)32)\n\n\n__global__ void WarpRangeAtomicOnGlobalMem(T* data, int n)\n{\n  unsigned int tid = (blockIdx.x * blockDim.x) + threadIdx.x;\n  for ( unsigned int i = tid; i < n; i += blockDim.x*gridDim.x){\n    atomicAdd(data+(i & 0x1F), (T)1); //arbitrary number to add\n  }\n}",
            "#define T ((int)32)\n\n\n__global__ void SingleRangeAtomicOnGlobalMem(T* data, int offset, int n)\n{\n  unsigned int tid = (blockIdx.x * blockDim.x) + threadIdx.x;\n  for ( unsigned int i = tid; i < n; i += blockDim.x*gridDim.x){\n    atomicAdd(data+offset, (T)1);    //arbitrary number to add\n  }\n}",
            "#define T ((int)32)\n\n\n__global__ void BlockRangeAtomicOnSharedMem(T* data, int n)\n{\n  __shared__ T smem_data[BLOCK_SIZE];\n  unsigned int tid = (blockIdx.x * blockDim.x) + threadIdx.x;\n  for ( unsigned int i = tid; i < n; i += blockDim.x*gridDim.x){\n    atomicAdd(smem_data+threadIdx.x, (T)1);\n  }\n  if (blockIdx.x == gridDim.x)\n    data[threadIdx.x] = smem_data[threadIdx.x];\n}",
            "#define T ((int)32)\n\n\n__global__ void WarpRangeAtomicOnSharedMem(T* data, int n)\n{\n  __shared__ T smem_data[32];\n  unsigned int tid = (blockIdx.x * blockDim.x) + threadIdx.x;\n  for ( unsigned int i = tid; i < n; i += blockDim.x*gridDim.x){\n    atomicAdd(smem_data+(i & 0x1F), (T)1);\n  }\n  if (blockIdx.x == gridDim.x && threadIdx.x < 0x1F)\n    data[threadIdx.x] = smem_data[threadIdx.x];\n}",
            "#define T ((int)32)\n\n\n__global__ void SingleRangeAtomicOnSharedMem(T* data, int offset, int n)\n{\n  __shared__ T smem_data[BLOCK_SIZE];\n  unsigned int tid = (blockIdx.x * blockDim.x) + threadIdx.x;\n  for ( unsigned int i = tid; i < n; i += blockDim.x*gridDim.x){\n    atomicAdd(smem_data + offset, (T)1);\n  }\n  if (blockIdx.x == gridDim.x && threadIdx.x == 0)\n    data[threadIdx.x] = smem_data[threadIdx.x];\n}"
        ]
    },
    "scel-cuda": {
        "/Users/gbolet/hecbench-roofline/src/scel-cuda/main.cu": [
            "#define GPU_NUM_THREADS 256\n\n\n__global__\nvoid SigmoidCrossEntropyWithLogitsKernel(\n  const int inner_size,\n  const bool log_D_trick,\n  const bool unjoined_lr_loss,\n  const float* logits_ptr,\n  const float* targets_ptr,\n        float* out_ptr)\n{\n  int i = blockIdx.x;\n  int last_idx = (i + 1) * inner_size;\n  float value = 0;\n  for (int in_idx = i * inner_size + threadIdx.x;\n           in_idx < last_idx; in_idx += blockDim.x) {\n    float lgt = logits_ptr[in_idx];\n    float tgt = targets_ptr[in_idx];\n    if (unjoined_lr_loss) {\n      value += unjoined_sigmoid_xent_forward(lgt, tgt);\n    } else {\n      value += log_D_trick ?\n               sigmoid_xent_forward_with_log_d_trick(lgt, tgt) :\n               sigmoid_xent_forward(lgt, tgt);\n    }\n  }\n\n  typedef cub::BlockReduce<float, GPU_NUM_THREADS> BlockReduce;\n  __shared__ typename BlockReduce::TempStorage temp_storage;\n  float sum = BlockReduce(temp_storage).Sum(value);\n  if (threadIdx.x == 0) {\n    out_ptr[i] = -sum / inner_size;\n  }\n}"
        ]
    },
    "stencil1d-cuda": {
        "/Users/gbolet/hecbench-roofline/src/stencil1d-cuda/stencil_1d.cu": [
            "__global__\nvoid stencil_1d(const int *__restrict__ in, int *__restrict__ out)\n{\n  __shared__ int temp[BLOCK_SIZE + 2 * RADIUS];\n  int gindex = threadIdx.x + blockIdx.x * blockDim.x;\n  int lindex = threadIdx.x + RADIUS;\n\n  // Read input elements into shared memory\n  temp[lindex] = in[gindex];\n\n  // At both end of a block, the sliding window moves beyond the block boundary.\n  if (threadIdx.x < RADIUS) {\n    temp[lindex - RADIUS] = (gindex < RADIUS) ? 0 : in[gindex - RADIUS];\n    temp[lindex + BLOCK_SIZE] = in[gindex + BLOCK_SIZE];\n  }\n\n  // Synchronize (ensure all the threads will be completed before continue)\n  __syncthreads();\n\n  // Apply the 1D stencil\n  int result = 0;\n  for (int offset = -RADIUS ; offset <= RADIUS ; offset++)\n    result += temp[lindex + offset];\n\n  // Store the result\n  out[gindex] = result; \n}"
        ]
    },
    "histogram-cuda": {
        "/Users/gbolet/hecbench-roofline/src/histogram-cuda/histogram_gmem_atomics.h": [
            "__device__ __forceinline__ void DecodePixel(uchar1 pixel, unsigned int (&bins)[ACTIVE_CHANNELS])\n{\n    bins[0] = (unsigned int) pixel.x;\n}\n\n__global__ void histogram_gmem_atomics(\n        const PixelType *in,\n        int width,\n        int height,\n        unsigned int *out)\n    {\n        // global position and size\n        int x = blockIdx.x * blockDim.x + threadIdx.x;\n        int y = blockIdx.y * blockDim.y + threadIdx.y;\n        int nx = blockDim.x * gridDim.x;\n        int ny = blockDim.y * gridDim.y;\n\n        // threads in workgroup\n        int t = threadIdx.x + threadIdx.y * blockDim.x; // thread index in workgroup, linear in 0..nt-1\n        int nt = blockDim.x * blockDim.y; // total threads in workgroup\n\n        // group index in 0..ngroups-1\n        int g = blockIdx.x + blockIdx.y * gridDim.x;\n\n        // initialize global memory\n        unsigned int *gmem = out + g * NUM_PARTS;\n        for (int i = t; i < ACTIVE_CHANNELS * NUM_BINS; i += nt)\n            gmem[i] = 0;\n        __syncthreads();\n\n        // process pixels (updates our group's partial histogram in gmem)\n        for (int col = x; col < width; col += nx)\n        {\n            for (int row = y; row < height; row += ny)\n            {\n                PixelType pixel = in[row * width + col];\n\n                unsigned int bins[ACTIVE_CHANNELS];\n                DecodePixel<NUM_BINS>(pixel, bins);\n\n                #pragma unroll\n                for (int CHANNEL = 0; CHANNEL < ACTIVE_CHANNELS; ++CHANNEL)\n                    atomicAdd(&gmem[(NUM_BINS * CHANNEL) + bins[CHANNEL]], 1);\n            }\n        }\n    }",
            "__global__ void histogram_gmem_accum(\n        const unsigned int *in,\n        int n,\n        unsigned int *out)\n    {\n        int i = blockIdx.x * blockDim.x + threadIdx.x;\n        if (i > ACTIVE_CHANNELS * NUM_BINS)\n            return; // out of range\n\n        unsigned int total = 0;\n        for (int j = 0; j < n; j++)\n            total += in[i + NUM_PARTS * j];\n\n        out[i] = total;\n    }"
        ],
        "/Users/gbolet/hecbench-roofline/src/histogram-cuda/histogram_smem_atomics.h": [
            "__device__ __forceinline__ void DecodePixel(uchar1 pixel, unsigned int (&bins)[ACTIVE_CHANNELS])\n{\n    bins[0] = (unsigned int) pixel.x;\n}\n\n__global__ void histogram_smem_atomics(\n        const PixelType *in,\n        int width,\n        int height,\n        unsigned int *out)\n    {\n        // global position and size\n        int x = blockIdx.x * blockDim.x + threadIdx.x;\n        int y = blockIdx.y * blockDim.y + threadIdx.y;\n        int nx = blockDim.x * gridDim.x;\n        int ny = blockDim.y * gridDim.y;\n\n        // threads in workgroup\n        int t = threadIdx.x + threadIdx.y * blockDim.x; // thread index in workgroup, linear in 0..nt-1\n        int nt = blockDim.x * blockDim.y; // total threads in workgroup\n\n        // group index in 0..ngroups-1\n        int g = blockIdx.x + blockIdx.y * gridDim.x;\n\n        // initialize smem\n        __shared__ unsigned int smem[ACTIVE_CHANNELS * NUM_BINS + 3];\n        for (int i = t; i < ACTIVE_CHANNELS * NUM_BINS + 3; i += nt)\n            smem[i] = 0;\n        __syncthreads();\n\n        // process pixels\n        // updates our group's partial histogram in smem\n        for (int col = x; col < width; col += nx)\n        {\n            for (int row = y; row < height; row += ny)\n            {\n                PixelType pixel = in[row * width + col];\n\n                unsigned int bins[ACTIVE_CHANNELS];\n                DecodePixel<NUM_BINS>(pixel, bins);\n\n                #pragma unroll\n                for (int CHANNEL = 0; CHANNEL < ACTIVE_CHANNELS; ++CHANNEL)\n                    atomicAdd(&smem[(NUM_BINS * CHANNEL) + bins[CHANNEL] + CHANNEL], 1);\n            }\n        }\n\n        __syncthreads();\n\n        // move to our workgroup's slice of output\n        out += g * NUM_PARTS;\n\n        // store local output to global\n        for (int i = t; i < NUM_BINS; i += nt)\n        {\n            #pragma unroll\n            for (int CHANNEL = 0; CHANNEL < ACTIVE_CHANNELS; ++CHANNEL)\n                out[i + NUM_BINS * CHANNEL] = smem[i + NUM_BINS * CHANNEL + CHANNEL];\n        }\n    }",
            "__global__ void histogram_smem_accum(\n        const unsigned int *in,\n        int n,\n        unsigned int *out)\n    {\n        int i = blockIdx.x * blockDim.x + threadIdx.x;\n        if (i > ACTIVE_CHANNELS * NUM_BINS) return; // out of range\n        unsigned int total = 0;\n        for (int j = 0; j < n; j++)\n            total += in[i + NUM_PARTS * j];\n        out[i] = total;\n    }"
        ]
    },
    "hybridsort-cuda": {
        "/Users/gbolet/hecbench-roofline/src/hybridsort-cuda/kernel_bucketprefix.h": [
            "__global__ void\nbucketprefix (\n    unsigned int* prefixoffsets,\n    unsigned int* offsets,\n    int blocks )\n{\n\n  const int tid = blockIdx.x * blockDim.x + threadIdx.x;\n  const int size = blocks * BUCKET_BLOCK_MEMORY;\n  int sum = 0;\n\n  for (int i = tid; i < size; i += DIVISIONS) {\n    int x = prefixoffsets[i];\n    prefixoffsets[i] = sum;\n    sum += x;\n  }\n  offsets[tid] = sum;\n}"
        ],
        "/Users/gbolet/hecbench-roofline/src/hybridsort-cuda/kernel_histogram.h": [
            "__global__ void\nhistogram1024 ( unsigned int* histoOutput, \n    const float* histoInput,\n    const int listsize,\n    const float minimum,\n    const float maximum)\n{\n  __shared__ unsigned int s_Hist[HISTOGRAM_BLOCK_MEMORY]; \n\n  const int gid = blockIdx.x * blockDim.x + threadIdx.x;\n  const int lid = threadIdx.x;\n  const int gsize = gridDim.x * blockDim.x;\n  const int lsize = blockDim.x;\n\n  //Per-warp substorage storage\n  int mulBase = (lid >> BUCKET_WARP_LOG_SIZE);\n  const int warpBase = IMUL(mulBase, HISTOGRAM_BIN_COUNT);\n\n  //Clear shared memory storage for current threadblock before processing\n  for(uint i = lid; i < HISTOGRAM_BLOCK_MEMORY; i+=lsize) {\n    s_Hist[i] = 0;\n  }\n\n\n  //Read through the entire input buffer, build per-warp histograms\n  __syncthreads();\n  for(int pos = gid; pos < listsize; pos += gsize) {\n    uint data4 = ((histoInput[pos] - minimum)/(maximum - minimum)) * HISTOGRAM_BIN_COUNT;\n\n    atomicAdd(&s_Hist[warpBase+(data4 & 0x3FFU)], 1U);\n  }\n\n  //Per-block histogram reduction\n  __syncthreads();\n\n  for(int pos = lid; pos < HISTOGRAM_BIN_COUNT; pos += lsize){\n    uint sum = 0;\n    for(int i = 0; i < HISTOGRAM_BLOCK_MEMORY; i+= HISTOGRAM_BIN_COUNT){ \n      sum += s_Hist[pos + i] & 0x07FFFFFFU;\n    }\n    atomicAdd(&histoOutput[pos], sum);\n  }\n}"
        ],
        "/Users/gbolet/hecbench-roofline/src/hybridsort-cuda/kernel_mergeSortPass.h": [
            "inline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\n__device__\nfloat4 getHighest(float4 a, float4 b)\n{\n  float ax = a.x;\n  float ay = a.y;\n  float az = a.z;\n  float aw = a.w;\n  float bx = b.x;\n  float by = b.y;\n  float bz = b.z;\n  float bw = b.w;\n  b.x = aw >= bx ? aw : bx;\n  b.y = az >= by ? az : by;\n  b.z = ay >= bz ? ay : bz;\n  b.w = ax >= bw ? ax : bw;\n  return b;\n}\n\n__device__\nfloat4 getLowest(float4 a, float4 b)\n{\n  float ax = a.x;\n  float ay = a.y;\n  float az = a.z;\n  float aw = a.w;\n  float bx = b.x;\n  float by = b.y;\n  float bz = b.z;\n  float bw = b.w;\n  a.x = ax < bw ? ax : bw;\n  a.y = ay < bz ? ay : bz;\n  a.z = az < by ? az : by;\n  a.w = aw < bx ? aw : bx;\n  return a;\n}\n\n__device__\nfloat4 sortElem(float4 r) {\n  float4 nr;\n\n  float xt = r.x;\n  float yt = r.y;\n  float zt = r.z;\n  float wt = r.w;\n\n  float nr_xt = xt > yt ? yt : xt;\n  float nr_yt = yt > xt ? yt : xt;\n  float nr_zt = zt > wt ? wt : zt;\n  float nr_wt = wt > zt ? wt : zt;\n\n  xt = nr_xt > nr_zt ? nr_zt : nr_xt;\n  yt = nr_yt > nr_wt ? nr_wt : nr_yt;\n  zt = nr_zt > nr_xt ? nr_zt : nr_xt;\n  wt = nr_wt > nr_yt ? nr_wt : nr_yt;\n\n  nr.x = xt;\n  nr.y = yt > zt ? zt : yt;\n  nr.z = zt > yt ? zt : yt;\n  nr.w = wt;\n  return nr;\n}\n\n__global__ void\nmergeSortPass (const float4* input, \n    float4* result,\n    const int* constStartAddr,\n    const int threadsPerDiv,\n    const int nrElems,\n    const int size)\n{\n\n  const int gid = blockIdx.x * blockDim.x + threadIdx.x;\n  // The division to work on\n  int division = gid / threadsPerDiv;\n  if(division >= DIVISIONS) return;\n  // The block within the division\n  int int_gid = gid - division * threadsPerDiv;\n  int Astart = constStartAddr[division] + int_gid * nrElems;\n\n  int Bstart = Astart + nrElems/2;\n  float4* resStart= &(result[Astart]);\n\n  if(Astart >= constStartAddr[division + 1])\n    return;\n  if(Bstart >= constStartAddr[division + 1]){\n    for(int i=0; i<(constStartAddr[division + 1] - Astart); i++)\n    {\n      resStart[i] = input[Astart + i];\n    }\n    return;\n  }\n\n  int aidx = 0;\n  int bidx = 0;\n  int outidx = 0;\n  float4 a, b;\n  a = input[Astart + aidx];\n  b = input[Bstart + bidx];\n\n  while(true)\n  {\n    /**\n     * For some reason, it's faster to do the texture fetches here than\n     * after the merge\n     */ \n    float4 nextA = input[Astart + aidx + 1];\n    float4 nextB = (Bstart + bidx + 1 >= size) ? \n                   make_float4(0.f, 0.f, 0.f, 0.f) : input[Bstart + bidx + 1];\n\n    float4 na = getLowest(a,b);\n    float4 nb = getHighest(a,b);\n    a = sortElem(na);\n    b = sortElem(nb);\n    // Now, a contains the lowest four elements, sorted\n    resStart[outidx++] = a;\n\n    bool elemsLeftInA;\n    bool elemsLeftInB;\n\n    elemsLeftInA = (aidx + 1 < nrElems/2); // Astart + aidx + 1 is allways less than division border\n    elemsLeftInB = (bidx + 1 < nrElems/2) && (Bstart + bidx + 1 < constStartAddr[division + 1]);\n\n    if(elemsLeftInA){\n      if(elemsLeftInB){\n        float nextA_t = nextA.x;\n        float nextB_t = nextB.x;\n        if(nextA_t < nextB_t) { aidx += 1; a = nextA; }\n        else { bidx += 1;  a = nextB; }\n      }\n      else {\n        aidx += 1; a = nextA;\n      }\n    }\n    else {\n      if(elemsLeftInB){\n        bidx += 1;  a = nextB;\n      }\n      else {\n        break;\n      }\n    }\n\n  }\n  resStart[outidx++] = b;\n}"
        ],
        "/Users/gbolet/hecbench-roofline/src/hybridsort-cuda/kernel_bucketcount.h": [
            "__global__ void\nbucketcount (const float* input , \n    int* indice,\n    unsigned int* prefixoffsets,\n    const float* pivotpoints,\n    const int listsize)\n{\n  __shared__ unsigned int s_offset[BUCKET_BLOCK_MEMORY]; \n\n  const int gid = blockIdx.x * blockDim.x + threadIdx.x;\n  const int lid = threadIdx.x;\n  const int gsize = gridDim.x * blockDim.x;\n  const int lsize = blockDim.x;\n  const int warpBase = (lid >> BUCKET_WARP_LOG_SIZE) * DIVISIONS;\n  const int numThreads = gsize;\n\n  for (int i = lid; i < BUCKET_BLOCK_MEMORY; i += lsize)\n    s_offset[i] = 0;\n\n  __syncthreads();\n\n  for (int tid = gid; tid < listsize; tid += numThreads) {\n    float elem = input[tid];\n\n    int idx  = DIVISIONS/2 - 1;\n    int jump = DIVISIONS/4;\n    float piv = pivotpoints[idx]; //s_pivotpoints[idx];\n\n    while(jump >= 1){\n      idx = (elem < piv) ? (idx - jump) : (idx + jump);\n      piv = pivotpoints[idx]; //s_pivotpoints[idx];\n      jump /= 2;\n    }\n    idx = (elem < piv) ? idx : (idx + 1);\n\n    indice[tid] = \n      (atomicAdd(&s_offset[warpBase+idx], 1U) << LOG_DIVISIONS)  + idx;\n  }\n\n  __syncthreads();\n\n  int prefixBase = blockIdx.x * BUCKET_BLOCK_MEMORY;\n\n  for (int i = lid; i < BUCKET_BLOCK_MEMORY; i += lsize)\n    prefixoffsets[prefixBase + i] = s_offset[i] & 0x07FFFFFFU;\n\n}"
        ],
        "/Users/gbolet/hecbench-roofline/src/hybridsort-cuda/mergesort.cu": [
            "__device__\nfloat4 sortElem(float4 r) {\n  float4 nr;\n\n  float xt = r.x;\n  float yt = r.y;\n  float zt = r.z;\n  float wt = r.w;\n\n  float nr_xt = xt > yt ? yt : xt;\n  float nr_yt = yt > xt ? yt : xt;\n  float nr_zt = zt > wt ? wt : zt;\n  float nr_wt = wt > zt ? wt : zt;\n\n  xt = nr_xt > nr_zt ? nr_zt : nr_xt;\n  yt = nr_yt > nr_wt ? nr_wt : nr_yt;\n  zt = nr_zt > nr_xt ? nr_zt : nr_xt;\n  wt = nr_wt > nr_yt ? nr_wt : nr_yt;\n\n  nr.x = xt;\n  nr.y = yt > zt ? zt : yt;\n  nr.z = zt > yt ? zt : yt;\n  nr.w = wt;\n  return nr;\n}\n\n__global__ void\nsortElement(float4* result, float4* input, const int size) \n{\n  int gid = blockIdx.x*blockDim.x+threadIdx.x;\n  if (gid < size) result[gid] = sortElem(input[gid]);\n}",
            "__global__ void\nmergepack ( float* result , \n    const float* orig , \n    const int *constStartAddr,\n    const unsigned int *finalStartAddr,\n    const unsigned int *nullElems )\n{\n\n  const int gid = blockIdx.x * blockDim.x + threadIdx.x;\n  int division = blockIdx.y;\n  if((finalStartAddr[division] + gid) < finalStartAddr[division + 1])\n    result[finalStartAddr[division] + gid] = \n      orig[constStartAddr[division]*4 + nullElems[division] + gid];\n}"
        ],
        "/Users/gbolet/hecbench-roofline/src/hybridsort-cuda/kernel_bucketsort.h": [
            "__global__ void\nbucketsort (const float* input , \n    const int* indice,\n    float* output,\n    const unsigned int* prefixoffsets,\n    const unsigned int* offsets,\n    const int listsize)\n{\n  const int grp_id = blockIdx.x;\n  const int gid = blockIdx.x * blockDim.x + threadIdx.x;\n  const int lid = threadIdx.x;\n  const int gsize = gridDim.x * blockDim.x;\n  const int lsize = blockDim.x;\n\n  __shared__ unsigned int s_offset[BUCKET_BLOCK_MEMORY]; \n\n  int prefixBase = grp_id * BUCKET_BLOCK_MEMORY;\n  const int warpBase = (lid >> BUCKET_WARP_LOG_SIZE) * DIVISIONS;\n  const int numThreads = gsize;\n\n  for (int i = lid; i < BUCKET_BLOCK_MEMORY; i += lsize){\n    s_offset[i] = offsets[i & (DIVISIONS - 1)] + prefixoffsets[prefixBase + i];\n  }\n\n  __syncthreads();\n\n  for (int tid = gid; tid < listsize; tid += numThreads){\n    float elem = input[tid];\n    int id = indice[tid];\n    output[s_offset[warpBase + (id & (DIVISIONS - 1))] + (id >>  LOG_DIVISIONS)] = elem;\n  }\n}"
        ]
    },
    "hotspot3D-cuda": {
        "/Users/gbolet/hecbench-roofline/src/hotspot3D-cuda/3D.cu": [
            "__global__ void\nhotspot3d(\n    const float*__restrict__ tIn, \n    const float*__restrict__ pIn, \n          float*__restrict__ tOut,\n    const int numCols, \n    const int numRows, \n    const int layers,\n    const float ce, \n    const float cw,\n    const float cn, \n    const float cs,\n    const float ct,\n    const float cb,\n    const float cc,\n    const float stepDivCap)\n{\n  float amb_temp = 80.0;\n\n  int i = blockDim.x * blockIdx.x + threadIdx.x;\n  int j = blockDim.y * blockIdx.y + threadIdx.y;\n  int c = i + j * numCols;\n  int xy = numCols * numRows;\n\n  int W = (i == 0)        ? c : c - 1;\n  int E = (i == numCols-1)     ? c : c + 1;\n  int N = (j == 0)        ? c : c - numCols;\n  int S = (j == numRows-1)     ? c : c + numCols;\n\n  float temp1, temp2, temp3;\n  temp1 = temp2 = tIn[c];\n  temp3 = tIn[c+xy];\n  tOut[c] = cc * temp2 + cw * tIn[W] + ce * tIn[E] + cs * tIn[S]\n    + cn * tIn[N] + cb * temp1 + ct * temp3 + stepDivCap * pIn[c] + ct * amb_temp;\n  c += xy;\n  W += xy;\n  E += xy;\n  N += xy;\n  S += xy;\n\n  for (int k = 1; k < layers-1; ++k) {\n    temp1 = temp2;\n    temp2 = temp3;\n    temp3 = tIn[c+xy];\n    tOut[c] = cc * temp2 + cw * tIn[W] + ce * tIn[E] + cs * tIn[S]\n      + cn * tIn[N] + cb * temp1 + ct * temp3 + stepDivCap * pIn[c] + ct * amb_temp;\n    c += xy;\n    W += xy;\n    E += xy;\n    N += xy;\n    S += xy;\n  }\n  temp1 = temp2;\n  temp2 = temp3;\n  tOut[c] = cc * temp2 + cw * tIn[W] + ce * tIn[E] + cs * tIn[S]\n    + cn * tIn[N] + cb * temp1 + ct * temp3 + stepDivCap * pIn[c] + ct * amb_temp;\n}"
        ]
    },
    "pad-cuda": {
        "/Users/gbolet/hecbench-roofline/src/pad-cuda/kernel.cu": [
            "#define T ((int)32)\n\n\n__device__ inline int gpu_first(Partitioner *p) {\n#ifdef DYNAMIC_PARTITION\n    if(p->strategy == DYNAMIC_PARTITIONING) {\n        if(threadIdx.x == 0) {\n            p->tmp[0] = atomicAdd_system(p->worklist, 1);\n        }\n        __syncthreads();\n        p->current = p->tmp[0];\n    } else\n#endif\n    {\n        p->current = p->cut + blockIdx.x;\n    }\n    return p->current;\n}\n\n__device__ inline bool gpu_more(const Partitioner *p) {\n    return (p->current < p->n_tasks);\n}\n\n__device__ inline int gpu_next(Partitioner *p) {\n#ifdef DYNAMIC_PARTITION\n    if(p->strategy == DYNAMIC_PARTITIONING) {\n        if(threadIdx.x == 0) {\n            p->tmp[0] = atomicAdd_system(p->worklist, 1);\n        }\n        __syncthreads();\n        p->current = p->tmp[0];\n    } else\n#endif\n    {\n        p->current = p->current + gridDim.x;\n    }\n    return p->current;\n}\n\n__device__\n#endif\ninline Partitioner partitioner_create(int n_tasks, float alpha\n#ifndef _GPU_COMPILER_\n    , int thread_id, int n_threads\n#endif\n#ifdef DYNAMIC_PARTITION\n#ifdef _GPU_COMPILER_\n    , int *worklist\n    , int *tmp\n#else\n    , std::atomic_int *worklist\n#endif\n#endif\n    ) {\n    Partitioner p;\n    p.n_tasks = n_tasks;\n#ifndef _GPU_COMPILER_\n    p.thread_id = thread_id;\n    p.n_threads = n_threads;\n#endif\n    if(alpha >= 0.0 && alpha <= 1.0) {\n        p.cut = p.n_tasks * alpha;\n#ifdef DYNAMIC_PARTITION\n        p.strategy = STATIC_PARTITIONING;\n#endif\n    } else {\n#ifdef DYNAMIC_PARTITION\n        p.strategy = DYNAMIC_PARTITIONING;\n        p.worklist = worklist;\n#ifdef _GPU_COMPILER_\n        p.tmp = tmp;\n#endif\n#endif\n    }\n    return p;\n}\n\n__global__\nvoid Padding_kernel(int n, int m, int pad, int n_tasks, float alpha,\n                    T *__restrict__ matrix_out,\n                    const T *matrix,\n                    int *__restrict__ flags\n#ifdef DYNAMIC_PARTITION\n                    , int *__restrict__ worklist\n#endif\n    ) {\n\n#ifdef DYNAMIC_PARTITION\n    extern __shared__ int l_mem[];\n    int* l_tmp = l_mem;\n#endif\n\n#ifdef DYNAMIC_PARTITION\n    Partitioner p = partitioner_create(n_tasks, alpha, worklist, l_tmp);\n#else\n    Partitioner p = partitioner_create(n_tasks, alpha);\n#endif\n\n    const int matrix_size = m * (n + pad);\n    const int matrix_size_align =\n        (matrix_size + blockDim.x * REGS - 1) / (blockDim.x * REGS) * (blockDim.x * REGS);\n\n    for(int my_s = gpu_first(&p); gpu_more(&p); my_s = gpu_next(&p)) {\n\n        // Declare on-chip memory\n        T   reg[REGS];\n        int pos      = matrix_size_align - 1 - (my_s * REGS * blockDim.x + threadIdx.x);\n        int my_s_row = pos / (n + pad);\n        int my_x     = pos % (n + pad);\n        int pos2     = my_s_row * n + my_x;\n// Load in on-chip memory\n#pragma unroll\n        for(int j = 0; j < REGS; j++) {\n            if(pos2 >= 0 && my_x < n && pos2 < matrix_size)\n                reg[j] = matrix[pos2];\n            else\n                reg[j] = 0;\n            pos -= blockDim.x;\n            my_s_row = pos / (n + pad);\n            my_x     = pos % (n + pad);\n            pos2     = my_s_row * n + my_x;\n        }\n\n        __syncthreads();\n\n        // Set global synch\n        if(threadIdx.x == 0) {\n#ifdef DYNAMIC_PARTITION\n            while(atomicAdd_system(&flags[my_s], 0) == 0) {\n            }\n            atomicAdd_system(&flags[my_s + 1], 1);\n#else\n            while(atomicAdd(&flags[my_s], 0) == 0) {\n            }\n            atomicAdd(&flags[my_s + 1], 1);\n#endif\n        }\n        __syncthreads();\n\n        pos = matrix_size_align - 1 - (my_s * REGS * blockDim.x + threadIdx.x);\n// Store to global memory\n#pragma unroll\n        for(int j = 0; j < REGS; j++) {\n            if(pos >= 0 && pos < matrix_size)\n                matrix_out[pos] = reg[j];\n            pos -= blockDim.x;\n        }\n    }\n}"
        ]
    },
    "crc64-cuda": {
        "/Users/gbolet/hecbench-roofline/src/crc64-cuda/CRC64.cu": [
            "__host__ __device__\nstatic inline uint32_t crc64_load_le32_(const uint32_t *p) {\n  uint32_t w = *p;\n  return  ((((w) & 0xff000000) >> 24)\n         | (((w) & 0x00ff0000) >>  8)\n         | (((w) & 0x0000ff00) <<  8)\n         | (((w) & 0x000000ff) << 24));\n}\n\n__device__\nuint64_t crc64_device(const unsigned char *input, size_t nbytes, \n\t\tconst uint64_t *d_crc64_table, \n\t\tconst uint64_t *d_crc64_interleaved_table) {\n  const unsigned char *data = input;\n  const unsigned char *end = data + nbytes;\n  uint64_t cs[5] = { UINT64_C(0xffffffffffffffff), 0, 0, 0, 0 };\n\n  // Process byte-by-byte until proper alignment is attained.\n  // In the inner loop, we process 5 4-byte words (20 bytes in total)\n  // per iteration. If the amount of data remaining is small,\n  // then we also use the slow algorithm.\n  while (data < end && ((((size_t) data) & 3) || (end - data < 20))) {\n    uint32_t idx = ((uint32_t) (cs[0] ^ *data++)) & 0xff;\n    cs[0] = d_crc64_table[3*256+idx] ^ (cs[0] >> 8);\n  }\n\n  if (data == end)\n    return cs[0] ^ UINT64_C(0xffffffffffffffff);\n\n  const uint32_t one = 1;\n  bool big_endian = !(*((char *)(&one)));\n\n  uint64_t cry = 0;\n  uint32_t in[5];\n\n  if (!big_endian) {\n    for (unsigned i = 0; i < 5; ++i)\n      in[i] = ((const uint32_t*) data)[i];\n    data += 20;\n\n    for (; end - data >= 20; data += 20) {\n      cs[0] ^= cry;\n\n      in[0] ^= (uint32_t) cs[0];\n      cs[1] ^= cs[0] >> 32;\n      cs[0] = d_crc64_interleaved_table[in[0] & 0xff];\n      in[0] >>= 8;\n\n      in[1] ^= (uint32_t) cs[1];\n      cs[2] ^= cs[1] >> 32;\n      cs[1] = d_crc64_interleaved_table[in[1] & 0xff];\n      in[1] >>= 8;\n\n      in[2] ^= (uint32_t) cs[2];\n      cs[3] ^= cs[2] >> 32;\n      cs[2] = d_crc64_interleaved_table[in[2] & 0xff];\n      in[2] >>= 8;\n\n      in[3] ^= (uint32_t) cs[3];\n      cs[4] ^= cs[3] >> 32;\n      cs[3] = d_crc64_interleaved_table[in[3] & 0xff];\n      in[3] >>= 8;\n\n      in[4] ^= (uint32_t) cs[4];\n      cry = cs[4] >> 32;\n      cs[4] = d_crc64_interleaved_table[in[4] & 0xff];\n      in[4] >>= 8;\n\n      for (unsigned b = 1; b < 3; ++b) {\n        cs[0] ^= d_crc64_interleaved_table[b*256+(in[0] & 0xff)];\n        in[0] >>= 8;\n\n        cs[1] ^= d_crc64_interleaved_table[b*256+(in[1] & 0xff)];\n        in[1] >>= 8;\n\n        cs[2] ^= d_crc64_interleaved_table[b*256+(in[2] & 0xff)];\n        in[2] >>= 8;\n\n        cs[3] ^= d_crc64_interleaved_table[b*256+(in[3] & 0xff)];\n        in[3] >>= 8;\n\n        cs[4] ^= d_crc64_interleaved_table[b*256+(in[4] & 0xff)];\n        in[4] >>= 8;\n      }\n\n      cs[0] ^= d_crc64_interleaved_table[3*256+(in[0] & 0xff)];\n      in[0] = ((const uint32_t*) data)[0];\n\n      cs[1] ^= d_crc64_interleaved_table[3*256+(in[1] & 0xff)];\n      in[1] = ((const uint32_t*) data)[1];\n\n      cs[2] ^= d_crc64_interleaved_table[3*256+(in[2] & 0xff)];\n      in[2] = ((const uint32_t*) data)[2];\n\n      cs[3] ^= d_crc64_interleaved_table[3*256+(in[3] & 0xff)];\n      in[3] = ((const uint32_t*) data)[3];\n\n      cs[4] ^= d_crc64_interleaved_table[3*256+(in[4] & 0xff)];\n      in[4] = ((const uint32_t*) data)[4];\n    }\n  } else {\n    for (unsigned i = 0; i < 5; ++i) {\n      in[i] = crc64_load_le32_(&((const uint32_t*) data)[i]);\n    }\n    data += 20;\n\n    for (; end - data >= 20; data += 20) {\n      cs[0] ^= cry;\n\n      in[0] ^= (uint32_t) cs[0];\n      cs[1] ^= cs[0] >> 32;\n      cs[0] = d_crc64_interleaved_table[in[0] & 0xff];\n      in[0] >>= 8;\n\n      in[1] ^= (uint32_t) cs[1];\n      cs[2] ^= cs[1] >> 32;\n      cs[1] = d_crc64_interleaved_table[in[1] & 0xff];\n      in[1] >>= 8;\n\n      in[2] ^= (uint32_t) cs[2];\n      cs[3] ^= cs[2] >> 32;\n      cs[2] = d_crc64_interleaved_table[in[2] & 0xff];\n      in[2] >>= 8;\n\n      in[3] ^= (uint32_t) cs[3];\n      cs[4] ^= cs[3] >> 32;\n      cs[3] = d_crc64_interleaved_table[in[3] & 0xff];\n      in[3] >>= 8;\n\n      in[4] ^= (uint32_t) cs[4];\n      cry = cs[4] >> 32;\n      cs[4] = d_crc64_interleaved_table[in[4] & 0xff];\n      in[4] >>= 8;\n\n      for (unsigned b = 1; b < 3; ++b) {\n        cs[0] ^= d_crc64_interleaved_table[b*256+(in[0] & 0xff)];\n        in[0] >>= 8;\n\n        cs[1] ^= d_crc64_interleaved_table[b*256+(in[1] & 0xff)];\n        in[1] >>= 8;\n\n        cs[2] ^= d_crc64_interleaved_table[b*256+(in[2] & 0xff)];\n        in[2] >>= 8;\n\n        cs[3] ^= d_crc64_interleaved_table[b*256+(in[3] & 0xff)];\n        in[3] >>= 8;\n\n        cs[4] ^= d_crc64_interleaved_table[b*256+(in[4] & 0xff)];\n        in[4] >>= 8;\n      }\n\n      cs[0] ^= d_crc64_interleaved_table[3*256+(in[0] & 0xff)];\n      in[0] = crc64_load_le32_(&((const uint32_t*) data)[0]);\n\n      cs[1] ^= d_crc64_interleaved_table[3*256+(in[1] & 0xff)];\n      in[1] = crc64_load_le32_(&((const uint32_t*) data)[1]);\n\n      cs[2] ^= d_crc64_interleaved_table[3*256+(in[2] & 0xff)];\n      in[2] = crc64_load_le32_(&((const uint32_t*) data)[2]);\n\n      cs[3] ^= d_crc64_interleaved_table[3*256+(in[3] & 0xff)];\n      in[3] = crc64_load_le32_(&((const uint32_t*) data)[3]);\n\n      cs[4] ^= d_crc64_interleaved_table[3*256+(in[4] & 0xff)];\n      in[4] = crc64_load_le32_(&((const uint32_t*) data)[4]);\n    }\n  }\n\n  cs[0] ^= cry;\n\n  for (unsigned i = 0; i < 5; ++i) {\n    if (i > 0)\n      cs[0] ^= cs[i];\n    in[i] ^= (uint32_t) cs[0];\n    cs[0] = cs[0] >> 32;\n\n    for (unsigned b = 0; b < 3; ++b) {\n      cs[0] ^= d_crc64_table[b*256+(in[i] & 0xff)];\n      in[i] >>= 8;\n    }\n\n    cs[0] ^= d_crc64_table[3*256+(in[i] & 0xff)];\n  }\n\n  while (data < end) {\n    uint32_t idx = ((uint32_t) (cs[0] ^ *data++)) & 0xff;\n    cs[0] = d_crc64_table[3*256+idx] ^ (cs[0] >> 8);\n  }\n\n  return cs[0] ^ UINT64_C(0xffffffffffffffff);\n}\n\n__global__ void \ncrc64_kernel(\n  size_t *__restrict__ d_thread_sz, \n  uint64_t *__restrict__ d_thread_cs, \n  const unsigned char*__restrict__  d_data, \n  const uint64_t *__restrict__ d_crc64_table, \n  const uint64_t *__restrict__ d_crc64_interleaved_table, \n  size_t nbytes, int nthreads) \n{\n  int tid = blockIdx.x * blockDim.x + threadIdx.x;\n  size_t bpt = nbytes/nthreads;\n  const unsigned char *start = d_data + bpt*tid;\n  const unsigned char *end;\n  if (tid != nthreads - 1)\n    end = start + bpt;\n  else\n    end = d_data + nbytes;\n    \n  size_t sz = end - start;\n  d_thread_sz[tid] = sz;\n  d_thread_cs[tid] = crc64_device(start, sz, d_crc64_table, d_crc64_interleaved_table);\n}"
        ]
    },
    "channelShuffle-cuda": {
        "/Users/gbolet/hecbench-roofline/src/channelShuffle-cuda/main.cu": [
            "#define T ((int)32)\n\n\n__global__ void ChannelShuffleNCHWKernel(\n    const int G,\n    const int K,\n    const int HxW,\n    const T* X,\n          T* Y)\n{\n  const int C = G * K;\n  const int n = kNFirst ? blockIdx.x : blockIdx.y;\n  const int s = kNFirst ? blockIdx.y : blockIdx.x;\n  const int g = blockIdx.z % G;\n  const int k = blockIdx.z / G;\n  const int offset = s * NUM_THREADS + threadIdx.x;\n  if (offset < HxW) {\n    Y[(n * C + blockIdx.z) * HxW + offset] =\n        __ldg(X + (n * C + g * K + k) * HxW + offset);\n  }\n}",
            "#define T ((int)32)\n\n\n__global__ void\nChannelShuffleNHWCKernel(const int G, const int K, const T* X, T* Y)\n{\n  __shared__ T sdata[kSharedSize];\n  const int C = G * K;\n  const int offset = blockIdx.x * C;\n  for (int i = threadIdx.x; i < C; i += blockDim.x) {\n    sdata[i] = __ldg(X + offset + i);\n  }\n  __syncthreads();\n  for (int i = threadIdx.x; i < C; i += blockDim.x) {\n    const int g = i % G;\n    const int k = i / G;\n    Y[offset + i] = sdata[g * K + k];\n  }\n}"
        ]
    },
    "keccaktreehash-cuda": {
        "/Users/gbolet/hecbench-roofline/src/keccaktreehash-cuda/KeccakTreeGPU.cu": [
            "__device__ void KeccakFunr( tKeccakLane * state, const tKeccakLane *KeccakF_RoundConstants )\n{\n  unsigned int round; //try to avoid to many registers\n  tKeccakLane BC[5];\n  tKeccakLane temp;\n\n  for ( round = 0; round < cKeccakNumberOfRounds; ++round )\n  {\n    {\n      // Theta\n      BC[0] = state[0] ^ state[5] ^ state[10] ^ state[15] ^ state[20];\n      BC[1] = state[1] ^ state[6] ^ state[11] ^ state[16] ^ state[21];\n      BC[2] = state[2] ^ state[7] ^ state[12] ^ state[17] ^ state[22];\n      BC[3] = state[3] ^ state[8] ^ state[13] ^ state[18] ^ state[23];\n      BC[4] = state[4] ^ state[9] ^ state[14] ^ state[19] ^ state[24];\n\n      temp = BC[4] ^ ROL32(BC[1], 1);//x=0\n      state[0] ^= temp;\n      state[5] ^= temp;\n      state[10] ^= temp;\n      state[15] ^= temp;\n      state[20] ^= temp;\n      temp = BC[0] ^ ROL32(BC[2], 1);//x=1\n      state[1] ^= temp;\n      state[6] ^= temp;\n      state[11] ^= temp;\n      state[16] ^= temp;\n      state[21] ^= temp;\n      temp = BC[1] ^ ROL32(BC[3], 1);//x=2\n      state[2] ^= temp;\n      state[7] ^= temp;\n      state[12] ^= temp;\n      state[17] ^= temp;\n      state[22] ^= temp;\n      temp = BC[2] ^ ROL32(BC[4], 1);//x=3\n      state[3] ^= temp;\n      state[8] ^= temp;\n      state[13] ^= temp;\n      state[18] ^= temp;\n      state[23] ^= temp;\n      temp = BC[3] ^ ROL32(BC[0], 1);//x=4\n      state[4] ^= temp;\n      state[9] ^= temp;\n      state[14] ^= temp;\n      state[19] ^= temp;\n      state[24] ^= temp;\n    }//end Theta\n\n    {\n      // Rho Pi\n      temp = state[1];\n      BC[0] = state[10];\n      state[10] = ROL32( temp, 1);\n      temp = BC[0];//x=0\n      BC[0] =  state[7];\n      state[7] = ROL32( temp, 3);\n      temp = BC[0];\n      BC[0] = state[11];\n      state[11] = ROL32( temp, 6);\n      temp = BC[0];\n      BC[0] = state[17];\n      state[17] = ROL32( temp,10);\n      temp = BC[0];\n      BC[0] = state[18];\n      state[18] = ROL32( temp,15);\n      temp = BC[0];\n      BC[0] =  state[3];\n      state[3] = ROL32( temp,21);\n      temp = BC[0];//x=5\n      BC[0] =  state[5];\n      state[5] = ROL32( temp,28);\n      temp = BC[0];\n      BC[0] = state[16];\n      state[16] = ROL32( temp, 4);\n      temp = BC[0];\n      BC[0] =  state[8];\n      state[8] = ROL32( temp,13);\n      temp = BC[0];\n      BC[0] = state[21];\n      state[21] = ROL32( temp,23);\n      temp = BC[0];\n      BC[0] = state[24];\n      state[24] = ROL32( temp, 2);\n      temp = BC[0];//x=10\n      BC[0] =  state[4];\n      state[4] = ROL32( temp,14);\n      temp = BC[0];\n      BC[0] = state[15];\n      state[15] = ROL32( temp,27);\n      temp = BC[0];\n      BC[0] = state[23];\n      state[23] = ROL32( temp, 9);\n      temp = BC[0];\n      BC[0] = state[19];\n      state[19] = ROL32( temp,24);\n      temp = BC[0];\n      BC[0] = state[13];\n      state[13] = ROL32( temp, 8);\n      temp = BC[0];//x=15\n      BC[0] = state[12];\n      state[12] = ROL32( temp,25);\n      temp = BC[0];\n      BC[0] =  state[2];\n      state[2] = ROL32( temp,11);\n      temp = BC[0];\n      BC[0] = state[20];\n      state[20] = ROL32( temp,30);\n      temp = BC[0];\n      BC[0] = state[14];\n      state[14] = ROL32( temp,18);\n      temp = BC[0];\n      BC[0] = state[22];\n      state[22] = ROL32( temp, 7);\n      temp = BC[0];//x=20\n      BC[0] =  state[9];\n      state[9] = ROL32( temp,29);\n      temp = BC[0];\n      BC[0] =  state[6];\n      state[6] = ROL32( temp,20);\n      temp = BC[0];\n      BC[0] =  state[1];\n      state[1] = ROL32( temp,12);\n      temp = BC[0];//x=23\n    }//end Rho Pi\n\n    {\n      //  Chi\n      BC[0] = state[0];\n      BC[1] = state[1];\n      BC[2] = state[2];\n      BC[3] = state[3];\n      BC[4] = state[4];\n      state[0] = BC[0] ^((~BC[1]) & BC[2]);\n      state[1] = BC[1] ^((~BC[2]) & BC[3]);\n      state[2] = BC[2] ^((~BC[3]) & BC[4]);\n      state[3] = BC[3] ^((~BC[4]) & BC[0]);\n      state[4] = BC[4] ^((~BC[0]) & BC[1]);\n      BC[0] = state[5];\n      BC[1] = state[6];\n      BC[2] = state[7];\n      BC[3] = state[8];\n      BC[4] = state[9];\n      state[5] = BC[0] ^((~BC[1]) & BC[2]);\n      state[6] = BC[1] ^((~BC[2]) & BC[3]);\n      state[7] = BC[2] ^((~BC[3]) & BC[4]);\n      state[8] = BC[3] ^((~BC[4]) & BC[0]);\n      state[9] = BC[4] ^((~BC[0]) & BC[1]);\n      BC[0] = state[10];\n      BC[1] = state[11];\n      BC[2] = state[12];\n      BC[3] = state[13];\n      BC[4] = state[14];\n      state[10] = BC[0] ^((~BC[1]) & BC[2]);\n      state[11] = BC[1] ^((~BC[2]) & BC[3]);\n      state[12] = BC[2] ^((~BC[3]) & BC[4]);\n      state[13] = BC[3] ^((~BC[4]) & BC[0]);\n      state[14] = BC[4] ^((~BC[0]) & BC[1]);\n      BC[0] = state[15];\n      BC[1] = state[16];\n      BC[2] = state[17];\n      BC[3] = state[18];\n      BC[4] = state[19];\n      state[15] = BC[0] ^((~BC[1]) & BC[2]);\n      state[16] = BC[1] ^((~BC[2]) & BC[3]);\n      state[17] = BC[2] ^((~BC[3]) & BC[4]);\n      state[18] = BC[3] ^((~BC[4]) & BC[0]);\n      state[19] = BC[4] ^((~BC[0]) & BC[1]);\n      BC[0] = state[20];\n      BC[1] = state[21];\n      BC[2] = state[22];\n      BC[3] = state[23];\n      BC[4] = state[24];\n      state[20] = BC[0] ^((~BC[1]) & BC[2]);\n      state[21] = BC[1] ^((~BC[2]) & BC[3]);\n      state[22] = BC[2] ^((~BC[3]) & BC[4]);\n      state[23] = BC[3] ^((~BC[4]) & BC[0]);\n      state[24] = BC[4] ^((~BC[0]) & BC[1]);\n    }//end Chi\n\n    //  Iota\n    state[0] ^= KeccakF_RoundConstants[round];\n  }\n\n}\n\n__global__ void ker_Keccak(const tKeccakLane *__restrict__ d_inBuffer,\n                                 tKeccakLane *__restrict__ d_outBuffer,\n                           const tKeccakLane *__restrict__ KeccakF_RoundConstants)\n{\n\n  int ind_word,k;\n  tKeccakLane Kstate[25];\n\n  //zeroize the state\n  for(ind_word=0; ind_word<25; ind_word++) {Kstate[ind_word]=0; } \n\n  for (k=0;k<NB_INPUT_BLOCK;k++)\n  {\n    //xor input into state\n    for (ind_word=0; ind_word<(INPUT_BLOCK_SIZE_B/4 ); ind_word++)\n    {\n\n      Kstate[ind_word] ^= \n        d_inBuffer[threadIdx.x \n        + ind_word    * NB_THREADS \n        + k        * NB_THREADS * INPUT_BLOCK_SIZE_B/4\n        + blockIdx.x  * NB_THREADS * INPUT_BLOCK_SIZE_B/4 * NB_INPUT_BLOCK ];\n    }\n    //apply GPU Keccak permutation\n    KeccakFunr(Kstate, KeccakF_RoundConstants);\n  }\n\n  //output hash in buffer\n  for (ind_word=0; ind_word<OUTPUT_BLOCK_SIZE_B/4; ind_word++)\n  {\n    d_outBuffer[threadIdx.x \n      + ind_word *NB_THREADS\n      + blockIdx.x   *NB_THREADS * OUTPUT_BLOCK_SIZE_B/4 ]= Kstate[ind_word];\n  }\n}"
        ]
    },
    "sa-cuda": {
        "/Users/gbolet/hecbench-roofline/src/sa-cuda/kernels.h": [
            "__global__ void Init_d_s12(int* s12, int n)\n{\n  int index = blockIdx.x*blockDim.x + threadIdx.x;\n  if (index >= n) return;\n  s12[index] = index + index / 2 + 1;\n}",
            "__global__\nvoid keybits(      int*__restrict__ SA12,\n             const int*__restrict__ s12,\n             const int*__restrict__ s,\n             int n, int i)\n{\n  int index = blockIdx.x*blockDim.x + threadIdx.x;\n  if (index >= n) return;\n  SA12[index] = s[s12[index] + i];\n}",
            "__global__\nvoid InitScan(const int*__restrict__ s,\n              const int*__restrict__ SA12,\n                    int*__restrict__ scan,\n              int n)\n{\n  int index = blockIdx.x*blockDim.x + threadIdx.x;\n  if (index >= n) return;\n  if ((s[SA12[index]] == s[SA12[index + 1]]) && \n      (s[SA12[index] + 1] == s[SA12[index + 1] + 1]) &&\n      (s[SA12[index] + 2] == s[SA12[index + 1] + 2]))\n  {\n    scan[index] = 0;\n  }\n  else\n    scan[index] = 1;\n}",
            "__global__\nvoid Set_suffix_rank(      int*__restrict__ s12,\n                     const int*__restrict__ SA12,\n                     const int*__restrict__ scan,\n                     int n02, int n0)\n{\n  int index = blockIdx.x*blockDim.x + threadIdx.x;\n  if (index >= n02) return;\n  s12[SA12[index] / 3 + ((SA12[index] % 3) - 1) * n0] = scan[index] + 1;\n}",
            "__global__\nvoid Store_unique_ranks(      int*__restrict__ s12,\n                        const int*__restrict__ SA12,\n                        int n)\n{\n  int index = blockIdx.x*blockDim.x + threadIdx.x;\n  if (index >= n) return;\n  s12[SA12[index]] = index + 1;\n}",
            "__global__\nvoid Compute_SA_From_UniqueRank(const int*__restrict__ s12,\n                                      int*__restrict__ SA12,\n                                int n)\n{\n  int index = blockIdx.x*blockDim.x + threadIdx.x;\n  if (index >= n) return;\n  SA12[s12[index] - 1] = index;\n}",
            "__global__\nvoid InitScan2(const int*__restrict__ SA12,\n                     int*__restrict__ scan,\n               int n0, int n02)\n{\n  int index = blockIdx.x*blockDim.x + threadIdx.x;\n  if (index >= n02)\n    return;\n  if (SA12[index] < n0)\n    scan[index] = 1;\n  else\n    scan[index] = 0;\n}",
            "__global__\nvoid Set_S0(      int*__restrict__ s0,\n            const int*__restrict__ SA12,\n            const int*__restrict__ scan,\n            int n0, int n02)\n{\n  int index = blockIdx.x*blockDim.x + threadIdx.x;\n  if (index >= n02) return;\n  if (SA12[index] < n0)\n    s0[scan[index]] = 3 * SA12[index];\n}",
            "__device__ int leq(int a1, int a2, int b1, int b2) {\n  return (a1 < b1 || (a1 == b1 && a2 <= b2));\n}\n\n__device__ int leq2(int a1, int a2, int a3, int b1, int b2, int b3) {\n  return (a1 < b1 || (a1 == b1 && leq(a2, a3, b2, b3)));\n}\n\n__global__\nvoid merge_suffixes(const int*__restrict__ SA0,\n                    const int*__restrict__ SA12,\n                          int*__restrict__ SA,\n                    const int*__restrict__ s,\n                    const int*__restrict__ s12,\n                    int n0, int n02, int n)\n{\n  int index = blockIdx.x*blockDim.x + threadIdx.x;\n  int left, right, mid;\n  int flag = 0;\n  if (index >= n0 + n02) return;\n\n  if (n != n0 + n02)\n  {\n    flag = 1;\n    if (index == n0) return;  \n  }\n\n  int i, j;\n  if (index < n0)\n  {\n    i = SA0[index];\n    left = n0;\n    right = n0 + n02;\n\n    while (left < right)\n    {\n      mid = (left + right) / 2;\n      if (SA12[mid - n0] < n0)\n      {\n        j = SA12[mid - n0] * 3 + 1;\n\n        if (leq(s[j], s12[(j + 1) / 3 + ((j + 1) % 3 - 1)*n0],\n                s[i], s12[i / 3]))\n          left = mid + 1;\n        else\n          right = mid;\n      }\n      else\n      {\n        j = (SA12[mid - n0] - n0) * 3 + 2;\n\n        if (leq2(s[j], s[j + 1], s12[(j + 2) / 3 + ((j + 2) % 3 - 1)*n0],\n                 s[i], s[i + 1], s12[i / 3 + n0]))\n          left = mid + 1;\n        else\n          right = mid;\n      }\n\n    }\n    SA[index + left - n0 - flag] = i;\n  }\n  else\n  {\n    if (SA12[index - n0] < n0)\n    {\n      i = SA12[index - n0] * 3 + 1;\n      left = 0;\n      right = n0;\n      while (left < right)\n      {\n        mid = (left + right) / 2;\n\n        if (leq(s[SA0[mid]], s12[SA0[mid] / 3], s[i],\n                s12[(i + 1) / 3 + ((i + 1) % 3 - 1)*n0]))\n          left = mid + 1;\n        else\n          right = mid;\n      }\n      SA[index - n0 + left - flag] = i;\n    }\n    else\n    {\n      i = (SA12[index - n0] - n0) * 3 + 2;\n      left = 0;\n      right = n0;\n      while (left < right)\n      {\n        mid = (left + right) / 2;\n\n        if (leq2(s[SA0[mid]], s[SA0[mid] + 1], s12[SA0[mid] / 3 + n0],\n                 s[i], s[i + 1], s12[(i + 2) / 3 + ((i + 2) % 3 - 1)*n0]))\n          left = mid + 1;\n        else\n          right = mid;\n      }\n      SA[index - n0 + left - flag] = i;\n    }\n  }\n}"
        ]
    },
    "local-ht-cuda": {
        "/Users/gbolet/hecbench-roofline/src/local-ht-cuda/src/kernel.cu": [
            "__device__ void print_mer(cstr_type& mer){\n   // if(threadIdx.x%32 == 0){\n    for(int i = 0; i < mer.length; i++){\n        printf(\"%c\",mer.start_ptr[i]);\n    }\n    printf(\"\\n\");\n   // }\n}\n\ninline __host__ __device__ uint4 make_uint4(int4 a)\n{\n    return make_uint4(uint(a.x), uint(a.y), uint(a.z), uint(a.w));\n}\n\ninline __host__ __device__ uint4 min(uint4 a, uint4 b)\n{\n    return make_uint4(min(a.x,b.x), min(a.y,b.y), min(a.z,b.z), min(a.w,b.w));\n}\n\n__device__ int bcast_warp(int arg) {\n    int laneId = threadIdx.x & 0x1f;\n    int value;\n    if (laneId == 0)        // Note unused variable for\n        value = arg;        // all threads except lane 0\n    value = __shfl_sync(0xffffffff, value, 0);   // Synchronize all threads in warp, and get \"value\" from lane 0\n    if (value != arg && laneId == 0)\n        printf(\"Thread %d failed. with val:%d, arg:%d \\n\", threadIdx.x, value, arg);\n    return value;\n}\n\n__device__ \nvoid count_mers(loc_ht* thrd_loc_ht, char* loc_r_reads, uint32_t max_ht_size, char* loc_r_quals, uint32_t* reads_r_offset, uint32_t& r_rds_cnt, \nuint32_t* rds_count_r_sum, double& loc_ctg_depth, int& mer_len, uint32_t& qual_offset, int64_t& excess_reads, const long int idx){\n    const int lane_id = threadIdx.x%32;\n    cstr_type read;\n    cstr_type qual;\n    uint32_t running_sum_len = 0;\n    // #ifdef DEBUG_PRINT_GPU\n    // int test = 1;\n    // if(DEBUG_PRINT_GPU && idx == test)\n    //     printf(\"inside_count_mers, hash size:%d \\n\", max_ht_size);\n    // #endif\n    for(int i = 0; i < r_rds_cnt; i++){\n        // #ifdef DEBUG_PRINT_GPU\n        // if(DEBUG_PRINT_GPU && idx == test)\n        //     printf(\"read loop iter:%d, thread:%d, loop max:%d\\n\",i, threadIdx.x, r_rds_cnt);\n        // #endif\n        read.start_ptr = loc_r_reads + running_sum_len;\n        qual.start_ptr = loc_r_quals + running_sum_len;\n        if(i == 0){\n            if(idx == 0){\n                read.length = reads_r_offset[(rds_count_r_sum[idx] - r_rds_cnt) + i];\n                qual.length = reads_r_offset[(rds_count_r_sum[idx] - r_rds_cnt) + i];\n                // #ifdef DEBUG_PRINT_GPU\n                // if(DEBUG_PRINT_GPU && idx == test)\n                //     printf(\"rds_count_r_sum[idx]:%d, rds_cnt:%d, read_length:%d, thread_id:%d\\n\",rds_count_r_sum[idx], r_rds_cnt, read.length, threadIdx.x);\n                // #endif\n                }\n            else{  \n                // printf(\"idx:%d, r_rdsx_cnt: %d, i: %d \\n\", idx, r_rds_cnt, i); \n                // printf(\"i:%d, rds_count:%d, idx:%d, thread:%d, blk:%d\\n\", i, r_rds_cnt, idx, threadIdx.x, blockIdx.x); \n                // #ifdef DEBUG_PRINT_GPU\n                // if(DEBUG_PRINT_GPU && idx == test)\n                //     printf(\"rds_count_r_sum[idx]:%d,rds_count_r_sum[idx-1]:%d,i:%d, rds_cnt:%d, reads_offset_0:%d, thread:%d\\n\",rds_count_r_sum[idx], rds_count_r_sum[idx-1],i, r_rds_cnt, read.length, threadIdx.x);\n                // #endif\n                if(rds_count_r_sum[idx - 1] == 0){\n                    read.length = reads_r_offset[(rds_count_r_sum[idx] - r_rds_cnt) + i];\n                    qual.length = reads_r_offset[(rds_count_r_sum[idx] - r_rds_cnt) + i];                    \n                }else{\n                    read.length = reads_r_offset[(rds_count_r_sum[idx] - r_rds_cnt) + i] - reads_r_offset[(rds_count_r_sum[idx - 1] -1)];\n                    qual.length = reads_r_offset[(rds_count_r_sum[idx] - r_rds_cnt) + i] - reads_r_offset[(rds_count_r_sum[idx - 1] -1)];\n                 }\n            }\n        }\n        else{\n            read.length = reads_r_offset[(rds_count_r_sum[idx] - r_rds_cnt) + i] - reads_r_offset[(rds_count_r_sum[idx] - r_rds_cnt) + (i-1)];\n            qual.length = reads_r_offset[(rds_count_r_sum[idx] - r_rds_cnt) + i] - reads_r_offset[(rds_count_r_sum[idx] - r_rds_cnt) + (i-1)];\n            // #ifdef DEBUG_PRINT_GPU\n            // if(DEBUG_PRINT_GPU && idx == test)\n            //      printf(\"rds_count_r_sum[idx]:%d, rds_cnt:%d, reads_offset_0:%d, thread:%d\\n\",rds_count_r_sum[idx], r_rds_cnt, read.length, threadIdx.x);\n            // #endif\n                \n            }\n        // #ifdef DEBUG_PRINT_GPU\n        // if(DEBUG_PRINT_GPU && idx == test){\n        //     printf(\"mer_len:%d, read_len:%d\\n\",mer_len, read.length);\n        //     printf(\"read from idx:%d, thread:%d\\n\", idx, threadIdx.x);\n        //     print_mer(read);\n        //   }\n        // #endif\n        if (mer_len > read.length) // skip the read which is smaller than merlen\n            continue;\n        int num_mers = read.length - mer_len;\n        for( int start = lane_id; start < num_mers; start+=32){\n            //TODO: on cpu side add a check that if a certain read contains 'N', \n            cstr_type mer(read.start_ptr + start, mer_len);\n            loc_ht &temp_Mer = ht_get_atomic(thrd_loc_ht, mer, max_ht_size);\n            \n            int ext_pos = start + mer_len;\n          //  assert(ext_pos < (int)read.length); // TODO: verify that assert works on gpu, for now commenting it out and replacing with printf\n          if(ext_pos >= (int) read.length)\n            printf(\"*********ASSERTION FAILURE IN COUNT_MERS****\");\n            char ext = read.start_ptr[ext_pos];\n            if (ext == 'N') continue; // TODO: why the redundant check?\n            int qual_diff = qual.start_ptr[ext_pos] - qual_offset;\n            if (qual_diff >= LASSM_MIN_QUAL) temp_Mer.val.low_q_exts.inc(ext, 1);\n            if (qual_diff >= LASSM_MIN_HI_QUAL) temp_Mer.val.hi_q_exts.inc(ext, 1);\n\n            //temp_Mer.val.set_ext(loc_ctg_depth);\n        }\n        __syncwarp();\n       running_sum_len += read.length; // right before the for loop ends, update the prev_len to offset next read correctly\n    }\n    __syncwarp();\n    for(auto k = lane_id; k < max_ht_size; k+=32){\n        if(thrd_loc_ht[k].key.length != EMPTY){\n            thrd_loc_ht[k].val.set_ext(loc_ctg_depth);\n        }\n    }\n    __syncwarp();\n    int test = 1;\n    #ifdef DEBUG_PRINT_GPU\n    if(idx == test){\n        if(lane_id == 0)    \n            for(int k = 0; k < max_ht_size; k++){\n            //{\n                if( thrd_loc_ht[k].key.length != EMPTY){\n                printf(\"from ht:\\n\");\n                print_mer(thrd_loc_ht[k].key);\n                printf(\"MerFreq.ext:%c, MerFreq.count:%d\\n\",thrd_loc_ht[k].val.ext,thrd_loc_ht[k].val.count);\n                thrd_loc_ht[k].val.hi_q_exts.print();\n                thrd_loc_ht[k].val.low_q_exts.print();\n                }\n            //}\n            }\n    }\n    #endif\n    __syncwarp();\n}\n\n__device__ void cstr_copy(cstr_type& str1, cstr_type& str2){\n\n    for(int i = 0; i < str2.length; i++){\n        str1.start_ptr[i] = str2.start_ptr[i];\n    }\n    str1.length = str2.length;\n}\n\n__device__ char walk_mers(loc_ht* thrd_loc_ht, loc_ht_bool* thrd_ht_bool, uint32_t max_ht_size, int& mer_len, cstr_type& mer_walk_temp, cstr_type& longest_walk, cstr_type& walk, const int idx, int max_walk_len){\n    char walk_result = 'X';\n    #ifdef DEBUG_PRINT_GPU\n    int test = 1;\n    #endif\n    for( int nsteps = 0; nsteps < max_walk_len; nsteps++){\n        //check if there is a cycle in graph\n        loc_ht_bool &temp_mer_loop = ht_get(thrd_ht_bool, mer_walk_temp, max_walk_len);\n        if(temp_mer_loop.key.length == EMPTY){ // if the mer has not been visited, add it to the table and mark visited\n            temp_mer_loop.key = mer_walk_temp;\n            temp_mer_loop.val = true;\n        }else{\n            walk_result = 'R'; // if the table already contains this mer then a cycle exits, return the walk with repeat.\n            #ifdef DEBUG_PRINT_GPU\n            if(idx == test)\n                printf(\"breaking at cycle found, res: %c\\n\", walk_result);\n            #endif\n            break;\n        }\n\n        loc_ht &temp_mer = ht_get(thrd_loc_ht, mer_walk_temp, max_ht_size);\n        if(temp_mer.key.length == EMPTY){//if mer is not found then dead end reached, terminate the walk\n            walk_result = 'X';\n            #ifdef DEBUG_PRINT_GPU\n            if(idx == test)\n               printf(\"breaking at mer not found,res: %c\\n\", walk_result);\n            #endif\n            break;\n        }\n        char ext = temp_mer.val.ext;\n        if(ext == 'F' || ext == 'X'){ // if the table points that ext is fork or dead end the terminate the walk\n            walk_result = ext;\n            #ifdef DEBUG_PRINT_GPU\n                if(idx == test){\n                    printf(\"breaking at dead end, res: %c\\n\", walk_result);\n                    printf(\"Mer Looked up:\\n\");\n                    print_mer(mer_walk_temp);\n                    printf(\"ext:%c\\n\",temp_mer.val.ext);\n                    printf(\"walk with mer_len:%d\\n\", mer_len);\n                    print_mer(walk);\n                }\n            #endif\n            break;\n        }\n\n         #ifdef DEBUG_PRINT_GPU\n         if(test == idx){\n             printf(\"Mer Looked up:\\n\");\n             print_mer(mer_walk_temp);\n             printf(\"ext:%c\\n\",temp_mer.val.ext);\n             printf(\"walk with mer_len:%d\\n\", mer_len);\n            // print_mer(walk);\n         }\n         #endif\n\n        mer_walk_temp.start_ptr = mer_walk_temp.start_ptr + 1; // increment the mer pointer and append the ext\n        mer_walk_temp.start_ptr[mer_walk_temp.length-1] = ext; // walk pointer points at the end of initial mer point.\n        if (ext != 0) walk.length++;\n\n        \n    }\n    \n    #ifdef DEBUG_PRINT_GPU\n    if(idx == test)\n        for (int k = 0; k < max_walk_len; k++) {\n            if(thrd_ht_bool[k].key.length != EMPTY){\n                printf(\"from bool ht:\\n\");\n                print_mer(thrd_ht_bool[k].key);\n                printf(\"Bool:%d\\n\",thrd_ht_bool[k].val);\n            }\n        }\n    #endif\n    return walk_result;\n}\n\n__global__ void iterative_walks_kernel(uint32_t* cid, uint32_t* ctg_offsets, char* contigs, char* reads_r, char* quals_r,  uint32_t* reads_r_offset,  uint32_t* rds_count_r_sum, \ndouble* ctg_depth, loc_ht* global_ht,  uint32_t* prefix_ht, loc_ht_bool* global_ht_bool, int kmer_len, uint32_t max_mer_len_off, uint32_t *term_counts, int64_t num_walks, int64_t max_walk_len, \nint64_t sum_ext, int32_t max_read_size, int32_t max_read_count, uint32_t qual_offset, char* longest_walks, char* mer_walk_temp, uint32_t* final_walk_lens, int tot_ctgs)\n{\n    const long int idx = threadIdx.x + blockIdx.x * blockDim.x;\n    const long int warp_id_glb = idx/32;\n    const long int lane_id = threadIdx.x%32;\n    if(warp_id_glb < tot_ctgs){ // so that no out of bound accesses \n    cstr_type loc_ctg;\n    char *loc_r_reads, *loc_r_quals;\n    uint32_t r_rds_cnt;\n    loc_ht* loc_mer_map;\n    uint32_t ht_loc_size;\n    loc_ht_bool* loc_bool_map;\n    double loc_ctg_depth;\n    int64_t excess_reads;\n    uint32_t max_ht_size = 0;\n    char* longest_walk_loc;\n    char* loc_mer_walk_temp;\n    #ifdef DEBUG_PRINT_GPU\n    int test = 1;\n    #endif\n\n    int min_mer_len = LASSM_MIN_KMER_LEN;\n    int max_mer_len = LASSM_MAX_KMER_LEN;\n    \n    //the warp portion is for HT phase only so mapping only reads related data based on warp id\n    if(warp_id_glb == 0){\n        loc_ctg.start_ptr = contigs;\n        loc_ctg.length = ctg_offsets[warp_id_glb];\n        loc_bool_map = global_ht_bool + warp_id_glb * max_walk_len;\n        longest_walk_loc = longest_walks + warp_id_glb * max_walk_len;\n        loc_mer_walk_temp = mer_walk_temp + warp_id_glb * (max_walk_len + max_mer_len_off);\n        r_rds_cnt = rds_count_r_sum[warp_id_glb];\n        loc_r_reads = reads_r;\n        loc_r_quals = quals_r;\n        loc_mer_map = global_ht;\n        ht_loc_size = prefix_ht[warp_id_glb];\n        loc_ctg_depth = ctg_depth[warp_id_glb];\n    }else{\n        loc_ctg.start_ptr = contigs + ctg_offsets[warp_id_glb-1];\n        loc_ctg.length = ctg_offsets[warp_id_glb] - ctg_offsets[warp_id_glb - 1];\n        loc_bool_map = global_ht_bool + warp_id_glb * max_walk_len;\n        longest_walk_loc = longest_walks + warp_id_glb * max_walk_len;\n        loc_mer_walk_temp = mer_walk_temp + warp_id_glb * (max_walk_len + max_mer_len_off);\n        loc_ctg_depth = ctg_depth[warp_id_glb];\n        r_rds_cnt = rds_count_r_sum[warp_id_glb] - rds_count_r_sum[warp_id_glb - 1];\n        if (rds_count_r_sum[warp_id_glb - 1] == 0)\n            loc_r_reads = reads_r;\n        else\n            loc_r_reads = reads_r + reads_r_offset[rds_count_r_sum[warp_id_glb - 1] - 1]; // you want to start from where previous contigs, last read ends.\n\n        if (rds_count_r_sum[warp_id_glb - 1] == 0)\n            loc_r_quals = quals_r;\n        else\n            loc_r_quals = quals_r + reads_r_offset[rds_count_r_sum[warp_id_glb - 1] - 1]; // you want to start from where previous contigs, last read ends.\n       \n        loc_mer_map = global_ht + prefix_ht[warp_id_glb - 1];\n        ht_loc_size = prefix_ht[warp_id_glb] - prefix_ht[warp_id_glb - 1];\n    }\n\n    max_ht_size = ht_loc_size;\n    max_mer_len = min(max_mer_len, loc_ctg.length);\n\n    cstr_type longest_walk_thread(longest_walk_loc,0);\n\n    //main for loop\n    int shift = 0;\n\n    for(int mer_len = kmer_len; mer_len >= min_mer_len && mer_len <= max_mer_len; mer_len += shift){\n            // #ifdef DEBUG_PRINT_GPU\n            // if(warp_id_glb == test){\n            //    printf(\"GPU: shift:%d, mer_len:%d, min_mer_len:%d, idx:%d, max_mer_len:%d \\n\", shift, mer_len, min_mer_len, threadIdx.x, max_mer_len);\n            //    printf(\"contig:\\n\");\n            //    print_mer(loc_ctg);\n            //    }\n            // #endif\n\n    if(warp_id_glb < tot_ctgs){ // all warps within this range can go in execute count mers, for walk_mers only the lane 0 of each warp does the work\n            for(uint32_t k = lane_id; k < max_ht_size; k+=32){ // resetting hash table in parallel with warps\n                loc_mer_map[k].key.length = EMPTY;\n            }\n            count_mers(loc_mer_map, loc_r_reads, max_ht_size, loc_r_quals, reads_r_offset, r_rds_cnt, rds_count_r_sum, loc_ctg_depth, mer_len, qual_offset, excess_reads, warp_id_glb);//passing warp_id instead of idx now\n            for(uint32_t k = lane_id; k < max_walk_len; k+=32){ // resetting bool map for next go\n                loc_bool_map[k].key.length = EMPTY;\n            }\n        if(lane_id == 0){ // this phase is processed by single thread of a warp\n            cstr_type ctg_mer(loc_ctg.start_ptr + (loc_ctg.length - mer_len), mer_len);\n            cstr_type loc_mer_walk(loc_mer_walk_temp, 0);\n            cstr_copy(loc_mer_walk, ctg_mer);\n            cstr_type walk(loc_mer_walk.start_ptr + mer_len, 0);\n\n            // #ifdef DEBUG_PRINT_GPU\n            // if(warp_id_glb == test){\n            //     printf(\"read_count:%d, idx:%d\\n\",r_rds_cnt, warp_id_glb);\n            //     printf(\"mer ctg len:%d mer_walk before:\\n\",loc_mer_walk.length);\n            //     print_mer(loc_mer_walk);\n\n            //    printf(\"ctg mer:\\n\");\n            //    print_mer(ctg_mer);\n            // }\n            // #endif\n\n            char walk_res = walk_mers(loc_mer_map, loc_bool_map, max_ht_size, mer_len, loc_mer_walk, longest_walk_thread, walk, warp_id_glb, max_walk_len);\n            // #ifdef DEBUG_PRINT_GPU\n            // if(warp_id_glb == test){\n            //     printf(\"walk_res:%c, idx:%d\\n\",walk_res, warp_id_glb);\n            //     printf(\"GPU: walk.len:%d, longest.len:%d, idx:%d\\n\", walk.length, longest_walk_thread.length, warp_id_glb);\n            // }\n            // #endif\n            //int walk_len = walk.length\n            if (walk.length > longest_walk_thread.length){ // this walk is longer than longest then copy it to longest walk\n                cstr_copy(longest_walk_thread, walk);\n            }\n            if (walk_res == 'X') {\n               // atomicAdd(&term_counts[0], 1);\n                // walk reaches a dead-end, downshift, unless we were upshifting\n                if (shift == LASSM_SHIFT_SIZE) \n                    break;\n                shift = -LASSM_SHIFT_SIZE;\n            }else {\n                //if (walk_res == 'F') \n                   // atomicAdd(&term_counts[1], 1);\n                //else \n                    //atomicAdd(&term_counts[2], 1);\n                // otherwise walk must end with a fork or repeat, so upshift\n                if (shift == -LASSM_SHIFT_SIZE){\n                    // #ifdef DEBUG_PRINT_GPU\n                    // printf(\"breaking at shift neg:%d\\n\", shift);\n                    // #endif\n                    break;\n                    }\n                if (mer_len > loc_ctg.length){\n                    // #ifdef DEBUG_PRINT_GPU\n                    // printf(\"breaking at mer_len too large:%d\\n\", mer_len);\n                    // #endif\n                    break;\n                }\n                shift = LASSM_SHIFT_SIZE;\n            }\n\n        }// lane id cond ended\n        __syncwarp(FULL_MASK);\n        unsigned mask = __activemask();\n        unsigned active = mask & 1; // zero if lane 0 has returned\n        if(active == 0) break; // return if lane 0 has returned\n        shift = bcast_warp(shift);\n        }//warp id cond end\n\n    }\n    if(lane_id == 0){\n    if(longest_walk_thread.length > 0){\n        final_walk_lens[warp_id_glb] = longest_walk_thread.length;\n        // printf(\"final longest walk len:%d/n\", longest_walk_thread.length);\n        // print_mer(longest_walk_thread);\n       // atomicAdd(num_walks, 1);\n     //   atomicAdd(sum_ext, longest_walk_thread.length);\n    }else{\n        final_walk_lens[warp_id_glb] = 0;\n    }\n    }\n\n    #ifdef DEBUG_PRINT_GPU\n    if(idx == test){\n       // printf(\"walk:\\n\");\n       // print_mer(walk);\n       // printf(\"walk len:%d\\n\", walk.length);\n       // printf(\"mer_walk after:\\n\");\n       // print_mer(loc_mer_walk);\n       // printf(\"mer_walk after, len:%d\\n\", loc_mer_walk.length);\n        }\n        //printf(\"walk result:%c\\n\", walk_res);\n    #endif\n}//end if to check if idx exceeds contigs\n}"
        ]
    },
    "hellinger-cuda": {
        "/Users/gbolet/hecbench-roofline/src/hellinger-cuda/main.cu": [
            "#define FP float\n\n\n__device__ __forceinline__ double SQRT(double x) {return sqrt(x);}\n\n__global__ \nvoid hellinger(\n  const FP *__restrict__ a, \n  const FP *__restrict__ b, \n        FP *__restrict__ c, \n  const int m, const int n, const int k)\n{\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    if( col < k && row < m)\n    {\n        FP sum = 0;\n        for(int i = 0; i < n; i++)\n        {\n            sum += SQRT(a[row * n + i] * b[i * k + col]);\n        }\n        const FP value = (FP)1.0 - sum;\n        const FP gate = (!signbit(value));\n        c[row * k + col] = SQRT(gate * value);\n    }\n}"
        ]
    },
    "cmp-cuda": {
        "/Users/gbolet/hecbench-roofline/src/cmp-cuda/main.cu": [
            "__global__ void\ninit_c(real *c, real inc, real c0) \n{\n  int i = blockIdx.x;\n  c[i] = c0 + inc*i;\n}",
            "__global__ void\ninit_half(const real* __restrict__ scalco, \n          const real* __restrict__ gx, \n          const real* __restrict__ gy, \n          const real* __restrict__ sx, \n          const real* __restrict__ sy, \n          real* __restrict__ h) \n{\n  int i = blockIdx.x;\n  real _s = scalco[i];\n\n  if(-EPSILON < _s && _s < EPSILON) _s = 1.0f;\n  else if(_s < 0) _s = 1.0f / _s;\n\n  real hx = (gx[i] - sx[i]) * _s;\n  real hy = (gy[i] - sy[i]) * _s;\n\n  h[i] = 0.25f * (hx * hx + hy * hy) / FACTOR;\n}",
            "inline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\ninline __host__ __device__ float4 sqrtf(float4 v)\n{\n    return make_float4(sqrtf(v.x), sqrtf(v.y), sqrtf(v.z), sqrtf(v.w));\n}\n\n__global__ void\ncompute_semblances(const real* __restrict__ h, \n                   const real* __restrict__ c, \n                   const real* __restrict__ samples, \n                   real* __restrict__ num,\n                   real* __restrict__ stt,\n                   int t_id0, \n                   int t_idf,\n                   real _idt,\n                   real _dt,\n                   int _tau,\n                   int _w,\n                   int nc,\n                   int ns) \n{\n  real _den = 0.0f, _ac_linear = 0.0f, _ac_squared = 0.0f;\n  real _num[MAX_W],  m = 0.0f;\n  int err = 0;\n\n  int i = blockIdx.x * NTHREADS + threadIdx.x;\n\n  int t0 = i / nc;\n  int c_id = i % nc;\n\n  if(i < ns * nc)\n  {\n    real _c = c[c_id];\n    real _t0 = _dt * t0;\n    _t0 *= _t0;\n\n    for(int j=0; j < _w; j++) _num[j] = 0.0f;\n\n    for(int t_id=t_id0; t_id < t_idf; t_id++) {\n      real t = sqrtf(_t0 + _c * h[t_id]) * _idt;\n\n      int it = (int)( t );\n      int ittau = it - _tau;\n      real x = t - (real)it;\n\n      if(ittau >= 0 && it + _tau + 1 < ns) {\n        int k1 = ittau + (t_id-t_id0)*ns;\n        real sk1p1 = samples[k1], sk1;\n\n        for(int j=0; j < _w; j++) {\n          k1++;\n          sk1 = sk1p1;\n          sk1p1 = samples[k1];\n          // linear interpolation optmized for this problema\n          real v = (sk1p1 - sk1) * x + sk1;\n\n          _num[j] += v;\n          _den += v * v;\n          _ac_linear += v;\n        }\n        m += 1;\n      } else { err++; }\n    }\n\n    // Reduction for num\n    for(int j=0; j < _w; j++) _ac_squared += _num[j] * _num[j];\n\n    // Evaluate semblances\n    if(_den > EPSILON && m > EPSILON && _w > EPSILON && err < 2) {\n      num[i] = _ac_squared / (_den * m);\n      stt[i] = _ac_linear  / (_w   * m);\n    }\n    else {\n      num[i] = -1.0f;\n      stt[i] = -1.0f;\n    }\n  }\n}",
            "__global__ void\nredux_semblances(const real* __restrict__ num, \n                 const real* __restrict__ stt, \n                 int*  __restrict__ ctr, \n                 real* __restrict__ str, \n                 real* __restrict__ stk,\n                 const int nc, \n                 const int cdp_id,\n                 const int ns) \n{\n  int t0 = blockIdx.x * NTHREADS + threadIdx.x;\n\n  if(t0 < ns)\n  {\n    real max_sem = 0.0f;\n    int max_c = -1;\n\n    for(int it=t0*nc; it < (t0+1)*nc ; it++) {\n      real _num = num[it];\n      if(_num > max_sem) {\n        max_sem = _num;\n        max_c = it;\n      }\n    }\n\n    ctr[cdp_id*ns + t0] = max_c % nc;\n    str[cdp_id*ns + t0] = max_sem;\n    stk[cdp_id*ns + t0] = max_c > -1 ? stt[max_c] : 0;\n  }\n}"
        ]
    },
    "aobench-cuda": {
        "/Users/gbolet/hecbench-roofline/src/aobench-cuda/ao.cu": [
            "inline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\ninline __host__ __device__ float4 fabs(float4 v)\n{\n    return make_float4(fabs(v.x), fabs(v.y), fabs(v.z), fabs(v.w));\n}\n\ninline __host__ __device__ float4 sqrtf(float4 v)\n{\n    return make_float4(sqrtf(v.x), sqrtf(v.y), sqrtf(v.z), sqrtf(v.w));\n}\n\n__device__\nstatic float vdot(vec v0, vec v1)\n{\n  return v0.x * v1.x + v0.y * v1.y + v0.z * v1.z;\n}\n\n__device__\nstatic void vcross(vec *c, vec v0, vec v1)\n{\n\n  c->x = v0.y * v1.z - v0.z * v1.y;\n  c->y = v0.z * v1.x - v0.x * v1.z;\n  c->z = v0.x * v1.y - v0.y * v1.x;\n}\n\n__device__\nstatic void vnormalize(vec *c)\n{\n  float length = sqrtf(vdot((*c), (*c)));\n\n  if (fabs(length) > 1.0e-17f) {\n    c->x /= length;\n    c->y /= length;\n    c->z /= length;\n  }\n}\n\n__device__\nvoid orthoBasis(vec *basis, vec n)\n{\n  basis[2] = n;\n  basis[1].x = 0.f; basis[1].y = 0.f; basis[1].z = 0.f;\n\n  if ((n.x < 0.6f) && (n.x > -0.6f)) {\n    basis[1].x = 1.0f;\n  } else if ((n.y < 0.6f) && (n.y > -0.6f)) {\n    basis[1].y = 1.0f;\n  } else if ((n.z < 0.6f) && (n.z > -0.6f)) {\n    basis[1].z = 1.0f;\n  } else {\n    basis[1].x = 1.0f;\n  }\n\n  vcross(&basis[0], basis[1], basis[2]);\n  vnormalize(&basis[0]);\n\n  vcross(&basis[1], basis[2], basis[0]);\n  vnormalize(&basis[1]);\n}\n\n__device__\nvoid ray_plane_intersect(Isect *isect, const Ray *ray, const Plane *plane)\n{\n  float d = -vdot(plane->p, plane->n);\n  float v = vdot(ray->dir, plane->n);\n\n  if (fabsf(v) < 1.0e-17f) return;\n\n  float t = -(vdot(ray->org, plane->n) + d) / v;\n\n  if ((t > 0.f) && (t < isect->t)) {\n    isect->t = t;\n    isect->hit = 1;\n\n    isect->p.x = ray->org.x + ray->dir.x * t;\n    isect->p.y = ray->org.y + ray->dir.y * t;\n    isect->p.z = ray->org.z + ray->dir.z * t;\n\n    isect->n = plane->n;\n  }\n}\n\n__device__\nvoid ray_sphere_intersect(Isect *isect, const Ray *ray, const Sphere *sphere)\n{\n  vec rs;\n\n  rs.x = ray->org.x - sphere->center.x;\n  rs.y = ray->org.y - sphere->center.y;\n  rs.z = ray->org.z - sphere->center.z;\n\n  float B = vdot(rs, ray->dir);\n  float C = vdot(rs, rs) - sphere->radius * sphere->radius;\n  float D = B * B - C;\n\n  if (D > 0.f) {\n    float t = -B - sqrtf(D);\n\n    if ((t > 0.f) && (t < isect->t)) {\n      isect->t = t;\n      isect->hit = 1;\n\n      isect->p.x = ray->org.x + ray->dir.x * t;\n      isect->p.y = ray->org.y + ray->dir.y * t;\n      isect->p.z = ray->org.z + ray->dir.z * t;\n\n      isect->n.x = isect->p.x - sphere->center.x;\n      isect->n.y = isect->p.y - sphere->center.y;\n      isect->n.z = isect->p.z - sphere->center.z;\n\n      vnormalize(&(isect->n));\n    }\n  }\n}\n\n__device__\nvoid ambient_occlusion(vec *col, const Isect *isect, const Sphere *spheres, const Plane *plane, RNG &rng)\n{\n  int    i, j;\n  int    ntheta = NAO_SAMPLES;\n  int    nphi   = NAO_SAMPLES;\n  float eps = 0.0001f;\n\n  vec p;\n\n  p.x = isect->p.x + eps * isect->n.x;\n  p.y = isect->p.y + eps * isect->n.y;\n  p.z = isect->p.z + eps * isect->n.z;\n\n  vec basis[3];\n  orthoBasis(basis, isect->n);\n\n\n  float occlusion = 0.f;\n\n  for (j = 0; j < ntheta; j++) {\n    for (i = 0; i < nphi; i++) {\n      float theta = sqrtf(rng());\n      float phi = 2.0f * (float)M_PI * rng();\n      float x = cosf(phi) * theta;\n      float y = sinf(phi) * theta;\n      float z = sqrtf(1.0f - theta * theta);\n\n      // local -> global\n      float rx = x * basis[0].x + y * basis[1].x + z * basis[2].x;\n      float ry = x * basis[0].y + y * basis[1].y + z * basis[2].y;\n      float rz = x * basis[0].z + y * basis[1].z + z * basis[2].z;\n\n      Ray ray;\n\n      ray.org = p;\n      ray.dir.x = rx;\n      ray.dir.y = ry;\n      ray.dir.z = rz;\n\n      Isect occIsect;\n      occIsect.t   = 1.0e+17f;\n      occIsect.hit = 0;\n\n      ray_sphere_intersect(&occIsect, &ray, spheres); \n      ray_sphere_intersect(&occIsect, &ray, spheres+1); \n      ray_sphere_intersect(&occIsect, &ray, spheres+2); \n      ray_plane_intersect (&occIsect, &ray, plane); \n\n      if (occIsect.hit) occlusion += 1.f;\n\n    }\n  }\n\n  occlusion = (ntheta * nphi - occlusion) / (float)(ntheta * nphi);\n\n  col->x = occlusion;\n  col->y = occlusion;\n  col->z = occlusion;\n}\n\n__device__\nunsigned char clamp(float f)\n{\n  int i = (int)(f * 255.5f);\n\n  if (i < 0) i = 0;\n  if (i > 255) i = 255;\n\n  return (unsigned char)i;\n}\n\n__global__ void\nrender_kernel (unsigned char *fimg, const Sphere *spheres, const Plane plane, \n    const int h, const int w, const int nsubsamples)\n{\n  int x = blockIdx.x * blockDim.x + threadIdx.x;\n  int y = blockIdx.y * blockDim.y + threadIdx.y;\n  if (y < h && x < w) {\n\n    RNG rng(y * w + x);\n    float s0 = 0;\n    float s1 = 0;\n    float s2 = 0;\n\n    for(int  v = 0; v < nsubsamples; v++ ) {\n      for(int  u = 0; u < nsubsamples; u++ ) {\n        float px = ( x + ( u / ( float )nsubsamples ) - ( w / 2.0f ) ) / ( w / 2.0f );\n        float py = -( y + ( v / ( float )nsubsamples ) - ( h / 2.0f ) ) / ( h / 2.0f );\n\n        Ray ray;\n        ray.org.x = 0.f;\n        ray.org.y = 0.f;\n        ray.org.z = 0.f;\n        ray.dir.x = px;\n        ray.dir.y = py;\n        ray.dir.z = -1.f;\n        vnormalize( &( ray.dir ) );\n\n        Isect isect;\n        isect.t = 1.0e+17f;\n        isect.hit = 0;\n\n        ray_sphere_intersect( &isect, &ray, spheres   );\n        ray_sphere_intersect( &isect, &ray, spheres + 1  );\n        ray_sphere_intersect( &isect, &ray, spheres + 2  );\n        ray_plane_intersect ( &isect, &ray, &plane );\n\n        if( isect.hit ) {\n          vec col;\n          ambient_occlusion( &col, &isect, spheres, &plane, rng );\n          s0 += col.x;\n          s1 += col.y;\n          s2 += col.z;\n        }\n\n      }\n    }\n    fimg[ 3 * ( y * w + x ) + 0 ] = clamp ( s0 / ( float )( nsubsamples * nsubsamples ) );\n    fimg[ 3 * ( y * w + x ) + 1 ] = clamp ( s1 / ( float )( nsubsamples * nsubsamples ) );\n    fimg[ 3 * ( y * w + x ) + 2 ] = clamp ( s2 / ( float )( nsubsamples * nsubsamples ) );\n  }\n}"
        ]
    },
    "nbnxm-cuda": {
        "/Users/gbolet/hecbench-roofline/src/nbnxm-cuda/main.cu": [
            "inline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\ninline __host__ __device__ uint4 make_uint4(int4 a)\n{\n    return make_uint4(uint(a.x), uint(a.y), uint(a.z), uint(a.w));\n}\n\ninline __host__ __device__ uint4 max(uint4 a, uint4 b)\n{\n    return make_uint4(max(a.x,b.x), max(a.y,b.y), max(a.z,b.z), max(a.w,b.w));\n}\n\n__device__ __forceinline__\nfloat4 __shfl_down(const float4 var, const uint32_t srcLane, const uint32_t width = 32) {\n  float4 output;\n  output.x = __shfl_down_sync(0xFFFFFFFF, var.x, srcLane, width);\n  output.y = __shfl_down_sync(0xFFFFFFFF, var.y, srcLane, width);\n  output.z = __shfl_down_sync(0xFFFFFFFF, var.z, srcLane, width);\n  output.w = __shfl_down_sync(0xFFFFFFFF, var.w, srcLane, width);\n  return output;\n}\n\nstatic __forceinline__ __device__\nvoid reduceForceJShuffle(\n    Float3 f,\n    const int tidxi,\n    const int aidx,\n    Float3 *a_f)\n{\n  static_assert(c_clSize == 8 || c_clSize == 4);\n  f[0] += __shfl_down(f[0], 1);\n  f[1] += __shfl_up(f[1], 1);\n  f[2] += __shfl_down(f[2], 1);\n  if (tidxi & 1)\n  {\n    f[0] = f[1];\n  }\n\n  f[0] += __shfl_down(f[0], 2);\n  f[2] += __shfl_up(f[2], 2);\n  if (tidxi & 2)\n  {\n    f[0] = f[2];\n  }\n\n  if constexpr (c_clSize == 8)\n  {\n    f[0] += __shfl_down(f[0], 4);\n  }\n\n  if (tidxi < 3)\n  {\n    atomicAdd(&a_f[aidx][tidxi], f[0]);\n  }\n}\n\nstatic __forceinline__ __device__\nValueType norm2(BasicVector<ValueType> v)\n{\n    return v.norm2();\n}\n\nstatic __forceinline__  __device__\nfloat pmeCorrF(const float z2)\n{\n  constexpr float FN6 = -1.7357322914161492954e-8F;\n  constexpr float FN5 = 1.4703624142580877519e-6F;\n  constexpr float FN4 = -0.000053401640219807709149F;\n  constexpr float FN3 = 0.0010054721316683106153F;\n  constexpr float FN2 = -0.019278317264888380590F;\n  constexpr float FN1 = 0.069670166153766424023F;\n  constexpr float FN0 = -0.75225204789749321333F;\n\n  constexpr float FD4 = 0.0011193462567257629232F;\n  constexpr float FD3 = 0.014866955030185295499F;\n  constexpr float FD2 = 0.11583842382862377919F;\n  constexpr float FD1 = 0.50736591960530292870F;\n  constexpr float FD0 = 1.0F;\n\n  const float z4 = z2 * z2;\n        float polyFD0 = FD4 * z4 + FD2;\n  const float polyFD1 = FD3 * z4 + FD1;\n  polyFD0 = polyFD0 * z4 + FD0;\n  polyFD0 = polyFD1 * z2 + polyFD0;\n  polyFD0 = 1.0F / polyFD0;\n\n  float polyFN0 = FN6 * z4 + FN4;\n  float polyFN1 = FN5 * z4 + FN3;\n  polyFN0       = polyFN0 * z4 + FN2;\n  polyFN1       = polyFN1 * z4 + FN1;\n  polyFN0       = polyFN0 * z4 + FN0;\n  polyFN0       = polyFN1 * z2 + polyFN0;\n\n  return polyFN0 * polyFD0;\n}\n\nstatic __forceinline__ __device__\nvoid reduceForceIAndFShiftXYZ(\n    float* sm_buf,\n    const float* fCiBufX,\n    const float* fCiBufY,\n    const float* fCiBufZ,\n    const bool   calcFShift,\n    const int    tidxi,\n    const int    tidxj,\n    const int    sci,\n    const int    shift,\n    Float3 *a_f,\n    Float3 *a_fShift) \n{\n  static constexpr int bufStride  = c_clSize * c_clSize;\n  static constexpr int clSizeLog2 = StaticLog2<c_clSize>::value;\n  const int            tidx       = tidxi + tidxj * c_clSize;\n  float                fShiftBuf  = 0.0F;\n#pragma unroll(8)\n  for (int ciOffset = 0; ciOffset < c_nbnxnGpuNumClusterPerSupercluster; ciOffset++)\n  {\n    const int aidx = (sci * c_nbnxnGpuNumClusterPerSupercluster + ciOffset) * c_clSize + tidxi;\n    /* store i forces in shmem */\n    sm_buf[tidx]                 = fCiBufX[ciOffset];\n    sm_buf[bufStride + tidx]     = fCiBufY[ciOffset];\n    sm_buf[2 * bufStride + tidx] = fCiBufZ[ciOffset];\n    __syncthreads();\n\n    /* Reduce the initial c_clSize values for each i atom to half\n     * every step by using c_clSize * i threads. */\n    int i = c_clSize / 2;\n    for (int j = clSizeLog2 - 1; j > 0; j--)\n    {\n      if (tidxj < i)\n      {\n        sm_buf[tidx] += sm_buf[tidx + i * c_clSize];\n        sm_buf[bufStride + tidx] += sm_buf[bufStride + tidx + i * c_clSize];\n        sm_buf[2 * bufStride + tidx] += sm_buf[2 * bufStride + tidx + i * c_clSize];\n      }\n      i >>= 1;\n      __syncthreads();\n    }\n\n    /* i == 1, last reduction step, writing to global mem */\n    /* Split the reduction between the first 3 line threads\n       Threads with line id 0 will do the reduction for (float3).x components\n       Threads with line id 1 will do the reduction for (float3).y components\n       Threads with line id 2 will do the reduction for (float3).z components. */\n    if (tidxj < 3)\n    {\n      const float f = sm_buf[tidxj * bufStride + tidxi] + \n                      sm_buf[tidxj * bufStride + c_clSize + tidxi];\n\n      atomicAdd(&a_f[aidx][tidxj], f);\n\n      if (calcFShift) fShiftBuf += f;\n    }\n    __syncthreads();\n  }\n  /* add up local shift forces into global mem */\n  if (calcFShift)\n  {\n    /* Only threads with tidxj < 3 will update fshift.\n       The threads performing the update must be the same as the threads\n       storing the reduction result above. */\n    if (tidxj < 3)\n    {\n      if constexpr (c_clSize == 4)\n      {\n        fShiftBuf += __shfl_down(fShiftBuf, 1);\n        fShiftBuf += __shfl_down(fShiftBuf, 2);\n        if (tidxi == 0)\n        {\n          atomicAdd(&a_fShift[shift][tidxj], fShiftBuf);\n        }\n      }\n      else\n      {\n        atomicAdd(&a_fShift[shift][tidxj], fShiftBuf);\n      }\n    }\n  }\n}\n\nstatic __forceinline__ __device__\nvoid reduceForceJ(\n    float* sm_buf,\n    Float3 f,\n    const int tidxi,\n    const int tidxj,\n    const int aidx,\n    Float3 *a_f)\n{\n  reduceForceJShuffle(f, tidxi, aidx, a_f);\n}\n\n__global__ void nbnxmKernelTest(\n    const Float4 *__restrict__ a_xq,\n    Float3 *__restrict__ a_f,\n    Float3 *__restrict__ a_shiftVec,\n    Float3 *__restrict__ a_fShift,\n    nbnxn_cj4_t *__restrict__ a_plistCJ4,\n    const nbnxn_sci_t *__restrict__ a_plistSci,\n    const nbnxn_excl_t *__restrict__ a_plistExcl,\n    const int *__restrict__ a_atomTypes,\n    const Float2 *__restrict__ a_nbfp,\n    const int numTypes,\n    const float rCoulombSq,\n    const float ewaldBeta,\n    const float epsFac,\n    const bool calcShift) \n{\n  constexpr int prunedClusterPairSize = c_clSize * c_splitClSize;\n\n  /* thread/block/warp id-s */\n  const unsigned tidxi = threadIdx.x;\n  const unsigned tidxj = threadIdx.y;\n  const unsigned tidx  = tidxj * c_clSize + tidxi;\n  const unsigned bidx = blockIdx.z;\n\n  const unsigned imeiIdx = tidx / prunedClusterPairSize;\n\n  constexpr size_t local_mem_size =\n    c_nbnxnGpuNumClusterPerSupercluster * c_clSize * sizeof(Float4) + // sm_xq\n    c_clSize * c_clSize * DIM * sizeof(float) +                       // sm_reductionBuffer\n    c_nbnxnGpuNumClusterPerSupercluster * c_clSize * sizeof(int);     // sm_atomTypeI\n\n  __shared__ uint8_t localPtr[local_mem_size];\n  uint8_t *ptr = localPtr; // localPtr is read only\n\n  Float4* sm_xq = reinterpret_cast<Float4*>(ptr);\n  ptr += c_nbnxnGpuNumClusterPerSupercluster * c_clSize * sizeof(Float4);\n  float* sm_reductionBuffer = reinterpret_cast<float*>(ptr);\n  ptr += c_clSize * c_clSize * DIM * sizeof(float);\n  int* sm_atomTypeI = reinterpret_cast<int*>(ptr);\n\n  float fCiBufX[c_nbnxnGpuNumClusterPerSupercluster] = {0};\n  float fCiBufY[c_nbnxnGpuNumClusterPerSupercluster] = {0};\n  float fCiBufZ[c_nbnxnGpuNumClusterPerSupercluster] = {0};\n\n  const nbnxn_sci_t nbSci     = a_plistSci[bidx];\n  const int         sci       = nbSci.sci;\n  const int         cij4Start = nbSci.cj4_ind_start;\n  const int         cij4End   = nbSci.cj4_ind_end;\n  const int         nbScishift = nbSci.shift;\n\n  // Only needed if props.elecEwaldAna\n  const float beta2 = ewaldBeta * ewaldBeta;\n  const float beta3 = ewaldBeta * ewaldBeta * ewaldBeta;\n\n  for (int i = 0; i < c_nbnxnGpuNumClusterPerSupercluster; i += c_clSize)\n  {\n    /* Pre-load i-atom x and q into shared memory */\n    const int ci = sci * c_nbnxnGpuNumClusterPerSupercluster + tidxj + i;\n    const int ai = ci * c_clSize + tidxi;\n\n    const Float3 shift = a_shiftVec[nbScishift];\n    Float4       xqi = a_xq[ai];\n    xqi += make_float4(shift[0], shift[1], shift[2], 0.0F);\n    xqi.w *= epsFac;\n    sm_xq[(tidxj + i) * c_clSize + tidxi] = xqi;\n    sm_atomTypeI[(tidxj + i) * c_clSize + tidxi] = a_atomTypes[ai];\n  }\n  __syncthreads();\n\n  // Only needed if (doExclusionForces)\n  const bool nonSelfInteraction = !(nbScishift == c_centralShiftIndex & tidxj <= tidxi);\n\n  // loop over the j clusters = seen by any of the atoms in the current super-cluster\n  for (int j4 = cij4Start; j4 < cij4End; j4 += 1)\n  {\n    unsigned imask = a_plistCJ4[j4].imei[imeiIdx].imask;\n    if (!imask)\n    {\n      continue;\n    }\n    const int wexclIdx = a_plistCJ4[j4].imei[imeiIdx].excl_ind;\n    const unsigned wexcl = a_plistExcl[wexclIdx].pair[tidx & (prunedClusterPairSize - 1)];\n\n    for (int jm = 0; jm < c_nbnxnGpuJgroupSize; jm++)\n    {\n      const bool maskSet =\n        imask & (superClInteractionMask << (jm * c_nbnxnGpuNumClusterPerSupercluster));\n      if (!maskSet)\n      {\n        continue;\n      }\n      unsigned  maskJI = (1U << (jm * c_nbnxnGpuNumClusterPerSupercluster));\n      const int cj     = a_plistCJ4[j4].cj[jm];\n      const int aj     = cj * c_clSize + tidxj;\n\n      // load j atom data\n      const Float4 xqj = a_xq[aj];\n\n      const Float3 xj(xqj.x, xqj.y, xqj.z);\n      const float  qj = xqj.w;\n      int          atomTypeJ; // Only needed if (!props.vdwComb)\n      atomTypeJ = a_atomTypes[aj];\n\n      Float3 fCjBuf(0.0F, 0.0F, 0.0F);\n\n#pragma unroll(8)\n      for (int i = 0; i < c_nbnxnGpuNumClusterPerSupercluster; i++)\n      {\n        if (imask & maskJI)\n        {\n          // i cluster index\n          const int ci = sci * c_nbnxnGpuNumClusterPerSupercluster + i;\n          // all threads load an atom from i cluster ci into shmem!\n          const Float4 xqi = sm_xq[i * c_clSize + tidxi];\n          const Float3 xi(xqi.x, xqi.y, xqi.z);\n\n          // distance between i and j atoms\n          const Float3 rv = xi - xj;\n          float        r2 = norm2(rv);\n\n          const float pairExclMask = (wexcl & maskJI) ? 1.0F : 0.0F;\n\n          // cutoff & exclusion check\n\n          const bool notExcluded = (nonSelfInteraction | (ci != cj));\n\n          // Check optimal way of branching here.\n          if ((r2 < rCoulombSq) && notExcluded)\n          {\n            const float qi = xqi.w;\n            int         atomTypeI; // Only needed if (!props.vdwComb)\n            Float2      c6c12;\n\n            /* LJ 6*C6 and 12*C12 */\n            atomTypeI = sm_atomTypeI[i * c_clSize + tidxi];\n            c6c12     = a_nbfp[numTypes * atomTypeI + atomTypeJ];\n\n            // c6 and c12 are unused and garbage iff props.vdwCombLB && !doCalcEnergies\n            const float c6  = c6c12.x;\n            const float c12 = c6c12.y;\n\n            // Ensure distance do not become so small that r^-12 overflows\n            r2 = max(r2, c_nbnxnMinDistanceSquared);\n\n            const float rInv = __frsqrt_rn(r2);\n            const float r2Inv = rInv * rInv;\n            float       r6Inv, fInvR;\n            r6Inv = r2Inv * r2Inv * r2Inv;\n            r6Inv *= pairExclMask;\n            fInvR = r6Inv * (c12 * r6Inv - c6) * r2Inv;\n\n            fInvR += qi * qj\n              * (pairExclMask * r2Inv * rInv + pmeCorrF(beta2 * r2) * beta3);\n\n            const Float3 forceIJ = rv * fInvR;\n\n            /* accumulate j forces in registers */\n            fCjBuf -= forceIJ;\n            /* accumulate i forces in registers */\n            fCiBufX[i] += forceIJ[0];\n            fCiBufY[i] += forceIJ[1];\n            fCiBufZ[i] += forceIJ[2];\n          } // (r2 < rCoulombSq) && notExcluded\n        }     // (imask & maskJI)\n        /* shift the mask bit by 1 */\n        maskJI += maskJI;\n      } // for (int i = 0; i < c_nbnxnGpuNumClusterPerSupercluster; i++)\n      /* reduce j forces */\n      reduceForceJ(sm_reductionBuffer, fCjBuf, tidxi, tidxj, aj, a_f);\n    } // for (int jm = 0; jm < c_nbnxnGpuJgroupSize; jm++)\n  } // for (int j4 = cij4Start; j4 < cij4End; j4 += 1)\n\n  {\n    const nbnxn_sci_t nbSci = a_plistSci[bidx];\n    const int sci = nbSci.sci;\n\n    // skip central shifts when summing shift forces\n    const bool doCalcShift = (calcShift && nbSci.shift != c_centralShiftIndex);\n\n    reduceForceIAndFShiftXYZ(\n        sm_reductionBuffer, fCiBufX, fCiBufY, fCiBufZ, doCalcShift, tidxi, tidxj, sci, nbSci.shift, a_f, a_fShift);\n  }\n}"
        ]
    },
    "concat-cuda": {
        "/Users/gbolet/hecbench-roofline/src/concat-cuda/main.cu": [
            "#define T ((int)32)\n\n\n__forceinline__ __host__ __device__\nint flat_3dim(int id1, int id2, int id3, int dim2, int dim3) {\n  return id1 * dim2 * dim3 + id2 * dim3 + id3;\n}\n\n__global__\nvoid concat (const T *__restrict__ inp1,\n             const T *__restrict__ inp2,\n                   T *output,\n             int sz0, int sz2, int sz1_1, int sz1_2)\n{\n  int nele = sz0 * sz2 * (sz1_1 + sz1_2);\n  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  if (idx >= nele) return;\n\n  float *dst_ptr = (float *)output + idx;\n  int idx2 = idx % sz2;\n  idx = idx / sz2;\n  int idx1 = idx % (sz1_1 + sz1_2);\n  int idx0 = idx / (sz1_1 + sz1_2);\n  float *src_ptr;\n  int sz1;\n  if (idx1 < sz1_1) {\n    sz1 = sz1_1;\n    src_ptr = (float *)inp1;\n  } else {\n    idx1 -= sz1_1;\n    sz1 = sz1_2;\n    src_ptr = (float *)inp2;\n  }\n  src_ptr += flat_3dim(idx0, idx1, idx2, sz1, sz2);\n  *dst_ptr = *src_ptr;\n}"
        ]
    },
    "layernorm-cuda": {
        "/Users/gbolet/hecbench-roofline/src/layernorm-cuda/main.cu": [
            "__global__ void layernorm_forward_kernel1(float* __restrict__ out, float* __restrict__ mean, float* __restrict__ rstd,\n                                    const float*  __restrict__ inp, const float*  __restrict__ weight,\n                                    const float* __restrict__ bias, int N, int C) {\n    namespace cg = cooperative_groups;\n    cg::thread_block block = cg::this_thread_block();\n    cg::thread_block_tile<32> warp = cg::tiled_partition<32>(block);\n    // meta_group_size is the number of warps in a block, and meta_group_rank is the warp index\n    int idx = blockIdx.x * warp.meta_group_size() + warp.meta_group_rank();\n    if(idx >= N) {\n        return;\n    }\n\n    // the row of input that this group of threads is responsible for\n    const float* x = inp + idx * C;\n\n    // mean\n    float sum = 0.0f;\n    for (int i = warp.thread_rank(); i < C; i += warp.size()) {\n        sum += x[i];\n    }\n    sum = cg::reduce(warp, sum, cg::plus<float>{});\n    float m = sum / C;\n    if(warp.thread_rank() == 0 && mean != nullptr) {\n        __stcs(mean + idx, m);\n    }\n\n    // rstd\n    sum = 0.0f;\n    for (int i = warp.thread_rank(); i < C; i += warp.size()) {\n        float diff = x[i] - m;\n        sum += diff * diff;\n    }\n    sum = cg::reduce(warp, sum, cg::plus<float>{});\n    float s = rsqrtf(sum / C + 1e-5f);\n    if(warp.thread_rank() == 0 && rstd != nullptr) {\n        __stcs(rstd + idx, s);\n    }\n\n    // final normalization and scaling by weight/bias\n    float* o = out + idx * C;\n    for (int c = warp.thread_rank(); c < C; c += warp.size()) {\n        // load and store using the .cs \"streaming\" hint to the compiler,\n        // indicating that this data will not be reused soon, and can be streamed through the caches\n        // this allows the threads to get more cache-hits for the (shared) weight and bias parameters\n        float n = s * (__ldcs(x+c) - m);\n        __stcs(o+c, n * weight[c] + bias[c]);\n    }\n}",
            "__global__ void layernorm_forward_kernel2(float* __restrict__ out, float* __restrict__ mean, float* __restrict__ rstd,\n                                    const float*  __restrict__ inp, const float*  __restrict__ weight,\n                                    const float* __restrict__ bias, int N, int C) {\n    namespace cg = cooperative_groups;\n    cg::thread_block block = cg::this_thread_block();\n    cg::thread_block_tile<32> warp = cg::tiled_partition<32>(block);\n    int idx = blockIdx.x * warp.meta_group_size() + warp.meta_group_rank();\n    if(idx >= N) {\n        return;\n    }\n\n    // the row of input that this group of threads is responsible for\n    const float* x = inp + idx * C;\n\n    // thread coarsening through the row, reduce the sum in series\n    float sum = 0.0; // stores sum(x)\n    float sum2 = 0.0; // stores sum(x**2)\n    for (int i = warp.thread_rank(); i < C; i += warp.size()) {\n        float xi = x[i];\n        sum += xi;\n        sum2 += xi * xi;\n    }\n    // warp-level reduction at the end\n    sum = cg::reduce(warp, sum, cg::plus<float>{}); // sum(x)\n    sum2 = cg::reduce(warp, sum2, cg::plus<float>{}); // sum(x**2)\n    sum /= C; // mean(x)\n    sum2 /= C; // mean(x**2)\n\n    // mean, var, rstd\n    float m = sum;\n    float var = sum2 - sum * sum;\n    float s = rsqrtf(var + 1e-5f);\n\n    // store the mean, no need to cache it\n    if(warp.thread_rank() == 0 && mean != nullptr) {\n        __stcs(mean + idx, m);\n    }\n    // store the rstd, no need to cache it\n    if(warp.thread_rank() == 0 && rstd != nullptr) {\n        __stcs(rstd + idx, s);\n    }\n    // final normalization and scaling by weight/bias\n    float* o = out + idx * C;\n    for (int c = warp.thread_rank(); c < C; c += warp.size()) {\n        float n = s * (__ldcs(x+c) - m);\n        __stcs(o+c, n * weight[c] + bias[c]);\n    }\n}"
        ]
    },
    "ge-spmm-cuda": {
        "/Users/gbolet/hecbench-roofline/src/ge-spmm-cuda/kernels.h": [
            "#define T ((int)32)\n\n\n__global__ void spmm_test1(\n  int A_nrows, int B_ncols,\n  const int*__restrict__ A_csrRowPtr,\n  const int*__restrict__ A_csrColInd,\n  const T*__restrict__ A_csrVal,\n  const T*__restrict__ B_dnVal,\n        T*__restrict__ C_dnVal)\n{\n  extern __shared__ int sh[];\n  int *colInd_sh = sh;\n  T *val_sh = (T *)&sh[(blockDim.y<<5)];\n  int shmem_offset = (threadIdx.y<<5);\n  int thread_idx = shmem_offset+threadIdx.x;\n\n  int rid = blockDim.y*blockIdx.x+threadIdx.y;\n\n  if (rid<A_nrows) {\n    int cid = (blockIdx.y<<5)+threadIdx.x;\n    int lb = A_csrRowPtr[rid];\n    int hb = A_csrRowPtr[(rid+1)];\n    int ptr = lb+threadIdx.x;\n    int offset;\n    T acc=0;\n\n    if (blockIdx.y != gridDim.y-1) {\n      for (int jj=lb; jj<hb; jj+=32) {\n        if (ptr<hb) {\n          val_sh[thread_idx] = A_csrVal[ptr];\n          colInd_sh[thread_idx] = B_ncols*A_csrColInd[ptr];\n        }\n        __syncwarp();\n        ptr += 32;\n\n        for (int kk=0; kk<32&&jj+kk<hb; kk++) {\n          offset = colInd_sh[(shmem_offset+kk)] + cid;\n          acc += val_sh[(shmem_offset+kk)]*B_dnVal[offset];\n        }\n        __syncwarp();\n      }\n      C_dnVal[(rid*B_ncols+cid)] = acc;\n    }\n    else {\n      for (int jj=lb; jj<hb; jj+=32) {\n        if (ptr<hb) {\n          val_sh[thread_idx] = A_csrVal[ptr];\n          colInd_sh[thread_idx] = B_ncols*A_csrColInd[ptr];\n        }\n        __syncwarp();\n        ptr += 32;\n\n        for (int kk=0; kk<32&&jj+kk<hb; kk++) {\n          offset = colInd_sh[(shmem_offset+kk)] + cid;\n          if (cid<B_ncols) {\n            acc += val_sh[(shmem_offset+kk)]*B_dnVal[offset];\n          }\n        }\n        __syncwarp();\n      }\n      if (cid<B_ncols) {\n        C_dnVal[(rid*B_ncols+cid)] = acc;\n      }\n    }\n  }\n}",
            "#define T ((int)32)\n\n\n__global__ void spmm_test2(\n  int A_nrows, int B_ncols,\n  const int*__restrict__ A_csrRowPtr,\n  const int*__restrict__ A_csrColInd,\n  const T*__restrict__ A_csrVal,\n  const T*__restrict__ B_dnVal,\n        T*__restrict__ C_dnVal)\n{\n  extern __shared__ int sh[];\n  int *colInd_sh = sh;\n  T *val_sh = (T *)&sh[(blockDim.y<<5)];\n  int shmem_offset = (threadIdx.y<<5);\n  int thread_idx = shmem_offset+threadIdx.x;\n\n  int rid = blockDim.y*blockIdx.x+threadIdx.y;\n\n  if (rid<A_nrows) {\n    int cid = (blockIdx.y<<6)+threadIdx.x;\n    int lb = A_csrRowPtr[rid];\n    int hb = A_csrRowPtr[(rid+1)];\n    int ptr = lb+threadIdx.x;\n    int offset;\n    T acc1=0, acc2=0, val;\n\n    if (blockIdx.y != gridDim.y-1) {\n      for (int jj=lb; jj<hb; jj+=32) {\n        if (ptr<hb) {\n          val_sh[thread_idx] = A_csrVal[ptr];\n          colInd_sh[thread_idx] = B_ncols*A_csrColInd[ptr];\n        }\n        __syncwarp();\n        ptr += 32;\n\n        for (int kk=0; kk<32&&jj+kk<hb; kk++) {\n          offset = colInd_sh[(shmem_offset+kk)] + cid;\n          val = val_sh[(shmem_offset+kk)];\n          acc1 += val*B_dnVal[offset];\n          acc2 += val*B_dnVal[offset+32];\n        }\n        __syncwarp();\n      }\n      offset = rid*B_ncols+cid;\n      C_dnVal[offset] = acc1;\n      C_dnVal[offset+32] = acc2;\n    }\n    else {\n      int nout = (B_ncols-cid+31)/32;\n      for (int jj=lb; jj<hb; jj+=32) {\n        if (ptr<hb) {\n          val_sh[thread_idx] = A_csrVal[ptr];\n          colInd_sh[thread_idx] = B_ncols*A_csrColInd[ptr];\n        }\n        __syncwarp();\n        ptr += 32;\n\n        for (int kk=0; kk<32&&jj+kk<hb; kk++) {\n          val = val_sh[(shmem_offset+kk)];\n          offset = colInd_sh[(shmem_offset+kk)] + cid;\n          if (nout>0) {\n            acc1 += val*B_dnVal[offset];\n          }\n          if (nout>1) {\n            acc2 += val*B_dnVal[offset+32];  \n          }\n        }\n        __syncwarp();\n      }\n      offset = rid*B_ncols+cid;\n      if (nout>0) {\n        C_dnVal[offset] = acc1;\n      }\n      if (nout>1) {\n        C_dnVal[(offset+32)] = acc2;\n      }\n    }\n  }\n}",
            "#define T ((int)32)\n\n\n__global__ void spmm_test3(\n  int A_nrows, int B_ncols,\n  const int*__restrict__ A_csrRowPtr,\n  const int*__restrict__ A_csrColInd,\n  const T*__restrict__ A_csrVal,\n  const T*__restrict__ B_dnVal,\n        T*__restrict__ C_dnVal)\n{\n  extern __shared__ int sh[];\n  int *colInd_sh = sh;\n  T *val_sh = (T *)&sh[(blockDim.y<<5)];\n  int shmem_offset = (threadIdx.y<<5);\n  int thread_idx = shmem_offset+threadIdx.x;\n\n  int rid = blockDim.y*blockIdx.x+threadIdx.y;\n\n  if (rid<A_nrows) {\n    int cid = (blockIdx.y<<7)+threadIdx.x;\n    int lb = A_csrRowPtr[rid];\n    int hb = A_csrRowPtr[(rid+1)];\n    int ptr = lb+threadIdx.x;\n    int offset;\n    T acc1=0, acc2=0, acc3=0, acc4=0, val;\n\n    if (blockIdx.y != gridDim.y-1) {\n      for (int jj=lb; jj<hb; jj+=32) {\n        if (ptr<hb) {\n          val_sh[thread_idx] = A_csrVal[ptr];\n          colInd_sh[thread_idx] = B_ncols*A_csrColInd[ptr];\n        }\n        __syncwarp();\n        ptr += 32;\n\n        for (int kk=0; kk<32&&jj+kk<hb; kk++) {\n          offset = colInd_sh[(shmem_offset+kk)] + cid;\n          val = val_sh[(shmem_offset+kk)];\n          acc1 += val*B_dnVal[offset];\n          acc2 += val*B_dnVal[offset+32];\n          acc3 += val*B_dnVal[offset+64];\n          acc4 += val*B_dnVal[offset+96];\n        }\n        __syncwarp();\n      }\n      offset = rid*B_ncols+cid;\n      C_dnVal[offset] = acc1;\n      C_dnVal[offset+32] = acc2;\n      C_dnVal[offset+64] = acc3;\n      C_dnVal[offset+96] = acc4;\n    }\n    else {\n      int nout = (B_ncols-cid+31)/32;\n      for (int jj=lb; jj<hb; jj+=32) {\n        if (ptr<hb) {\n          val_sh[thread_idx] = A_csrVal[ptr];\n          colInd_sh[thread_idx] = B_ncols*A_csrColInd[ptr];\n        }\n        __syncwarp();\n        ptr += 32;\n\n        for (int kk=0; kk<32&&jj+kk<hb; kk++) {\n          val = val_sh[(shmem_offset+kk)];\n          offset = colInd_sh[(shmem_offset+kk)] + cid;\n          if (nout>0) {\n            acc1 += val*B_dnVal[offset];\n          }\n          if (nout>1) {\n            acc2 += val*B_dnVal[offset+32];  \n          }\n          if (nout>2) {\n            acc3 += val*B_dnVal[offset+64];\n          }\n          if (nout>3) {\n            acc4 += val*B_dnVal[offset+96];  \n          }\n        }\n        __syncwarp();\n      }\n      offset = rid*B_ncols+cid;\n      if (nout>0) {\n        C_dnVal[offset] = acc1;\n      }\n      if (nout>1) {\n        C_dnVal[(offset+32)] = acc2;\n      }\n      if (nout>2) {\n        C_dnVal[(offset+64)] = acc3;\n      }\n      if (nout>3) {\n        C_dnVal[(offset+96)] = acc4;\n      }\n    }\n  }\n}",
            "#define T ((int)32)\n\n\n__global__ void spmm_test4(\n  int A_nrows, int B_ncols,\n  const int*__restrict__ A_csrRowPtr,\n  const int*__restrict__ A_csrColInd,\n  const T*__restrict__ A_csrVal,\n  const T*__restrict__ B_dnVal,\n        T*__restrict__ C_dnVal)\n{\n  extern __shared__ int sh[];\n  int *colInd_sh = sh;\n  T *val_sh = (T *)&sh[(blockDim.y<<5)];\n  int shmem_offset = (threadIdx.y<<5);\n  int thread_idx = shmem_offset+threadIdx.x;\n\n  int rid = blockDim.y*blockIdx.x+threadIdx.y;\n\n  if (rid<A_nrows) {\n    int cid = (blockIdx.y<<8)+threadIdx.x;\n    int lb = A_csrRowPtr[rid];\n    int hb = A_csrRowPtr[(rid+1)];\n    int ptr = lb+threadIdx.x;\n    int offset;\n    T acc1=0, acc2=0, acc3=0, acc4=0, acc5=0,acc6=0,acc7=0,acc8=0,val;\n\n    if (blockIdx.y != gridDim.y-1) {\n      for (int jj=lb; jj<hb; jj+=32) {\n        if (ptr<hb) {\n          val_sh[thread_idx] = A_csrVal[ptr];\n          colInd_sh[thread_idx] = B_ncols*A_csrColInd[ptr];\n        }\n        __syncwarp();\n        ptr += 32;\n\n        for (int kk=0; kk<32&&jj+kk<hb; kk++) {\n          offset = colInd_sh[(shmem_offset+kk)] + cid;\n          val = val_sh[(shmem_offset+kk)];\n          acc1 += val*B_dnVal[offset];\n          acc2 += val*B_dnVal[offset+32];\n          acc3 += val*B_dnVal[offset+64];\n          acc4 += val*B_dnVal[offset+96];\n          acc5 += val*B_dnVal[offset+128];\n          acc6 += val*B_dnVal[offset+160];\n          acc7 += val*B_dnVal[offset+192];\n          acc8 += val*B_dnVal[offset+224];\n        }\n        __syncwarp();\n      }\n      offset = rid*B_ncols+cid;\n      C_dnVal[offset] = acc1;\n      C_dnVal[offset+32] = acc2;\n      C_dnVal[offset+64] = acc3;\n      C_dnVal[offset+96] = acc4;\n      C_dnVal[offset+128] = acc5;\n      C_dnVal[offset+160] = acc6;\n      C_dnVal[offset+192] = acc7;\n      C_dnVal[offset+224] = acc8;\n    }\n    else {\n      int nout = (B_ncols-cid+31)/32;\n      for (int jj=lb; jj<hb; jj+=32) {\n        if (ptr<hb) {\n          val_sh[thread_idx] = A_csrVal[ptr];\n          colInd_sh[thread_idx] = B_ncols*A_csrColInd[ptr];\n        }\n        __syncwarp();\n        ptr += 32;\n\n        for (int kk=0; kk<32&&jj+kk<hb; kk++) {\n          val = val_sh[(shmem_offset+kk)];\n          offset = colInd_sh[(shmem_offset+kk)] + cid;\n          if (nout>0) {\n            acc1 += val*B_dnVal[offset];\n          }\n          if (nout>1) {\n            acc2 += val*B_dnVal[offset+32];  \n          }\n          if (nout>2) {\n            acc3 += val*B_dnVal[offset+64];\n          }\n          if (nout>3) {\n            acc4 += val*B_dnVal[offset+96];  \n          }\n          if (nout>4) {\n            acc5 += val*B_dnVal[offset+128];  \n          }\n          if (nout>5) {\n            acc6 += val*B_dnVal[offset+160];  \n          }\n          if (nout>6) {\n            acc7 += val*B_dnVal[offset+192];  \n          }\n          if (nout>7) {\n            acc8 += val*B_dnVal[offset+224];  \n          }\n        }\n        __syncwarp();\n      }\n      offset = rid*B_ncols+cid;\n      if (nout>0) {\n        C_dnVal[offset] = acc1;\n      }\n      if (nout>1) {\n        C_dnVal[(offset+32)] = acc2;\n      }\n      if (nout>2) {\n        C_dnVal[(offset+64)] = acc3;\n      }\n      if (nout>3) {\n        C_dnVal[(offset+96)] = acc4;\n      }\n      if (nout>4) {\n        C_dnVal[(offset+128)] = acc5;\n      }\n      if (nout>5) {\n        C_dnVal[(offset+160)] = acc6;\n      }\n      if (nout>6) {\n        C_dnVal[(offset+192)] = acc7;\n      }\n      if (nout>7) {\n        C_dnVal[(offset+224)] = acc8;\n      }\n    }\n  }\n}"
        ]
    },
    "aes-cuda": {
        "/Users/gbolet/hecbench-roofline/src/aes-cuda/kernels.cu": [
            "__host__ __device__\nuchar galoisMultiplication(uchar a, uchar b)\n{\n  uchar p = 0; \n  for(unsigned int i=0; i < 8; ++i)\n  {\n    if((b&1) == 1)\n    {\n      p^=a;\n    }\n    uchar hiBitSet = (a & 0x80);\n    a <<= 1;\n    if(hiBitSet == 0x80)\n    {\n      a ^= 0x1b;\n    }\n    b >>= 1;\n  }\n  return p;\n}\n\n__device__\nuchar4 mixColumns(const uchar4 * block, const uchar4 * galiosCoeff, unsigned int j)\n{\n  unsigned int bw = 4;\n\n  uchar x, y, z, w;\n\n  x = galoisMultiplication(block[0].x, galiosCoeff[(bw-j)%bw].x);\n  y = galoisMultiplication(block[0].y, galiosCoeff[(bw-j)%bw].x);\n  z = galoisMultiplication(block[0].z, galiosCoeff[(bw-j)%bw].x);\n  w = galoisMultiplication(block[0].w, galiosCoeff[(bw-j)%bw].x);\n\n  for(unsigned int k=1; k< 4; ++k)\n  {\n    x ^= galoisMultiplication(block[k].x, galiosCoeff[(k+bw-j)%bw].x);\n    y ^= galoisMultiplication(block[k].y, galiosCoeff[(k+bw-j)%bw].x);\n    z ^= galoisMultiplication(block[k].z, galiosCoeff[(k+bw-j)%bw].x);\n    w ^= galoisMultiplication(block[k].w, galiosCoeff[(k+bw-j)%bw].x);\n  }\n\n  return make_uchar4(x, y, z, w);\n}\n\ninline __device__\nuchar4 sboxRead(const uchar * SBox, uchar4 block)\n{\n  return make_uchar4(SBox[block.x], SBox[block.y], SBox[block.z], SBox[block.w]);\n}\n\n__device__\nuchar4 shiftRows(uchar4 row, unsigned int j)\n{\n  uchar4 r = row;\n  for(uint i=0; i < j; ++i)  \n  {\n    //r.xyzw() = r.yzwx();\n    uchar x = r.x;\n    uchar y = r.y;\n    uchar z = r.z;\n    uchar w = r.w;\n    r = make_uchar4(y,z,w,x);\n  }\n  return r;\n}\n\n__global__\nvoid AESEncrypt(      uchar4  *__restrict output  ,\n                const uchar4  *__restrict input   ,\n                const uchar4  *__restrict roundKey,\n                const uchar   *__restrict SBox    ,\n                const uint     width , \n                const uint     rounds )\n\n{\n  __shared__ uchar4 block0[4];\n  __shared__ uchar4 block1[4];\n\n  unsigned int bx = blockIdx.x;\n  unsigned int by = blockIdx.y;\n\n  //unsigned int localIdx = threadIdx.x;\n  unsigned int localIdy = threadIdx.y;\n\n  unsigned int globalIndex = (((by * width/4) + bx) * 4) + (localIdy);\n  unsigned int localIndex  = localIdy;\n\n  uchar4 galiosCoeff[4];\n  galiosCoeff[0] = make_uchar4(2, 0, 0, 0);\n  galiosCoeff[1] = make_uchar4(3, 0, 0, 0);\n  galiosCoeff[2] = make_uchar4(1, 0, 0, 0);\n  galiosCoeff[3] = make_uchar4(1, 0, 0, 0);\n\n  block0[localIndex]  = input[globalIndex];\n\n  block0[localIndex] ^= roundKey[localIndex];\n\n  for(unsigned int r=1; r < rounds; ++r)\n  {\n    block0[localIndex] = sboxRead(SBox, block0[localIndex]);\n\n    block0[localIndex] = shiftRows(block0[localIndex], localIndex); \n\n    __syncthreads();\n    block1[localIndex]  = mixColumns(block0, galiosCoeff, localIndex); \n\n    __syncthreads();\n    block0[localIndex] = block1[localIndex]^roundKey[r*4 + localIndex];\n  }\n  block0[localIndex] = sboxRead(SBox, block0[localIndex]);\n\n  block0[localIndex] = shiftRows(block0[localIndex], localIndex); \n\n  output[globalIndex] =  block0[localIndex]^roundKey[(rounds)*4 + localIndex];\n}",
            "__host__ __device__\nuchar galoisMultiplication(uchar a, uchar b)\n{\n  uchar p = 0; \n  for(unsigned int i=0; i < 8; ++i)\n  {\n    if((b&1) == 1)\n    {\n      p^=a;\n    }\n    uchar hiBitSet = (a & 0x80);\n    a <<= 1;\n    if(hiBitSet == 0x80)\n    {\n      a ^= 0x1b;\n    }\n    b >>= 1;\n  }\n  return p;\n}\n\n__device__\nuchar4 mixColumns(const uchar4 * block, const uchar4 * galiosCoeff, unsigned int j)\n{\n  unsigned int bw = 4;\n\n  uchar x, y, z, w;\n\n  x = galoisMultiplication(block[0].x, galiosCoeff[(bw-j)%bw].x);\n  y = galoisMultiplication(block[0].y, galiosCoeff[(bw-j)%bw].x);\n  z = galoisMultiplication(block[0].z, galiosCoeff[(bw-j)%bw].x);\n  w = galoisMultiplication(block[0].w, galiosCoeff[(bw-j)%bw].x);\n\n  for(unsigned int k=1; k< 4; ++k)\n  {\n    x ^= galoisMultiplication(block[k].x, galiosCoeff[(k+bw-j)%bw].x);\n    y ^= galoisMultiplication(block[k].y, galiosCoeff[(k+bw-j)%bw].x);\n    z ^= galoisMultiplication(block[k].z, galiosCoeff[(k+bw-j)%bw].x);\n    w ^= galoisMultiplication(block[k].w, galiosCoeff[(k+bw-j)%bw].x);\n  }\n\n  return make_uchar4(x, y, z, w);\n}\n\ninline __device__\nuchar4 sboxRead(const uchar * SBox, uchar4 block)\n{\n  return make_uchar4(SBox[block.x], SBox[block.y], SBox[block.z], SBox[block.w]);\n}\n\n__device__\nuchar4 shiftRowsInv(uchar4 row, unsigned int j)\n{\n  uchar4 r = row;\n  for(uint i=0; i < j; ++i)  \n  {\n    // r = r.wxyz();\n    uchar x = r.x;\n    uchar y = r.y;\n    uchar z = r.z;\n    uchar w = r.w;\n    r = make_uchar4(w,x,y,z);\n  }\n  return r;\n}\n\n__global__\nvoid AESDecrypt(       uchar4  *__restrict output    ,\n                const  uchar4  *__restrict input     ,\n                const  uchar4  *__restrict roundKey  ,\n                const  uchar   *__restrict SBox      ,\n                const  uint    width , \n                const  uint    rounds)\n\n{\n  __shared__ uchar4 block0[4];\n  __shared__ uchar4 block1[4];\n\n  unsigned int bx = blockIdx.x;\n  unsigned int by = blockIdx.y;\n\n  //unsigned int localIdx = threadIdx.x;\n  unsigned int localIdy = threadIdx.y;\n\n  unsigned int globalIndex = (((by * width/4) + bx) * 4) + (localIdy);\n  unsigned int localIndex  = localIdy;\n\n  uchar4 galiosCoeff[4];\n  galiosCoeff[0] = make_uchar4(14, 0, 0, 0);\n  galiosCoeff[1] = make_uchar4(11, 0, 0, 0);\n  galiosCoeff[2] = make_uchar4(13, 0, 0, 0);\n  galiosCoeff[3] = make_uchar4( 9, 0, 0, 0);\n\n  block0[localIndex]  = input[globalIndex];\n\n  block0[localIndex] ^= roundKey[4*rounds + localIndex];\n\n  for(unsigned int r=rounds -1 ; r > 0; --r)\n  {\n    block0[localIndex] = shiftRowsInv(block0[localIndex], localIndex); \n\n    block0[localIndex] = sboxRead(SBox, block0[localIndex]);\n\n    __syncthreads();\n    block1[localIndex] = block0[localIndex]^roundKey[r*4 + localIndex];\n\n    __syncthreads();\n    block0[localIndex]  = mixColumns(block1, galiosCoeff, localIndex); \n  }  \n\n  block0[localIndex] = shiftRowsInv(block0[localIndex], localIndex); \n\n  block0[localIndex] = sboxRead(SBox, block0[localIndex]);\n\n  output[globalIndex] =  block0[localIndex]^roundKey[localIndex];\n}"
        ]
    },
    "reverse2D-cuda": {
        "/Users/gbolet/hecbench-roofline/src/reverse2D-cuda/reverse.cuh": [
            "__global__\nvoid reverseKernel(data_t* out,\n                   const data_t* in,\n                   int nrows,\n                   int ncols,\n                   bool rowMajor,\n                   bool alongRows,\n                   int len)\n{\n  typedef TxN_t<data_t, veclen_> VecType;\n  int idx = (threadIdx.x + blockIdx.x * blockDim.x) * VecType::Ratio;\n  if (idx >= len) return;\n  int srcIdx, dstIdx;\n  if (!rowMajor && !alongRows) {\n    int srcRow = idx % nrows;\n    int srcCol = idx / nrows;\n    int dstRow = srcRow;\n    int dstCol = ncols - srcCol - 1;\n    srcIdx     = idx;\n    dstIdx     = dstCol * nrows + dstRow;\n  } else if (!rowMajor && alongRows) {\n    int mod    = ceildiv(nrows, 2);\n    int srcRow = idx % mod;\n    int srcCol = idx / mod;\n    int dstRow = nrows - srcRow - VecType::Ratio;\n    int dstCol = srcCol;\n    srcIdx     = srcCol * nrows + srcRow;\n    dstIdx     = dstCol * nrows + dstRow;\n  } else if (rowMajor && !alongRows) {\n    int mod    = ceildiv(ncols, 2);\n    int srcRow = idx / mod;\n    int srcCol = idx % mod;\n    int dstRow = srcRow;\n    int dstCol = ncols - srcCol - VecType::Ratio;\n    srcIdx     = srcCol + srcRow * ncols;\n    dstIdx     = dstCol + dstRow * ncols;\n  } else {\n    int srcRow = idx / ncols;\n    int srcCol = idx % ncols;\n    int dstRow = nrows - srcRow - 1;\n    int dstCol = srcCol;\n    srcIdx     = idx;\n    dstIdx     = dstCol + dstRow * ncols;\n  }\n  VecType a, b;\n  a.load(in, srcIdx);\n  b.load(in, dstIdx);\n  // while reversing along coalesced dimension, also reverse the elements\n  if ((rowMajor && !alongRows) || (!rowMajor && alongRows)) {\n    #pragma unroll\n    for (int i = 0; i < VecType::Ratio; ++i) {\n      swapVals(a.val.data[i], a.val.data[VecType::Ratio - i - 1]);\n      swapVals(b.val.data[i], b.val.data[VecType::Ratio - i - 1]);\n    }\n  }\n  a.store(out, dstIdx);\n  b.store(out, srcIdx);\n}"
        ]
    },
    "nlll-cuda": {
        "/Users/gbolet/hecbench-roofline/src/nlll-cuda/main.cu": [
            "__global__\nvoid nll_loss_forward_reduce2d_kernel(\n    scalar_t* __restrict__ output,\n    scalar_t* __restrict__ total_weight,\n    const scalar_t* __restrict__ input,\n    const index_t*  __restrict__ target,\n    const scalar_t* __restrict__ weights,\n    bool size_average,\n    int64_t nframe,\n    int64_t kdim,\n    int64_t ignore_index)\n{\n  __shared__ accscalar_t sm_inputs[NLL_LOSS_THREADS],\n                         acc_weight[NLL_LOSS_THREADS];\n\n  int tid = threadIdx.x;\n  sm_inputs[tid] = static_cast<accscalar_t>(0);\n  acc_weight[tid] = static_cast<accscalar_t>(0);\n\n  for (int i = tid; i < nframe; i += NLL_LOSS_THREADS) {\n    index_t t = target[i];\n    if (t != ignore_index) {\n      scalar_t cur_weight =\n          weights != nullptr ? weights[t] : static_cast<scalar_t>(1);\n      sm_inputs[tid] -= static_cast<accscalar_t>(input[i * kdim + t] * cur_weight);\n      acc_weight[tid] += static_cast<accscalar_t>(cur_weight);\n    }\n  }\n\n  __syncthreads();\n\n  if (tid == 0) {\n    accscalar_t output_acc = 0;\n    accscalar_t total_weight_acc = 0;\n    for (int i = 0; i < NLL_LOSS_THREADS; ++i) {\n      output_acc += sm_inputs[i];\n      total_weight_acc += acc_weight[i];\n    }\n    *total_weight = static_cast<scalar_t>(total_weight_acc);\n    if (size_average) {\n      *output = static_cast<scalar_t>(output_acc / total_weight_acc);\n    } else {\n      *output = static_cast<scalar_t>(output_acc);\n    }\n  }\n}"
        ]
    },
    "overlay-cuda": {
        "/Users/gbolet/hecbench-roofline/src/overlay-cuda/main.cu": [
            "#define T ((int)32)\n\n\n__global__ void DetectionOverlayBox(\n  const T*__restrict__ input,\n        T*__restrict__  output,\n  int imgWidth, int imgHeight,\n  int x0, int y0, int boxWidth, int boxHeight,\n  const float4 color) \n{\n  const int box_x = blockIdx.x * blockDim.x + threadIdx.x;\n  const int box_y = blockIdx.y * blockDim.y + threadIdx.y;\n  \n  if( box_x >= boxWidth || box_y >= boxHeight ) return;\n  \n  const int x = box_x + x0;\n  const int y = box_y + y0;\n  \n  if( x >= imgWidth || y >= imgHeight ) return;\n  \n  T px = input[ y * imgWidth + x ];\n  \n  const float alpha = color.w / 255.0f;\n  const float ialph = 1.0f - alpha;\n  \n  px.x = alpha * color.x + ialph * px.x;\n  px.y = alpha * color.y + ialph * px.y;\n  px.z = alpha * color.z + ialph * px.z;\n  \n  output[y * imgWidth + x] = px;\n}"
        ]
    },
    "ntt-cuda": {
        "/Users/gbolet/hecbench-roofline/src/ntt-cuda/main.cu": [
            "#define uint64 uint64_hack_\n\n\n__global__ void intt_3_64k_modcrt(\n        uint32 *__restrict__ dst,\n  const uint64 *__restrict__ src)\n{\n  __shared__ uint64 buffer[512];\n  register uint64 samples[8], s8[8];\n  register uint32 fmem, tmem, fbuf, tbuf;\n  fmem = (bidx<<9)|((tidx&0x3E)<<3)|(tidx&0x1);\n  tbuf = tidx<<3;\n  fbuf = ((tidx&0x38)<<3) | (tidx&0x7);\n  tmem = (bidx<<9)|((tidx&0x38)<<3) | (tidx&0x7);\n#pragma unroll\n  for (int i=0; i<8; i++)\n    samples[i] = src[fmem|(i<<1)];\n  ntt8(samples);\n\n#pragma unroll\n  for (int i=0; i<8; i++)\n    buffer[tbuf|i] = _ls_modP(samples[i], ((tidx&0x1)<<2)*i*3);\n  __syncthreads();\n\n#pragma unroll\n  for (int i=0; i<8; i++)\n    samples[i] = buffer[fbuf|(i<<3)];\n\n#pragma unroll\n  for (int i=0; i<4; i++) {\n    s8[2*i] = _add_modP(samples[2*i], samples[2*i+1]);\n    s8[2*i+1] = _sub_modP(samples[2*i], samples[2*i+1]);\n  }\n\n#pragma unroll\n  for (int i=0; i<8; i++) {\n    dst[(((tmem|(i<<3))&0xf)<<12)|((tmem|(i<<3))>>4)] =\n      (uint32)(_mul_modP(s8[i], 18446462594437939201UL, valP));\n  }\n}"
        ]
    },
    "simpleMultiDevice-cuda": {
        "/Users/gbolet/hecbench-roofline/src/simpleMultiDevice-cuda/main.cu": [
            "__global__\nvoid reduceKernel(float *d_Result, const float *d_Input, int N)\n{\n  const int     tid = blockIdx.x * blockDim.x + threadIdx.x;\n  const int threadN = gridDim.x * blockDim.x;\n  float sum = 0;\n\n  for (int pos = tid; pos < N; pos += threadN)\n    sum += d_Input[pos];\n\n  d_Result[tid] = sum;\n}"
        ]
    },
    "logprob-cuda": {
        "/Users/gbolet/hecbench-roofline/src/logprob-cuda/main.cu": [
            "#define T ((int)32)\n\n\ninline __host__ __device__ uint4 make_uint4(int4 a)\n{\n    return make_uint4(uint(a.x), uint(a.y), uint(a.z), uint(a.w));\n}\n\ninline __host__ __device__ uint4 max(uint4 a, uint4 b)\n{\n    return make_uint4(max(a.x,b.x), max(a.y,b.y), max(a.z,b.z), max(a.w,b.w));\n}\n\n__inline__ __device__ T warpReduceMax(T val)\n{\n  #pragma unroll\n  for (int mask = 16; mask > 0; mask >>= 1)\n    val = max(val, __shfl_xor_sync(FINAL_MASK, val, mask, 32));\n  return val;\n}\n\n__inline__ __device__ T warpReduceSum(T val)\n{\n#pragma unroll\n  for (int mask = 16; mask > 0; mask >>= 1)\n    val += __shfl_xor_sync(FINAL_MASK, val, mask, 32);\n  return val;\n}\n\ninline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\ninline __host__ __device__ float4 expf(float4 v)\n{\n    return make_float4(expf(v.x), expf(v.y), expf(v.z), expf(v.w));\n}\n\ninline __host__ __device__ float4 fmaxf(float4 a, float4 b)\n{\n    return make_float4(fmaxf(a.x,b.x), fmaxf(a.y,b.y), fmaxf(a.z,b.z), fmaxf(a.w,b.w));\n}\n\n__inline__ __device__ T blockReduceMax(T val)\n{\n  static __shared__ T shared[32];\n  int lane = threadIdx.x & 0x1f;  // in-warp idx\n  int wid = threadIdx.x >> 5;     // warp idx\n\n  val = warpReduceMax(val);  // get maxx in each warp\n\n  if (lane == 0)  // record in-warp maxx by warp Idx\n    shared[wid] = val;\n\n  __syncthreads();\n\n  // Modify from blockDim.x << 5 to blockDim.x / 32. to prevent\n  // blockDim.x is not divided by 32\n  val = (threadIdx.x < (blockDim.x / 32.f)) ? shared[lane] : -1e20f;\n  val = warpReduceMax(val);\n\n  return val;\n}\n\n__inline__ __device__ T blockReduceSum(T val)\n{\n  static __shared__ T shared[32];\n  int lane = threadIdx.x & 0x1f;\n  int wid = threadIdx.x >> 5;\n\n  val = warpReduceSum<T>(val);\n\n  if (lane == 0)\n    shared[wid] = val;\n\n  __syncthreads();\n\n  // Modify from blockDim.x << 5 to blockDim.x / 32. to prevent\n  // blockDim.x is not divided by 32\n  val = (threadIdx.x < (blockDim.x / 32.f)) ? shared[lane] : (T)(0.0f);\n  val = warpReduceSum<T>(val);\n\n  return val;\n}\n\n__global__ void log_probs_kernel(\n    float*       log_probs,\n    const T*     logits,\n    const int*   ids,\n    const int*   lengths,\n    const int    max_input_length,\n    const int    batch_size,\n    const int    vocab_size,\n    const int    vocab_size_padded)\n{\n  // Calculate the log probability from logits.\n  //   log_probs[t, :] = log(softmax(logits))[ids[t + 1, :]]\n  //\n  // log_probs: [batch_size, max_length -1],\n  //     log probabilities of each token.\n  // logits: [batch_size, max_length, vocab_size_padded]\n  // lengths: [batch_size], sequence lengths\n  // ids: [max_length, batch_size], token ids.\n  // batch_size: [1], batch_size. in case of beam > 1, batch x beam.\n  // vocab_size: [1], vocab_size,\n  // vocab_size: [1], vocab_size_padded, padded vocab size.\n\n  const bool IS_FP16   = std::is_same<T, half>::value;\n  const T    MAX_T_VAL = (IS_FP16) ? HALF_FLT_MAX : FLT_MAX;\n\n  int tidx = threadIdx.x; // vocab dim\n  int step = blockIdx.x;  // step dim\n  int bidx = blockIdx.y;  // batch dim\n\n  __shared__ float s_max_logit;\n\n  if (bidx < batch_size && step < lengths[bidx] - 1) {\n    // Compute the address of logits to data for the current batch\n    int step_offset  = step * vocab_size_padded;\n    int batch_offset = bidx * max_input_length * vocab_size_padded;\n    logits += step_offset + batch_offset;\n\n    // Find max(logits)\n    float local_max = -MAX_T_VAL;\n    float val       = -MAX_T_VAL;\n    for (int i = tidx; i < vocab_size; i += blockDim.x) {\n      val       = static_cast<float>(logits[i]);\n      local_max = fmaxf(local_max, val);\n    }\n\n    float max_val = blockReduceMax<float>(local_max);\n    if (tidx == 0) {\n      s_max_logit = max_val;\n    }\n    __syncthreads();\n\n    // Calculate the denominator: sum_i exp(logits[i])\n    float local_sum_exp = 0.0f;\n    for (int i = tidx; i < vocab_size; i += blockDim.x) {\n      val = expf(static_cast<float>(logits[i]) - s_max_logit);\n      local_sum_exp += val;\n    }\n\n    float sum_exp = blockReduceSum<float>(local_sum_exp);\n    if (tidx == 0) {\n      int idx = step + bidx * (max_input_length - 1);\n      // log_probs[step, ...] is the log probability of a token at step t + 1.\n      int token_idx = step + 1 + bidx * max_input_length;\n      log_probs[idx] = static_cast<float>(logits[ids[token_idx]]) - s_max_logit - logf(sum_exp + 1e-9f);\n    }\n  }\n}",
            "#define T ((int)32)\n\n\n__inline__ __device__ T warpReduceSum(T val)\n{\n#pragma unroll\n  for (int mask = 16; mask > 0; mask >>= 1)\n    val += __shfl_xor_sync(FINAL_MASK, val, mask, 32);\n  return val;\n}\n\n__inline__ __device__ T blockReduceSum(T val)\n{\n  static __shared__ T shared[32];\n  int lane = threadIdx.x & 0x1f;\n  int wid = threadIdx.x >> 5;\n\n  val = warpReduceSum<T>(val);\n\n  if (lane == 0)\n    shared[wid] = val;\n\n  __syncthreads();\n\n  // Modify from blockDim.x << 5 to blockDim.x / 32. to prevent\n  // blockDim.x is not divided by 32\n  val = (threadIdx.x < (blockDim.x / 32.f)) ? shared[lane] : (T)(0.0f);\n  val = warpReduceSum<T>(val);\n\n  return val;\n}\n\n__global__ void accumulate_log_probs(\n          float* cum_log_probs,\n    const float* log_probs,\n    const int*   lengths,\n    const int    max_input_length,\n    const int    batch_size)\n{\n  // Accumulate the log probability along the sequence dimension.\n  //   cum_log_probs[j] = sum_i log(softmax(logits))[ids[i,j]]\n  //\n  // cum_log_probs: [batch_size], cumulative log probability\n  // log_probs: [batch_size, max_length - 1],\n  //   log probability of each token\n  // lengths: [batch_size], sequence lengths\n  // batch_size: [1], batch_size. in case of beam > 1, batch x beam.\n\n  int bidx = blockIdx.x;   // batch dim\n  int tidx = threadIdx.x;  // step dim\n  int length = lengths[bidx];\n\n  // reposition logits to data for the current batch.\n  log_probs += bidx * (max_input_length - 1);\n  float local_accum = 0.0f;\n  for (int step = tidx; step < length - 1; step += blockDim.x) {\n    local_accum += static_cast<float>(log_probs[step]);\n  }\n  float accum = blockReduceSum<float>(local_accum);\n  if (tidx == 0) {\n    cum_log_probs[bidx] = accum;\n  }\n}"
        ]
    },
    "zmddft-cuda": {
        "/Users/gbolet/hecbench-roofline/src/zmddft-cuda/main.cu": [
            "__global__\nvoid ker_zmddft_fwd_256x256x256_cu0(const double *X, double *P1)\n{\n  __shared__ double T3[2048];\n  double a495, a496, a497, a498, a499, a500, a501, a502, \n         s145, s146, s147, s148, s149, s150, s151, s152, \n         s153, s154, s155, s156, s157, s158, s159, s160, \n         s161, s162, s163, s164, s165, s166, s167, s168, \n         s169, s170, s171, s172, s173, s174, s175, s176, \n         s177, s178, s179, s180, s181, s182, s183, s184, \n         s185, s186, s187, s188, s189, s190, s191, s192, \n         t538, t539, t540, t541, t542, t543, t544, t545, \n         t546, t547, t548, t549, t550, t551, t552, t553, \n         t554, t555, t556, t557, t558, t559, t560, t561, \n         t562, t563, t564, t565, t566, t567, t568, t569, \n         t570, t571, t572, t573, t574, t575, t576, t577, \n         t578, t579, t580, t581, t582, t583, t584, t585, \n         t586, t587, t588, t589, t590, t591, t592, t593, \n         t594, t595, t596, t597, t598, t599, t600, t601, \n         t602, t603, t604, t605, t606, t607, t608, t609, \n         t610, t611, t612, t613, t614, t615, t616, t617, \n         t618, t619, t620, t621, t622, t623, t624, t625;\n  int a492, a493, a494, a503;\n  a492 = (512*(threadIdx.x / 16));\n  a493 = (threadIdx.x % 16);\n  a494 = ((2048*blockIdx.x) + a492 + (2*a493));\n  s145 = X[a494];\n  s146 = X[(a494 + 1)];\n  s147 = X[(a494 + 256)];\n  s148 = X[(a494 + 257)];\n  t538 = (s145 + s147);\n  t539 = (s146 + s148);\n  t540 = (s145 - s147);\n  t541 = (s146 - s148);\n  s149 = X[(a494 + 128)];\n  s150 = X[(a494 + 129)];\n  s151 = X[(a494 + 384)];\n  s152 = X[(a494 + 385)];\n  t542 = (s149 + s151);\n  t543 = (s150 + s152);\n  t544 = (s149 - s151);\n  t545 = (s150 - s152);\n  t546 = (t538 + t542);\n  t547 = (t539 + t543);\n  t548 = (t538 - t542);\n  t549 = (t539 - t543);\n  t550 = (t540 + t545);\n  t551 = (t541 - t544);\n  t552 = (t540 - t545);\n  t553 = (t541 + t544);\n  s153 = X[(a494 + 32)];\n  s154 = X[(a494 + 33)];\n  s155 = X[(a494 + 288)];\n  s156 = X[(a494 + 289)];\n  t554 = (s153 + s155);\n  t555 = (s154 + s156);\n  t556 = (s153 - s155);\n  t557 = (s154 - s156);\n  s157 = X[(a494 + 160)];\n  s158 = X[(a494 + 161)];\n  s159 = X[(a494 + 416)];\n  s160 = X[(a494 + 417)];\n  t558 = (s157 + s159);\n  t559 = (s158 + s160);\n  t560 = (s157 - s159);\n  t561 = (s158 - s160);\n  t562 = (t554 + t558);\n  t563 = (t555 + t559);\n  a495 = (0.70710678118654757*(t554 - t558));\n  a496 = (0.70710678118654757*(t555 - t559));\n  s161 = (a495 + a496);\n  s162 = (a496 - a495);\n  t564 = (t556 + t561);\n  t565 = (t557 - t560);\n  t566 = (t556 - t561);\n  t567 = (t557 + t560);\n  s163 = ((0.92387953251128674*t564) + (0.38268343236508978*t565));\n  s164 = ((0.92387953251128674*t565) - (0.38268343236508978*t564));\n  s165 = ((0.38268343236508978*t566) + (0.92387953251128674*t567));\n  s166 = ((0.38268343236508978*t567) - (0.92387953251128674*t566));\n  s167 = X[(a494 + 64)];\n  s168 = X[(a494 + 65)];\n  s169 = X[(a494 + 320)];\n  s170 = X[(a494 + 321)];\n  t568 = (s167 + s169);\n  t569 = (s168 + s170);\n  t570 = (s167 - s169);\n  t571 = (s168 - s170);\n  s171 = X[(a494 + 192)];\n  s172 = X[(a494 + 193)];\n  s173 = X[(a494 + 448)];\n  s174 = X[(a494 + 449)];\n  t572 = (s171 + s173);\n  t573 = (s172 + s174);\n  t574 = (s171 - s173);\n  t575 = (s172 - s174);\n  t576 = (t568 + t572);\n  t577 = (t569 + t573);\n  t578 = (t568 - t572);\n  t579 = (t569 - t573);\n  a497 = (0.70710678118654757*(t570 + t575));\n  a498 = (0.70710678118654757*(t571 - t574));\n  s175 = (a497 + a498);\n  s176 = (a498 - a497);\n  a499 = (0.70710678118654757*(t571 + t574));\n  a500 = (0.70710678118654757*(t570 - t575));\n  s177 = (a499 - a500);\n  s178 = (a500 + a499);\n  s179 = X[(a494 + 96)];\n  s180 = X[(a494 + 97)];\n  s181 = X[(a494 + 352)];\n  s182 = X[(a494 + 353)];\n  t580 = (s179 + s181);\n  t581 = (s180 + s182);\n  t582 = (s179 - s181);\n  t583 = (s180 - s182);\n  s183 = X[(a494 + 224)];\n  s184 = X[(a494 + 225)];\n  s185 = X[(a494 + 480)];\n  s186 = X[(a494 + 481)];\n  t584 = (s183 + s185);\n  t585 = (s184 + s186);\n  t586 = (s183 - s185);\n  t587 = (s184 - s186);\n  t588 = (t580 + t584);\n  t589 = (t581 + t585);\n  a501 = (0.70710678118654757*(t581 - t585));\n  a502 = (0.70710678118654757*(t580 - t584));\n  s187 = (a501 - a502);\n  s188 = (a502 + a501);\n  t590 = (t582 + t587);\n  t591 = (t583 - t586);\n  t592 = (t582 - t587);\n  t593 = (t583 + t586);\n  s189 = ((0.38268343236508978*t590) + (0.92387953251128674*t591));\n  s190 = ((0.38268343236508978*t591) - (0.92387953251128674*t590));\n  s191 = ((0.92387953251128674*t592) + (0.38268343236508978*t593));\n  s192 = ((0.38268343236508978*t592) - (0.92387953251128674*t593));\n  t594 = (t546 + t576);\n  t595 = (t547 + t577);\n  t596 = (t546 - t576);\n  t597 = (t547 - t577);\n  t598 = (t562 + t588);\n  t599 = (t563 + t589);\n  t600 = (t562 - t588);\n  t601 = (t563 - t589);\n  a503 = (a492 + (32*a493));\n  T3[a503] = (t594 + t598);\n  T3[(a503 + 1)] = (t595 + t599);\n  T3[(a503 + 16)] = (t594 - t598);\n  T3[(a503 + 17)] = (t595 - t599);\n  T3[(a503 + 8)] = (t596 + t601);\n  T3[(a503 + 9)] = (t597 - t600);\n  T3[(a503 + 24)] = (t596 - t601);\n  T3[(a503 + 25)] = (t597 + t600);\n  t602 = (t550 + s175);\n  t603 = (t551 + s176);\n  t604 = (t550 - s175);\n  t605 = (t551 - s176);\n  t606 = (s163 + s189);\n  t607 = (s164 + s190);\n  t608 = (s163 - s189);\n  t609 = (s164 - s190);\n  T3[(a503 + 2)] = (t602 + t606);\n  T3[(a503 + 3)] = (t603 + t607);\n  T3[(a503 + 18)] = (t602 - t606);\n  T3[(a503 + 19)] = (t603 - t607);\n  T3[(a503 + 10)] = (t604 + t609);\n  T3[(a503 + 11)] = (t605 - t608);\n  T3[(a503 + 26)] = (t604 - t609);\n  T3[(a503 + 27)] = (t605 + t608);\n  t610 = (t548 + t579);\n  t611 = (t549 - t578);\n  t612 = (t548 - t579);\n  t613 = (t549 + t578);\n  t614 = (s161 + s187);\n  t615 = (s162 - s188);\n  t616 = (s161 - s187);\n  t617 = (s162 + s188);\n  T3[(a503 + 4)] = (t610 + t614);\n  T3[(a503 + 5)] = (t611 + t615);\n  T3[(a503 + 20)] = (t610 - t614);\n  T3[(a503 + 21)] = (t611 - t615);\n  T3[(a503 + 12)] = (t612 + t617);\n  T3[(a503 + 13)] = (t613 - t616);\n  T3[(a503 + 28)] = (t612 - t617);\n  T3[(a503 + 29)] = (t613 + t616);\n  t618 = (t552 + s177);\n  t619 = (t553 - s178);\n  t620 = (t552 - s177);\n  t621 = (t553 + s178);\n  t622 = (s165 - s191);\n  t623 = (s166 + s192);\n  t624 = (s165 + s191);\n  t625 = (s166 - s192);\n  T3[(a503 + 6)] = (t618 + t622);\n  T3[(a503 + 7)] = (t619 + t623);\n  T3[(a503 + 22)] = (t618 - t622);\n  T3[(a503 + 23)] = (t619 - t623);\n  T3[(a503 + 14)] = (t620 + t625);\n  T3[(a503 + 15)] = (t621 - t624);\n  T3[(a503 + 30)] = (t620 - t625);\n  T3[(a503 + 31)] = (t621 + t624);\n  __syncthreads();\n  double a1478, a1479, a1480, a1481, a1482, a1483, a1484, a1485, \n         a1486, a1487, a1488, a1489, a1490, a1491, a1492, a1493, \n         a1494, a1495, a1496, a1497, a1498, a1499, a1500, a1501, \n         a1502, a1503, a1504, a1505, a1506, a1507, a1508, a1509, \n         a1510, a1511, a1512, a1513, a1514, a1515, a1516, a1517, \n         s434, s435, s436, s437, s438, s439, s440, s441, \n         s442, s443, s444, s445, s446, s447, s448, s449, \n         s450, s451, s452, s453, s454, s455, s456, s457, \n         s458, s459, s460, s461, s462, s463, s464, s465, \n         s466, s467, s468, s469, s470, s471, s472, s473, \n         s474, s475, s476, s477, s478, s479, s480, s481, \n         s482, s483, s484, s485, s486, s487, s488, s489, \n         s490, s491, s492, s493, s494, s495, s496, s497, \n         s498, s499, s500, s501, s502, s503, s504, s505, \n         s506, s507, s508, s509, s510, s511, s512, s513, \n         t1000, t1001, t1002, t1003, t1004, t1005, t1006, t1007, \n         t1008, t1009, t1010, t1011, t1012, t1013, t1014, t1015, \n         t1016, t1017, t1018, t1019, t1020, t1021, t1022, t1023, \n         t1024, t1025, t1026, t1027, t1028, t1029, t1030, t1031, \n         t1032, t1033, t1034, t1035, t1036, t1037, t1038, t1039, \n         t1040, t1041, t1042, t1043, t1044, t1045, t1046, t1047, \n         t1048, t1049, t1050, t1051, t1052, t1053, t1054, t1055, \n         t1056, t1057, t970, t971, t972, t973, t974, t975, \n         t976, t977, t978, t979, t980, t981, t982, t983, \n         t984, t985, t986, t987, t988, t989, t990, t991, \n         t992, t993, t994, t995, t996, t997, t998, t999;\n  int a1474, a1475, a1476, a1477, a1518;\n  a1474 = (threadIdx.x / 16);\n  a1475 = (threadIdx.x % 16);\n  a1476 = ((512*a1474) + (2*a1475));\n  s434 = T3[a1476];\n  s435 = T3[(a1476 + 1)];\n  s436 = T3[(a1476 + 256)];\n  s437 = T3[(a1476 + 257)];\n  a1477 = (32*a1475);\n  a1478 = D3[a1477];\n  a1479 = D3[(a1477 + 1)];\n  s438 = ((a1478*s434) - (a1479*s435));\n  s439 = ((a1479*s434) + (a1478*s435));\n  a1480 = D3[(a1477 + 2)];\n  a1481 = D3[(a1477 + 3)];\n  s440 = ((a1480*s436) - (a1481*s437));\n  s441 = ((a1481*s436) + (a1480*s437));\n  t970 = (s438 + s440);\n  t971 = (s439 + s441);\n  t972 = (s438 - s440);\n  t973 = (s439 - s441);\n  s442 = T3[(a1476 + 128)];\n  s443 = T3[(a1476 + 129)];\n  s444 = T3[(a1476 + 384)];\n  s445 = T3[(a1476 + 385)];\n  a1482 = D3[(4 + a1477)];\n  a1483 = D3[(5 + a1477)];\n  s446 = ((a1482*s442) - (a1483*s443));\n  s447 = ((a1483*s442) + (a1482*s443));\n  a1484 = D3[(6 + a1477)];\n  a1485 = D3[(7 + a1477)];\n  s448 = ((a1484*s444) - (a1485*s445));\n  s449 = ((a1485*s444) + (a1484*s445));\n  t974 = (s446 + s448);\n  t975 = (s447 + s449);\n  t976 = (s446 - s448);\n  t977 = (s447 - s449);\n  t978 = (t970 + t974);\n  t979 = (t971 + t975);\n  t980 = (t970 - t974);\n  t981 = (t971 - t975);\n  t982 = (t972 + t977);\n  t983 = (t973 - t976);\n  t984 = (t972 - t977);\n  t985 = (t973 + t976);\n  s450 = T3[(a1476 + 32)];\n  s451 = T3[(a1476 + 33)];\n  s452 = T3[(a1476 + 288)];\n  s453 = T3[(a1476 + 289)];\n  a1486 = D3[(a1477 + 8)];\n  a1487 = D3[(9 + a1477)];\n  s454 = ((a1486*s450) - (a1487*s451));\n  s455 = ((a1487*s450) + (a1486*s451));\n  a1488 = D3[(10 + a1477)];\n  a1489 = D3[(11 + a1477)];\n  s456 = ((a1488*s452) - (a1489*s453));\n  s457 = ((a1489*s452) + (a1488*s453));\n  t986 = (s454 + s456);\n  t987 = (s455 + s457);\n  t988 = (s454 - s456);\n  t989 = (s455 - s457);\n  s458 = T3[(a1476 + 160)];\n  s459 = T3[(a1476 + 161)];\n  s460 = T3[(a1476 + 416)];\n  s461 = T3[(a1476 + 417)];\n  a1490 = D3[(12 + a1477)];\n  a1491 = D3[(13 + a1477)];\n  s462 = ((a1490*s458) - (a1491*s459));\n  s463 = ((a1491*s458) + (a1490*s459));\n  a1492 = D3[(14 + a1477)];\n  a1493 = D3[(15 + a1477)];\n  s464 = ((a1492*s460) - (a1493*s461));\n  s465 = ((a1493*s460) + (a1492*s461));\n  t990 = (s462 + s464);\n  t991 = (s463 + s465);\n  t992 = (s462 - s464);\n  t993 = (s463 - s465);\n  t994 = (t986 + t990);\n  t995 = (t987 + t991);\n  a1494 = (0.70710678118654757*(t986 - t990));\n  a1495 = (0.70710678118654757*(t987 - t991));\n  s466 = (a1494 + a1495);\n  s467 = (a1495 - a1494);\n  t996 = (t988 + t993);\n  t997 = (t989 - t992);\n  t998 = (t988 - t993);\n  t999 = (t989 + t992);\n  s468 = ((0.92387953251128674*t996) + (0.38268343236508978*t997));\n  s469 = ((0.92387953251128674*t997) - (0.38268343236508978*t996));\n  s470 = ((0.38268343236508978*t998) + (0.92387953251128674*t999));\n  s471 = ((0.38268343236508978*t999) - (0.92387953251128674*t998));\n  s472 = T3[(a1476 + 64)];\n  s473 = T3[(a1476 + 65)];\n  s474 = T3[(a1476 + 320)];\n  s475 = T3[(a1476 + 321)];\n  a1496 = D3[(a1477 + 16)];\n  a1497 = D3[(17 + a1477)];\n  s476 = ((a1496*s472) - (a1497*s473));\n  s477 = ((a1497*s472) + (a1496*s473));\n  a1498 = D3[(18 + a1477)];\n  a1499 = D3[(19 + a1477)];\n  s478 = ((a1498*s474) - (a1499*s475));\n  s479 = ((a1499*s474) + (a1498*s475));\n  t1000 = (s476 + s478);\n  t1001 = (s477 + s479);\n  t1002 = (s476 - s478);\n  t1003 = (s477 - s479);\n  s480 = T3[(a1476 + 192)];\n  s481 = T3[(a1476 + 193)];\n  s482 = T3[(a1476 + 448)];\n  s483 = T3[(a1476 + 449)];\n  a1500 = D3[(20 + a1477)];\n  a1501 = D3[(21 + a1477)];\n  s484 = ((a1500*s480) - (a1501*s481));\n  s485 = ((a1501*s480) + (a1500*s481));\n  a1502 = D3[(22 + a1477)];\n  a1503 = D3[(23 + a1477)];\n  s486 = ((a1502*s482) - (a1503*s483));\n  s487 = ((a1503*s482) + (a1502*s483));\n  t1004 = (s484 + s486);\n  t1005 = (s485 + s487);\n  t1006 = (s484 - s486);\n  t1007 = (s485 - s487);\n  t1008 = (t1000 + t1004);\n  t1009 = (t1001 + t1005);\n  t1010 = (t1000 - t1004);\n  t1011 = (t1001 - t1005);\n  a1504 = (0.70710678118654757*(t1002 + t1007));\n  a1505 = (0.70710678118654757*(t1003 - t1006));\n  s488 = (a1504 + a1505);\n  s489 = (a1505 - a1504);\n  a1506 = (0.70710678118654757*(t1003 + t1006));\n  a1507 = (0.70710678118654757*(t1002 - t1007));\n  s490 = (a1506 - a1507);\n  s491 = (a1507 + a1506);\n  s492 = T3[(a1476 + 96)];\n  s493 = T3[(a1476 + 97)];\n  s494 = T3[(a1476 + 352)];\n  s495 = T3[(a1476 + 353)];\n  a1508 = D3[(a1477 + 24)];\n  a1509 = D3[(25 + a1477)];\n  s496 = ((a1508*s492) - (a1509*s493));\n  s497 = ((a1509*s492) + (a1508*s493));\n  a1510 = D3[(26 + a1477)];\n  a1511 = D3[(27 + a1477)];\n  s498 = ((a1510*s494) - (a1511*s495));\n  s499 = ((a1511*s494) + (a1510*s495));\n  t1012 = (s496 + s498);\n  t1013 = (s497 + s499);\n  t1014 = (s496 - s498);\n  t1015 = (s497 - s499);\n  s500 = T3[(a1476 + 224)];\n  s501 = T3[(a1476 + 225)];\n  s502 = T3[(a1476 + 480)];\n  s503 = T3[(a1476 + 481)];\n  a1512 = D3[(28 + a1477)];\n  a1513 = D3[(29 + a1477)];\n  s504 = ((a1512*s500) - (a1513*s501));\n  s505 = ((a1513*s500) + (a1512*s501));\n  a1514 = D3[(30 + a1477)];\n  a1515 = D3[(31 + a1477)];\n  s506 = ((a1514*s502) - (a1515*s503));\n  s507 = ((a1515*s502) + (a1514*s503));\n  t1016 = (s504 + s506);\n  t1017 = (s505 + s507);\n  t1018 = (s504 - s506);\n  t1019 = (s505 - s507);\n  t1020 = (t1012 + t1016);\n  t1021 = (t1013 + t1017);\n  a1516 = (0.70710678118654757*(t1013 - t1017));\n  a1517 = (0.70710678118654757*(t1012 - t1016));\n  s508 = (a1516 - a1517);\n  s509 = (a1517 + a1516);\n  t1022 = (t1014 + t1019);\n  t1023 = (t1015 - t1018);\n  t1024 = (t1014 - t1019);\n  t1025 = (t1015 + t1018);\n  s510 = ((0.38268343236508978*t1022) + (0.92387953251128674*t1023));\n  s511 = ((0.38268343236508978*t1023) - (0.92387953251128674*t1022));\n  s512 = ((0.92387953251128674*t1024) + (0.38268343236508978*t1025));\n  s513 = ((0.38268343236508978*t1024) - (0.92387953251128674*t1025));\n  t1026 = (t978 + t1008);\n  t1027 = (t979 + t1009);\n  t1028 = (t978 - t1008);\n  t1029 = (t979 - t1009);\n  t1030 = (t994 + t1020);\n  t1031 = (t995 + t1021);\n  t1032 = (t994 - t1020);\n  t1033 = (t995 - t1021);\n  a1518 = ((8*blockIdx.x) + (131072*a1475) + (2*a1474));\n  P1[a1518] = (t1026 + t1030);\n  P1[(a1518 + 1)] = (t1027 + t1031);\n  P1[(a1518 + 16777216)] = (t1026 - t1030);\n  P1[(a1518 + 16777217)] = (t1027 - t1031);\n  P1[(a1518 + 8388608)] = (t1028 + t1033);\n  P1[(a1518 + 8388609)] = (t1029 - t1032);\n  P1[(a1518 + 25165824)] = (t1028 - t1033);\n  P1[(a1518 + 25165825)] = (t1029 + t1032);\n  t1034 = (t982 + s488);\n  t1035 = (t983 + s489);\n  t1036 = (t982 - s488);\n  t1037 = (t983 - s489);\n  t1038 = (s468 + s510);\n  t1039 = (s469 + s511);\n  t1040 = (s468 - s510);\n  t1041 = (s469 - s511);\n  P1[(a1518 + 2097152)] = (t1034 + t1038);\n  P1[(a1518 + 2097153)] = (t1035 + t1039);\n  P1[(a1518 + 18874368)] = (t1034 - t1038);\n  P1[(a1518 + 18874369)] = (t1035 - t1039);\n  P1[(a1518 + 10485760)] = (t1036 + t1041);\n  P1[(a1518 + 10485761)] = (t1037 - t1040);\n  P1[(a1518 + 27262976)] = (t1036 - t1041);\n  P1[(a1518 + 27262977)] = (t1037 + t1040);\n  t1042 = (t980 + t1011);\n  t1043 = (t981 - t1010);\n  t1044 = (t980 - t1011);\n  t1045 = (t981 + t1010);\n  t1046 = (s466 + s508);\n  t1047 = (s467 - s509);\n  t1048 = (s466 - s508);\n  t1049 = (s467 + s509);\n  P1[(a1518 + 4194304)] = (t1042 + t1046);\n  P1[(a1518 + 4194305)] = (t1043 + t1047);\n  P1[(a1518 + 20971520)] = (t1042 - t1046);\n  P1[(a1518 + 20971521)] = (t1043 - t1047);\n  P1[(a1518 + 12582912)] = (t1044 + t1049);\n  P1[(a1518 + 12582913)] = (t1045 - t1048);\n  P1[(a1518 + 29360128)] = (t1044 - t1049);\n  P1[(a1518 + 29360129)] = (t1045 + t1048);\n  t1050 = (t984 + s490);\n  t1051 = (t985 - s491);\n  t1052 = (t984 - s490);\n  t1053 = (t985 + s491);\n  t1054 = (s470 - s512);\n  t1055 = (s471 + s513);\n  t1056 = (s470 + s512);\n  t1057 = (s471 - s513);\n  P1[(a1518 + 6291456)] = (t1050 + t1054);\n  P1[(a1518 + 6291457)] = (t1051 + t1055);\n  P1[(a1518 + 23068672)] = (t1050 - t1054);\n  P1[(a1518 + 23068673)] = (t1051 - t1055);\n  P1[(a1518 + 14680064)] = (t1052 + t1057);\n  P1[(a1518 + 14680065)] = (t1053 - t1056);\n  P1[(a1518 + 31457280)] = (t1052 - t1057);\n  P1[(a1518 + 31457281)] = (t1053 + t1056);\n  __syncthreads();\n}",
            "__global__\nvoid ker_zmddft_fwd_256x256x256_cu1(const double *P1, double *P2)\n{\n  __shared__ double T33[2048];\n  double a2012, a2013, a2014, a2015, a2016, a2017, a2018, a2019, \n         s658, s659, s660, s661, s662, s663, s664, s665, \n         s666, s667, s668, s669, s670, s671, s672, s673, \n         s674, s675, s676, s677, s678, s679, s680, s681, \n         s682, s683, s684, s685, s686, s687, s688, s689, \n         s690, s691, s692, s693, s694, s695, s696, s697, \n         s698, s699, s700, s701, s702, s703, s704, s705, \n         t1402, t1403, t1404, t1405, t1406, t1407, t1408, t1409, \n         t1410, t1411, t1412, t1413, t1414, t1415, t1416, t1417, \n         t1418, t1419, t1420, t1421, t1422, t1423, t1424, t1425, \n         t1426, t1427, t1428, t1429, t1430, t1431, t1432, t1433, \n         t1434, t1435, t1436, t1437, t1438, t1439, t1440, t1441, \n         t1442, t1443, t1444, t1445, t1446, t1447, t1448, t1449, \n         t1450, t1451, t1452, t1453, t1454, t1455, t1456, t1457, \n         t1458, t1459, t1460, t1461, t1462, t1463, t1464, t1465, \n         t1466, t1467, t1468, t1469, t1470, t1471, t1472, t1473, \n         t1474, t1475, t1476, t1477, t1478, t1479, t1480, t1481, \n         t1482, t1483, t1484, t1485, t1486, t1487, t1488, t1489;\n  int a2009, a2010, a2011, a2020;\n  a2009 = (512*(threadIdx.x / 16));\n  a2010 = (threadIdx.x % 16);\n  a2011 = ((2048*blockIdx.x) + a2009 + (2*a2010));\n  s658 = P1[a2011];\n  s659 = P1[(a2011 + 1)];\n  s660 = P1[(a2011 + 256)];\n  s661 = P1[(a2011 + 257)];\n  t1402 = (s658 + s660);\n  t1403 = (s659 + s661);\n  t1404 = (s658 - s660);\n  t1405 = (s659 - s661);\n  s662 = P1[(a2011 + 128)];\n  s663 = P1[(a2011 + 129)];\n  s664 = P1[(a2011 + 384)];\n  s665 = P1[(a2011 + 385)];\n  t1406 = (s662 + s664);\n  t1407 = (s663 + s665);\n  t1408 = (s662 - s664);\n  t1409 = (s663 - s665);\n  t1410 = (t1402 + t1406);\n  t1411 = (t1403 + t1407);\n  t1412 = (t1402 - t1406);\n  t1413 = (t1403 - t1407);\n  t1414 = (t1404 + t1409);\n  t1415 = (t1405 - t1408);\n  t1416 = (t1404 - t1409);\n  t1417 = (t1405 + t1408);\n  s666 = P1[(a2011 + 32)];\n  s667 = P1[(a2011 + 33)];\n  s668 = P1[(a2011 + 288)];\n  s669 = P1[(a2011 + 289)];\n  t1418 = (s666 + s668);\n  t1419 = (s667 + s669);\n  t1420 = (s666 - s668);\n  t1421 = (s667 - s669);\n  s670 = P1[(a2011 + 160)];\n  s671 = P1[(a2011 + 161)];\n  s672 = P1[(a2011 + 416)];\n  s673 = P1[(a2011 + 417)];\n  t1422 = (s670 + s672);\n  t1423 = (s671 + s673);\n  t1424 = (s670 - s672);\n  t1425 = (s671 - s673);\n  t1426 = (t1418 + t1422);\n  t1427 = (t1419 + t1423);\n  a2012 = (0.70710678118654757*(t1418 - t1422));\n  a2013 = (0.70710678118654757*(t1419 - t1423));\n  s674 = (a2012 + a2013);\n  s675 = (a2013 - a2012);\n  t1428 = (t1420 + t1425);\n  t1429 = (t1421 - t1424);\n  t1430 = (t1420 - t1425);\n  t1431 = (t1421 + t1424);\n  s676 = ((0.92387953251128674*t1428) + (0.38268343236508978*t1429));\n  s677 = ((0.92387953251128674*t1429) - (0.38268343236508978*t1428));\n  s678 = ((0.38268343236508978*t1430) + (0.92387953251128674*t1431));\n  s679 = ((0.38268343236508978*t1431) - (0.92387953251128674*t1430));\n  s680 = P1[(a2011 + 64)];\n  s681 = P1[(a2011 + 65)];\n  s682 = P1[(a2011 + 320)];\n  s683 = P1[(a2011 + 321)];\n  t1432 = (s680 + s682);\n  t1433 = (s681 + s683);\n  t1434 = (s680 - s682);\n  t1435 = (s681 - s683);\n  s684 = P1[(a2011 + 192)];\n  s685 = P1[(a2011 + 193)];\n  s686 = P1[(a2011 + 448)];\n  s687 = P1[(a2011 + 449)];\n  t1436 = (s684 + s686);\n  t1437 = (s685 + s687);\n  t1438 = (s684 - s686);\n  t1439 = (s685 - s687);\n  t1440 = (t1432 + t1436);\n  t1441 = (t1433 + t1437);\n  t1442 = (t1432 - t1436);\n  t1443 = (t1433 - t1437);\n  a2014 = (0.70710678118654757*(t1434 + t1439));\n  a2015 = (0.70710678118654757*(t1435 - t1438));\n  s688 = (a2014 + a2015);\n  s689 = (a2015 - a2014);\n  a2016 = (0.70710678118654757*(t1435 + t1438));\n  a2017 = (0.70710678118654757*(t1434 - t1439));\n  s690 = (a2016 - a2017);\n  s691 = (a2017 + a2016);\n  s692 = P1[(a2011 + 96)];\n  s693 = P1[(a2011 + 97)];\n  s694 = P1[(a2011 + 352)];\n  s695 = P1[(a2011 + 353)];\n  t1444 = (s692 + s694);\n  t1445 = (s693 + s695);\n  t1446 = (s692 - s694);\n  t1447 = (s693 - s695);\n  s696 = P1[(a2011 + 224)];\n  s697 = P1[(a2011 + 225)];\n  s698 = P1[(a2011 + 480)];\n  s699 = P1[(a2011 + 481)];\n  t1448 = (s696 + s698);\n  t1449 = (s697 + s699);\n  t1450 = (s696 - s698);\n  t1451 = (s697 - s699);\n  t1452 = (t1444 + t1448);\n  t1453 = (t1445 + t1449);\n  a2018 = (0.70710678118654757*(t1445 - t1449));\n  a2019 = (0.70710678118654757*(t1444 - t1448));\n  s700 = (a2018 - a2019);\n  s701 = (a2019 + a2018);\n  t1454 = (t1446 + t1451);\n  t1455 = (t1447 - t1450);\n  t1456 = (t1446 - t1451);\n  t1457 = (t1447 + t1450);\n  s702 = ((0.38268343236508978*t1454) + (0.92387953251128674*t1455));\n  s703 = ((0.38268343236508978*t1455) - (0.92387953251128674*t1454));\n  s704 = ((0.92387953251128674*t1456) + (0.38268343236508978*t1457));\n  s705 = ((0.38268343236508978*t1456) - (0.92387953251128674*t1457));\n  t1458 = (t1410 + t1440);\n  t1459 = (t1411 + t1441);\n  t1460 = (t1410 - t1440);\n  t1461 = (t1411 - t1441);\n  t1462 = (t1426 + t1452);\n  t1463 = (t1427 + t1453);\n  t1464 = (t1426 - t1452);\n  t1465 = (t1427 - t1453);\n  a2020 = (a2009 + (32*a2010));\n  T33[a2020] = (t1458 + t1462);\n  T33[(a2020 + 1)] = (t1459 + t1463);\n  T33[(a2020 + 16)] = (t1458 - t1462);\n  T33[(a2020 + 17)] = (t1459 - t1463);\n  T33[(a2020 + 8)] = (t1460 + t1465);\n  T33[(a2020 + 9)] = (t1461 - t1464);\n  T33[(a2020 + 24)] = (t1460 - t1465);\n  T33[(a2020 + 25)] = (t1461 + t1464);\n  t1466 = (t1414 + s688);\n  t1467 = (t1415 + s689);\n  t1468 = (t1414 - s688);\n  t1469 = (t1415 - s689);\n  t1470 = (s676 + s702);\n  t1471 = (s677 + s703);\n  t1472 = (s676 - s702);\n  t1473 = (s677 - s703);\n  T33[(a2020 + 2)] = (t1466 + t1470);\n  T33[(a2020 + 3)] = (t1467 + t1471);\n  T33[(a2020 + 18)] = (t1466 - t1470);\n  T33[(a2020 + 19)] = (t1467 - t1471);\n  T33[(a2020 + 10)] = (t1468 + t1473);\n  T33[(a2020 + 11)] = (t1469 - t1472);\n  T33[(a2020 + 26)] = (t1468 - t1473);\n  T33[(a2020 + 27)] = (t1469 + t1472);\n  t1474 = (t1412 + t1443);\n  t1475 = (t1413 - t1442);\n  t1476 = (t1412 - t1443);\n  t1477 = (t1413 + t1442);\n  t1478 = (s674 + s700);\n  t1479 = (s675 - s701);\n  t1480 = (s674 - s700);\n  t1481 = (s675 + s701);\n  T33[(a2020 + 4)] = (t1474 + t1478);\n  T33[(a2020 + 5)] = (t1475 + t1479);\n  T33[(a2020 + 20)] = (t1474 - t1478);\n  T33[(a2020 + 21)] = (t1475 - t1479);\n  T33[(a2020 + 12)] = (t1476 + t1481);\n  T33[(a2020 + 13)] = (t1477 - t1480);\n  T33[(a2020 + 28)] = (t1476 - t1481);\n  T33[(a2020 + 29)] = (t1477 + t1480);\n  t1482 = (t1416 + s690);\n  t1483 = (t1417 - s691);\n  t1484 = (t1416 - s690);\n  t1485 = (t1417 + s691);\n  t1486 = (s678 - s704);\n  t1487 = (s679 + s705);\n  t1488 = (s678 + s704);\n  t1489 = (s679 - s705);\n  T33[(a2020 + 6)] = (t1482 + t1486);\n  T33[(a2020 + 7)] = (t1483 + t1487);\n  T33[(a2020 + 22)] = (t1482 - t1486);\n  T33[(a2020 + 23)] = (t1483 - t1487);\n  T33[(a2020 + 14)] = (t1484 + t1489);\n  T33[(a2020 + 15)] = (t1485 - t1488);\n  T33[(a2020 + 30)] = (t1484 - t1489);\n  T33[(a2020 + 31)] = (t1485 + t1488);\n  __syncthreads();\n  double a2995, a2996, a2997, a2998, a2999, a3000, a3001, a3002, \n         a3003, a3004, a3005, a3006, a3007, a3008, a3009, a3010, \n         a3011, a3012, a3013, a3014, a3015, a3016, a3017, a3018, \n         a3019, a3020, a3021, a3022, a3023, a3024, a3025, a3026, \n         a3027, a3028, a3029, a3030, a3031, a3032, a3033, a3034, \n         s1000, s1001, s1002, s1003, s1004, s1005, s1006, s1007, \n         s1008, s1009, s1010, s1011, s1012, s1013, s1014, s1015, \n         s1016, s1017, s1018, s1019, s1020, s1021, s1022, s1023, \n         s1024, s1025, s946, s947, s948, s949, s950, s951, \n         s952, s953, s954, s955, s956, s957, s958, s959, \n         s960, s961, s962, s963, s964, s965, s966, s967, \n         s968, s969, s970, s971, s972, s973, s974, s975, \n         s976, s977, s978, s979, s980, s981, s982, s983, \n         s984, s985, s986, s987, s988, s989, s990, s991, \n         s992, s993, s994, s995, s996, s997, s998, s999, \n         t1834, t1835, t1836, t1837, t1838, t1839, t1840, t1841, \n         t1842, t1843, t1844, t1845, t1846, t1847, t1848, t1849, \n         t1850, t1851, t1852, t1853, t1854, t1855, t1856, t1857, \n         t1858, t1859, t1860, t1861, t1862, t1863, t1864, t1865, \n         t1866, t1867, t1868, t1869, t1870, t1871, t1872, t1873, \n         t1874, t1875, t1876, t1877, t1878, t1879, t1880, t1881, \n         t1882, t1883, t1884, t1885, t1886, t1887, t1888, t1889, \n         t1890, t1891, t1892, t1893, t1894, t1895, t1896, t1897, \n         t1898, t1899, t1900, t1901, t1902, t1903, t1904, t1905, \n         t1906, t1907, t1908, t1909, t1910, t1911, t1912, t1913, \n         t1914, t1915, t1916, t1917, t1918, t1919, t1920, t1921;\n  int a2991, a2992, a2993, a2994, a3035;\n  a2991 = (threadIdx.x / 16);\n  a2992 = (threadIdx.x % 16);\n  a2993 = ((512*a2991) + (2*a2992));\n  s946 = T33[a2993];\n  s947 = T33[(a2993 + 1)];\n  s948 = T33[(a2993 + 256)];\n  s949 = T33[(a2993 + 257)];\n  a2994 = (32*a2992);\n  a2995 = D3[a2994];\n  a2996 = D3[(a2994 + 1)];\n  s950 = ((a2995*s946) - (a2996*s947));\n  s951 = ((a2996*s946) + (a2995*s947));\n  a2997 = D3[(a2994 + 2)];\n  a2998 = D3[(a2994 + 3)];\n  s952 = ((a2997*s948) - (a2998*s949));\n  s953 = ((a2998*s948) + (a2997*s949));\n  t1834 = (s950 + s952);\n  t1835 = (s951 + s953);\n  t1836 = (s950 - s952);\n  t1837 = (s951 - s953);\n  s954 = T33[(a2993 + 128)];\n  s955 = T33[(a2993 + 129)];\n  s956 = T33[(a2993 + 384)];\n  s957 = T33[(a2993 + 385)];\n  a2999 = D3[(4 + a2994)];\n  a3000 = D3[(5 + a2994)];\n  s958 = ((a2999*s954) - (a3000*s955));\n  s959 = ((a3000*s954) + (a2999*s955));\n  a3001 = D3[(6 + a2994)];\n  a3002 = D3[(7 + a2994)];\n  s960 = ((a3001*s956) - (a3002*s957));\n  s961 = ((a3002*s956) + (a3001*s957));\n  t1838 = (s958 + s960);\n  t1839 = (s959 + s961);\n  t1840 = (s958 - s960);\n  t1841 = (s959 - s961);\n  t1842 = (t1834 + t1838);\n  t1843 = (t1835 + t1839);\n  t1844 = (t1834 - t1838);\n  t1845 = (t1835 - t1839);\n  t1846 = (t1836 + t1841);\n  t1847 = (t1837 - t1840);\n  t1848 = (t1836 - t1841);\n  t1849 = (t1837 + t1840);\n  s962 = T33[(a2993 + 32)];\n  s963 = T33[(a2993 + 33)];\n  s964 = T33[(a2993 + 288)];\n  s965 = T33[(a2993 + 289)];\n  a3003 = D3[(a2994 + 8)];\n  a3004 = D3[(9 + a2994)];\n  s966 = ((a3003*s962) - (a3004*s963));\n  s967 = ((a3004*s962) + (a3003*s963));\n  a3005 = D3[(10 + a2994)];\n  a3006 = D3[(11 + a2994)];\n  s968 = ((a3005*s964) - (a3006*s965));\n  s969 = ((a3006*s964) + (a3005*s965));\n  t1850 = (s966 + s968);\n  t1851 = (s967 + s969);\n  t1852 = (s966 - s968);\n  t1853 = (s967 - s969);\n  s970 = T33[(a2993 + 160)];\n  s971 = T33[(a2993 + 161)];\n  s972 = T33[(a2993 + 416)];\n  s973 = T33[(a2993 + 417)];\n  a3007 = D3[(12 + a2994)];\n  a3008 = D3[(13 + a2994)];\n  s974 = ((a3007*s970) - (a3008*s971));\n  s975 = ((a3008*s970) + (a3007*s971));\n  a3009 = D3[(14 + a2994)];\n  a3010 = D3[(15 + a2994)];\n  s976 = ((a3009*s972) - (a3010*s973));\n  s977 = ((a3010*s972) + (a3009*s973));\n  t1854 = (s974 + s976);\n  t1855 = (s975 + s977);\n  t1856 = (s974 - s976);\n  t1857 = (s975 - s977);\n  t1858 = (t1850 + t1854);\n  t1859 = (t1851 + t1855);\n  a3011 = (0.70710678118654757*(t1850 - t1854));\n  a3012 = (0.70710678118654757*(t1851 - t1855));\n  s978 = (a3011 + a3012);\n  s979 = (a3012 - a3011);\n  t1860 = (t1852 + t1857);\n  t1861 = (t1853 - t1856);\n  t1862 = (t1852 - t1857);\n  t1863 = (t1853 + t1856);\n  s980 = ((0.92387953251128674*t1860) + (0.38268343236508978*t1861));\n  s981 = ((0.92387953251128674*t1861) - (0.38268343236508978*t1860));\n  s982 = ((0.38268343236508978*t1862) + (0.92387953251128674*t1863));\n  s983 = ((0.38268343236508978*t1863) - (0.92387953251128674*t1862));\n  s984 = T33[(a2993 + 64)];\n  s985 = T33[(a2993 + 65)];\n  s986 = T33[(a2993 + 320)];\n  s987 = T33[(a2993 + 321)];\n  a3013 = D3[(a2994 + 16)];\n  a3014 = D3[(17 + a2994)];\n  s988 = ((a3013*s984) - (a3014*s985));\n  s989 = ((a3014*s984) + (a3013*s985));\n  a3015 = D3[(18 + a2994)];\n  a3016 = D3[(19 + a2994)];\n  s990 = ((a3015*s986) - (a3016*s987));\n  s991 = ((a3016*s986) + (a3015*s987));\n  t1864 = (s988 + s990);\n  t1865 = (s989 + s991);\n  t1866 = (s988 - s990);\n  t1867 = (s989 - s991);\n  s992 = T33[(a2993 + 192)];\n  s993 = T33[(a2993 + 193)];\n  s994 = T33[(a2993 + 448)];\n  s995 = T33[(a2993 + 449)];\n  a3017 = D3[(20 + a2994)];\n  a3018 = D3[(21 + a2994)];\n  s996 = ((a3017*s992) - (a3018*s993));\n  s997 = ((a3018*s992) + (a3017*s993));\n  a3019 = D3[(22 + a2994)];\n  a3020 = D3[(23 + a2994)];\n  s998 = ((a3019*s994) - (a3020*s995));\n  s999 = ((a3020*s994) + (a3019*s995));\n  t1868 = (s996 + s998);\n  t1869 = (s997 + s999);\n  t1870 = (s996 - s998);\n  t1871 = (s997 - s999);\n  t1872 = (t1864 + t1868);\n  t1873 = (t1865 + t1869);\n  t1874 = (t1864 - t1868);\n  t1875 = (t1865 - t1869);\n  a3021 = (0.70710678118654757*(t1866 + t1871));\n  a3022 = (0.70710678118654757*(t1867 - t1870));\n  s1000 = (a3021 + a3022);\n  s1001 = (a3022 - a3021);\n  a3023 = (0.70710678118654757*(t1867 + t1870));\n  a3024 = (0.70710678118654757*(t1866 - t1871));\n  s1002 = (a3023 - a3024);\n  s1003 = (a3024 + a3023);\n  s1004 = T33[(a2993 + 96)];\n  s1005 = T33[(a2993 + 97)];\n  s1006 = T33[(a2993 + 352)];\n  s1007 = T33[(a2993 + 353)];\n  a3025 = D3[(a2994 + 24)];\n  a3026 = D3[(25 + a2994)];\n  s1008 = ((a3025*s1004) - (a3026*s1005));\n  s1009 = ((a3026*s1004) + (a3025*s1005));\n  a3027 = D3[(26 + a2994)];\n  a3028 = D3[(27 + a2994)];\n  s1010 = ((a3027*s1006) - (a3028*s1007));\n  s1011 = ((a3028*s1006) + (a3027*s1007));\n  t1876 = (s1008 + s1010);\n  t1877 = (s1009 + s1011);\n  t1878 = (s1008 - s1010);\n  t1879 = (s1009 - s1011);\n  s1012 = T33[(a2993 + 224)];\n  s1013 = T33[(a2993 + 225)];\n  s1014 = T33[(a2993 + 480)];\n  s1015 = T33[(a2993 + 481)];\n  a3029 = D3[(28 + a2994)];\n  a3030 = D3[(29 + a2994)];\n  s1016 = ((a3029*s1012) - (a3030*s1013));\n  s1017 = ((a3030*s1012) + (a3029*s1013));\n  a3031 = D3[(30 + a2994)];\n  a3032 = D3[(31 + a2994)];\n  s1018 = ((a3031*s1014) - (a3032*s1015));\n  s1019 = ((a3032*s1014) + (a3031*s1015));\n  t1880 = (s1016 + s1018);\n  t1881 = (s1017 + s1019);\n  t1882 = (s1016 - s1018);\n  t1883 = (s1017 - s1019);\n  t1884 = (t1876 + t1880);\n  t1885 = (t1877 + t1881);\n  a3033 = (0.70710678118654757*(t1877 - t1881));\n  a3034 = (0.70710678118654757*(t1876 - t1880));\n  s1020 = (a3033 - a3034);\n  s1021 = (a3034 + a3033);\n  t1886 = (t1878 + t1883);\n  t1887 = (t1879 - t1882);\n  t1888 = (t1878 - t1883);\n  t1889 = (t1879 + t1882);\n  s1022 = ((0.38268343236508978*t1886) + (0.92387953251128674*t1887));\n  s1023 = ((0.38268343236508978*t1887) - (0.92387953251128674*t1886));\n  s1024 = ((0.92387953251128674*t1888) + (0.38268343236508978*t1889));\n  s1025 = ((0.38268343236508978*t1888) - (0.92387953251128674*t1889));\n  t1890 = (t1842 + t1872);\n  t1891 = (t1843 + t1873);\n  t1892 = (t1842 - t1872);\n  t1893 = (t1843 - t1873);\n  t1894 = (t1858 + t1884);\n  t1895 = (t1859 + t1885);\n  t1896 = (t1858 - t1884);\n  t1897 = (t1859 - t1885);\n  a3035 = ((8*blockIdx.x) + (131072*a2992) + (2*a2991));\n  P2[a3035] = (t1890 + t1894);\n  P2[(a3035 + 1)] = (t1891 + t1895);\n  P2[(a3035 + 16777216)] = (t1890 - t1894);\n  P2[(a3035 + 16777217)] = (t1891 - t1895);\n  P2[(a3035 + 8388608)] = (t1892 + t1897);\n  P2[(a3035 + 8388609)] = (t1893 - t1896);\n  P2[(a3035 + 25165824)] = (t1892 - t1897);\n  P2[(a3035 + 25165825)] = (t1893 + t1896);\n  t1898 = (t1846 + s1000);\n  t1899 = (t1847 + s1001);\n  t1900 = (t1846 - s1000);\n  t1901 = (t1847 - s1001);\n  t1902 = (s980 + s1022);\n  t1903 = (s981 + s1023);\n  t1904 = (s980 - s1022);\n  t1905 = (s981 - s1023);\n  P2[(a3035 + 2097152)] = (t1898 + t1902);\n  P2[(a3035 + 2097153)] = (t1899 + t1903);\n  P2[(a3035 + 18874368)] = (t1898 - t1902);\n  P2[(a3035 + 18874369)] = (t1899 - t1903);\n  P2[(a3035 + 10485760)] = (t1900 + t1905);\n  P2[(a3035 + 10485761)] = (t1901 - t1904);\n  P2[(a3035 + 27262976)] = (t1900 - t1905);\n  P2[(a3035 + 27262977)] = (t1901 + t1904);\n  t1906 = (t1844 + t1875);\n  t1907 = (t1845 - t1874);\n  t1908 = (t1844 - t1875);\n  t1909 = (t1845 + t1874);\n  t1910 = (s978 + s1020);\n  t1911 = (s979 - s1021);\n  t1912 = (s978 - s1020);\n  t1913 = (s979 + s1021);\n  P2[(a3035 + 4194304)] = (t1906 + t1910);\n  P2[(a3035 + 4194305)] = (t1907 + t1911);\n  P2[(a3035 + 20971520)] = (t1906 - t1910);\n  P2[(a3035 + 20971521)] = (t1907 - t1911);\n  P2[(a3035 + 12582912)] = (t1908 + t1913);\n  P2[(a3035 + 12582913)] = (t1909 - t1912);\n  P2[(a3035 + 29360128)] = (t1908 - t1913);\n  P2[(a3035 + 29360129)] = (t1909 + t1912);\n  t1914 = (t1848 + s1002);\n  t1915 = (t1849 - s1003);\n  t1916 = (t1848 - s1002);\n  t1917 = (t1849 + s1003);\n  t1918 = (s982 - s1024);\n  t1919 = (s983 + s1025);\n  t1920 = (s982 + s1024);\n  t1921 = (s983 - s1025);\n  P2[(a3035 + 6291456)] = (t1914 + t1918);\n  P2[(a3035 + 6291457)] = (t1915 + t1919);\n  P2[(a3035 + 23068672)] = (t1914 - t1918);\n  P2[(a3035 + 23068673)] = (t1915 - t1919);\n  P2[(a3035 + 14680064)] = (t1916 + t1921);\n  P2[(a3035 + 14680065)] = (t1917 - t1920);\n  P2[(a3035 + 31457280)] = (t1916 - t1921);\n  P2[(a3035 + 31457281)] = (t1917 + t1920);\n  __syncthreads();\n}",
            "__global__\nvoid ker_zmddft_fwd_256x256x256_cu2(const double *P2, double *Y)\n{\n  __shared__ double T63[2048];\n  double a3529, a3530, a3531, a3532, a3533, a3534, a3535, a3536, \n         s1170, s1171, s1172, s1173, s1174, s1175, s1176, s1177, \n         s1178, s1179, s1180, s1181, s1182, s1183, s1184, s1185, \n         s1186, s1187, s1188, s1189, s1190, s1191, s1192, s1193, \n         s1194, s1195, s1196, s1197, s1198, s1199, s1200, s1201, \n         s1202, s1203, s1204, s1205, s1206, s1207, s1208, s1209, \n         s1210, s1211, s1212, s1213, s1214, s1215, s1216, s1217, \n         t2266, t2267, t2268, t2269, t2270, t2271, t2272, t2273, \n         t2274, t2275, t2276, t2277, t2278, t2279, t2280, t2281, \n         t2282, t2283, t2284, t2285, t2286, t2287, t2288, t2289, \n         t2290, t2291, t2292, t2293, t2294, t2295, t2296, t2297, \n         t2298, t2299, t2300, t2301, t2302, t2303, t2304, t2305, \n         t2306, t2307, t2308, t2309, t2310, t2311, t2312, t2313, \n         t2314, t2315, t2316, t2317, t2318, t2319, t2320, t2321, \n         t2322, t2323, t2324, t2325, t2326, t2327, t2328, t2329, \n         t2330, t2331, t2332, t2333, t2334, t2335, t2336, t2337, \n         t2338, t2339, t2340, t2341, t2342, t2343, t2344, t2345, \n         t2346, t2347, t2348, t2349, t2350, t2351, t2352, t2353;\n  int a3526, a3527, a3528, a3537;\n  a3526 = (512*(threadIdx.x / 16));\n  a3527 = (threadIdx.x % 16);\n  a3528 = ((2048*blockIdx.x) + a3526 + (2*a3527));\n  s1170 = P2[a3528];\n  s1171 = P2[(a3528 + 1)];\n  s1172 = P2[(a3528 + 256)];\n  s1173 = P2[(a3528 + 257)];\n  t2266 = (s1170 + s1172);\n  t2267 = (s1171 + s1173);\n  t2268 = (s1170 - s1172);\n  t2269 = (s1171 - s1173);\n  s1174 = P2[(a3528 + 128)];\n  s1175 = P2[(a3528 + 129)];\n  s1176 = P2[(a3528 + 384)];\n  s1177 = P2[(a3528 + 385)];\n  t2270 = (s1174 + s1176);\n  t2271 = (s1175 + s1177);\n  t2272 = (s1174 - s1176);\n  t2273 = (s1175 - s1177);\n  t2274 = (t2266 + t2270);\n  t2275 = (t2267 + t2271);\n  t2276 = (t2266 - t2270);\n  t2277 = (t2267 - t2271);\n  t2278 = (t2268 + t2273);\n  t2279 = (t2269 - t2272);\n  t2280 = (t2268 - t2273);\n  t2281 = (t2269 + t2272);\n  s1178 = P2[(a3528 + 32)];\n  s1179 = P2[(a3528 + 33)];\n  s1180 = P2[(a3528 + 288)];\n  s1181 = P2[(a3528 + 289)];\n  t2282 = (s1178 + s1180);\n  t2283 = (s1179 + s1181);\n  t2284 = (s1178 - s1180);\n  t2285 = (s1179 - s1181);\n  s1182 = P2[(a3528 + 160)];\n  s1183 = P2[(a3528 + 161)];\n  s1184 = P2[(a3528 + 416)];\n  s1185 = P2[(a3528 + 417)];\n  t2286 = (s1182 + s1184);\n  t2287 = (s1183 + s1185);\n  t2288 = (s1182 - s1184);\n  t2289 = (s1183 - s1185);\n  t2290 = (t2282 + t2286);\n  t2291 = (t2283 + t2287);\n  a3529 = (0.70710678118654757*(t2282 - t2286));\n  a3530 = (0.70710678118654757*(t2283 - t2287));\n  s1186 = (a3529 + a3530);\n  s1187 = (a3530 - a3529);\n  t2292 = (t2284 + t2289);\n  t2293 = (t2285 - t2288);\n  t2294 = (t2284 - t2289);\n  t2295 = (t2285 + t2288);\n  s1188 = ((0.92387953251128674*t2292) + (0.38268343236508978*t2293));\n  s1189 = ((0.92387953251128674*t2293) - (0.38268343236508978*t2292));\n  s1190 = ((0.38268343236508978*t2294) + (0.92387953251128674*t2295));\n  s1191 = ((0.38268343236508978*t2295) - (0.92387953251128674*t2294));\n  s1192 = P2[(a3528 + 64)];\n  s1193 = P2[(a3528 + 65)];\n  s1194 = P2[(a3528 + 320)];\n  s1195 = P2[(a3528 + 321)];\n  t2296 = (s1192 + s1194);\n  t2297 = (s1193 + s1195);\n  t2298 = (s1192 - s1194);\n  t2299 = (s1193 - s1195);\n  s1196 = P2[(a3528 + 192)];\n  s1197 = P2[(a3528 + 193)];\n  s1198 = P2[(a3528 + 448)];\n  s1199 = P2[(a3528 + 449)];\n  t2300 = (s1196 + s1198);\n  t2301 = (s1197 + s1199);\n  t2302 = (s1196 - s1198);\n  t2303 = (s1197 - s1199);\n  t2304 = (t2296 + t2300);\n  t2305 = (t2297 + t2301);\n  t2306 = (t2296 - t2300);\n  t2307 = (t2297 - t2301);\n  a3531 = (0.70710678118654757*(t2298 + t2303));\n  a3532 = (0.70710678118654757*(t2299 - t2302));\n  s1200 = (a3531 + a3532);\n  s1201 = (a3532 - a3531);\n  a3533 = (0.70710678118654757*(t2299 + t2302));\n  a3534 = (0.70710678118654757*(t2298 - t2303));\n  s1202 = (a3533 - a3534);\n  s1203 = (a3534 + a3533);\n  s1204 = P2[(a3528 + 96)];\n  s1205 = P2[(a3528 + 97)];\n  s1206 = P2[(a3528 + 352)];\n  s1207 = P2[(a3528 + 353)];\n  t2308 = (s1204 + s1206);\n  t2309 = (s1205 + s1207);\n  t2310 = (s1204 - s1206);\n  t2311 = (s1205 - s1207);\n  s1208 = P2[(a3528 + 224)];\n  s1209 = P2[(a3528 + 225)];\n  s1210 = P2[(a3528 + 480)];\n  s1211 = P2[(a3528 + 481)];\n  t2312 = (s1208 + s1210);\n  t2313 = (s1209 + s1211);\n  t2314 = (s1208 - s1210);\n  t2315 = (s1209 - s1211);\n  t2316 = (t2308 + t2312);\n  t2317 = (t2309 + t2313);\n  a3535 = (0.70710678118654757*(t2309 - t2313));\n  a3536 = (0.70710678118654757*(t2308 - t2312));\n  s1212 = (a3535 - a3536);\n  s1213 = (a3536 + a3535);\n  t2318 = (t2310 + t2315);\n  t2319 = (t2311 - t2314);\n  t2320 = (t2310 - t2315);\n  t2321 = (t2311 + t2314);\n  s1214 = ((0.38268343236508978*t2318) + (0.92387953251128674*t2319));\n  s1215 = ((0.38268343236508978*t2319) - (0.92387953251128674*t2318));\n  s1216 = ((0.92387953251128674*t2320) + (0.38268343236508978*t2321));\n  s1217 = ((0.38268343236508978*t2320) - (0.92387953251128674*t2321));\n  t2322 = (t2274 + t2304);\n  t2323 = (t2275 + t2305);\n  t2324 = (t2274 - t2304);\n  t2325 = (t2275 - t2305);\n  t2326 = (t2290 + t2316);\n  t2327 = (t2291 + t2317);\n  t2328 = (t2290 - t2316);\n  t2329 = (t2291 - t2317);\n  a3537 = (a3526 + (32*a3527));\n  T63[a3537] = (t2322 + t2326);\n  T63[(a3537 + 1)] = (t2323 + t2327);\n  T63[(a3537 + 16)] = (t2322 - t2326);\n  T63[(a3537 + 17)] = (t2323 - t2327);\n  T63[(a3537 + 8)] = (t2324 + t2329);\n  T63[(a3537 + 9)] = (t2325 - t2328);\n  T63[(a3537 + 24)] = (t2324 - t2329);\n  T63[(a3537 + 25)] = (t2325 + t2328);\n  t2330 = (t2278 + s1200);\n  t2331 = (t2279 + s1201);\n  t2332 = (t2278 - s1200);\n  t2333 = (t2279 - s1201);\n  t2334 = (s1188 + s1214);\n  t2335 = (s1189 + s1215);\n  t2336 = (s1188 - s1214);\n  t2337 = (s1189 - s1215);\n  T63[(a3537 + 2)] = (t2330 + t2334);\n  T63[(a3537 + 3)] = (t2331 + t2335);\n  T63[(a3537 + 18)] = (t2330 - t2334);\n  T63[(a3537 + 19)] = (t2331 - t2335);\n  T63[(a3537 + 10)] = (t2332 + t2337);\n  T63[(a3537 + 11)] = (t2333 - t2336);\n  T63[(a3537 + 26)] = (t2332 - t2337);\n  T63[(a3537 + 27)] = (t2333 + t2336);\n  t2338 = (t2276 + t2307);\n  t2339 = (t2277 - t2306);\n  t2340 = (t2276 - t2307);\n  t2341 = (t2277 + t2306);\n  t2342 = (s1186 + s1212);\n  t2343 = (s1187 - s1213);\n  t2344 = (s1186 - s1212);\n  t2345 = (s1187 + s1213);\n  T63[(a3537 + 4)] = (t2338 + t2342);\n  T63[(a3537 + 5)] = (t2339 + t2343);\n  T63[(a3537 + 20)] = (t2338 - t2342);\n  T63[(a3537 + 21)] = (t2339 - t2343);\n  T63[(a3537 + 12)] = (t2340 + t2345);\n  T63[(a3537 + 13)] = (t2341 - t2344);\n  T63[(a3537 + 28)] = (t2340 - t2345);\n  T63[(a3537 + 29)] = (t2341 + t2344);\n  t2346 = (t2280 + s1202);\n  t2347 = (t2281 - s1203);\n  t2348 = (t2280 - s1202);\n  t2349 = (t2281 + s1203);\n  t2350 = (s1190 - s1216);\n  t2351 = (s1191 + s1217);\n  t2352 = (s1190 + s1216);\n  t2353 = (s1191 - s1217);\n  T63[(a3537 + 6)] = (t2346 + t2350);\n  T63[(a3537 + 7)] = (t2347 + t2351);\n  T63[(a3537 + 22)] = (t2346 - t2350);\n  T63[(a3537 + 23)] = (t2347 - t2351);\n  T63[(a3537 + 14)] = (t2348 + t2353);\n  T63[(a3537 + 15)] = (t2349 - t2352);\n  T63[(a3537 + 30)] = (t2348 - t2353);\n  T63[(a3537 + 31)] = (t2349 + t2352);\n  __syncthreads();\n  double a4512, a4513, a4514, a4515, a4516, a4517, a4518, a4519, \n         a4520, a4521, a4522, a4523, a4524, a4525, a4526, a4527, \n         a4528, a4529, a4530, a4531, a4532, a4533, a4534, a4535, \n         a4536, a4537, a4538, a4539, a4540, a4541, a4542, a4543, \n         a4544, a4545, a4546, a4547, a4548, a4549, a4550, a4551, \n         s1458, s1459, s1460, s1461, s1462, s1463, s1464, s1465, \n         s1466, s1467, s1468, s1469, s1470, s1471, s1472, s1473, \n         s1474, s1475, s1476, s1477, s1478, s1479, s1480, s1481, \n         s1482, s1483, s1484, s1485, s1486, s1487, s1488, s1489, \n         s1490, s1491, s1492, s1493, s1494, s1495, s1496, s1497, \n         s1498, s1499, s1500, s1501, s1502, s1503, s1504, s1505, \n         s1506, s1507, s1508, s1509, s1510, s1511, s1512, s1513, \n         s1514, s1515, s1516, s1517, s1518, s1519, s1520, s1521, \n         s1522, s1523, s1524, s1525, s1526, s1527, s1528, s1529, \n         s1530, s1531, s1532, s1533, s1534, s1535, s1536, s1537, \n         t2698, t2699, t2700, t2701, t2702, t2703, t2704, t2705, \n         t2706, t2707, t2708, t2709, t2710, t2711, t2712, t2713, \n         t2714, t2715, t2716, t2717, t2718, t2719, t2720, t2721, \n         t2722, t2723, t2724, t2725, t2726, t2727, t2728, t2729, \n         t2730, t2731, t2732, t2733, t2734, t2735, t2736, t2737, \n         t2738, t2739, t2740, t2741, t2742, t2743, t2744, t2745, \n         t2746, t2747, t2748, t2749, t2750, t2751, t2752, t2753, \n         t2754, t2755, t2756, t2757, t2758, t2759, t2760, t2761, \n         t2762, t2763, t2764, t2765, t2766, t2767, t2768, t2769, \n         t2770, t2771, t2772, t2773, t2774, t2775, t2776, t2777, \n         t2778, t2779, t2780, t2781, t2782, t2783, t2784, t2785;\n  int a4508, a4509, a4510, a4511, a4552;\n  a4508 = (threadIdx.x / 16);\n  a4509 = (threadIdx.x % 16);\n  a4510 = ((512*a4508) + (2*a4509));\n  s1458 = T63[a4510];\n  s1459 = T63[(a4510 + 1)];\n  s1460 = T63[(a4510 + 256)];\n  s1461 = T63[(a4510 + 257)];\n  a4511 = (32*a4509);\n  a4512 = D3[a4511];\n  a4513 = D3[(a4511 + 1)];\n  s1462 = ((a4512*s1458) - (a4513*s1459));\n  s1463 = ((a4513*s1458) + (a4512*s1459));\n  a4514 = D3[(a4511 + 2)];\n  a4515 = D3[(a4511 + 3)];\n  s1464 = ((a4514*s1460) - (a4515*s1461));\n  s1465 = ((a4515*s1460) + (a4514*s1461));\n  t2698 = (s1462 + s1464);\n  t2699 = (s1463 + s1465);\n  t2700 = (s1462 - s1464);\n  t2701 = (s1463 - s1465);\n  s1466 = T63[(a4510 + 128)];\n  s1467 = T63[(a4510 + 129)];\n  s1468 = T63[(a4510 + 384)];\n  s1469 = T63[(a4510 + 385)];\n  a4516 = D3[(4 + a4511)];\n  a4517 = D3[(5 + a4511)];\n  s1470 = ((a4516*s1466) - (a4517*s1467));\n  s1471 = ((a4517*s1466) + (a4516*s1467));\n  a4518 = D3[(6 + a4511)];\n  a4519 = D3[(7 + a4511)];\n  s1472 = ((a4518*s1468) - (a4519*s1469));\n  s1473 = ((a4519*s1468) + (a4518*s1469));\n  t2702 = (s1470 + s1472);\n  t2703 = (s1471 + s1473);\n  t2704 = (s1470 - s1472);\n  t2705 = (s1471 - s1473);\n  t2706 = (t2698 + t2702);\n  t2707 = (t2699 + t2703);\n  t2708 = (t2698 - t2702);\n  t2709 = (t2699 - t2703);\n  t2710 = (t2700 + t2705);\n  t2711 = (t2701 - t2704);\n  t2712 = (t2700 - t2705);\n  t2713 = (t2701 + t2704);\n  s1474 = T63[(a4510 + 32)];\n  s1475 = T63[(a4510 + 33)];\n  s1476 = T63[(a4510 + 288)];\n  s1477 = T63[(a4510 + 289)];\n  a4520 = D3[(a4511 + 8)];\n  a4521 = D3[(9 + a4511)];\n  s1478 = ((a4520*s1474) - (a4521*s1475));\n  s1479 = ((a4521*s1474) + (a4520*s1475));\n  a4522 = D3[(10 + a4511)];\n  a4523 = D3[(11 + a4511)];\n  s1480 = ((a4522*s1476) - (a4523*s1477));\n  s1481 = ((a4523*s1476) + (a4522*s1477));\n  t2714 = (s1478 + s1480);\n  t2715 = (s1479 + s1481);\n  t2716 = (s1478 - s1480);\n  t2717 = (s1479 - s1481);\n  s1482 = T63[(a4510 + 160)];\n  s1483 = T63[(a4510 + 161)];\n  s1484 = T63[(a4510 + 416)];\n  s1485 = T63[(a4510 + 417)];\n  a4524 = D3[(12 + a4511)];\n  a4525 = D3[(13 + a4511)];\n  s1486 = ((a4524*s1482) - (a4525*s1483));\n  s1487 = ((a4525*s1482) + (a4524*s1483));\n  a4526 = D3[(14 + a4511)];\n  a4527 = D3[(15 + a4511)];\n  s1488 = ((a4526*s1484) - (a4527*s1485));\n  s1489 = ((a4527*s1484) + (a4526*s1485));\n  t2718 = (s1486 + s1488);\n  t2719 = (s1487 + s1489);\n  t2720 = (s1486 - s1488);\n  t2721 = (s1487 - s1489);\n  t2722 = (t2714 + t2718);\n  t2723 = (t2715 + t2719);\n  a4528 = (0.70710678118654757*(t2714 - t2718));\n  a4529 = (0.70710678118654757*(t2715 - t2719));\n  s1490 = (a4528 + a4529);\n  s1491 = (a4529 - a4528);\n  t2724 = (t2716 + t2721);\n  t2725 = (t2717 - t2720);\n  t2726 = (t2716 - t2721);\n  t2727 = (t2717 + t2720);\n  s1492 = ((0.92387953251128674*t2724) + (0.38268343236508978*t2725));\n  s1493 = ((0.92387953251128674*t2725) - (0.38268343236508978*t2724));\n  s1494 = ((0.38268343236508978*t2726) + (0.92387953251128674*t2727));\n  s1495 = ((0.38268343236508978*t2727) - (0.92387953251128674*t2726));\n  s1496 = T63[(a4510 + 64)];\n  s1497 = T63[(a4510 + 65)];\n  s1498 = T63[(a4510 + 320)];\n  s1499 = T63[(a4510 + 321)];\n  a4530 = D3[(a4511 + 16)];\n  a4531 = D3[(17 + a4511)];\n  s1500 = ((a4530*s1496) - (a4531*s1497));\n  s1501 = ((a4531*s1496) + (a4530*s1497));\n  a4532 = D3[(18 + a4511)];\n  a4533 = D3[(19 + a4511)];\n  s1502 = ((a4532*s1498) - (a4533*s1499));\n  s1503 = ((a4533*s1498) + (a4532*s1499));\n  t2728 = (s1500 + s1502);\n  t2729 = (s1501 + s1503);\n  t2730 = (s1500 - s1502);\n  t2731 = (s1501 - s1503);\n  s1504 = T63[(a4510 + 192)];\n  s1505 = T63[(a4510 + 193)];\n  s1506 = T63[(a4510 + 448)];\n  s1507 = T63[(a4510 + 449)];\n  a4534 = D3[(20 + a4511)];\n  a4535 = D3[(21 + a4511)];\n  s1508 = ((a4534*s1504) - (a4535*s1505));\n  s1509 = ((a4535*s1504) + (a4534*s1505));\n  a4536 = D3[(22 + a4511)];\n  a4537 = D3[(23 + a4511)];\n  s1510 = ((a4536*s1506) - (a4537*s1507));\n  s1511 = ((a4537*s1506) + (a4536*s1507));\n  t2732 = (s1508 + s1510);\n  t2733 = (s1509 + s1511);\n  t2734 = (s1508 - s1510);\n  t2735 = (s1509 - s1511);\n  t2736 = (t2728 + t2732);\n  t2737 = (t2729 + t2733);\n  t2738 = (t2728 - t2732);\n  t2739 = (t2729 - t2733);\n  a4538 = (0.70710678118654757*(t2730 + t2735));\n  a4539 = (0.70710678118654757*(t2731 - t2734));\n  s1512 = (a4538 + a4539);\n  s1513 = (a4539 - a4538);\n  a4540 = (0.70710678118654757*(t2731 + t2734));\n  a4541 = (0.70710678118654757*(t2730 - t2735));\n  s1514 = (a4540 - a4541);\n  s1515 = (a4541 + a4540);\n  s1516 = T63[(a4510 + 96)];\n  s1517 = T63[(a4510 + 97)];\n  s1518 = T63[(a4510 + 352)];\n  s1519 = T63[(a4510 + 353)];\n  a4542 = D3[(a4511 + 24)];\n  a4543 = D3[(25 + a4511)];\n  s1520 = ((a4542*s1516) - (a4543*s1517));\n  s1521 = ((a4543*s1516) + (a4542*s1517));\n  a4544 = D3[(26 + a4511)];\n  a4545 = D3[(27 + a4511)];\n  s1522 = ((a4544*s1518) - (a4545*s1519));\n  s1523 = ((a4545*s1518) + (a4544*s1519));\n  t2740 = (s1520 + s1522);\n  t2741 = (s1521 + s1523);\n  t2742 = (s1520 - s1522);\n  t2743 = (s1521 - s1523);\n  s1524 = T63[(a4510 + 224)];\n  s1525 = T63[(a4510 + 225)];\n  s1526 = T63[(a4510 + 480)];\n  s1527 = T63[(a4510 + 481)];\n  a4546 = D3[(28 + a4511)];\n  a4547 = D3[(29 + a4511)];\n  s1528 = ((a4546*s1524) - (a4547*s1525));\n  s1529 = ((a4547*s1524) + (a4546*s1525));\n  a4548 = D3[(30 + a4511)];\n  a4549 = D3[(31 + a4511)];\n  s1530 = ((a4548*s1526) - (a4549*s1527));\n  s1531 = ((a4549*s1526) + (a4548*s1527));\n  t2744 = (s1528 + s1530);\n  t2745 = (s1529 + s1531);\n  t2746 = (s1528 - s1530);\n  t2747 = (s1529 - s1531);\n  t2748 = (t2740 + t2744);\n  t2749 = (t2741 + t2745);\n  a4550 = (0.70710678118654757*(t2741 - t2745));\n  a4551 = (0.70710678118654757*(t2740 - t2744));\n  s1532 = (a4550 - a4551);\n  s1533 = (a4551 + a4550);\n  t2750 = (t2742 + t2747);\n  t2751 = (t2743 - t2746);\n  t2752 = (t2742 - t2747);\n  t2753 = (t2743 + t2746);\n  s1534 = ((0.38268343236508978*t2750) + (0.92387953251128674*t2751));\n  s1535 = ((0.38268343236508978*t2751) - (0.92387953251128674*t2750));\n  s1536 = ((0.92387953251128674*t2752) + (0.38268343236508978*t2753));\n  s1537 = ((0.38268343236508978*t2752) - (0.92387953251128674*t2753));\n  t2754 = (t2706 + t2736);\n  t2755 = (t2707 + t2737);\n  t2756 = (t2706 - t2736);\n  t2757 = (t2707 - t2737);\n  t2758 = (t2722 + t2748);\n  t2759 = (t2723 + t2749);\n  t2760 = (t2722 - t2748);\n  t2761 = (t2723 - t2749);\n  a4552 = ((8*blockIdx.x) + (131072*a4509) + (2*a4508));\n  Y[a4552] = (t2754 + t2758);\n  Y[(a4552 + 1)] = (t2755 + t2759);\n  Y[(a4552 + 16777216)] = (t2754 - t2758);\n  Y[(a4552 + 16777217)] = (t2755 - t2759);\n  Y[(a4552 + 8388608)] = (t2756 + t2761);\n  Y[(a4552 + 8388609)] = (t2757 - t2760);\n  Y[(a4552 + 25165824)] = (t2756 - t2761);\n  Y[(a4552 + 25165825)] = (t2757 + t2760);\n  t2762 = (t2710 + s1512);\n  t2763 = (t2711 + s1513);\n  t2764 = (t2710 - s1512);\n  t2765 = (t2711 - s1513);\n  t2766 = (s1492 + s1534);\n  t2767 = (s1493 + s1535);\n  t2768 = (s1492 - s1534);\n  t2769 = (s1493 - s1535);\n  Y[(a4552 + 2097152)] = (t2762 + t2766);\n  Y[(a4552 + 2097153)] = (t2763 + t2767);\n  Y[(a4552 + 18874368)] = (t2762 - t2766);\n  Y[(a4552 + 18874369)] = (t2763 - t2767);\n  Y[(a4552 + 10485760)] = (t2764 + t2769);\n  Y[(a4552 + 10485761)] = (t2765 - t2768);\n  Y[(a4552 + 27262976)] = (t2764 - t2769);\n  Y[(a4552 + 27262977)] = (t2765 + t2768);\n  t2770 = (t2708 + t2739);\n  t2771 = (t2709 - t2738);\n  t2772 = (t2708 - t2739);\n  t2773 = (t2709 + t2738);\n  t2774 = (s1490 + s1532);\n  t2775 = (s1491 - s1533);\n  t2776 = (s1490 - s1532);\n  t2777 = (s1491 + s1533);\n  Y[(a4552 + 4194304)] = (t2770 + t2774);\n  Y[(a4552 + 4194305)] = (t2771 + t2775);\n  Y[(a4552 + 20971520)] = (t2770 - t2774);\n  Y[(a4552 + 20971521)] = (t2771 - t2775);\n  Y[(a4552 + 12582912)] = (t2772 + t2777);\n  Y[(a4552 + 12582913)] = (t2773 - t2776);\n  Y[(a4552 + 29360128)] = (t2772 - t2777);\n  Y[(a4552 + 29360129)] = (t2773 + t2776);\n  t2778 = (t2712 + s1514);\n  t2779 = (t2713 - s1515);\n  t2780 = (t2712 - s1514);\n  t2781 = (t2713 + s1515);\n  t2782 = (s1494 - s1536);\n  t2783 = (s1495 + s1537);\n  t2784 = (s1494 + s1536);\n  t2785 = (s1495 - s1537);\n  Y[(a4552 + 6291456)] = (t2778 + t2782);\n  Y[(a4552 + 6291457)] = (t2779 + t2783);\n  Y[(a4552 + 23068672)] = (t2778 - t2782);\n  Y[(a4552 + 23068673)] = (t2779 - t2783);\n  Y[(a4552 + 14680064)] = (t2780 + t2785);\n  Y[(a4552 + 14680065)] = (t2781 - t2784);\n  Y[(a4552 + 31457280)] = (t2780 - t2785);\n  Y[(a4552 + 31457281)] = (t2781 + t2784);\n  __syncthreads();\n}"
        ]
    },
    "nonzero-cuda": {
        "/Users/gbolet/hecbench-roofline/src/nonzero-cuda/main.cu": [
            "__global__\nvoid write_indices(int64_t* inp, TensorDims<index_t> dims, int ndim, index_t nzero)\n{\n  auto index = threadIdx.x + blockIdx.x * blockDim.x;\n  if (index < nzero) {\n    index_t div = 1;\n    int64_t idx_flat = inp[index];\n    #pragma unroll\n    for (int dim = MAX_DIMS; dim >= 0; dim--) {\n      if (dim > ndim - 1) continue;\n      auto dim_size = dims.sizes[dim];\n      inp[index + dim * nzero] = (idx_flat / div) % dim_size;\n      div *= dim_size;\n    }\n  }\n}"
        ]
    },
    "fsm-cuda": {
        "/Users/gbolet/hecbench-roofline/src/fsm-cuda/kernels.h": [
            "__device__\nunsigned int LCG_random(unsigned int * seed) {\n  const unsigned int m = 2147483648;\n  const unsigned int a = 26757677;\n  const unsigned int c = 1;\n  *seed = (a * (*seed) + c) % m;\n  return *seed;\n}\n\n__device__\nvoid LCG_random_init(unsigned int * seed) {\n  const unsigned int m = 2147483648;\n  const unsigned int a = 26757677;\n  const unsigned int c = 1;\n  *seed = (a * (*seed) + c) % m;\n}\n\n__device__ __forceinline__\ndouble atomicMax(double *address, double val)\n{\n  unsigned long long ret = __double_as_longlong(*address);\n  while(val > __longlong_as_double(ret))\n  {\n    unsigned long long old = ret;\n    if((ret = atomicCAS((unsigned long long *)address, old, __double_as_longlong(val))) == old)\n      break;\n  }\n  return __longlong_as_double(ret);\n}\n\n__global__\nvoid FSMKernel(\n  const int length,\n  const unsigned short *__restrict__ data,\n  int *__restrict__ best,\n  unsigned int *__restrict__ rndstate,\n  unsigned char *__restrict__ bfsm,\n  unsigned char *__restrict__ same,\n  int *__restrict__ smax,\n  int *__restrict__ sbest,\n  int *__restrict__ oldmax)\n{\n  int i, d, pc, s, bit, id, misses, rnd;\n  unsigned long long myresult, current;\n  unsigned char *fsm, state[TABSIZE];\n  __shared__ unsigned char next[FSMSIZE * 2 * POPSIZE];\n\n  fsm = &next[threadIdx.x * (FSMSIZE * 2)];\n\n  if (threadIdx.x == 0) {\n    oldmax[blockIdx.x] = 0;\n    same[blockIdx.x] = 0;\n  }\n  __syncthreads();\n\n  id = threadIdx.x + blockIdx.x * blockDim.x;\n  rndstate[id] = SEED ^ id;\n  LCG_random_init(&rndstate[id]);\n\n  // initial population\n  for (i = 0; i < FSMSIZE * 2; i++) {\n    fsm[i] = LCG_random(rndstate+id) & (FSMSIZE - 1);\n  }\n\n  // run generations until cutoff times no improvement\n  do {\n    // reset miss counter and initial state\n    memset(state, 0, TABSIZE);\n    misses = 0;\n\n    // evaluate FSM\n#pragma unroll\n    for (i = 0; i < length; i++) {\n      d = (int)data[i];\n      pc = (d >> 1) & (TABSIZE - 1);\n      bit = d & 1;\n      s = (int)state[pc];\n      misses += bit ^ (s & 1);\n      state[pc] = fsm[s + s + bit];\n    }\n    for (; i < length; i++) {\n      d = (int)data[i];\n      pc = (d >> 1) & (TABSIZE - 1);\n      bit = d & 1;\n      s = (int)state[pc];\n      misses += bit ^ (s & 1);\n      state[pc] = fsm[s + s + bit];\n    }\n\n    // determine best FSM\n    if (threadIdx.x == 0) {\n      atomicAdd(&best[2], 1);  // increment generation count\n      smax[blockIdx.x] = 0;\n      sbest[blockIdx.x] = 0;\n    }\n    __syncthreads();\n    atomicMax(&smax[blockIdx.x], length - misses);\n    __syncthreads();\n    if (length - misses == smax[blockIdx.x]) atomicMax(&sbest[blockIdx.x], threadIdx.x);\n    __syncthreads();\n    bit = 0;\n    if (sbest[blockIdx.x] == threadIdx.x) {\n      // check if there was an improvement\n      same[blockIdx.x]++;\n      if (oldmax[blockIdx.x] < smax[blockIdx.x]) {\n        oldmax[blockIdx.x] = smax[blockIdx.x];\n        same[blockIdx.x] = 0;\n      }\n    } else {\n      // select 1/8 of threads for mutation (best FSM does crossover)\n      if ((LCG_random(rndstate+id) & 7) == 0) bit = 1;\n    }\n    __syncthreads();\n\n    if (bit) {\n      // mutate best FSM by flipping random bits with 1/4th probability\n      for (i = 0; i < FSMSIZE * 2; i++) {\n        rnd = LCG_random(rndstate+id) & LCG_random(rndstate+id);\n        fsm[i] = (next[i + sbest[blockIdx.x] * FSMSIZE * 2] ^ rnd) & (FSMSIZE - 1);\n      }\n    } else {\n      // crossover best FSM with random FSMs using 3/4 of bits from best FSM\n      for (i = 0; i < FSMSIZE * 2; i++) {\n        rnd = LCG_random(rndstate+id) & LCG_random(rndstate+id);\n        fsm[i] = (fsm[i] & rnd) | (next[i + sbest[blockIdx.x] * FSMSIZE * 2] & ~rnd);\n      }\n    }\n  } while (same[blockIdx.x] < CUTOFF);  // end of loop over generations\n\n  // record best result of this block\n  if (sbest[blockIdx.x] == threadIdx.x) {\n    id = blockIdx.x;\n    myresult = length - misses;\n    myresult = (myresult << 32) + id;\n    current = *((unsigned long long *)best);\n    while (myresult > current) {\n      atomicCAS((unsigned long long *)best, current, myresult);\n      current = *((unsigned long long *)best);\n    }\n    for (i = 0; i < FSMSIZE * 2; i++) {\n      bfsm[id * (FSMSIZE*2) + i] = fsm[i];\n    }\n  }\n}",
            "__global__\nvoid MaxKernel(\n  int *__restrict__ best, \n  const unsigned char *__restrict__ bfsm)\n{\n  // copy best FSM state assignment over\n  int id = best[0];\n  for (int i = 0; i < FSMSIZE * 2; i++) {\n    best[i + 3] = bfsm[id * (FSMSIZE*2) + i];\n  }\n}"
        ]
    },
    "vanGenuchten-cuda": {
        "/Users/gbolet/hecbench-roofline/src/vanGenuchten-cuda/main.cu": [
            "inline __host__ __device__ int4 make_int4(float4 a)\n{\n    return make_int4(int(a.x), int(a.y), int(a.z), int(a.w));\n}\n\ninline __host__ __device__ int4 abs(int4 v)\n{\n    return make_int4(abs(v.x), abs(v.y), abs(v.z), abs(v.w));\n}\n\n__global__ \nvoid vanGenuchten(\n  const double *__restrict__ Ksat,\n  const double *__restrict__ psi,\n        double *__restrict__ C,\n        double *__restrict__ theta,\n        double *__restrict__ K,\n  const int size)\n{\n  double Se, _theta, _psi, lambda, m, t;\n\n  int i = threadIdx.x + blockIdx.x * blockDim.x;\n  if (i < size)\n  {\n    lambda = n - 1.0;\n    m = lambda/n;\n\n    // Compute the volumetric moisture content [eqn 21]\n    _psi = psi[i] * 100.0;\n    if ( _psi < 0.0 )\n      _theta = (theta_S - theta_R) / pow(1.0 + pow((alpha*(-_psi)),n), m) + theta_R;\n    else\n      _theta = theta_S;\n\n    theta[i] = _theta;\n\n    // Compute the effective saturation [eqn 2]\n    Se = (_theta - theta_R)/(theta_S - theta_R);\n\n    // Compute the hydraulic conductivity [eqn 8]\n    t = 1.0 - pow(1.0-pow(Se,1.0/m), m);\n    K[i] = Ksat[i] * sqrt(Se) * t * t;\n\n  // Compute the specific moisture storage derivative of eqn (21).\n  // So we have to calculate C = d(theta)/dh. Then the unit is converted into [1/m].\n  if (_psi < 0.0)\n    C[i] = 100.0 * alpha * n * (1.0/n-1.0)*pow(alpha*abs(_psi), n-1.0)\n      * (theta_R-theta_S) * pow(pow(alpha*abs(_psi), n)+1.0, 1.0/n-2.0);\n  else\n    C[i] = 0.0;\n  }\n}"
        ]
    },
    "stencil3d-cuda": {
        "/Users/gbolet/hecbench-roofline/src/stencil3d-cuda/main.cu": [
            "#define Real float\n\n\n__global__ void stencil3d(\n    const Real*__restrict__ d_psi, \n          Real*__restrict__ d_npsi, \n    const Real*__restrict__ d_sigmaX, \n    const Real*__restrict__ d_sigmaY, \n    const Real*__restrict__ d_sigmaZ,\n    int nx, int ny, int nz)\n{\n\n  // z is the fastest varying direction\n  __shared__ Real sm_psi[4][BSIZE][BSIZE];\n\n  #define V0(y,z) sm_psi[pii][y][z]\n  #define V1(y,z) sm_psi[cii][y][z]\n  #define V2(y,z) sm_psi[nii][y][z]\n  \n  #define sigmaX(x,y,z,dir) d_sigmaX[ z + nz * ( y + ny * ( x + nx * dir ) ) ]\n  #define sigmaY(x,y,z,dir) d_sigmaY[ z + nz * ( y + ny * ( x + nx * dir ) ) ]\n  #define sigmaZ(x,y,z,dir) d_sigmaZ[ z + nz * ( y + ny * ( x + nx * dir ) ) ]\n  \n  #define psi(x,y,z) d_psi[ z + nz * ( (y) + ny * (x) ) ]\n  #define npsi(x,y,z) d_npsi[ z + nz * ( (y) + ny * (x) ) ]\n\n  const int tjj = threadIdx.y;\n  const int tkk = threadIdx.x;\n\n  // shift for each tile by updating device pointers\n  d_psi = &(psi(XTILE*blockIdx.x, (BSIZE-2)*blockIdx.y, (BSIZE-2)*blockIdx.z));\n  d_npsi = &(npsi(XTILE*blockIdx.x, (BSIZE-2)*blockIdx.y, (BSIZE-2)*blockIdx.z));\n\n  d_sigmaX = &(sigmaX(XTILE*blockIdx.x, (BSIZE-2)*blockIdx.y, (BSIZE-2)*blockIdx.z, 0));\n  d_sigmaY = &(sigmaY(XTILE*blockIdx.x, (BSIZE-2)*blockIdx.y, (BSIZE-2)*blockIdx.z, 0));\n  d_sigmaZ = &(sigmaZ(XTILE*blockIdx.x, (BSIZE-2)*blockIdx.y, (BSIZE-2)*blockIdx.z, 0));\n\n  int nLast_x=XTILE+1; int nLast_y=(BSIZE-1); int nLast_z=(BSIZE-1);\n  if (blockIdx.x == gridDim.x-1) nLast_x = nx-2 - XTILE * blockIdx.x + 1;\n  if (blockIdx.y == gridDim.y-1) nLast_y = ny-2 - (BSIZE-2) * blockIdx.y + 1;\n  if (blockIdx.z == gridDim.z-1) nLast_z = nz-2 - (BSIZE-2) * blockIdx.z + 1;\n\n  if(tjj>nLast_y || tkk>nLast_z) return;\n\n  // previous, current, next, and temp indices\n  int pii,cii,nii,tii;\n  pii=0; cii=1; nii=2;\n\n  sm_psi[cii][tjj][tkk] = psi(0,tjj,tkk);\n  sm_psi[nii][tjj][tkk] = psi(1,tjj,tkk);\n  Real xcharge,ycharge,zcharge,dV = 0;\n\n  __syncthreads();\n\n  //initial\n  if ((tkk>0) && (tkk<nLast_z) && (tjj>0) && (tjj<nLast_y))\n  {\n    Real xd=-V1(tjj,tkk) + V2(tjj,tkk);\n    Real yd=(-V1(-1 + tjj,tkk) + V1(1 + tjj,tkk) - V2(-1 + tjj,tkk) + V2(1 + tjj,tkk))/4.;\n    Real zd=(-V1(tjj,-1 + tkk) + V1(tjj,1 + tkk) - V2(tjj,-1 + tkk) + V2(tjj,1 + tkk))/4.;\n    dV -= sigmaX(1,tjj,tkk,0) * xd + sigmaX(1,tjj,tkk,1) * yd + sigmaX(1,tjj,tkk,2) * zd ; \n  }\n\n  tii=pii; pii=cii; cii=nii; nii=tii;\n\n  for(int ii=1;ii<nLast_x;ii++)\n  {\n    sm_psi[nii][tjj][tkk] = psi(ii+1,tjj,tkk);\n    __syncthreads();\n\n    // y face current\n    if ((tkk>0) && (tkk<nLast_z) && (tjj<nLast_y))\n    {\n      Real xd=(-V0(tjj,tkk) - V0(1 + tjj,tkk) + V2(tjj,tkk) + V2(1 + tjj,tkk))/4.;\n      Real yd=-V1(tjj,tkk) + V1(1 + tjj,tkk);\n      Real zd=(-V1(tjj,-1 + tkk) + V1(tjj,1 + tkk) - V1(1 + tjj,-1 + tkk) + V1(1 + tjj,1 + tkk))/4.;\n      ycharge = sigmaY(ii,tjj+1,tkk,0) * xd + sigmaY(ii,tjj+1,tkk,1) * yd + sigmaY(ii,tjj+1,tkk,2) * zd ; \n      dV += ycharge;\n      sm_psi[3][tjj][tkk]=ycharge;\n    }\n    __syncthreads();\n\n    if ((tkk>0) && (tkk<nLast_z) && (tjj>0) && (tjj<nLast_y))\n      dV -= sm_psi[3][tjj-1][tkk];  //bring from left\n\n    __syncthreads();\n\n    // z face current\n    if ((tkk<nLast_z) && (tjj>0) && (tjj<nLast_y))\n    {\n      Real xd=(-V0(tjj,tkk) - V0(tjj,1 + tkk) + V2(tjj,tkk) + V2(tjj,1 + tkk))/4.;\n      Real yd=(-V1(-1 + tjj,tkk) - V1(-1 + tjj,1 + tkk) + V1(1 + tjj,tkk) + V1(1 + tjj,1 + tkk))/4.;\n      Real zd=-V1(tjj,tkk) + V1(tjj,1 + tkk);\n      zcharge = sigmaZ(ii,tjj,tkk+1,0) * xd + sigmaZ(ii,tjj,tkk+1,1) * yd + sigmaZ(ii,tjj,tkk+1,2) * zd ; \n      dV += zcharge;\n      sm_psi[3][tjj][tkk]=zcharge;\n    }\n\n    __syncthreads();\n\n    if ((tkk>0) && (tkk<nLast_z) && (tjj>0) && (tjj<nLast_y))\n      dV -= sm_psi[3][tjj][tkk-1];\n    __syncthreads();\n\n    // x face current\n    if ((tkk>0) && (tkk<nLast_z) && (tjj>0) && (tjj<nLast_y))\n    {\n      Real xd=-V1(tjj,tkk) + V2(tjj,tkk);\n      Real yd=(-V1(-1 + tjj,tkk) + V1(1 + tjj,tkk) - V2(-1 + tjj,tkk) + V2(1 + tjj,tkk))/4.;\n      Real zd=(-V1(tjj,-1 + tkk) + V1(tjj,1 + tkk) - V2(tjj,-1 + tkk) + V2(tjj,1 + tkk))/4.;\n      xcharge = sigmaX(ii+1,tjj,tkk,0) * xd + sigmaX(ii+1,tjj,tkk,1) * yd + sigmaX(ii+1,tjj,tkk,2) * zd ; \n      dV += xcharge;\n      npsi(ii,tjj,tkk) = dV; //store dV\n      dV = -xcharge; //pass to the next cell in x-dir\n    }\n    __syncthreads();\n    tii=pii; pii=cii; cii=nii; nii=tii;\n  }\n}"
        ]
    },
    "seam-carving-cuda": {
        "/Users/gbolet/hecbench-roofline/src/seam-carving-cuda/kernels.h": [
            "inline __host__ __device__ int4 make_int4(float4 a)\n{\n    return make_int4(int(a.x), int(a.y), int(a.z), int(a.w));\n}\n\ninline __host__ __device__ int4 abs(int4 v)\n{\n    return make_int4(abs(v.x), abs(v.y), abs(v.z), abs(v.w));\n}\n\n__device__ pixel pixel_from_uchar4(uchar4 uc4){\n  pixel pix;\n  pix.r = (int)uc4.x;\n  pix.g = (int)uc4.y;\n  pix.b = (int)uc4.z;\n  return pix;\n}\n\n__global__ void compute_costs_kernel(\n    const uchar4 *__restrict__ d_pixels, \n    short *__restrict__ d_costs_left, \n    short *__restrict__ d_costs_up, \n    short *__restrict__ d_costs_right, \n    int w, int h, int current_w)\n{\n  //first row, first column and last column of shared memory are reserved for halo...\n  __shared__ pixel pix_cache[COSTS_BLOCKSIZE_Y][COSTS_BLOCKSIZE_X];\n  //...and the global index in the image is computed accordingly to this \n  int row = blockIdx.y*(COSTS_BLOCKSIZE_Y-1) + threadIdx.y -1 ; \n  int column = blockIdx.x*(COSTS_BLOCKSIZE_X-2) + threadIdx.x -1; \n  int ix = row*w + column;\n  int cache_row = threadIdx.y;\n  int cache_column = threadIdx.x;\n  short active = 0;\n\n  if(row >= 0 && row < h && column >= 0 && column < current_w){\n    active = 1;\n    pix_cache[cache_row][cache_column] = pixel_from_uchar4(d_pixels[ix]);\n  }\n  else{\n    pix_cache[cache_row][cache_column] = BORDER_PIXEL;\n  }\n\n  //wait until each thread has initialized its portion of shared memory\n  __syncthreads();\n\n  //all the threads that are NOT in halo positions can now compute costs, with fast access to shared memory\n  if(active && cache_row != 0 && cache_column != 0 && cache_column != COSTS_BLOCKSIZE_X-1){\n    int rdiff, gdiff, bdiff;\n    int p_r, p_g, p_b;\n    pixel pix1, pix2, pix3;\n\n    pix1 = pix_cache[cache_row][cache_column+1];\n    pix2 = pix_cache[cache_row][cache_column-1];\n    pix3 = pix_cache[cache_row-1][cache_column];\n\n    //compute partials\n    p_r = abs(pix1.r - pix2.r);\n    p_g = abs(pix1.g - pix2.g);\n    p_b = abs(pix1.b - pix2.b);\n\n    //compute left cost       \n    rdiff = p_r + abs(pix3.r - pix2.r);\n    gdiff = p_g + abs(pix3.g - pix2.g);\n    bdiff = p_b + abs(pix3.b - pix2.b);\n    d_costs_left[ix] = rdiff + gdiff + bdiff;\n\n    //compute up cost\n    d_costs_up[ix] = p_r + p_g + p_b;\n\n    //compute right cost\n    rdiff = p_r + abs(pix3.r - pix1.r);\n    gdiff = p_g + abs(pix3.g - pix1.g);\n    bdiff = p_b + abs(pix3.b - pix1.b);\n    d_costs_right[ix] = rdiff + gdiff + bdiff;         \n  }\n}",
            "inline __host__ __device__ int4 make_int4(float4 a)\n{\n    return make_int4(int(a.x), int(a.y), int(a.z), int(a.w));\n}\n\ninline __host__ __device__ int4 abs(int4 v)\n{\n    return make_int4(abs(v.x), abs(v.y), abs(v.z), abs(v.w));\n}\n\n__device__ pixel pixel_from_uchar4(uchar4 uc4){\n  pixel pix;\n  pix.r = (int)uc4.x;\n  pix.g = (int)uc4.y;\n  pix.b = (int)uc4.z;\n  return pix;\n}\n\n__global__ void compute_costs_full_kernel(\n    const uchar4* __restrict__ d_pixels, \n    short *__restrict__ d_costs_left, \n    short *__restrict__ d_costs_up, \n    short *__restrict__ d_costs_right, \n    int w, int h, int current_w)\n{\n  __shared__ pixel pix_cache[COSTS_BLOCKSIZE_Y+1][COSTS_BLOCKSIZE_X+2];\n  int row = blockIdx.y*COSTS_BLOCKSIZE_Y + threadIdx.y; \n  int column = blockIdx.x*COSTS_BLOCKSIZE_X + threadIdx.x; \n  int ix = row*w + column;\n  int cache_row = threadIdx.y + 1;\n  int cache_column = threadIdx.x + 1;\n  short active = 0;\n\n  if(row < h && column < current_w){\n    active = 1;\n    if(threadIdx.x == 0){\n      if(column == 0)\n        pix_cache[cache_row][0] = BORDER_PIXEL;\n      else\n        pix_cache[cache_row][0] = pixel_from_uchar4(d_pixels[ix-1]);\n    }\n    if(threadIdx.x == COSTS_BLOCKSIZE_X-1 || column == current_w-1){\n      if(column == current_w-1)\n        pix_cache[cache_row][cache_column+1] = BORDER_PIXEL;\n      else\n        pix_cache[cache_row][COSTS_BLOCKSIZE_X+1] = pixel_from_uchar4(d_pixels[ix+1]);\n    }\n    if(threadIdx.y == 0){\n      if(row == 0)\n        pix_cache[0][cache_column] = BORDER_PIXEL;  \n      else\n        pix_cache[0][cache_column] = pixel_from_uchar4(d_pixels[ix-w]);          \n    } \n    pix_cache[cache_row][cache_column] = pixel_from_uchar4(d_pixels[ix]);  \n  }\n\n  __syncthreads();\n\n  if(active){\n    int rdiff, gdiff, bdiff;\n    int p_r, p_g, p_b;\n    pixel pix1, pix2, pix3;\n\n    pix1 = pix_cache[cache_row][cache_column+1];\n    pix2 = pix_cache[cache_row][cache_column-1];\n    pix3 = pix_cache[cache_row-1][cache_column];\n\n    //compute partials\n    p_r = abs(pix1.r - pix2.r);\n    p_g = abs(pix1.g - pix2.g);\n    p_b = abs(pix1.b - pix2.b);\n\n    //compute left cost       \n    rdiff = p_r + abs(pix3.r - pix2.r);\n    gdiff = p_g + abs(pix3.g - pix2.g);\n    bdiff = p_b + abs(pix3.b - pix2.b);\n    d_costs_left[ix] = rdiff + gdiff + bdiff;\n\n    //compute up cost\n    d_costs_up[ix] = p_r + p_g + p_b;\n\n    //compute right cost\n    rdiff = p_r + abs(pix3.r - pix1.r);\n    gdiff = p_g + abs(pix3.g - pix1.g);\n    bdiff = p_b + abs(pix3.b - pix1.b);\n    d_costs_right[ix] = rdiff + gdiff + bdiff; \n  }\n}",
            "inline __host__ __device__ uint4 make_uint4(int4 a)\n{\n    return make_uint4(uint(a.x), uint(a.y), uint(a.z), uint(a.w));\n}\n\ninline __host__ __device__ uint4 min(uint4 a, uint4 b)\n{\n    return make_uint4(min(a.x,b.x), min(a.y,b.y), min(a.z,b.z), min(a.w,b.w));\n}\n\n__device__ void pointer_swap(void **p1, void **p2){\n  void *tmp;\n  tmp = *p1;\n  *p1 = *p2;\n  *p2 = tmp; \n}\n\n__global__ void compute_M_kernel_step1(\n    const short *__restrict__ d_costs_left, \n    const short *__restrict__ d_costs_up, \n    const short *__restrict__ d_costs_right, \n    int* __restrict__ d_M, \n    int w, int h, int current_w, int base_row)\n{\n  __shared__ int cache[2*COMPUTE_M_BLOCKSIZE_X];\n  int *m_cache = cache;\n  int *m_cache_swap = &(cache[COMPUTE_M_BLOCKSIZE_X]);\n  int column = blockIdx.x*COMPUTE_M_BLOCKSIZE_X + threadIdx.x; \n  int ix = base_row*w + column;\n  int cache_column = threadIdx.x; \n  short is_first;\n  short is_last;\n  int right, up, left;\n\n  is_first = blockIdx.x == 0;\n  is_last = blockIdx.x == gridDim.x-1;\n\n  if(column < current_w){\n    if(base_row == 0){\n      left = min(d_costs_left[ix], min(d_costs_up[ix], d_costs_right[ix]));\n      m_cache[cache_column] = left;\n      d_M[ix] = left; \n    }\n    else{\n      m_cache[cache_column] = d_M[ix];    \n    }\n  }\n\n  __syncthreads();\n\n  int max_row = base_row + COMPUTE_M_BLOCKSIZE_X/2;\n  for(int row = base_row+1, inc = 1; row < max_row && row < h; row++, inc++){\n    ix = ix + w;\n    if(column < current_w && (is_first || inc <= threadIdx.x) && (is_last || threadIdx.x < COMPUTE_M_BLOCKSIZE_X - inc)){\n\n      //with left\n      if(column > 0)\n        left = m_cache[cache_column - 1] + d_costs_left[ix]; \n      else \n        left = INT_MAX;\n      //with up\n      up = m_cache[cache_column] + d_costs_up[ix];\n      //with right\n      if(column < current_w-1)\n        right = m_cache[cache_column + 1] + d_costs_right[ix];\n      else\n        right = INT_MAX;\n\n      left = min(left, min(up, right));           \n      d_M[ix] = left;\n      //swap read/write shared memory\n      pointer_swap((void**)&m_cache, (void**)&m_cache_swap);\n      m_cache[cache_column] = left;\n    }   \n    //wait until every thread has written shared memory\n    __syncthreads();                \n  }\n}",
            "inline __host__ __device__ uint4 make_uint4(int4 a)\n{\n    return make_uint4(uint(a.x), uint(a.y), uint(a.z), uint(a.w));\n}\n\ninline __host__ __device__ uint4 min(uint4 a, uint4 b)\n{\n    return make_uint4(min(a.x,b.x), min(a.y,b.y), min(a.z,b.z), min(a.w,b.w));\n}\n\n__global__ void compute_M_kernel_step2(\n    const short *__restrict__ d_costs_left, \n    const short *__restrict__ d_costs_up, \n    const short *__restrict__ d_costs_right, \n    int* __restrict__ d_M, \n    int w, int h, int current_w, int base_row)\n{\n  int column = blockIdx.x*COMPUTE_M_BLOCKSIZE_X + threadIdx.x + COMPUTE_M_BLOCKSIZE_X/2; \n  int right, up, left;\n\n  int ix; \n  int prev_ix = base_row*w + column;\n  int max_row = base_row + COMPUTE_M_BLOCKSIZE_X/2;\n  for(int row = base_row+1, inc = 1; row < max_row && row < h; row++, inc++){\n    ix = prev_ix + w;\n    if(column < current_w && (COMPUTE_M_BLOCKSIZE_X/2 - inc <= threadIdx.x) && (threadIdx.x < COMPUTE_M_BLOCKSIZE_X/2 + inc)){\n      //with left\n      left = d_M[prev_ix - 1] + d_costs_left[ix]; \n      //with up\n      up = d_M[prev_ix] + d_costs_up[ix];\n      //with right\n      if(column < current_w-1)\n        right = d_M[prev_ix + 1] + d_costs_right[ix];\n      else\n        right = INT_MAX;\n\n      left = min(left, min(up, right));               \n      d_M[ix] = left;\n    }\n    prev_ix = ix;\n    __syncthreads();\n  }\n}",
            "inline __host__ __device__ uint4 make_uint4(int4 a)\n{\n    return make_uint4(uint(a.x), uint(a.y), uint(a.z), uint(a.w));\n}\n\ninline __host__ __device__ uint4 min(uint4 a, uint4 b)\n{\n    return make_uint4(min(a.x,b.x), min(a.y,b.y), min(a.z,b.z), min(a.w,b.w));\n}\n\n__device__ void pointer_swap(void **p1, void **p2){\n  void *tmp;\n  tmp = *p1;\n  *p1 = *p2;\n  *p2 = tmp; \n}\n\n__global__ void compute_M_kernel_small(\n    const short *__restrict__ d_costs_left, \n    const short *__restrict__ d_costs_up, \n    const short *__restrict__ d_costs_right, \n    int* __restrict__ d_M, \n    int w, int h, int current_w)\n{\n  extern __shared__ int cache[];\n  int *m_cache = cache;\n  int *m_cache_swap = &(cache[current_w]);\n  int column = threadIdx.x;\n  int ix = column;\n  int left, up, right;\n\n  //first row\n  left = min(d_costs_left[ix], min(d_costs_up[ix], d_costs_right[ix]));\n  d_M[ix] = left; \n  m_cache[ix] = left;\n\n  __syncthreads(); \n\n  //other rows\n  for(int row = 1; row < h; row++){\n    if(column < current_w){\n      ix = ix + w;//ix = row*w + column;   \n\n      //with left\n      if(column > 0)\n        left = m_cache[column - 1] + d_costs_left[ix]; \n      else\n        left = INT_MAX;\n      //with up\n      up = m_cache[column] + d_costs_up[ix];\n      //with right\n      if(column < current_w-1)\n        right = m_cache[column + 1] + d_costs_right[ix];\n      else\n        right = INT_MAX;\n\n      left = min(left, min(up, right));            \n      d_M[ix] = left;\n      //swap read/write shared memory\n      pointer_swap((void**)&m_cache, (void**)&m_cache_swap); \n      m_cache[column] = left;\n    }\n    __syncthreads();    \n  }     \n}",
            "inline __host__ __device__ uint4 make_uint4(int4 a)\n{\n    return make_uint4(uint(a.x), uint(a.y), uint(a.z), uint(a.w));\n}\n\ninline __host__ __device__ uint4 min(uint4 a, uint4 b)\n{\n    return make_uint4(min(a.x,b.x), min(a.y,b.y), min(a.z,b.z), min(a.w,b.w));\n}\n\n__device__ void pointer_swap(void **p1, void **p2){\n  void *tmp;\n  tmp = *p1;\n  *p1 = *p2;\n  *p2 = tmp; \n}\n\n__global__ void compute_M_kernel_single(\n    const short *__restrict__ d_costs_left, \n    const short *__restrict__ d_costs_up, \n    const short *__restrict__ d_costs_right, \n    int* __restrict__ d_M, \n    int w, int h, int current_w, int n_elem)\n{\n  extern __shared__ int cache[];\n  int *m_cache = cache;\n  int *m_cache_swap = &(cache[current_w]);\n  int tid = threadIdx.x;\n  int column; \n  int ix;\n  int left, up, right;\n\n  //first row\n  for(int i = 0; i < n_elem; i++){\n    column = tid + i*blockDim.x;\n    if(column < current_w){\n      left = min(d_costs_left[column], min(d_costs_up[column], d_costs_right[column]));\n      d_M[column] = left; \n      m_cache[column] = left;\n    }\n  }\n\n  __syncthreads(); \n\n  //other rows\n  for(int row = 1; row < h; row++){\n    for(int i = 0; i < n_elem; i++){\n      column = tid + i*blockDim.x;\n      if(column < current_w){\n        ix = row*w + column;\n\n        //with left\n        if(column > 0){\n          left = m_cache[column - 1] + d_costs_left[ix]; \n        }\n        else\n          left = INT_MAX;\n        //with up\n        up = m_cache[column] + d_costs_up[ix];\n        //with right\n        if(column < current_w-1){\n          right = m_cache[column + 1] + d_costs_right[ix];\n        }\n        else\n          right = INT_MAX;\n\n        left = min(left, min(up, right));\n        d_M[ix] = left;\n        m_cache_swap[column] = left;\n      }          \n    }    \n    //swap read/write shared memory\n    pointer_swap((void**)&m_cache, (void**)&m_cache_swap);\n    __syncthreads();\n  }        \n}",
            "inline __host__ __device__ uint4 make_uint4(int4 a)\n{\n    return make_uint4(uint(a.x), uint(a.y), uint(a.z), uint(a.w));\n}\n\ninline __host__ __device__ uint4 min(uint4 a, uint4 b)\n{\n    return make_uint4(min(a.x,b.x), min(a.y,b.y), min(a.z,b.z), min(a.w,b.w));\n}\n\n__global__ void compute_M_kernel_iterate0(\n    const short *__restrict__ d_costs_left, \n    const short *__restrict__ d_costs_up, \n    const short *__restrict__ d_costs_right, \n    int* __restrict__ d_M, \n    int w, int current_w)\n{\n  int column = blockIdx.x*COMPUTE_M_BLOCKSIZE_X + threadIdx.x; \n\n  if(column < current_w){\n    d_M[column] = min(d_costs_left[column], min(d_costs_up[column], d_costs_right[column]));\n  }\n}",
            "inline __host__ __device__ uint4 make_uint4(int4 a)\n{\n    return make_uint4(uint(a.x), uint(a.y), uint(a.z), uint(a.w));\n}\n\ninline __host__ __device__ uint4 min(uint4 a, uint4 b)\n{\n    return make_uint4(min(a.x,b.x), min(a.y,b.y), min(a.z,b.z), min(a.w,b.w));\n}\n\n__global__ void compute_M_kernel_iterate1(\n    const short *__restrict__ d_costs_left, \n    const short *__restrict__ d_costs_up, \n    const short *__restrict__ d_costs_right, \n    int* __restrict__ d_M, \n    int w, int current_w, int row)\n{\n  int column = blockIdx.x*COMPUTE_M_BLOCKSIZE_X + threadIdx.x; \n  int ix = row*w + column;\n  int prev_ix = ix - w;\n  int left, up, right;\n\n  if(column < current_w){\n    //with left\n    if(column > 0)\n      left = d_M[prev_ix - 1] + d_costs_left[ix]; \n    else\n      left = INT_MAX;           \n    //with up\n    up = d_M[prev_ix] + d_costs_up[ix];        \n    //with right\n    if(column < current_w-1)\n      right = d_M[prev_ix + 1] + d_costs_right[ix];\n    else\n      right = INT_MAX;\n\n    d_M[ix] = min(left, min(up, right));  \n  } \n}",
            "__global__ void min_reduce(\n    const int* __restrict__ d_values,\n    int* __restrict__ d_indices,\n    int size)\n{\n  __shared__ int val_cache[REDUCE_BLOCKSIZE_X];\n  __shared__ int ix_cache[REDUCE_BLOCKSIZE_X];\n  int tid = threadIdx.x;\n  int column = blockIdx.x*REDUCE_BLOCKSIZE_X + tid;\n  int grid_size = gridDim.x*REDUCE_BLOCKSIZE_X;\n  int min_v = INT_MAX;\n  int min_i = 0;\n  int new_i, new_v;\n\n  for(int i = 0; i < REDUCE_ELEMENTS_PER_THREAD; i++){\n    if(column < size){\n      new_i = d_indices[column];\n      new_v  = d_values[new_i];\n      if(new_v < min_v){\n        min_i = new_i;\n        min_v = new_v;\n      }\n    } \n    column = column + grid_size;         \n  }\n  val_cache[tid] = min_v;\n  ix_cache[tid] = min_i;\n\n  __syncthreads();\n\n  for(int i = REDUCE_BLOCKSIZE_X/2; i > 0; i = i/2){\n    if(tid < i){\n      if(val_cache[tid + i] < val_cache[tid] || (val_cache[tid + i] == val_cache[tid] && ix_cache[tid + i] < ix_cache[tid])){\n        val_cache[tid] = val_cache[tid + i];\n        ix_cache[tid] = ix_cache[tid + i];\n      }\n    }\n    __syncthreads();\n  }\n\n  if(tid == 0){\n    d_indices[blockIdx.x] = ix_cache[0];  \n  }  \n}",
            "__global__ void find_seam_kernel(\n    const int *__restrict__ d_M,\n    const int *__restrict__ d_indices,\n    int *__restrict__ d_seam,\n    int w, int h, int current_w)\n{\n  int base_row, mid;\n  int min_index = d_indices[0];\n\n  d_seam[h-1] = min_index; \n  for(int row = h-2; row >= 0; row--){\n    base_row = row*w;\n    mid = min_index;\n    if(mid != 0){\n      if(d_M[base_row + mid - 1] < d_M[base_row + min_index])\n        min_index = mid - 1;\n    }\n    if(mid != current_w){\n      if(d_M[base_row + mid + 1] < d_M[base_row + min_index])\n        min_index = mid + 1;\n    }\n    d_seam[row] = min_index;\n  }\n}",
            "__global__ void remove_seam_kernel(\n    const uchar4 *__restrict__ d_pixels, \n          uchar4 *__restrict__ d_pixels_swap, \n    const int *__restrict__ d_seam, \n    int w, int h, int current_w)\n{\n  int row = blockIdx.y*REMOVE_BLOCKSIZE_Y + threadIdx.y;\n  int column = blockIdx.x*REMOVE_BLOCKSIZE_X + threadIdx.x;\n\n  if(row < h && column < current_w-1){\n    int seam_c = d_seam[row];\n    int ix = row*w + column;\n    d_pixels_swap[ix] = (column >= seam_c) ? d_pixels[ix + 1] : d_pixels[ix];\n  }\n}",
            "inline __host__ __device__ int4 make_int4(float4 a)\n{\n    return make_int4(int(a.x), int(a.y), int(a.z), int(a.w));\n}\n\ninline __host__ __device__ int4 abs(int4 v)\n{\n    return make_int4(abs(v.x), abs(v.y), abs(v.z), abs(v.w));\n}\n\n__device__ pixel pixel_from_uchar4(uchar4 uc4){\n  pixel pix;\n  pix.r = (int)uc4.x;\n  pix.g = (int)uc4.y;\n  pix.b = (int)uc4.z;\n  return pix;\n}\n\n__global__ void update_costs_kernel(\n    const uchar4 *__restrict__ d_pixels, \n    const short *__restrict__ d_costs_left, \n    const short *__restrict__ d_costs_up, \n    const short *__restrict__ d_costs_right, \n    short *__restrict__ d_costs_swap_left, \n    short *__restrict__ d_costs_swap_up, \n    short *__restrict__ d_costs_swap_right, \n    const int *__restrict__ d_seam, \n    int w, int h, int current_w)\n{\n  int row = blockIdx.y*UPDATE_BLOCKSIZE_Y + threadIdx.y;\n  int column = blockIdx.x*UPDATE_BLOCKSIZE_X + threadIdx.x;\n\n  if(row < h && column < current_w-1){\n    int seam_c = d_seam[row];\n    int ix = row*w + column;\n    if(column >= seam_c-2 && column <= seam_c+1){\n      //update costs near removed seam\n      pixel pix1, pix2, pix3;\n      int p_r, p_g, p_b;\n      int rdiff, gdiff, bdiff;          \n\n      if(column == current_w-2) \n        pix1 = BORDER_PIXEL;\n      else\n        pix1 = pixel_from_uchar4(d_pixels[ix + 1]);\n      if(column == 0)\n        pix2 = BORDER_PIXEL;\n      else\n        pix2 = pixel_from_uchar4(d_pixels[ix - 1]);\n      if(row == 0)\n        pix3 = BORDER_PIXEL;\n      else\n        pix3 = pixel_from_uchar4(d_pixels[ix - w]);\n\n      //compute partials\n      p_r = abs(pix1.r - pix2.r);\n      p_g = abs(pix1.g - pix2.g);\n      p_b = abs(pix1.b - pix2.b);\n\n      //compute left cost       \n      rdiff = p_r + abs(pix3.r - pix2.r);\n      gdiff = p_g + abs(pix3.g - pix2.g);\n      bdiff = p_b + abs(pix3.b - pix2.b);\n      d_costs_swap_left[ix] = rdiff + gdiff + bdiff;\n\n      //compute up cost\n      d_costs_swap_up[ix] = p_r + p_g + p_b;\n\n      //compute right cost\n      rdiff = p_r + abs(pix3.r - pix1.r);\n      gdiff = p_g + abs(pix3.g - pix1.g);\n      bdiff = p_b + abs(pix3.b - pix1.b);\n      d_costs_swap_right[ix] = rdiff + gdiff + bdiff;             \n    }\n    else if(column > seam_c+1){\n      //shift costs to the left\n      d_costs_swap_left[ix] = d_costs_left[ix + 1];\n      d_costs_swap_up[ix] = d_costs_up[ix + 1];\n      d_costs_swap_right[ix] = d_costs_right[ix + 1];\n    }\n    else{\n      //copy remaining costs\n      d_costs_swap_left[ix] = d_costs_left[ix];\n      d_costs_swap_up[ix] = d_costs_up[ix];\n      d_costs_swap_right[ix] = d_costs_right[ix];\n    }\n  }\n}",
            "inline __host__ __device__ int4 make_int4(float4 a)\n{\n    return make_int4(int(a.x), int(a.y), int(a.z), int(a.w));\n}\n\ninline __host__ __device__ int4 abs(int4 v)\n{\n    return make_int4(abs(v.x), abs(v.y), abs(v.z), abs(v.w));\n}\n\n__device__ pixel pixel_from_uchar4(uchar4 uc4){\n  pixel pix;\n  pix.r = (int)uc4.x;\n  pix.g = (int)uc4.y;\n  pix.b = (int)uc4.z;\n  return pix;\n}\n\n__global__ void approx_setup_kernel(\n    const uchar4 *__restrict__ d_pixels, \n    int *__restrict__ d_index_map, \n    int *__restrict__ d_offset_map, \n    int *__restrict__ d_M, int w, int h, int current_w)\n{\n  __shared__ pixel pix_cache[APPROX_SETUP_BLOCKSIZE_Y][APPROX_SETUP_BLOCKSIZE_X];\n  __shared__ short left_cache[APPROX_SETUP_BLOCKSIZE_Y][APPROX_SETUP_BLOCKSIZE_X];\n  __shared__ short up_cache[APPROX_SETUP_BLOCKSIZE_Y][APPROX_SETUP_BLOCKSIZE_X];\n  __shared__ short right_cache[APPROX_SETUP_BLOCKSIZE_Y][APPROX_SETUP_BLOCKSIZE_X];\n  int row = blockIdx.y*(APPROX_SETUP_BLOCKSIZE_Y-1) + threadIdx.y -1 ; \n  int column = blockIdx.x*(APPROX_SETUP_BLOCKSIZE_X-4) + threadIdx.x -2; //WE NEED MORE HORIZONTAL HALO...\n  int ix = row*w + column;\n  int cache_row = threadIdx.y;\n  int cache_column = threadIdx.x;\n  short active = 0;\n\n  if(row >= 0 && row < h && column >= 0 && column < current_w){\n    active = 1;\n    pix_cache[cache_row][cache_column] = pixel_from_uchar4(d_pixels[ix]);\n  }\n  else{\n    pix_cache[cache_row][cache_column] = BORDER_PIXEL;\n  }\n\n  //wait until each thread has initialized its portion of shared memory\n  __syncthreads();\n\n  if(active && cache_row > 0){\n    int rdiff, gdiff, bdiff;\n    int p_r, p_g, p_b;\n    pixel pix1, pix2, pix3;\n\n    if(cache_column < APPROX_SETUP_BLOCKSIZE_X-1){\n      pix1 = pix_cache[cache_row][cache_column+1];   //...OR ELSE WE CANNOT CALCULATE LEFT COST FOR THE LAST THREAD IN THE BLOCK (pix1 dependance)\n    }\n\n    if(cache_column > 0){\n      pix2 = pix_cache[cache_row][cache_column-1];   //SAME THING WITH RIGHT COST FOR THE FIRST THREAD (pix2 dependance)\n    }\n\n    pix3 = pix_cache[cache_row-1][cache_column];\n\n    //compute partials\n    p_r = abs(pix1.r - pix2.r);\n    p_g = abs(pix1.g - pix2.g);\n    p_b = abs(pix1.b - pix2.b);\n\n    //compute left cost       \n    rdiff = p_r + abs(pix3.r - pix2.r);\n    gdiff = p_g + abs(pix3.g - pix2.g);\n    bdiff = p_b + abs(pix3.b - pix2.b);\n    left_cache[cache_row][cache_column] = rdiff + gdiff + bdiff;\n\n    //compute up cost\n    up_cache[cache_row][cache_column] = p_r + p_g + p_b;\n\n    //compute right cost\n    rdiff = p_r + abs(pix3.r - pix1.r);\n    gdiff = p_g + abs(pix3.g - pix1.g);\n    bdiff = p_b + abs(pix3.b - pix1.b);\n    right_cache[cache_row][cache_column] = rdiff + gdiff + bdiff;             \n  }\n\n  __syncthreads();\n\n  if(active && row < h-1 && cache_column > 1 && cache_column < APPROX_SETUP_BLOCKSIZE_X-2 && cache_row != APPROX_SETUP_BLOCKSIZE_Y-1){\n    int min_cost = INT_MAX;\n    int map_ix;\n    int cost;\n\n    if(column > 0){\n      min_cost = right_cache[cache_row+1][cache_column-1];\n      map_ix = ix + w - 1;\n    }\n\n    cost = up_cache[cache_row+1][cache_column];\n    if(cost < min_cost){\n      min_cost = cost;\n      map_ix = ix + w;\n    }\n\n    if(column < current_w-1){\n      cost = left_cache[cache_row+1][cache_column+1];\n      if(cost < min_cost){\n        min_cost = cost;\n        map_ix = ix + w + 1;\n      }\n    }\n\n    d_index_map[ix] = map_ix;\n    d_offset_map[ix] = map_ix;\n    d_M[ix] = min_cost;           \n  }\n}",
            "__global__ void approx_M_kernel(\n    int *__restrict__ d_offset_map,\n    int *__restrict__ d_M,\n    int w, int h, int current_w, int step)\n{\n  int row = blockIdx.y*2*step;\n  int next_row = row + step;\n  int column = blockIdx.x*APPROX_M_BLOCKSIZE_X + threadIdx.x;\n  int ix = row*w + column;\n\n  if(next_row < h-1 && column < current_w){\n    int offset = d_offset_map[ix];\n    d_M[ix] += d_M[offset];\n    d_offset_map[ix] = d_offset_map[offset];\n  }\n}",
            "__global__ void approx_seam_kernel(\n    const int *__restrict__ d_index_map, \n    const int *__restrict__ d_indices, \n    int *__restrict__ d_seam, \n    int w, int h)\n{\n  int ix;\n  ix = d_indices[0]; //min index\n  for(int i = 0; i < h; i++){\n    d_seam[i] = ix - i*w;\n    ix = d_index_map[ix];\n  }\n}"
        ]
    },
    "allreduce-cuda": {
        "/Users/gbolet/hecbench-roofline/src/allreduce-cuda/collectives.cu": [
            "__global__ void kernel_add(const float* x, const float* y, const int N, float* out) {\n  for (int i = blockIdx.x * blockDim.x + threadIdx.x; i < N; i += blockDim.x * gridDim.x) {\n      out[i] = x[i] + y[i];\n    }\n}"
        ]
    },
    "henry-cuda": {
        "/Users/gbolet/hecbench-roofline/src/henry-cuda/main.cu": [
            "inline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\ninline __host__ __device__ float4 sqrtf(float4 v)\n{\n    return make_float4(sqrtf(v.x), sqrtf(v.y), sqrtf(v.z), sqrtf(v.w));\n}\n\n__device__\ndouble LCG_random_double(uint64_t * seed)\n{\n  const uint64_t m = 9223372036854775808ULL; // 2^63\n  const uint64_t a = 2806196910506780709ULL;\n  const uint64_t c = 1ULL;\n  *seed = (a * (*seed) + c) % m;\n  return (double) (*seed) / (double) m;\n}\n\n__device__\nstruct pair_r compute(\n  float *genekj,\n  float *geneij,\n  const char *sample,\n  int wid,int k,int i,int D,\n  const float *gene)\n{\n  int j;\n  float sx = 0.f, sxx = 0.f, sy = 0.f, sxy = 0.f, syy = 0.f;\n  float sx_n = 0.f, sxx_n = 0.f, sy_n = 0.f, sxy_n = 0.f, syy_n = 0.f;\n\n  struct pair_r rval = {0.f, 0.f};\n\n  for (j = 0; j < D; j++) {\n    genekj[j]=gene[k*(D+1)+j];\n    if(sample[j]=='1')\n      sx += genekj[j];\n    else\n      sx_n += genekj[j];\n  }\n\n  sx /= wid;\n  sx_n /= (D-wid);\n\n  for (j = 0; j < D; j++) {\n    if(sample[j]=='1')\n      sxx += (sx-genekj[j]) * (sx-genekj[j]);\n    else\n      sxx_n += (sx_n-genekj[j]) * (sx_n-genekj[j]);\n  }\n\n  sxx = sqrtf(sxx);\n  sxx_n = sqrtf(sxx_n);\n\n  for (j = 0; j < D; j++) {\n    geneij[j]=gene[i*(D+1)+j];\n    if(sample[j]=='1')\n      sy += geneij[j];\n    else\n      sy_n += geneij[j];\n  }\n\n  sy /= wid; \n  sy_n /= (D-wid); \n\n  for (j = 0; j < D; j++)\n  {\n    if(sample[j]=='1') {\n      sxy += (sx - genekj[j]) * (sy - geneij[j]);\n      syy += (sy - geneij[j]) * (sy - geneij[j]);\n    }\n    else {\n      sxy_n += (sx_n - genekj[j]) * (sy_n - geneij[j]);\n      syy_n += (sy_n - geneij[j]) * (sy_n - geneij[j]);\n    }\n  }\n\n  syy = sqrtf(syy);\n  syy_n = sqrtf(syy_n);\n  rval.r = fabsf(sxy/(sxx * syy));\n  rval.n_r = fabsf(sxy_n/(sxx_n * syy_n));\n\n  return rval;\n}\n\n__global__ void insertions(\n    double *__restrict__ boltzmannFactors, \n    const StructureAtom * __restrict__ structureAtoms, \n    int natoms, double L) \n{\n  // boltzmannFactors : pointer array in which to store computed Boltzmann factors\n  // structureAtoms : pointer array storing atoms in unit cell of crystal structure\n  // natoms : number of atoms in crystal structure\n  // L : box length\n  int id = threadIdx.x + blockIdx.x * NUMTHREADS;\n\n  // random seed for each thread\n  uint64_t seed = id;\n\n  // Generate random position inside the cubic unit cell of the structure\n  double x = L * LCG_random_double(&seed);\n  double y = L * LCG_random_double(&seed);\n  double z = L * LCG_random_double(&seed);\n\n  // Compute Boltzmann factor, store in boltzmannFactors\n  boltzmannFactors[id] = compute(x, y, z, structureAtoms, natoms, L);\n}"
        ]
    },
    "wmma-cuda": {
        "/Users/gbolet/hecbench-roofline/src/wmma-cuda/main.cu": [
            "__global__ void gemm(const uint32_t m, const uint32_t n, const uint32_t k,\n                     fp16 const *__restrict__ a,\n                     fp16 const *__restrict__ b,\n                     fp32 const *c,\n                     fp32 *d, const uint32_t lda, const uint32_t ldb,\n                     const uint32_t ldc, const uint32_t ldd,\n                     const fp32 alpha, const fp32 beta) {\n  // Create frags\n  auto fragA = wmma::fragment<wmma::matrix_a, WMMA_M, WMMA_N, WMMA_K, fp16,\n                              wmma::row_major>();\n  auto fragB = wmma::fragment<wmma::matrix_b, WMMA_M, WMMA_N, WMMA_K, fp16,\n                              wmma::col_major>();\n  auto fragC = wmma::fragment<wmma::accumulator, WMMA_M, WMMA_N, WMMA_K, fp32>();\n  auto fragAcc = wmma::fragment<wmma::accumulator, WMMA_M, WMMA_N, WMMA_K, fp32>();\n\n  wmma::fill_fragment(fragAcc, 0.0f);\n\n  // Tile using a 2D grid\n  auto majorWarp = (blockIdx.x * blockDim.x + threadIdx.x) / WAVE_SIZE;\n  auto minorWarp = (blockIdx.y * blockDim.y + threadIdx.y);\n\n  // Target C block\n  auto cRow = majorWarp * WMMA_M;\n  auto cCol = minorWarp * WMMA_N;\n\n  // Bounds check\n  if (cRow < m && cCol < n) {\n    for (int i = 0; i < k; i += WMMA_K) {\n      // Load the inputs\n      wmma::load_matrix_sync(fragA, a + (cRow * lda + i), lda);\n      wmma::load_matrix_sync(fragB, b + (cCol * ldb + i), ldb);\n\n      // Matrix multiply - accumulate using MFMA units\n      wmma::mma_sync(fragAcc, fragA, fragB, fragAcc);\n    }\n\n    // Fetch C matrix\n    wmma::load_matrix_sync(fragC, c + (cRow * ldc + cCol), ldc,\n                           wmma::mem_row_major);\n\n    // D = alpha * A x B + beta * C\n    for (int i = 0; i < fragC.num_elements; ++i) {\n      fragC.x[i] = alpha * fragAcc.x[i] + beta * fragC.x[i];\n    }\n\n    // Store to D\n    wmma::store_matrix_sync(d + (cRow * ldd + cCol), fragC, ldd,\n                            wmma::mem_row_major);\n  }\n}"
        ]
    },
    "qem-cuda": {
        "/Users/gbolet/hecbench-roofline/src/qem-cuda/gpu_solver.cu": [
            "__global__ void QRdel(int n, const float *A, const float *B, const float *C,\n                      const float *D, float *__restrict__ b,\n                      float *__restrict__ c, float *__restrict__ d,\n                      float *__restrict__ Q, float *__restrict__ R,\n                      float *__restrict__ Qint, float *__restrict__ Rint,\n                      float *__restrict__ del) {\n  int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < n) {\n\n    b[i] = 0.75f * (B[i] / A[i]);\n    c[i] = 0.50f * (C[i] / A[i]);\n    d[i] = 0.25f * (D[i] / A[i]);\n\n    Q[i] = (c[i] / 3.f) - ((b[i] * b[i]) / 9.f);\n    R[i] = (b[i] * c[i]) / 6.f - (b[i] * b[i] * b[i]) / 27.f - 0.5f * d[i];\n\n    // round Q and R to get around problems caused by floating point precision\n    Q[i] = roundf(Q[i] * 1E5f) / 1E5f;\n    R[i] = roundf(R[i] * 1E5f) / 1E5f;\n\n    Qint[i] = (Q[i] * Q[i] * Q[i]);\n    Rint[i] = (R[i] * R[i]);\n\n    del[i] = Rint[i] + Qint[i];\n    // del[i] = (R[i] * R[i]) + (Q[i] * Q[i] * Q[i]); // why not just Q*Q*Q +\n    // R*R? Heisenbug. Heisenbug in release code\n  }\n}",
            "inline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\ninline __host__ __device__ float4 sqrtf(float4 v)\n{\n    return make_float4(sqrtf(v.x), sqrtf(v.y), sqrtf(v.z), sqrtf(v.w));\n}\n\n__global__ void QuarticSolver(int n, const float *A, const float *B,\n                              const float *C, const float *D, const float *b,\n                              const float *Q, const float *R, const float *del,\n                              float *__restrict__ theta,\n                              float *__restrict__ sqrtQ, float *__restrict__ x1,\n                              float *__restrict__ x2, float *__restrict__ x3,\n                              float *__restrict__ temp,\n                              float *__restrict__ min) {\n  // solver for finding minimum (xmin) for f(x) = Ax^4 + Bx^3 + Cx^2 + Dx + E\n  // undefined behaviour if A=0\n\n  int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < n) {\n    // comparing against 1E-5 to deal with potential problems with comparing\n    // floats to zero\n    if (del[i] <= 1E-5f) { // all 3 roots real\n\n      /*sqrtQ = 2 * sqrt(-Q);\n         theta = acos(R / (sqrtQ ^ 3));\n\n         x1 = 2 * (sqrtQ*cos(theta / 3) - b/3;\n         x2 = 2 * (sqrtQ*cos((theta + 2 * pi) / 3) - b/3);\n         x3 = 2 * (sqrtQ*cos((theta + 4 * pi) / 3) - b/3);*/\n\n      theta[i] = acosf((R[i] / sqrtf(-(Q[i] * Q[i] * Q[i]))));\n      sqrtQ[i] = 2.f * sqrtf(-Q[i]);\n\n      x1[i] = ((sqrtQ[i] * cosf((theta[i]) / 3.f)) - (b[i] / 3.f));\n      x2[i] = ((sqrtQ[i] * cosf((theta[i] + 2.f * 3.1415927f) / 3.f)) -\n               (b[i] / 3.f));\n      x3[i] = ((sqrtQ[i] * cosf((theta[i] + 4.f * 3.1415927f) / 3.f)) -\n               (b[i] / 3.f));\n\n      // unrolled bubble sort\n      if (x1[i] < x2[i]) {\n        temp[i] = x1[i];\n        x1[i] = x2[i];\n        x2[i] = temp[i];\n      } // { swap(x1[i], x2[i]); }//swap\n      if (x2[i] < x3[i]) {\n        temp[i] = x2[i];\n        x2[i] = x3[i];\n        x3[i] = temp[i];\n      } //{ swap(x2[i], x3[i]); }//swap\n      if (x1[i] < x2[i]) {\n        temp[i] = x1[i];\n        x1[i] = x2[i];\n        x2[i] = temp[i];\n      } //{ swap(x1[i], x2[i]); }//swap\n\n      min[i] =\n          A[i] *\n                          ((x1[i] * x1[i] * x1[i] * x1[i]) -\n                           (x3[i] * x3[i] * x3[i] * x3[i])) /\n                          4.f +\n                      B[i] *\n                          ((x1[i] * x1[i] * x1[i]) - (x3[i] * x3[i] * x3[i])) /\n                          3.f +\n                      C[i] * ((x1[i] * x1[i]) - (x3[i] * x3[i])) / 2.f +\n                      D[i] * (x1[i] - x3[i]) <=\n                  0.f\n              ? x1[i]\n              : x3[i];\n\n    }\n\n    // if (del[i] > 0) { // only 1 real root\n    else {\n\n      /*S = (R + sqrtD)^(1 / 3);\n         T = (R - sqrtD)^(1 / 3);\n         x = S + T - b/3;*/\n\n      x1[i] = cbrtf((R[i] + sqrtf(del[i]))) + cbrtf((R[i] - sqrtf(del[i]))) -\n              (b[i] / 3.f); // real root\n\n      // complex conjugate roots not relevant for minimisation\n\n      x2[i] = 0;\n      x3[i] = 0;\n\n      min[i] = x1[i];\n    }\n  }\n}"
        ]
    },
    "haversine-cuda": {
        "/Users/gbolet/hecbench-roofline/src/haversine-cuda/distance.cu": [
            "__global__ void compute_haversine_distance(\n  const double4 *__restrict__ p,\n        double*__restrict__ distance,\n  const int n)\n{\n  int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < n) {\n    auto ay = p[i].x * DEGREE_TO_RADIAN;  // a_lat\n    auto ax = p[i].y * DEGREE_TO_RADIAN;  // a_lon\n    auto by = p[i].z * DEGREE_TO_RADIAN;  // b_lat\n    auto bx = p[i].w * DEGREE_TO_RADIAN;  // b_lon\n\n    // haversine formula\n    auto x        = (bx - ax) / 2.0;\n    auto y        = (by - ay) / 2.0;\n    auto sinysqrd = sin(y) * sin(y);\n    auto sinxsqrd = sin(x) * sin(x);\n    auto scale    = cos(ay) * cos(by);\n    distance[i] = 2.0 * EARTH_RADIUS_KM * asin(sqrt(sinysqrd + sinxsqrd * scale));\n  }\n}"
        ]
    },
    "meanshift-cuda": {
        "/Users/gbolet/hecbench-roofline/src/meanshift-cuda/main.cu": [
            "inline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\ninline __host__ __device__ float4 expf(float4 v)\n{\n    return make_float4(expf(v.x), expf(v.y), expf(v.z), expf(v.w));\n}\n\n__global__ void mean_shift(const float *data, float *data_next) {\n    size_t tid = (blockIdx.x * blockDim.x) + threadIdx.x;\n    if (tid < N) {\n      size_t row = tid * D;\n      float new_position[D] = {0.f};\n      float tot_weight = 0.f;\n      for (size_t i = 0; i < N; ++i) {\n        size_t row_n = i * D;\n        float sq_dist = 0.f;\n        for (size_t j = 0; j < D; ++j) {\n          sq_dist += (data[row + j] - data[row_n + j]) * (data[row + j] - data[row_n + j]);\n        }\n        if (sq_dist <= RADIUS) {\n          float weight = expf(-sq_dist / DBL_SIGMA_SQ);\n          for (size_t j = 0; j < D; ++j) {\n            new_position[j] += weight * data[row_n + j];\n          }\n          tot_weight += weight;\n        }\n      }\n      for (size_t j = 0; j < D; ++j) {\n        data_next[row + j] = new_position[j] / tot_weight;\n      }\n    }\n  }",
            "inline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\ninline __host__ __device__ float4 expf(float4 v)\n{\n    return make_float4(expf(v.x), expf(v.y), expf(v.z), expf(v.w));\n}\n\n__global__ void mean_shift_tiling(const float* data, float* data_next) {\n\n    // Shared memory allocation\n    __shared__ float local_data[TILE_WIDTH * D];\n    __shared__ float valid_data[TILE_WIDTH];\n    // A few convenient variables\n    int tid = (blockIdx.x * blockDim.x) + threadIdx.x;\n    int row = tid * D;\n    int local_row = threadIdx.x * D;\n    float new_position[D] = {0.f};\n    float tot_weight = 0.f;\n    // Load data in shared memory\n    for (int t = 0; t < BLOCKS; ++t) {\n      int tid_in_tile = t * TILE_WIDTH + threadIdx.x;\n      if (tid_in_tile < N) {\n        int row_in_tile = tid_in_tile * D;\n        for (int j = 0; j < D; ++j) {\n          local_data[local_row + j] = data[row_in_tile + j];\n        }\n        valid_data[threadIdx.x] = 1;\n      }\n      else {\n        for (int j = 0; j < D; ++j) {\n          local_data[local_row + j] = 0;\n        }\n        valid_data[threadIdx.x] = 0;\n      }\n      __syncthreads();\n      for (int i = 0; i < TILE_WIDTH; ++i) {\n        int local_row_tile = i * D;\n        float valid_radius = RADIUS * valid_data[i];\n        float sq_dist = 0.;\n        for (int j = 0; j < D; ++j) {\n          sq_dist += (data[row + j] - local_data[local_row_tile + j]) *\n                     (data[row + j] - local_data[local_row_tile + j]);\n        }\n        if (sq_dist <= valid_radius) {\n          float weight = expf(-sq_dist / DBL_SIGMA_SQ);\n          for (int j = 0; j < D; ++j) {\n            new_position[j] += (weight * local_data[local_row_tile + j]);\n          }\n          tot_weight += (weight * valid_data[i]);\n        }\n      }\n      __syncthreads();\n    }\n    if (tid < N) {\n      for (int j = 0; j < D; ++j) {\n        data_next[row + j] = new_position[j] / tot_weight;\n      }\n    }\n  }"
        ]
    },
    "qtclustering-cuda": {
        "/Users/gbolet/hecbench-roofline/src/qtclustering-cuda/kernels_common.h": [
            "__global__\nvoid reduce_card_device(int *cardinalities, int TB_count){\n    int i, max_card = -1, winner_index;\n\n    for(i=0; i<TB_count*2; i+=2){\n        if( cardinalities[i] > max_card ){\n            max_card = cardinalities[i];\n            winner_index = cardinalities[i+1];\n        }\n    }\n\n    cardinalities[0] = max_card;\n    cardinalities[1] = winner_index;\n\n}",
            "inline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\ninline __host__ __device__ float4 fmaxf(float4 a, float4 b)\n{\n    return make_float4(fmaxf(a.x,b.x), fmaxf(a.y,b.y), fmaxf(a.z,b.z), fmaxf(a.w,b.w));\n}\n\n__device__ __forceinline__ float MAX<float>(float in, float in2)\n{\n    return fmaxf(in, in2);\n}\n\n__global__ void\ncompute_degrees(int *indr_mtrx, int *degrees, int N0, int max_degree){\n    int tid, tblock_id, TB_count, offset;\n    int local_point_count, curThreadCount;\n    int starting_point;\n\n    curThreadCount = blockDim.x;\n    tid = threadIdx.x;\n    tblock_id = blockIdx.x;\n    TB_count = gridDim.x;\n    local_point_count = (N0+TB_count-1)/TB_count;\n    starting_point = tblock_id * local_point_count;\n    offset =  starting_point*max_degree;\n    indr_mtrx = &indr_mtrx[offset];\n    degrees = &degrees[starting_point];\n\n    // The last threadblock might end up with less points.\n    if( (tblock_id+1)*local_point_count > N0 )\n        local_point_count = MAX(0,N0-starting_point);\n\n    for(int i=0; i+tid < local_point_count; i+=curThreadCount){\n        int cnt = 0;\n        for(int j=0; j < max_degree; j++){\n            if( indr_mtrx[(i+tid)*max_degree+j] >= 0 ){\n                ++cnt;\n            }\n        }\n        degrees[i+tid] = cnt;\n    }\n}",
            "__global__ void\nupdate_clustered_pnts_mask(char *clustered_pnts_mask, char *Ai_mask, int N0 ) {\n    int tid = threadIdx.x;\n    int curThreadCount = blockDim.x*blockDim.y*blockDim.z;\n\n    // If a point is part of the latest winner cluster, then it should be marked as\n    // clustered for the future iterations. Otherwise it should be left as it is.\n    for(int i = 0; i+tid < N0; i+=curThreadCount){\n        clustered_pnts_mask[i+tid] |= Ai_mask[i+tid];\n    }\n    __syncthreads();\n}",
            "inline __device__\nint closest_point_reduction(float min_dist, float threshold, int closest_point){\n    __shared__ float dist_array[THREADSPERBLOCK];\n    __shared__ int point_index_array[THREADSPERBLOCK];\n\n    int tid = threadIdx.x;\n    int curThreadCount = blockDim.x*blockDim.y*blockDim.z;\n\n    dist_array[tid] = min_dist;\n    point_index_array[tid] = closest_point;\n\n    __syncthreads();\n\n    if(tid == 0 ){\n        for(int j=1; j<curThreadCount; j++){\n            float dist = dist_array[j];\n            // look for a point that is closer, or equally far, but with a smaller index.\n            if( (dist < min_dist) || (dist == min_dist && point_index_array[j] < point_index_array[0]) ){\n                min_dist = dist;\n                point_index_array[0] = point_index_array[j];\n            }\n        }\n        if( min_dist > threshold )\n            point_index_array[0] = -1;\n    }\n\n    __syncthreads();\n\n    return point_index_array[0];\n}\n\ninline __device__\nint generate_candidate_cluster_compact_storage(int seed_point, int degree, char *Ai_mask, float *compact_storage_dist_matrix, char *clustered_pnts_mask, int *indr_mtrx, float *dist_to_clust, int point_count, int N0, int max_degree, int *candidate_cluster, float threshold)\n{\n\n    bool flag;\n    int cnt, latest_point;\n\n    int curThreadCount = blockDim.x*blockDim.y*blockDim.z;\n    int tid = threadIdx.x;\n    int seed_p_off;\n\n    float curr_dist_to_clust_i;\n    float curr_dist_to_clust_0, curr_dist_to_clust_1, curr_dist_to_clust_2, curr_dist_to_clust_3;\n    float curr_dist_to_clust_4, curr_dist_to_clust_5, curr_dist_to_clust_6, curr_dist_to_clust_7;\n    float curr_dist_to_clust_8, curr_dist_to_clust_9, curr_dist_to_clust_10, curr_dist_to_clust_11;\n    int cand_pnt_i=-1;\n    int cand_pnt_0=-1, cand_pnt_1=-1, cand_pnt_2=-1, cand_pnt_3=-1;\n    int cand_pnt_4=-1, cand_pnt_5=-1, cand_pnt_6=-1, cand_pnt_7=-1;\n    int cand_pnt_8=-1, cand_pnt_9=-1, cand_pnt_10=-1, cand_pnt_11=-1;\n\n    // Cleanup the candidate-cluster-mask, Ai_mask\n    for(int i=0; i+tid < N0; i+=curThreadCount){\n        Ai_mask[i+tid] = 0;\n    }\n\n    // Cleanup the \"distance cache\"\n    for(int i=0; i+tid < max_degree; i+=curThreadCount){\n        dist_to_clust[i+tid] = 0;\n    }\n\n    // Put the seed point in the candidate cluster and mark it as taken in the candidate cluster mask Ai_mask.\n    flag = true;\n    cnt = 1;\n    if( 0 == tid ){\n        if( NULL != candidate_cluster )\n            candidate_cluster[0] = seed_point;\n        Ai_mask[seed_point] = 1;\n    }\n    __syncthreads();\n    seed_p_off = seed_point*max_degree;\n    latest_point = seed_point;\n\n    // Prefetch 12 points per thread, into registers, to reduce the memory pressure (and delay) of\n    // constantly going to memory to fetch these points inside the while() loop that follows.\n    do{\n        FETCH_POINT(  cand_pnt_0,  0 );\n        FETCH_POINT(  cand_pnt_1,  1 );\n        FETCH_POINT(  cand_pnt_2,  2 );\n        FETCH_POINT(  cand_pnt_3,  3 );\n        FETCH_POINT(  cand_pnt_4,  4 );\n        FETCH_POINT(  cand_pnt_5,  5 );\n        FETCH_POINT(  cand_pnt_6,  6 );\n        FETCH_POINT(  cand_pnt_7,  7 );\n        FETCH_POINT(  cand_pnt_8,  8 );\n        FETCH_POINT(  cand_pnt_9,  9 );\n        FETCH_POINT( cand_pnt_10, 10 );\n        FETCH_POINT( cand_pnt_11, 11 );\n    }while(0);\n\n    // different threads might exit this loop at different times, so let them catch up.\n    __syncthreads();\n\n    while( (cnt < point_count) && flag ){\n        int min_G_index;\n        int point_index = -1;\n        float min_dist=3*threshold;\n        int last_index_checked = 0;\n        float diameter;\n        float dist_to_new_point;\n\n        int latest_p_off = latest_point*max_degree;\n\n        do{\n            COMPUTE_DIAMETER_WITH_POINT(  cand_pnt_0,  curr_dist_to_clust_0,  0 );\n            COMPUTE_DIAMETER_WITH_POINT(  cand_pnt_1,  curr_dist_to_clust_1,  1 );\n            COMPUTE_DIAMETER_WITH_POINT(  cand_pnt_2,  curr_dist_to_clust_2,  2 );\n            COMPUTE_DIAMETER_WITH_POINT(  cand_pnt_3,  curr_dist_to_clust_3,  3 );\n            COMPUTE_DIAMETER_WITH_POINT(  cand_pnt_4,  curr_dist_to_clust_4,  4 );\n            COMPUTE_DIAMETER_WITH_POINT(  cand_pnt_5,  curr_dist_to_clust_5,  5 );\n            COMPUTE_DIAMETER_WITH_POINT(  cand_pnt_6,  curr_dist_to_clust_6,  6 );\n            COMPUTE_DIAMETER_WITH_POINT(  cand_pnt_7,  curr_dist_to_clust_7,  7 );\n            COMPUTE_DIAMETER_WITH_POINT(  cand_pnt_8,  curr_dist_to_clust_8,  8 );\n            COMPUTE_DIAMETER_WITH_POINT(  cand_pnt_9,  curr_dist_to_clust_9,  9 );\n            COMPUTE_DIAMETER_WITH_POINT( cand_pnt_10, curr_dist_to_clust_10, 10 );\n            COMPUTE_DIAMETER_WITH_POINT( cand_pnt_11, curr_dist_to_clust_11, 11 );\n        }while(0);\n\n        // different threads might exit this loop at different times, so let them catch up.\n        __syncthreads();\n\n        // The following loop implements the \"find point pj s.t. diameter(Ai && pj) is minimum\"\n        for(int i=12; i*curThreadCount+tid < max_degree; i++){\n            FETCH_POINT( cand_pnt_i, i );\n            COMPUTE_DIAMETER_WITH_POINT( cand_pnt_i, curr_dist_to_clust_i, i );\n        }\n        __syncthreads();\n\n        min_G_index = closest_point_reduction(min_dist, threshold, point_index);\n\n        if(min_G_index >= 0 ){\n            if( 0 == tid ){\n                Ai_mask[min_G_index] = 1;\n                if( NULL != candidate_cluster ){\n                    candidate_cluster[cnt] = min_G_index;\n                }\n            }\n            latest_point = min_G_index;\n            cnt++;\n        }else{\n            flag = false;\n        }\n\n        __syncthreads();\n    }\n    __syncthreads();\n\n    return cnt;\n}\n\n__global__ void\ntrim_ungrouped_pnts_indr_array(int seed_index, int *ungrpd_pnts_indr, float *dist_matrix, int *result_cluster, char *Ai_mask, char *clustered_pnts_mask, int *indr_mtrx, int *cluster_cardinalities, float *dist_to_clust, int *degrees, int point_count, int N0, int max_degree, float threshold) {\n    int cnt;\n    int tid = threadIdx.x;\n    int curThreadCount = blockDim.x*blockDim.y*blockDim.z;\n    __shared__ int tmp_pnts[THREADSPERBLOCK];\n\n    int degree = degrees[seed_index];\n        (void)generate_candidate_cluster_compact_storage(seed_index, degree, Ai_mask, dist_matrix, clustered_pnts_mask, indr_mtrx,\n                                               dist_to_clust, point_count, N0, max_degree, result_cluster, threshold);\n\n    __shared__ int cnt_sh;\n    __shared__ bool flag_sh;\n\n    if( 0 == tid ){\n        cnt_sh = 0;\n        flag_sh = false;\n    }\n    __syncthreads();\n\n    for(int i = 0; i+tid < point_count; i+=curThreadCount){\n        // Have all threads make a coalesced read of contiguous global memory and copy the points assuming they are all good.\n        tmp_pnts[tid] = ungrpd_pnts_indr[i+tid];\n        int pnt = tmp_pnts[tid];\n        // If a point is bad (which should not happen very often), raise a global flag so that thread zero fixes the problem.\n        if( 1 == Ai_mask[pnt] ){\n            flag_sh = true;\n            tmp_pnts[tid] = INVALID_POINT_MARKER;\n        }else{\n            ungrpd_pnts_indr[cnt_sh+tid] = pnt;\n        }\n\n        __syncthreads();\n\n        if( 0 == tid ){\n            if( flag_sh ){\n                cnt = cnt_sh;\n                for(int j = 0; (j < curThreadCount) && (i+j < point_count); j++ ){\n                    if( INVALID_POINT_MARKER != tmp_pnts[j] ){\n                        ungrpd_pnts_indr[cnt] = tmp_pnts[j];\n                        cnt++;\n                    }\n                }\n                cnt_sh = cnt;\n            }else{\n                cnt_sh += curThreadCount;\n            }\n            flag_sh  = false;\n        }\n\n        __syncthreads();\n\n    }\n}",
            "inline __device__\nint closest_point_reduction(float min_dist, float threshold, int closest_point){\n    __shared__ float dist_array[THREADSPERBLOCK];\n    __shared__ int point_index_array[THREADSPERBLOCK];\n\n    int tid = threadIdx.x;\n    int curThreadCount = blockDim.x*blockDim.y*blockDim.z;\n\n    dist_array[tid] = min_dist;\n    point_index_array[tid] = closest_point;\n\n    __syncthreads();\n\n    if(tid == 0 ){\n        for(int j=1; j<curThreadCount; j++){\n            float dist = dist_array[j];\n            // look for a point that is closer, or equally far, but with a smaller index.\n            if( (dist < min_dist) || (dist == min_dist && point_index_array[j] < point_index_array[0]) ){\n                min_dist = dist;\n                point_index_array[0] = point_index_array[j];\n            }\n        }\n        if( min_dist > threshold )\n            point_index_array[0] = -1;\n    }\n\n    __syncthreads();\n\n    return point_index_array[0];\n}\n\ninline __device__\nint generate_candidate_cluster_compact_storage(int seed_point, int degree, char *Ai_mask, float *compact_storage_dist_matrix, char *clustered_pnts_mask, int *indr_mtrx, float *dist_to_clust, int point_count, int N0, int max_degree, int *candidate_cluster, float threshold)\n{\n\n    bool flag;\n    int cnt, latest_point;\n\n    int curThreadCount = blockDim.x*blockDim.y*blockDim.z;\n    int tid = threadIdx.x;\n    int seed_p_off;\n\n    float curr_dist_to_clust_i;\n    float curr_dist_to_clust_0, curr_dist_to_clust_1, curr_dist_to_clust_2, curr_dist_to_clust_3;\n    float curr_dist_to_clust_4, curr_dist_to_clust_5, curr_dist_to_clust_6, curr_dist_to_clust_7;\n    float curr_dist_to_clust_8, curr_dist_to_clust_9, curr_dist_to_clust_10, curr_dist_to_clust_11;\n    int cand_pnt_i=-1;\n    int cand_pnt_0=-1, cand_pnt_1=-1, cand_pnt_2=-1, cand_pnt_3=-1;\n    int cand_pnt_4=-1, cand_pnt_5=-1, cand_pnt_6=-1, cand_pnt_7=-1;\n    int cand_pnt_8=-1, cand_pnt_9=-1, cand_pnt_10=-1, cand_pnt_11=-1;\n\n    // Cleanup the candidate-cluster-mask, Ai_mask\n    for(int i=0; i+tid < N0; i+=curThreadCount){\n        Ai_mask[i+tid] = 0;\n    }\n\n    // Cleanup the \"distance cache\"\n    for(int i=0; i+tid < max_degree; i+=curThreadCount){\n        dist_to_clust[i+tid] = 0;\n    }\n\n    // Put the seed point in the candidate cluster and mark it as taken in the candidate cluster mask Ai_mask.\n    flag = true;\n    cnt = 1;\n    if( 0 == tid ){\n        if( NULL != candidate_cluster )\n            candidate_cluster[0] = seed_point;\n        Ai_mask[seed_point] = 1;\n    }\n    __syncthreads();\n    seed_p_off = seed_point*max_degree;\n    latest_point = seed_point;\n\n    // Prefetch 12 points per thread, into registers, to reduce the memory pressure (and delay) of\n    // constantly going to memory to fetch these points inside the while() loop that follows.\n    do{\n        FETCH_POINT(  cand_pnt_0,  0 );\n        FETCH_POINT(  cand_pnt_1,  1 );\n        FETCH_POINT(  cand_pnt_2,  2 );\n        FETCH_POINT(  cand_pnt_3,  3 );\n        FETCH_POINT(  cand_pnt_4,  4 );\n        FETCH_POINT(  cand_pnt_5,  5 );\n        FETCH_POINT(  cand_pnt_6,  6 );\n        FETCH_POINT(  cand_pnt_7,  7 );\n        FETCH_POINT(  cand_pnt_8,  8 );\n        FETCH_POINT(  cand_pnt_9,  9 );\n        FETCH_POINT( cand_pnt_10, 10 );\n        FETCH_POINT( cand_pnt_11, 11 );\n    }while(0);\n\n    // different threads might exit this loop at different times, so let them catch up.\n    __syncthreads();\n\n    while( (cnt < point_count) && flag ){\n        int min_G_index;\n        int point_index = -1;\n        float min_dist=3*threshold;\n        int last_index_checked = 0;\n        float diameter;\n        float dist_to_new_point;\n\n        int latest_p_off = latest_point*max_degree;\n\n        do{\n            COMPUTE_DIAMETER_WITH_POINT(  cand_pnt_0,  curr_dist_to_clust_0,  0 );\n            COMPUTE_DIAMETER_WITH_POINT(  cand_pnt_1,  curr_dist_to_clust_1,  1 );\n            COMPUTE_DIAMETER_WITH_POINT(  cand_pnt_2,  curr_dist_to_clust_2,  2 );\n            COMPUTE_DIAMETER_WITH_POINT(  cand_pnt_3,  curr_dist_to_clust_3,  3 );\n            COMPUTE_DIAMETER_WITH_POINT(  cand_pnt_4,  curr_dist_to_clust_4,  4 );\n            COMPUTE_DIAMETER_WITH_POINT(  cand_pnt_5,  curr_dist_to_clust_5,  5 );\n            COMPUTE_DIAMETER_WITH_POINT(  cand_pnt_6,  curr_dist_to_clust_6,  6 );\n            COMPUTE_DIAMETER_WITH_POINT(  cand_pnt_7,  curr_dist_to_clust_7,  7 );\n            COMPUTE_DIAMETER_WITH_POINT(  cand_pnt_8,  curr_dist_to_clust_8,  8 );\n            COMPUTE_DIAMETER_WITH_POINT(  cand_pnt_9,  curr_dist_to_clust_9,  9 );\n            COMPUTE_DIAMETER_WITH_POINT( cand_pnt_10, curr_dist_to_clust_10, 10 );\n            COMPUTE_DIAMETER_WITH_POINT( cand_pnt_11, curr_dist_to_clust_11, 11 );\n        }while(0);\n\n        // different threads might exit this loop at different times, so let them catch up.\n        __syncthreads();\n\n        // The following loop implements the \"find point pj s.t. diameter(Ai && pj) is minimum\"\n        for(int i=12; i*curThreadCount+tid < max_degree; i++){\n            FETCH_POINT( cand_pnt_i, i );\n            COMPUTE_DIAMETER_WITH_POINT( cand_pnt_i, curr_dist_to_clust_i, i );\n        }\n        __syncthreads();\n\n        min_G_index = closest_point_reduction(min_dist, threshold, point_index);\n\n        if(min_G_index >= 0 ){\n            if( 0 == tid ){\n                Ai_mask[min_G_index] = 1;\n                if( NULL != candidate_cluster ){\n                    candidate_cluster[cnt] = min_G_index;\n                }\n            }\n            latest_point = min_G_index;\n            cnt++;\n        }else{\n            flag = false;\n        }\n\n        __syncthreads();\n    }\n    __syncthreads();\n\n    return cnt;\n}\n\n__global__\nvoid QTC_device( float *dist_matrix, char *Ai_mask, char *clustered_pnts_mask, int *indr_mtrx, int *cluster_cardinalities, int *ungrpd_pnts_indr, float *dist_to_clust, int *degrees, int point_count, int N0, int max_degree, float threshold, int node_rank, int node_count, int total_thread_block_count) {\n    int max_cardinality = -1;\n    int max_cardinality_index;\n    int i, tblock_id, tid, base_offset;\n\n    tid = threadIdx.x;\n    //tblock_id = (blockIdx.y * gridDim.x + blockIdx.x);\n    tblock_id = blockIdx.x;\n    Ai_mask = &Ai_mask[tblock_id * N0];\n    dist_to_clust = &dist_to_clust[tblock_id * max_degree];\n    //base_offset = node_offset+tblock_id;\n    base_offset = tblock_id*node_count + node_rank;\n\n    // for i loop of the algorithm.\n    // Each thread iterates over all points that the whole thread-block owns\n    for(i = base_offset; i < point_count; i+= total_thread_block_count ){\n        int cnt;\n        int seed_index = ungrpd_pnts_indr[i];\n        int degree = degrees[seed_index];\n        if( degree <= max_cardinality ) continue;\n            cnt = generate_candidate_cluster_compact_storage( seed_index, degree, Ai_mask, dist_matrix,\n                                                    clustered_pnts_mask, indr_mtrx, dist_to_clust,\n                                                    point_count, N0, max_degree, NULL, threshold);\n        if( cnt > max_cardinality ){\n            max_cardinality = cnt;\n            max_cardinality_index = seed_index;\n        }\n    } // for (i\n\n    // since only three elements per block go to the global memory, the offset is:\n    //int card_offset = (blockIdx.y * gridDim.x + blockIdx.x)*2;\n    int card_offset = blockIdx.x*2;\n    // only one thread needs to write into the global memory since they all have the same information.\n    if( 0 == tid ){\n        cluster_cardinalities[card_offset] = max_cardinality;\n        cluster_cardinalities[card_offset+1] = max_cardinality_index;\n    }\n}"
        ]
    },
    "black-scholes-cuda": {
        "/Users/gbolet/hecbench-roofline/src/black-scholes-cuda/blackScholesAnalyticEngineKernels.cu": [
            "inline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\ninline __host__ __device__ float4 expf(float4 v)\n{\n    return make_float4(expf(v.x), expf(v.y), expf(v.z), expf(v.w));\n}\n\n__device__ float interestRateCompoundFactor(float t, yieldTermStruct currYieldTermStruct)\n{\n  return (expf((currYieldTermStruct.forward)*t));\n}\n\n__device__ float interestRateDiscountFactor(float t, yieldTermStruct currYieldTermStruct)\n{\n  return 1.0f / interestRateCompoundFactor(t, currYieldTermStruct);\n}\n\n__device__ float gaussianFunctNormDist(normalDistStruct normDist, float x)\n{\n  float deltax = x - normDist.average;\n  float exponent = -(deltax*deltax)/normDist.denominator;\n\n  // debian alpha had some strange problem in the very-low range\n  return exponent <= -690.0f ? 0.0f :  // exp(x) < 1.0e-300 anyway\n    normDist.normalizationFactor * expf(exponent);\n}\n\n__device__ float errorFunct(normalDistStruct normDist, float x)\n{\n  float R,S,P,Q,s,y,z,r, ax;\n\n  ax = fabsf(x);\n\n  if(ax < 0.84375f) \n  {      \n    if(ax < 3.7252902984e-09f) \n    { \n      if (ax < FLT_MIN*16.0f)\n        return 0.125f*(8.0f*x+ (ERROR_FUNCT_efx8)*x);  /*avoid underflow */\n      return x + (ERROR_FUNCT_efx)*x;\n    }\n    z = x*x;\n    r = ERROR_FUNCT_pp0+z*(ERROR_FUNCT_pp1+z*(ERROR_FUNCT_pp2+z*(ERROR_FUNCT_pp3+z*ERROR_FUNCT_pp4)));\n    s = ERROR_FUNCT_one+z*(ERROR_FUNCT_qq1+z*(ERROR_FUNCT_qq2+z*(ERROR_FUNCT_qq3+z*(ERROR_FUNCT_qq4+z*ERROR_FUNCT_qq5))));\n    y = r/s;\n    return x + x*y;\n  }\n  if(ax <1.25f) \n  {      \n    s = ax-ERROR_FUNCT_one;\n    P = ERROR_FUNCT_pa0+s*(ERROR_FUNCT_pa1+s*(ERROR_FUNCT_pa2+s*(ERROR_FUNCT_pa3+s*(ERROR_FUNCT_pa4+s*(ERROR_FUNCT_pa5+s*ERROR_FUNCT_pa6)))));\n    Q = ERROR_FUNCT_one+s*(ERROR_FUNCT_qa1+s*(ERROR_FUNCT_qa2+s*(ERROR_FUNCT_qa3+s*(ERROR_FUNCT_qa4+s*(ERROR_FUNCT_qa5+s*ERROR_FUNCT_qa6)))));\n    if(x>=0.0f) return ERROR_FUNCT_erx + P/Q; else return -1.0f*ERROR_FUNCT_erx - P/Q;\n  }\n  if (ax >= 6.0f) \n  {      \n    if(x>=0.0f) \n      return ERROR_FUNCT_one-ERROR_FUNCT_tiny; \n    else \n      return ERROR_FUNCT_tiny-ERROR_FUNCT_one;\n  }\n\n  /* Starts to lose accuracy when ax~5 */\n  s = ERROR_FUNCT_one/(ax*ax);\n\n  if(ax < 2.85714285714285f) { /* |x| < 1/0.35 */\n    R = ERROR_FUNCT_ra0+s*(ERROR_FUNCT_ra1+s*(ERROR_FUNCT_ra2+s*(ERROR_FUNCT_ra3+s*(ERROR_FUNCT_ra4+s*(ERROR_FUNCT_ra5+s*(ERROR_FUNCT_ra6+s*ERROR_FUNCT_ra7))))));\n    S = ERROR_FUNCT_one+s*(ERROR_FUNCT_sa1+s*(ERROR_FUNCT_sa2+s*(ERROR_FUNCT_sa3+s*(ERROR_FUNCT_sa4+s*(ERROR_FUNCT_sa5+s*(ERROR_FUNCT_sa6+s*(ERROR_FUNCT_sa7+s*ERROR_FUNCT_sa8)))))));\n  } else {    /* |x| >= 1/0.35 */\n    R=ERROR_FUNCT_rb0+s*(ERROR_FUNCT_rb1+s*(ERROR_FUNCT_rb2+s*(ERROR_FUNCT_rb3+s*(ERROR_FUNCT_rb4+s*(ERROR_FUNCT_rb5+s*ERROR_FUNCT_rb6)))));\n    S=ERROR_FUNCT_one+s*(ERROR_FUNCT_sb1+s*(ERROR_FUNCT_sb2+s*(ERROR_FUNCT_sb3+s*(ERROR_FUNCT_sb4+s*(ERROR_FUNCT_sb5+s*(ERROR_FUNCT_sb6+s*ERROR_FUNCT_sb7))))));\n  }\n\n  r = expf( -ax*ax-0.5625f +R/S);\n  if(x>=0.0f) \n    return ERROR_FUNCT_one-r/ax; \n  else \n    return r/ax-ERROR_FUNCT_one;\n}\n\n__device__ float cumNormDistDeriv(normalDistStruct normDist, float x)\n{\n  float xn = (x - normDist.average) / normDist.sigma;\n  return gaussianFunctNormDist(normDist, xn) / normDist.sigma;\n}\n\n__device__ float cumNormDistOp(normalDistStruct normDist, float z)\n{\n  z = (z - normDist.average) / normDist.sigma;\n  float result = 0.5f * ( 1.0f + errorFunct(normDist, z*M_SQRT_2 ) );\n  return result;\n}\n\n__device__ void initCumNormDist(normalDistStruct& currCumNormDist)\n{\n  currCumNormDist.average = 0.0f;\n  currCumNormDist.sigma = 1.0f;\n  currCumNormDist.normalizationFactor = M_SQRT_2*M_1_SQRTPI/currCumNormDist.sigma;\n  currCumNormDist.derNormalizationFactor = currCumNormDist.sigma*currCumNormDist.sigma;\n  currCumNormDist.denominator = 2.0f*currCumNormDist.derNormalizationFactor;\n}\n\n__device__ void initBlackCalcVars(blackCalcStruct& blackCalculator, payoffStruct payoff)\n{\n  blackCalculator.d1 = log(blackCalculator.forward / blackCalculator.strike)/blackCalculator.stdDev + 0.5f*blackCalculator.stdDev;\n  blackCalculator.d2 = blackCalculator.d1 - blackCalculator.stdDev;\n\n  //initialize the cumulative normal distribution structure\n  normalDistStruct currCumNormDist;\n  initCumNormDist(currCumNormDist);\n\n  blackCalculator.cum_d1 = cumNormDistOp(currCumNormDist, blackCalculator.d1);\n  blackCalculator.cum_d2 = cumNormDistOp(currCumNormDist, blackCalculator.d2);\n  blackCalculator.n_d1 = cumNormDistDeriv(currCumNormDist, blackCalculator.d1);\n  blackCalculator.n_d2 = cumNormDistDeriv(currCumNormDist, blackCalculator.d2);\n\n  blackCalculator.x = payoff.strike;\n  blackCalculator.DxDstrike = 1.0f;\n\n  // the following one will probably disappear as soon as\n  // super-share will be properly handled\n  blackCalculator.DxDs = 0.0f;\n\n  // this part is always executed.\n  // in case of plain-vanilla payoffs, it is also the only part\n  // which is executed.\n  switch (payoff.type) \n  {\n    case CALL:\n      blackCalculator.alpha     = blackCalculator.cum_d1;//  N(d1)\n      blackCalculator.DalphaDd1 = blackCalculator.n_d1;//  n(d1)\n      blackCalculator.beta      = -1.0f*blackCalculator.cum_d2;// -N(d2)\n      blackCalculator.DbetaDd2  = -1.0f*blackCalculator.n_d2;// -n(d2)\n      break;\n    case PUT:\n      blackCalculator.alpha     = -1.0f+blackCalculator.cum_d1;// -N(-d1)\n      blackCalculator.DalphaDd1 = blackCalculator.n_d1;//  n( d1)\n      blackCalculator.beta      = 1.0f-blackCalculator.cum_d2;//  N(-d2)\n      blackCalculator.DbetaDd2  = -1.0f* blackCalculator.n_d2;// -n( d2)\n      break;\n  }\n}\n\ninline __host__ __device__ float4 sqrtf(float4 v)\n{\n    return make_float4(sqrtf(v.x), sqrtf(v.y), sqrtf(v.z), sqrtf(v.w));\n}\n\n__device__ float getBlackVolBlackVar(blackVolStruct volTS)\n{\n  float vol = volTS.volatility;\n  return vol*vol*volTS.timeYearFraction;\n}\n\n__device__ float getDiscountOnDividendYield(float yearFraction, yieldTermStruct dividendYieldTermStruct)\n{\n  float intDiscountFactor = interestRateDiscountFactor(yearFraction, dividendYieldTermStruct);\n  return intDiscountFactor;\n}\n\n__device__ float getDiscountOnRiskFreeRate(float yearFraction, yieldTermStruct riskFreeRateYieldTermStruct)\n{\n  return interestRateDiscountFactor(yearFraction, riskFreeRateYieldTermStruct);\n}\n\n__device__ float getResultVal(blackCalcStruct blackCalculator)\n{\n  float result = blackCalculator.discount * (blackCalculator.forward * \n      blackCalculator.alpha + blackCalculator.x * blackCalculator.beta);\n  return result;\n}\n\n__device__ void initBlackCalculator(blackCalcStruct& blackCalc, payoffStruct payoff, float forwardPrice, float stdDev, float riskFreeDiscount)\n{\n  blackCalc.strike = payoff.strike;\n  blackCalc.forward = forwardPrice;\n  blackCalc.stdDev = stdDev;\n  blackCalc.discount = riskFreeDiscount;\n  blackCalc.variance = stdDev * stdDev;\n\n  initBlackCalcVars(blackCalc, payoff);\n}\n\n__global__ void getOutValOption(const optionInputStruct* options, float* outputVals, int numVals)\n{\n  int optionNum = blockIdx.x * blockDim.x + threadIdx.x;\n\n  //check if within current options\n  if (optionNum < numVals)\n  {\n    const optionInputStruct threadOption = options[optionNum];\n\n    payoffStruct currPayoff;\n    currPayoff.type = threadOption.type;\n    currPayoff.strike = threadOption.strike;\n\n    yieldTermStruct qTS;\n    qTS.timeYearFraction = threadOption.t;\n    qTS.forward = threadOption.q;\n\n    yieldTermStruct rTS;\n    rTS.timeYearFraction = threadOption.t;\n    rTS.forward = threadOption.r;\n\n    blackVolStruct volTS;\n    volTS.timeYearFraction = threadOption.t;\n    volTS.volatility = threadOption.vol;\n\n    blackScholesMertStruct stochProcess;\n    stochProcess.x0 = threadOption.spot;\n    stochProcess.dividendTS = qTS;\n    stochProcess.riskFreeTS = rTS;\n    stochProcess.blackVolTS = volTS;\n\n    optionStruct currOption;\n    currOption.payoff = currPayoff;\n    currOption.yearFractionTime = threadOption.t;\n    currOption.pricingEngine = stochProcess; \n\n    float variance = getBlackVolBlackVar(currOption.pricingEngine.blackVolTS);\n    float dividendDiscount = getDiscountOnDividendYield(currOption.yearFractionTime, currOption.pricingEngine.dividendTS);\n    float riskFreeDiscount = getDiscountOnRiskFreeRate(currOption.yearFractionTime, currOption.pricingEngine.riskFreeTS);\n    float spot = currOption.pricingEngine.x0; \n\n    float forwardPrice = spot * dividendDiscount / riskFreeDiscount;\n\n    //declare the blackCalcStruct\n    blackCalcStruct blackCalc;\n\n    //initialize the calculator\n    initBlackCalculator(blackCalc, currOption.payoff, forwardPrice, sqrtf(variance), riskFreeDiscount);\n\n    //retrieve the results values\n    float resultVal = getResultVal(blackCalc);\n\n    //write the resulting value to global memory\n    outputVals[optionNum] = resultVal;\n  }\n}"
        ]
    },
    "grrt-cuda": {
        "/Users/gbolet/hecbench-roofline/src/grrt-cuda/kernels.cu": [
            "inline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\ninline __host__ __device__ float4 fabs(float4 v)\n{\n    return make_float4(fabs(v.x), fabs(v.y), fabs(v.z), fabs(v.w));\n}\n\n__device__ static void geodesic(double* Variables, double* VariablesIn, double *y, double *dydx)\n{\n  double r = y[0];\n  double theta = y[1];\n  double pr = y[4];\n  double ptheta = y[5];\n\n  double r2  = r * r;\n  double twor = 2.0 * r;\n\n  double sintheta = sin(theta);\n  double costheta = cos(theta);\n  double cos2     = costheta * costheta;\n  double sin2     = sintheta * sintheta;\n  double sigma  = r2 + a2 * cos2;\n  double delta  = r2 - twor + a2;\n  double sd    = sigma * delta;\n  double siginv  = 1.0 / sigma;\n  double bot    = 1.0 / sd;\n\n\n  //avoid problems near the axis\n  if (sintheta < 1e-8)\n  {\n    sintheta = 1e-8;\n    sin2 = 1e-16;\n  }\n\n  dydx[0] = -pr * delta * siginv;  \n  dydx[1] = -ptheta * siginv;\n  dydx[2] = -(twor * A + (sigma - twor) * L / sin2) * bot;\n  dydx[3] = -(1.0 + (twor * (r2 + a2) - twor * A * L) * bot);\n  dydx[4] = -(((r - 1.0) * (-kappa) + twor * (r2 + a2) - 2.0 * A * L) * bot - 2.0 * pr * pr * (r - 1.0) * siginv);\n  dydx[5] = -sintheta * costheta*(L * L / (sin2 * sin2) - a2) * siginv;\n}\n\n__device__ static void rkstep(double* Variables, double* VariablesIn,double *y,\n                              double *dydx, double h, double *yout, double *yerr)\n{\n  int i;\n  double ak[N];\n  double ytemp1[N], ytemp2[N], ytemp3[N], ytemp4[N], ytemp5[N];\n  double hdydx;\n  double yi, yt;\n\n  for (i = 0; i < N; i++)\n  {\n    hdydx     = h * dydx[i];\n    yi        = y[i];\n    ytemp1[i] = yi + 0.2 * hdydx;\n    ytemp2[i] = yi + (3.0/40.0) * hdydx;\n    ytemp3[i] = yi + 0.3 * hdydx;\n    ytemp4[i] = yi -(11.0/54.0) * hdydx;\n    ytemp5[i] = yi + (1631.0/55296.0) * hdydx;\n    yout[i]   = yi + (37.0/378.0) * hdydx;\n    yerr[i]   = ((37.0/378.0)-(2825.0/27648.0)) * hdydx;\n  }\n\n  geodesic(Variables, VariablesIn, ytemp1, ak);\n\n  for (i = 0; i < N; i++)\n  {\n    yt         = h * ak[i];\n    ytemp2[i] += (9.0/40.0) * yt;\n    ytemp3[i] -= 0.9 * yt;\n    ytemp4[i] += 2.5 * yt;\n    ytemp5[i] += (175.0/512.0) * yt;\n  }\n\n  geodesic(Variables, VariablesIn, ytemp2, ak);\n\n  for (i = 0; i < N; i++)\n  {\n    yt         = h * ak[i];\n    ytemp3[i] += 1.2 * yt;\n    ytemp4[i] -= (70.0/27.0) * yt;\n    ytemp5[i] += (575.0/13824.0) * yt;\n    yout[i]   += (250.0/621.0) * yt;\n    yerr[i]   += ((250.0/621.0)-(18575.0/48384.0)) * yt;\n  }\n\n  geodesic(Variables, VariablesIn, ytemp3, ak);\n\n  for (i = 0; i < N; i++)\n  {\n    yt         = h * ak[i];\n    ytemp4[i] += (35.0/27.0) * yt;\n    ytemp5[i] += (44275.0/110592.0) * yt;\n    yout[i]   += (125.0/594.0) * yt;\n    yerr[i]   += ((125.0/594.0)-(13525.0/55296.0)) * yt;\n  }\n\n  geodesic(Variables, VariablesIn, ytemp4, ak);\n\n  for (i = 0; i < N; i++)\n  {\n    yt         = h * ak[i];\n    ytemp5[i] += (253.0/4096.0) * yt;\n    yerr[i]   -= (277.0/14336.0) * yt;\n  }\n\n  geodesic(Variables, VariablesIn, ytemp5, ak);\n\n  for (i = 0; i < N; i++)\n  {\n    yt       = h * ak[i];\n    yout[i] += (512.0/1771.0) * yt;\n    yerr[i] += ((512.0/1771.0)-0.25) * yt;\n  }\n}\n\n__device__ static float ISCO(double* VariablesIn)\n{\n  double z1       = 1 + pow(1 - A * A, 1 / 3.0) * pow(1 + A, 1 / 3.0) + pow(1 - A, 1 / 3.0);\n  double z2       = sqrt(3 * A * A + z1 * z1);\n  return 3. + z2 - sqrt((3 - z1) * (3 + z1 + 2 * z2));\n}\n\n__device__ static void initial(double* Variables, double* VariablesIn, double *y0, double *ydot0)\n{\n  double alpha = grid_x;\n  double beta  = grid_y;\n\n  //see Eq[18-19], with phi_obs=0, x= alpha, y=beta, z=0\n  double x     = sqrt(r0*r0+a2)*sin(theta0)-beta*cos(theta0);\n  double y     = alpha;\n  double z     = r0*cos(theta0)+beta*sin(theta0);\n  double w     = x*x+y*y+z*z-a2;\n\n  //see Eq[20-22]\n  y0[0] = sqrt((w+sqrt(w*w+(2.*A*z)*(2.*A*z)))/2.);   \n  y0[1] = acos(z/y0[0]);                              \n  y0[2] = atan2(y,x);                                \n  y0[3] = 0;\n  double r = y0[0];\n  double theta = y0[1];\n  double phi=y0[2];\n\n  double sigma = r*r+(A*cos(theta))*(A*cos(theta));\n  double u     = sqrt(a2+r*r);\n  double v     = -sin(theta0)*cos(phi);\n  double zdot  = -1.;\n\n  //see Eq[24-26]\n  double rdot0 = zdot*(-u*u*cos(theta0)*cos(theta)+r*u*v*sin(theta))/sigma;         \n  double thetadot0 = zdot*(cos(theta0)*r*sin(theta)+u*v*cos(theta))/sigma;          \n  double phidot0 = zdot*sin(theta0)*sin(phi)/(u*sin(theta));                         \n\n  ydot0[0] = rdot0;\n  ydot0[1] = thetadot0;\n  ydot0[2] = phidot0;\n\n  double sintheta=sin(theta);\n  double sin2 = sintheta*sintheta;\n\n  double r2 = r * r;\n  double delta = r2 - 2.0 * r + a2;\n  double s1 = sigma - 2.0 * r;\n\n  y0[4] = rdot0*sigma/delta;\n  y0[5] = thetadot0*sigma;\n\n  // Eq.7\n  double energy2 = s1*(rdot0*rdot0/delta+thetadot0*thetadot0)\n    + delta*sin2*phidot0*phidot0;\n\n  double energy = sqrt(energy2);\n\n  // rescaled by energy\n  y0[4] = y0[4]/energy;\n  y0[5] = y0[5]/energy;\n\n  // Eq.8 \n  L = ((sigma*delta*phidot0-2.0*A*r*energy)*sin2/s1)/energy;\n\n  // Eq below Eq.15\n  kappa = y0[5]*y0[5]+a2*sin2+L*L/sin2;\n}\n\n__device__ static double rk5(double* Variables, double* VariablesIn, double *y, double *dydx, \n                             double htry, double escal, double *yscal, double *hdid)\n{\n  int i;\n  double hnext;\n  double errmax, h = htry, htemp;\n  double yerr[N], ytemp[N];\n\n  while (1)\n  {\n    // find adaptive step size\n    rkstep(Variables, VariablesIn, y, dydx, h, ytemp, yerr);\n\n    errmax = 0.0;\n    for (i = 0; i < N; i++)\n    {\n      double temp = fabs(yerr[i]/yscal[i]);\n      if (temp > errmax) errmax = temp;\n    }\n\n    errmax *= escal;\n    if (errmax <= 1.0) break;\n\n    htemp = 0.9 * h / sqrt(sqrt(errmax));\n\n    h *= 0.1;\n\n    if (h >= 0.0)\n    {\n      if (htemp > h) h = htemp;\n    }\n    else\n    {\n      if (htemp < h) h = htemp;\n    }\n  }\n\n  if (errmax > 1.89e-4)\n  {\n    hnext = 0.9 * h * pow(errmax, -0.2);\n  }\n  else\n  {\n    hnext = 5.0 * h;\n  }\n\n  *hdid = h;\n\n  memcpy(y, ytemp, N * sizeof(double));\n\n  return hnext;\n}\n\n__device__ double task1fun_GetZ(double* Variables, double* VariablesIn, double *y)\n{\n  double r1 = y[0];\n  double E_local = -(r1 * r1 + A * sqrt(r1)) / (r1 * sqrt(r1 * r1 - 3. * r1 + 2. * A * sqrt(r1))) + \n                  L / sqrt(r1) / sqrt(r1 * r1 - 3. * r1  +2. * A * sqrt(r1));\n  double E_inf = -1.0;      \n  return E_local / E_inf; \n}\n\n__global__ void task1(double*__restrict__ ResultsPixel,\n                      double*__restrict__ VariablesIn,\n                      int GridIdxX, int GridIdxY)\n{\n  // to check whether the photon is inside image plane\n  if(X1 >= SIZE || Y1 >= SIZE) return;  \n\n  double Variables[VarNUM];\n\n  r0       = 1000.0;                 \n  theta0   = (PI/180.0) * INCLINATION;     \n  a2       = A * A;                \n  Rhor     = 1.0 + sqrt(1.0 - a2) + 1e-5;      \n  Rmstable = ISCO(VariablesIn);                \n\n  double htry = 0.5, escal = 1e14, hdid = 0.0, hnext = 0.0;\n  double y[N], dydx[N], yscal[N], ylaststep[N];\n  double Rdisk   = 50.;\n  double ima_width = 55.;\n  double s1  = ima_width;                      \n  double s2  = 2.*ima_width/((int)SIZE+1.);   \n\n  grid_x = -s1 + s2*(X1+1.);\n  grid_y = -s1 + s2*(Y1+1.);\n\n  initial(Variables, VariablesIn, y, dydx);\n\n  ResultsPixel(0) = grid_x;\n  ResultsPixel(1) = grid_y;\n  ResultsPixel(2) = 0;\n\n  while (1)\n  {\n    for(int i = 0; i < N; i++) ylaststep[i] = y[i];\n\n    geodesic(Variables, VariablesIn, y, dydx);\n\n    for (int i = 0; i < N; i++) yscal[i] = fabs(y[i]) + fabs(dydx[i] * htry) + 1.0e-3;\n\n    //fifth-order Runge-Kutta method\n    hnext = rk5(Variables, VariablesIn, y, dydx, htry, escal, yscal, &hdid);\n\n    // hit the disk, compute redshift\n    if( y[0] < Rdisk && y[0] > Rmstable && (ylaststep[1] - PI/2.) * (y[1] - PI/2.) < 0. )\n    {    \n      ResultsPixel(2) = 1./task1fun_GetZ(Variables, VariablesIn, y);\n      break;\n    }\n\n    // Inside the event horizon radius or escape to infinity\n    if ((y[0] > r0) && (dydx[0]>0)) break;\n\n    if (y[0] < Rhor) break;\n\n    htry = hnext;\n  }\n}",
            "#define T ((int)32)\n\n\n__host__ __device__ __forceinline__ constexpr T floor(T a, T b) {\n  return (a - b + 1) / b;\n}\n\n__device__ static double K2_find(double Te)\n{\n  double d = Te_grids*(log(Te / Te_min)/ log(Te_max / Te_min));\n  int    i = floor(d);\n\n  return (1 - (double)(d-i)) * K2_tab[i] + (double)(d-i) * K2_tab[i+1];\n}\n\ninline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\ninline __host__ __device__ float4 fabs(float4 v)\n{\n    return make_float4(fabs(v.x), fabs(v.y), fabs(v.z), fabs(v.w));\n}\n\n__device__ static void geodesic(double* Variables, double* VariablesIn, double *y, double *dydx)\n{\n  double r = y[0];\n  double theta = y[1];\n  double pr = y[4];\n  double ptheta = y[5];\n\n  double r2  = r * r;\n  double twor = 2.0 * r;\n\n  double sintheta = sin(theta);\n  double costheta = cos(theta);\n  double cos2     = costheta * costheta;\n  double sin2     = sintheta * sintheta;\n  double sigma  = r2 + a2 * cos2;\n  double delta  = r2 - twor + a2;\n  double sd    = sigma * delta;\n  double siginv  = 1.0 / sigma;\n  double bot    = 1.0 / sd;\n\n\n  //avoid problems near the axis\n  if (sintheta < 1e-8)\n  {\n    sintheta = 1e-8;\n    sin2 = 1e-16;\n  }\n\n  dydx[0] = -pr * delta * siginv;  \n  dydx[1] = -ptheta * siginv;\n  dydx[2] = -(twor * A + (sigma - twor) * L / sin2) * bot;\n  dydx[3] = -(1.0 + (twor * (r2 + a2) - twor * A * L) * bot);\n  dydx[4] = -(((r - 1.0) * (-kappa) + twor * (r2 + a2) - 2.0 * A * L) * bot - 2.0 * pr * pr * (r - 1.0) * siginv);\n  dydx[5] = -sintheta * costheta*(L * L / (sin2 * sin2) - a2) * siginv;\n}\n\n__device__ static void rkstep(double* Variables, double* VariablesIn,double *y,\n                              double *dydx, double h, double *yout, double *yerr)\n{\n  int i;\n  double ak[N];\n  double ytemp1[N], ytemp2[N], ytemp3[N], ytemp4[N], ytemp5[N];\n  double hdydx;\n  double yi, yt;\n\n  for (i = 0; i < N; i++)\n  {\n    hdydx     = h * dydx[i];\n    yi        = y[i];\n    ytemp1[i] = yi + 0.2 * hdydx;\n    ytemp2[i] = yi + (3.0/40.0) * hdydx;\n    ytemp3[i] = yi + 0.3 * hdydx;\n    ytemp4[i] = yi -(11.0/54.0) * hdydx;\n    ytemp5[i] = yi + (1631.0/55296.0) * hdydx;\n    yout[i]   = yi + (37.0/378.0) * hdydx;\n    yerr[i]   = ((37.0/378.0)-(2825.0/27648.0)) * hdydx;\n  }\n\n  geodesic(Variables, VariablesIn, ytemp1, ak);\n\n  for (i = 0; i < N; i++)\n  {\n    yt         = h * ak[i];\n    ytemp2[i] += (9.0/40.0) * yt;\n    ytemp3[i] -= 0.9 * yt;\n    ytemp4[i] += 2.5 * yt;\n    ytemp5[i] += (175.0/512.0) * yt;\n  }\n\n  geodesic(Variables, VariablesIn, ytemp2, ak);\n\n  for (i = 0; i < N; i++)\n  {\n    yt         = h * ak[i];\n    ytemp3[i] += 1.2 * yt;\n    ytemp4[i] -= (70.0/27.0) * yt;\n    ytemp5[i] += (575.0/13824.0) * yt;\n    yout[i]   += (250.0/621.0) * yt;\n    yerr[i]   += ((250.0/621.0)-(18575.0/48384.0)) * yt;\n  }\n\n  geodesic(Variables, VariablesIn, ytemp3, ak);\n\n  for (i = 0; i < N; i++)\n  {\n    yt         = h * ak[i];\n    ytemp4[i] += (35.0/27.0) * yt;\n    ytemp5[i] += (44275.0/110592.0) * yt;\n    yout[i]   += (125.0/594.0) * yt;\n    yerr[i]   += ((125.0/594.0)-(13525.0/55296.0)) * yt;\n  }\n\n  geodesic(Variables, VariablesIn, ytemp4, ak);\n\n  for (i = 0; i < N; i++)\n  {\n    yt         = h * ak[i];\n    ytemp5[i] += (253.0/4096.0) * yt;\n    yerr[i]   -= (277.0/14336.0) * yt;\n  }\n\n  geodesic(Variables, VariablesIn, ytemp5, ak);\n\n  for (i = 0; i < N; i++)\n  {\n    yt       = h * ak[i];\n    yout[i] += (512.0/1771.0) * yt;\n    yerr[i] += ((512.0/1771.0)-0.25) * yt;\n  }\n}\n\n__device__ static float ISCO(double* VariablesIn)\n{\n  double z1       = 1 + pow(1 - A * A, 1 / 3.0) * pow(1 + A, 1 / 3.0) + pow(1 - A, 1 / 3.0);\n  double z2       = sqrt(3 * A * A + z1 * z1);\n  return 3. + z2 - sqrt((3 - z1) * (3 + z1 + 2 * z2));\n}\n\n__device__ static double Jansky_Correction(double* VariablesIn,double ima_width)\n{\n  double distance=C_sgrA_d*C_pc;\n  double theta=atan(ima_width*C_sgrA_mbh*C_rgeo/distance);\n  double pix_str=theta/(SIZE/2.)*theta/(SIZE/2.);  //radian^2 (steradians) per pixel\n  return pix_str/C_Jansky;\n}\n\n__device__ static double K2(double Te)\n{\n  double tab_K2;\n  //avoid the boundary effect when T~T_max\n  if (Te>85.){ \n    tab_K2=2.*Te*Te;\n    return tab_K2;\n  }\n\n  if (Te<Te_min){ \n    //set a dummy value\n    return exp(-11.);\n  }\n\n  tab_K2= K2_find(Te);\n  return exp(tab_K2);\n}\n\n__device__ static double Luminosity_Correction(double* VariablesIn,double ima_width)\n{\n  double distance=C_sgrA_d*C_pc;\n  double theta=atan(ima_width*C_sgrA_mbh*C_rgeo/distance);\n  double pix_str=theta/(SIZE/2.)*theta/(SIZE/2.);  //radian^2 (steradians) per pixel\n  return pix_str*distance*distance*4.*PI*freq_obs;\n}\n\n__device__ static void initial(double* Variables, double* VariablesIn, double *y0, double *ydot0)\n{\n  double alpha = grid_x;\n  double beta  = grid_y;\n\n  //see Eq[18-19], with phi_obs=0, x= alpha, y=beta, z=0\n  double x     = sqrt(r0*r0+a2)*sin(theta0)-beta*cos(theta0);\n  double y     = alpha;\n  double z     = r0*cos(theta0)+beta*sin(theta0);\n  double w     = x*x+y*y+z*z-a2;\n\n  //see Eq[20-22]\n  y0[0] = sqrt((w+sqrt(w*w+(2.*A*z)*(2.*A*z)))/2.);   \n  y0[1] = acos(z/y0[0]);                              \n  y0[2] = atan2(y,x);                                \n  y0[3] = 0;\n  double r = y0[0];\n  double theta = y0[1];\n  double phi=y0[2];\n\n  double sigma = r*r+(A*cos(theta))*(A*cos(theta));\n  double u     = sqrt(a2+r*r);\n  double v     = -sin(theta0)*cos(phi);\n  double zdot  = -1.;\n\n  //see Eq[24-26]\n  double rdot0 = zdot*(-u*u*cos(theta0)*cos(theta)+r*u*v*sin(theta))/sigma;         \n  double thetadot0 = zdot*(cos(theta0)*r*sin(theta)+u*v*cos(theta))/sigma;          \n  double phidot0 = zdot*sin(theta0)*sin(phi)/(u*sin(theta));                         \n\n  ydot0[0] = rdot0;\n  ydot0[1] = thetadot0;\n  ydot0[2] = phidot0;\n\n  double sintheta=sin(theta);\n  double sin2 = sintheta*sintheta;\n\n  double r2 = r * r;\n  double delta = r2 - 2.0 * r + a2;\n  double s1 = sigma - 2.0 * r;\n\n  y0[4] = rdot0*sigma/delta;\n  y0[5] = thetadot0*sigma;\n\n  // Eq.7\n  double energy2 = s1*(rdot0*rdot0/delta+thetadot0*thetadot0)\n    + delta*sin2*phidot0*phidot0;\n\n  double energy = sqrt(energy2);\n\n  // rescaled by energy\n  y0[4] = y0[4]/energy;\n  y0[5] = y0[5]/energy;\n\n  // Eq.8 \n  L = ((sigma*delta*phidot0-2.0*A*r*energy)*sin2/s1)/energy;\n\n  // Eq below Eq.15\n  kappa = y0[5]*y0[5]+a2*sin2+L*L/sin2;\n}\n\n__device__ static double rk5(double* Variables, double* VariablesIn, double *y, double *dydx, \n                             double htry, double escal, double *yscal, double *hdid)\n{\n  int i;\n  double hnext;\n  double errmax, h = htry, htemp;\n  double yerr[N], ytemp[N];\n\n  while (1)\n  {\n    // find adaptive step size\n    rkstep(Variables, VariablesIn, y, dydx, h, ytemp, yerr);\n\n    errmax = 0.0;\n    for (i = 0; i < N; i++)\n    {\n      double temp = fabs(yerr[i]/yscal[i]);\n      if (temp > errmax) errmax = temp;\n    }\n\n    errmax *= escal;\n    if (errmax <= 1.0) break;\n\n    htemp = 0.9 * h / sqrt(sqrt(errmax));\n\n    h *= 0.1;\n\n    if (h >= 0.0)\n    {\n      if (htemp > h) h = htemp;\n    }\n    else\n    {\n      if (htemp < h) h = htemp;\n    }\n  }\n\n  if (errmax > 1.89e-4)\n  {\n    hnext = 0.9 * h * pow(errmax, -0.2);\n  }\n  else\n  {\n    hnext = 5.0 * h;\n  }\n\n  *hdid = h;\n\n  memcpy(y, ytemp, N * sizeof(double));\n\n  return hnext;\n}\n\n__device__ double task2fun_GetZ(double* Variables, double* VariablesIn, double *y)\n{\n  double ut,uphi,ur,E_local;\n  double E_inf= -1.0;   \n  double r=y[0];\n  double theta=y[1];\n  double pr=y[4];\n\n  double r2 = r*r;\n  double twor = 2.0*r;\n  double sintheta, costheta;\n  sintheta=sin(theta);\n  costheta=cos(theta);\n  double cos2 = costheta*costheta;\n  double sin2 = sintheta*sintheta;\n\n  double sigma = r2+a2*cos2;\n  double delta = r2-twor+a2;\n  double ssig=(r2+a2)*(r2+a2)-a2*delta*sin2; \n\n  //======define (covariant) metric component    \n  double gtt=-(1.-2.*r/sigma);\n  double gtph=-2.*A*r*sin2/sigma;\n  double grr=sigma/delta;\n  double gphph=ssig*sin2/sigma;    \n\n\n  //==========Keplerian flow: outside ISCO\n  double ut_k  =(r*r+A*sqrt(r))/(r*sqrt(r*r-3.*r+2.*A*sqrt(r)));\n  double ur_k  =0.;\n  double uphi_k  =1./(sqrt(r)*sqrt(r*r-3.*r+2.*A*sqrt(r)));\n\n  //==========Keplerian flow: inside ISCO    \n  if( r<Rmstable)\n  {\n\n    double delta = r*r-2.*r+a2;\n    double lambda=(Rmstable*Rmstable-2.*A*sqrt(Rmstable)+a2)/(sqrt(Rmstable*Rmstable*Rmstable)-2.*sqrt(Rmstable)+A);\n    double gamma=sqrt(1-2./3./Rmstable);\n    double h=(2.*r-A*lambda)/delta;\n\n    ut_k=gamma*(1.+2/r*(1.+h));\n    ur_k=-sqrt(2./3./Rmstable)*sqrt(pow((Rmstable/r-1.),3.));\n    uphi_k=gamma/r/r*(lambda+A*h);\n  }  \n\n  //normalize the four-velocity\n  ut    = ut_k;\n  uphi  = uphi_k;\n  ur    = ur_k;  \n  double omega = uphi/ut;\n  double k0    = -(gtt + omega*omega*gphph + 2.*omega*gtph);\n  ut = sqrt(((1. + grr*ur*ur) / k0));\n  uphi = omega*ut;  \n\n  // compute redshift\n  E_local=-ut+L*uphi+pr*ur;\n  return E_local/E_inf; \n}\n\n__global__ void task2(double*__restrict__ ResultsPixel,\n                      double*__restrict__ VariablesIn,\n                      int GridIdxX, int GridIdxY)\n{\n  if(X1 >= SIZE || Y1 >= SIZE) return;  \n\n  double Variables[VarNUM];\n  r0       = 1000.0;                 \n  theta0   = (PI/180.0) * INCLINATION;     \n  a2       = A * A;                \n  Rhor     = 1.0 + sqrt(1.0 - a2) + 1e-5;   \n  Rmstable = ISCO(VariablesIn);  \n\n  double htry = 0.5, escal = 1e14, hdid = 0.0, hnext = 0.0;\n  double y[N], dydx[N], yscal[N];\n\n  double Rdisk   = 500.;\n  double ima_width = 10.;\n  double s1  = ima_width;                      \n  double s2  = 2.*ima_width/((int)SIZE+1.);   \n  double Jy_corr=Jansky_Correction(VariablesIn,ima_width);\n  double L_corr=Luminosity_Correction(VariablesIn,ima_width);\n\n  grid_x = -s1 + s2*(X1+1.);\n  grid_y = -s1 + s2*(Y1+1.);\n\n  initial(Variables, VariablesIn, y, dydx);\n\n  ResultsPixel(0) = grid_x;\n  ResultsPixel(1) = grid_y;\n  ResultsPixel(2) = 0;\n\n  double ds=0.;  \n  double dtau=0.;\n  double dI=0.;\n\n  while (1)\n  {\n    geodesic(Variables, VariablesIn, y, dydx);\n\n    for (int i = 0; i < N; i++)\n      yscal[i] = fabs(y[i]) + fabs(dydx[i] * htry) + 1.0e-3;\n\n    hnext = rk5(Variables, VariablesIn, y, dydx, htry, escal, yscal, &hdid);\n\n    if ((y[0] > r0) && (dydx[0]>0)){\n      ResultsPixel(2) = dI*freq_obs*freq_obs*freq_obs*L_corr; \n      break;\n    }\n\n    if (y[0] < Rhor){\n      ResultsPixel(2) = dI*freq_obs*freq_obs*freq_obs*L_corr; \n      break;\n    }\n\n    double r=y[0];\n    double theta=y[1];\n\n    if(y[0]<Rdisk){\n\n      double zzz        = task2fun_GetZ(Variables, VariablesIn, y); //zzz=E_em/E_obs\n      double freq_local = freq_obs*zzz;  \n\n      // thermal synchrotron\n      double nth0=3e7;\n      double zc=r*cos(theta);\n      double rc=r*sin(theta);\n\n      double nth=nth0*exp(-zc*zc/2./rc/rc)*pow(r,-1.1);\n      double Te=1.7e11*pow(r,-0.84); \n      double b=sqrt(8.*PI*0.1*nth*C_mp*C_c*C_c/6./r);\n\n      double vb=C_e*b/2./PI/C_me/C_c;\n      double theta_E= C_kB*Te/C_me/C_c/C_c;\n      double v=freq_local;\n      double x=2.*v/3./vb/theta_E/theta_E;\n\n      double K_value=K2(theta_E);\n\n      double comp1=4.*PI*nth*C_e*C_e*v/sqrt(3.)/K_value/C_c;\n      double comp2=4.0505/pow(x,(1./6.))*(1.+0.4/pow(x,0.25)+0.5316/sqrt(x))*exp(-1.8899*pow(x,1./3.));\n      double j_nu=comp1*comp2;\n      double B_nu=2.0*v*v*v*C_h/C_c/C_c/(exp(C_h*v/C_kB/Te)-1.0);\n\n      // integrate intensity along ray\n      ds    =  htry;\n\n      dtau  =  dtau + ds*C_sgrA_mbh*C_rgeo*j_nu/B_nu*zzz;  \n      dI    =  dI + ds*C_sgrA_mbh*C_rgeo*j_nu/freq_local/freq_local/freq_local*exp(-dtau)*zzz;  \n    }\n    htry = hnext;\n  }\n}"
        ]
    },
    "ccs-cuda": {
        "/Users/gbolet/hecbench-roofline/src/ccs-cuda/main.cu": [
            "inline __host__ __device__ float4 make_float4(uint4 a)\n{\n    return make_float4(float(a.x), float(a.y), float(a.z), float(a.w));\n}\n\ninline __host__ __device__ float4 sqrtf(float4 v)\n{\n    return make_float4(sqrtf(v.x), sqrtf(v.y), sqrtf(v.z), sqrtf(v.w));\n}\n\n__device__\nstruct pair_r compute(\n  float *genekj,\n  float *geneij,\n  const char *sample,\n  int wid,int k,int i,int D,\n  const float *gene)\n{\n  int j;\n  float sx = 0.f, sxx = 0.f, sy = 0.f, sxy = 0.f, syy = 0.f;\n  float sx_n = 0.f, sxx_n = 0.f, sy_n = 0.f, sxy_n = 0.f, syy_n = 0.f;\n\n  struct pair_r rval = {0.f, 0.f};\n\n  for (j = 0; j < D; j++) {\n    genekj[j]=gene[k*(D+1)+j];\n    if(sample[j]=='1')\n      sx += genekj[j];\n    else\n      sx_n += genekj[j];\n  }\n\n  sx /= wid;\n  sx_n /= (D-wid);\n\n  for (j = 0; j < D; j++) {\n    if(sample[j]=='1')\n      sxx += (sx-genekj[j]) * (sx-genekj[j]);\n    else\n      sxx_n += (sx_n-genekj[j]) * (sx_n-genekj[j]);\n  }\n\n  sxx = sqrtf(sxx);\n  sxx_n = sqrtf(sxx_n);\n\n  for (j = 0; j < D; j++) {\n    geneij[j]=gene[i*(D+1)+j];\n    if(sample[j]=='1')\n      sy += geneij[j];\n    else\n      sy_n += geneij[j];\n  }\n\n  sy /= wid; \n  sy_n /= (D-wid); \n\n  for (j = 0; j < D; j++)\n  {\n    if(sample[j]=='1') {\n      sxy += (sx - genekj[j]) * (sy - geneij[j]);\n      syy += (sy - geneij[j]) * (sy - geneij[j]);\n    }\n    else {\n      sxy_n += (sx_n - genekj[j]) * (sy_n - geneij[j]);\n      syy_n += (sy_n - geneij[j]) * (sy_n - geneij[j]);\n    }\n  }\n\n  syy = sqrtf(syy);\n  syy_n = sqrtf(syy_n);\n  rval.r = fabsf(sxy/(sxx * syy));\n  rval.n_r = fabsf(sxy_n/(sxx_n * syy_n));\n\n  return rval;\n}\n\n__global__ void compute_bicluster(\n  const float *__restrict__ gene, \n  const int n,\n  const int maxbcn,\n  const int D,\n  const float thr,\n  char *__restrict__ maxbc_sample,\n  char *__restrict__ maxbc_data,\n  float *__restrict__ maxbc_score,\n  int *__restrict__ maxbc_datacount,\n  int *__restrict__ maxbc_samplecount,\n  char *__restrict__ tmpbc_sample,\n  char *__restrict__ tmpbc_data)\n{\n  __shared__ float s_genekj[MAXSAMPLE];\n  __shared__ float s_geneij[MAXSAMPLE];\n  __shared__  char s_vect[3*MAXSAMPLE];\n\n  int k=blockIdx.x*blockDim.x+threadIdx.x;\n\n  if(k<maxbcn) {\n    float jcc,mean_k,mean_i;\n    int i,j,l,vl,wid,wid_0,wid_1,wid_2,l_i,t_tot,t_dif;\n    int dif,tot;\n    struct pair_r rval;\n    int tmpbc_datacount,tmpbc_samplecount;\n\n    float genekj,geneij;\n\n    maxbc_score[k]=1.f;\n    maxbc_datacount[k]=0;  \n\n    //calculate mean expression for gene k\n\n    mean_k=gene[k*(D+1)+D];\n\n    for (i = k+1; i < n; i++) //pair k,i\n    {   \n      //calculate mean expression for gene i\n      mean_i=gene[i*(D+1)+D];\n\n      wid_0=0; wid_1=0; wid_2=0;      \n\n      for (j = 0; j < D; j++)  \n      {\n        genekj=gene[k*(D+1)+j];\n        geneij=gene[i*(D+1)+j];\n\n        if ((genekj - mean_k)>=0 && (geneij - mean_i)>=0) //i and k upregulated : positive correlation\n        {\n          s_vect[0*3+j] = '1';\n          s_vect[1*3+j] = '0';\n          s_vect[2*3+j] = '0';\n          wid_0++;\n        }\n        else if ((genekj - mean_k)<0 && (geneij - mean_i)<0)  // i and k down regulated : positive correlation\n        {\n          s_vect[0*3+j] = '0';\n          s_vect[1*3+j] = '1';\n          s_vect[2*3+j] = '0';\n          wid_1++;\n        }\n        else if ((genekj - mean_k)*(geneij - mean_i)<0) //betwenn i and k one is up regulated and the other one is down regulated : negative correlation\n        {\n          s_vect[0*3+j] = '0';\n          s_vect[1*3+j] = '0';\n          s_vect[2*3+j] = '1';\n          wid_2++;\n        } \n      }\n\n      for (vl = 0; vl < 3; vl++)\n      { \n        dif=0; tot=0;\n        if(vl==0)\n          wid=wid_0; \n        else if(vl==1)\n          wid=wid_1; \n        if(vl==2)\n          wid=wid_2; \n\n        if(wid>minsample) { //minimum samples required to form a bicluster module. Default minimum set to 10 in ccs.h   \n\n          rval=compute(s_genekj, s_geneij, s_vect+vl*MAXSAMPLE, wid, k, i, D, gene);\n        }\n        else {\n          continue;\n        }\n\n        if (rval.r > thr) \n        {\n          tot++;      \n          if(rval.n_r>thr)\n            dif++;\n\n          for (j = 0;j < D; j++)\n            tmpbc_sample[k*D+j] = s_vect[vl*MAXSAMPLE+j];\n\n          for (j = 0;j < n; j++)\n            tmpbc_data[k*n+j] = '0';\n\n          tmpbc_data[k*n+k] = '1';\n          tmpbc_data[k*n+i] = '1';\n          tmpbc_datacount = 2;\n          tmpbc_samplecount = wid;\n\n          for (l = 0; l < n; l++)  { //bicluster augmentation\n            if (l != i && l != k) {\n              t_tot=0; t_dif=0;\n              for(l_i=0;l_i<n;l_i++) {\n                if(tmpbc_data[k*n+l_i]=='1')  {\n                  rval=compute(s_genekj, s_geneij, s_vect + vl*MAXSAMPLE, wid, l, l_i, D, gene);\n\n                  if(rval.r>thr) \n                    t_tot+=1;\n                  else {\n                    t_tot=0;\n                    break;\n                  }   \n                  if(rval.n_r>thr) \n                    t_dif+=1;\n                }  \n              }                                                                    \n\n              if(t_tot>0)  {\n                tmpbc_data[k*n+l] = '1';\n                tmpbc_datacount+=1;\n                tot+=t_tot; dif+=t_dif;\n              }\n            }\n          }  // end of augmentation\n\n          // Compute Jaccard score\n\n          if(tot>0)\n            jcc=(float)dif/tot;   \n          else\n            jcc=1.f; \n\n          /* Select bicluster candidate as the largest (maxbc[k].datacount<tmpbc.datacount) \n             of all condition dependent (jaccard score <0.01) bicluster for k. Minimum number of gene \n             for a bicluster is set at 10. See the mingene at ccs.h */\n\n          if(jcc<0.01f && maxbc_datacount[k]<tmpbc_datacount && tmpbc_datacount>mingene)\n          {\n            maxbc_score[k]=jcc;\n            for (j = 0; j < n; j++)  \n              maxbc_data[k*n+j]=tmpbc_data[k*n+j];\n            for (j = 0; j < D; j++)  \n              maxbc_sample[k*D+j]=tmpbc_sample[k*D+j];\n            maxbc_datacount[k]=tmpbc_datacount;\n            maxbc_samplecount[k]=tmpbc_samplecount;\n          }\n        }    //end of r>thr condition\n      }    //end of loop for vl  \n    }  // end of i loop\n  }\n}"
        ]
    }
}