{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbdfb6e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autogen version: 0.5.7\n",
      "Max SP TFLOP/s with FMA 25.068\n",
      "Max DP TFLOP/s with FMA 0.392\n",
      "Max SP TFLOP/s w/out FMA 12.534\n",
      "Max DP TFLOP/s w/out FMA 0.196\n",
      "Max TINTOP/s 12.534\n",
      "SP Balance Point is at: 32.97 flop/byte\n",
      "DP Balance Point is at: 0.52 flop/byte\n",
      "INT Balance Point is at: 16.49 intop/byte\n",
      "\n",
      "These values get passed as LLM context so the model can infer about rooflines:\n",
      "Peak SP GFLOP/s 25067.52 with FMA\n",
      "Peak DP GFLOP/s 391.68 with FMA\n",
      "Peak GINTOP/s 12533.76 with FMA\n",
      "scraped and pruned CUDA programs count 297\n",
      "scraped and pruned OMP  programs count 242\n"
     ]
    }
   ],
   "source": [
    "from roofline_utils import *\n",
    "from autogen_cuda_static_analysis_graphflow import *\n",
    "from autogen_agentchat.ui import Console\n",
    "\n",
    "import time\n",
    "import argparse\n",
    "\n",
    "import openai\n",
    "import sys\n",
    "\n",
    "import logging\n",
    "from autogen_core import TRACE_LOGGER_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d3a68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(filename='mylog.log', \n",
    "                    level=logging.DEBUG,\n",
    "                    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s\\n')\n",
    "logger = logging.getLogger(TRACE_LOGGER_NAME)\n",
    "logger.addHandler(logging.StreamHandler())\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "345007b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# please create a file called '.llm-api-key' with your api key and no newline characters\n",
    "with open('./.llm-api-key', 'r') as file:\n",
    "    LLM_API_KEY=file.read().strip()\n",
    "\n",
    "with open('./.openrouter-api-key', 'r') as file:\n",
    "    OPENROUTER_API_KEY=file.read().strip()\n",
    "\n",
    "# ### Open the Trin/Val Data CSV Files\n",
    "dtypes['language'] = 'string'\n",
    "dtypes['numTokens'] = np.int64\n",
    "dtypes['kernelCode'] = 'string'\n",
    "dtypes['kernelSASS'] = 'string'\n",
    "dtypes['isBB'] = np.int64\n",
    "dtypes['class'] = 'string'\n",
    "dtypes['answer'] = 'string'\n",
    "\n",
    "reasoning_models = ['o3', 'o1', 'o4']\n",
    "\n",
    "def is_reasoning_model(modelName):\n",
    "    # the last part of the model name is the model name\n",
    "    # example: openai/o3-mini-2024-11-20 is o3-mini-2024-11-20\n",
    "    mName = modelName.split('/')[-1]\n",
    "    for m in reasoning_models:\n",
    "        if m in mName:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1cfaf765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(170, 24)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "trainDF = pd.read_csv('train-dataset-balanced.csv', quotechar='\"', dtype=dtypes)\n",
    "valDF = pd.read_csv('validation-dataset-balanced.csv', quotechar='\"', dtype=dtypes)\n",
    "\n",
    "trainDF['isTrain'] = 1\n",
    "valDF['isTrain'] = 0\n",
    "\n",
    "df = pd.concat([trainDF, valDF], ignore_index=True)\n",
    "\n",
    "# keep only the CUDA codes\n",
    "df = df[df['language'] == 'CUDA']\n",
    "\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b8561b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def create_model_client(modelName, useAzure=False, temp=1.0, topp=0.1, timeout=60, storeLogProbs=False):\n",
    "\n",
    "    model_client = None\n",
    "    logprob_args = {}\n",
    "    temp_topp_args = {}\n",
    "\n",
    "    if not is_reasoning_model(modelName):\n",
    "        temp_topp_args = {'temperature': temp, 'top_p': topp}\n",
    "    else:\n",
    "        if storeLogProbs:\n",
    "            logprob_args = {'logprobs': storeLogProbs, 'top_logprobs': 4}\n",
    "\n",
    "    if useAzure:\n",
    "        model_client = AzureOpenAIChatCompletionClient(\n",
    "                # https://galor-m6d0ej1n-eastus2.cognitiveservices.azure.com/openai/deployments/o1/chat/completions?api-version=2024-12-01-preview\n",
    "                model=modelName,\n",
    "                azure_endpoint='https://galor-m6d0ej1n-eastus2.cognitiveservices.azure.com',\n",
    "                #azure_endpoint='https://galor-m6d0ej1n-eastus2.cognitiveservices.azure.com',\n",
    "                azure_deployment=modelName,\n",
    "                api_key=LLM_API_KEY,\n",
    "                timeout=timeout,\n",
    "                api_version='2025-01-01-preview',\n",
    "                **temp_topp_args,\n",
    "                **logprob_args,\n",
    "                model_info = {'vision':False, 'function_calling':True, 'json_output':True, 'family':'unknown'}\n",
    "        )\n",
    "    else:\n",
    "        model_client = OpenAIChatCompletionClient(\n",
    "                #model='openai/gpt-4o-mini',\n",
    "                #model='openai/gpt-4o-mini-2024-07-18',\n",
    "                #model='google/gemini-2.0-flash-001',\n",
    "                #model='openai/o3-mini',\n",
    "                #model='openai/gpt-4o-2024-11-20',\n",
    "                #model='deepseek/deepseek-r1',\n",
    "                #model='openai/o3-mini-high',\n",
    "                #model='openai/o1-mini-2024-09-12',\n",
    "                model=modelName,\n",
    "                base_url='https://openrouter.ai/api/v1',\n",
    "                api_key=OPENROUTER_API_KEY,\n",
    "                timeout=timeout,\n",
    "                # comment these back in for the non-reasoning models\n",
    "                #top_p = topp,\n",
    "                #temperature=temp,\n",
    "                **temp_topp_args,\n",
    "                **logprob_args,\n",
    "                model_info = {'vision':False, 'function_calling':False, 'json_output':False, 'family':'unknown'}\n",
    "        )\n",
    "\n",
    "    return model_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4f61519",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gbolet/miniconda3/envs/autogen5.7/lib/python3.11/site-packages/autogen_ext/models/openai/_openai_client.py:413: UserWarning: Missing required field 'structured_output' in ModelInfo. This field will be required in a future version of AutoGen.\n",
      "  validate_model_info(self._model_info)\n"
     ]
    }
   ],
   "source": [
    "model_client = await create_model_client(modelName='openai/o4-mini', timeout=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4baa11b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = build_graphflow(model_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8fd103be",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df.iterrows():\n",
    "    #sassCode = row['kernelSASS']\n",
    "\n",
    "    input_args = {\n",
    "         \"source_code\": row['kernelCode'],\n",
    "         \"kernel_name\": row['Kernel Name'],\n",
    "         \"program_name\": row['targetName'],\n",
    "         \"exec_args\": row['exeArgs'],\n",
    "         \"grid_size\": row['Grid Size'],\n",
    "         \"block_size\": row['Block Size'],\n",
    "         \"device\": row['device'],\n",
    "     }\n",
    "\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df170db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#You are an expert CUDA programmer and you are given a CUDA kernel source code.\n",
    "#Your task is to analyze the kernel and provide a detailed report on its performance characteristics.\n",
    "#Please provide your analysis in a structured format.\n",
    "\n",
    "def make_input_prompt(input_args):\n",
    "    prompt = f\"\"\"\n",
    "\n",
    "Kernel Name: {input_args['kernel_name']}\n",
    "Program Name: {input_args['program_name']}\n",
    "Execution Arguments: [{input_args['exec_args']}]\n",
    "Grid Size: {input_args['grid_size']}\n",
    "Block Size: {input_args['block_size']}\n",
    "\n",
    "GPU Hardware Specs:\n",
    " - GPU Name: {input_args['device']}\n",
    " - Compute Capbility: 86\n",
    " - RAM: 10 GB\n",
    " - SM count: 68\n",
    " - Max Bandwidth: 760.3 GB/s\n",
    " - Peak SP GFLOP/s 25067.52 with FMA\n",
    " - Peak DP GFLOP/s 391.68 with FMA\n",
    " - Peak GINTOP/s 12533.76 with FMA\n",
    "\n",
    "\n",
    "Source Code:\n",
    "{input_args['source_code']} \"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "023f35d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = workflow.run_stream(task=make_input_prompt(input_args))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b6f13ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- TextMessage (user) ----------\n",
      "\n",
      "\n",
      "Kernel Name: void resize<unsigned char, 8>(T1 *, unsigned long, int, int, const T1 *, int, int, float, float, bool, bool)\n",
      "Program Name: resize-cuda\n",
      "Execution Arguments: [1920 1080 256 256 512 100]\n",
      "Grid Size: (29184, 1, 1)\n",
      "Block Size: (256, 1, 1)\n",
      "\n",
      "GPU Hardware Specs:\n",
      " - GPU Name: NVIDIA GeForce RTX 3080\n",
      " - Compute Capbility: 86\n",
      " - RAM: 10 GB\n",
      " - SM count: 68\n",
      " - Max Bandwidth: 760.3 GB/s\n",
      " - Peak SP GFLOP/s 25067.52 with FMA\n",
      " - Peak DP GFLOP/s 391.68 with FMA\n",
      " - Peak GINTOP/s 12533.76 with FMA\n",
      "\n",
      "\n",
      "Source Code:\n",
      "-----------------------------------\n",
      "main.cu\n",
      "-----------------------------------\n",
      "#include <stdio.h>\n",
      "#include <stdlib.h>\n",
      "#include <math.h>\n",
      "#include <chrono>\n",
      "#include <cuda.h>\n",
      "\n",
      "template <class T, std::size_t CHANNELS_PER_ITER>\n",
      "__global__\n",
      "void resize (\n",
      "    T *__restrict__ output,\n",
      "    size_t output_size, int out_height, int out_width,\n",
      "    const T *__restrict__ input, int in_height, int in_width,\n",
      "    float o2i_fy, float o2i_fx, bool round, bool half_pixel_centers)\n",
      "{\n",
      "    auto in_image_size = in_height * in_width;\n",
      "    auto out_image_size = out_height * out_width;\n",
      "\n",
      "    /* think of the output and input as a collection of 2d images with the last axis\n",
      "     * representing the width and the last but one axis representing the height\n",
      "     *\n",
      "     * the remaining axis together form a collection of these images/channels\n",
      "     */\n",
      "    auto num_effective_channels = output_size / out_image_size;\n",
      "\n",
      "    /* we process multiple channels every iteration to reuse the identical computation\n",
      "     * involved with the spatial dimensions\n",
      "     *\n",
      "     * if we are processing `CHANNELS_PER_ITER` channels per iteration, we will need\n",
      "     * (num_effective_channels / CHANNELS_PER_ITER) iterations per (x, y) location\n",
      "     */\n",
      "    auto num_channel_iters_per_xy = (num_effective_channels / CHANNELS_PER_ITER);\n",
      "\n",
      "    /* we need `num_channel_iters_per_xy` iterations per (x, y) and there are `out_image_size`\n",
      "     * combinations of (x, y); hence, we'll need `num_channel_iters_per_xy * out_image_size`\n",
      "     * iterations in total to finish the resize operation\n",
      "     */\n",
      "    auto iters_required = num_channel_iters_per_xy * out_image_size;\n",
      "\n",
      "    for (int iter = blockIdx.x * blockDim.x + threadIdx.x;\n",
      "             iter < iters_required; iter += blockDim.x * gridDim.x) {\n",
      "\n",
      "        const int c_start = (iter / out_image_size) * CHANNELS_PER_ITER;\n",
      "\n",
      "        /* note here that consecutive `iter` values will often have consecutive `x` values\n",
      "         * => stores into output will be coalesced across threads\n",
      "         */\n",
      "        const int y = (iter % out_image_size) / out_width;\n",
      "        const int x = iter % out_width;\n",
      "\n",
      "        auto in_yf = half_pixel_centers ? (y + 0.5f) * o2i_fy : y * o2i_fy;\n",
      "        int in_y = round ? lroundf(in_yf) : static_cast<int>(in_yf);\n",
      "\n",
      "        auto in_xf = half_pixel_centers ? (x + 0.5f) * o2i_fx : x * o2i_fx;\n",
      "        int in_x = round ? lroundf(in_xf) : static_cast<int>(in_xf);\n",
      "\n",
      "        in_x = min(in_x, in_width - 1);\n",
      "        in_y = min(in_y, in_height - 1);\n",
      "\n",
      "        int in_idx = c_start * in_image_size + in_y * in_width + in_x;\n",
      "        int out_idx = c_start * out_image_size + y * out_width + x;\n",
      "\n",
      "        for (int i = 0; i < CHANNELS_PER_ITER; i++) {\n",
      "            output[out_idx] = input[in_idx];\n",
      "            in_idx += in_image_size;\n",
      "            out_idx += out_image_size;\n",
      "        }\n",
      "    }\n",
      "}\n",
      "\n",
      "template <class T, std::size_t CHANNELS_PER_ITER>\n",
      "__global__ void resize_bilinear(\n",
      "    T *__restrict__ output,\n",
      "    size_t output_size, int out_height, int out_width,\n",
      "    const T *__restrict__ input, int in_height, int in_width,\n",
      "    float o2i_fy, float o2i_fx, bool half_pixel_centers)\n",
      "{\n",
      "    auto in_image_size = in_height * in_width;\n",
      "    auto out_image_size = out_height * out_width;\n",
      "\n",
      "    /* think of the output and input as a collection of 2d images with the last axis\n",
      "     * representing the width and the last but one axis representing the height\n",
      "     *\n",
      "     * the remaining axis together form a collection of these images/channels\n",
      "     */\n",
      "    auto num_effective_channels = output_size / out_image_size;\n",
      "\n",
      "    /* we process multiple channels every iteration to reuse the identical computation\n",
      "     * involved with the spatial dimensions\n",
      "     *\n",
      "     * if we are processing `CHANNELS_PER_ITER` channels per iteration, we will need\n",
      "     * (num_effective_channels / CHANNELS_PER_ITER) iterations per (x, y) location\n",
      "     */\n",
      "    auto num_channel_iters_per_xy = (num_effective_channels / CHANNELS_PER_ITER);\n",
      "\n",
      "    /* we need `num_channel_iters_per_xy` iterations per (x, y) and there are `out_image_size`\n",
      "     * combinations of (x, y); hence, we'll need `num_channel_iters_per_xy * out_image_size`\n",
      "     * iterations in total to finish the resize operation\n",
      "     */\n",
      "    auto iters_required = num_channel_iters_per_xy * out_image_size;\n",
      "\n",
      "    for (int iter = blockIdx.x * blockDim.x + threadIdx.x;\n",
      "             iter < iters_required; iter += blockDim.x * gridDim.x) {\n",
      "\n",
      "        const int c_start = (iter / out_image_size) * CHANNELS_PER_ITER;\n",
      "        const int c_end = c_start + CHANNELS_PER_ITER;\n",
      "\n",
      "        /* note here that consecutive `iter` values will often have consecutive `x` values\n",
      "         * => stores into output will be coalesced across threads\n",
      "         */\n",
      "        const int y = (iter % out_image_size) / out_width;\n",
      "        const int x = iter % out_width;\n",
      "\n",
      "        auto in_x = half_pixel_centers ? fmaxf((x + 0.5f) * o2i_fx - 0.5f, 0.0f) : x * o2i_fx;\n",
      "        auto in_y = half_pixel_centers ? fmaxf((y + 0.5f) * o2i_fy - 0.5f, 0.0f) : y * o2i_fy;\n",
      "\n",
      "        auto in_x0 = static_cast<int>(in_x);\n",
      "        auto in_x1 = min(in_x0 + 1, in_width - 1);\n",
      "\n",
      "        auto in_y0 = static_cast<int>(in_y);\n",
      "\n",
      "        auto in_y1 = min(in_y0, in_height - 1);\n",
      "        auto in_y2 = min(in_y0 + 1, in_height - 1);\n",
      "\n",
      "        int in_offset_r0 = c_start * in_image_size + in_y1 * in_width;\n",
      "        int in_offset_r1 = c_start * in_image_size + in_y2 * in_width;\n",
      "        int out_idx = c_start * out_image_size + y * out_width + x;\n",
      "\n",
      "        #pragma unroll 1 /* disable unrolling to reduce register pressure; not sure how but it works */\n",
      "        for (auto c = c_start; c < c_end; c++) {\n",
      "            auto v_00 = input[in_offset_r0 + in_x0],\n",
      "                 v_01 = input[in_offset_r0 + in_x1],\n",
      "                 v_10 = input[in_offset_r1 + in_x0],\n",
      "                 v_11 = input[in_offset_r1 + in_x1];\n",
      "\n",
      "            output[out_idx] =\n",
      "                v_00 +\n",
      "                T(in_y - in_y0) * T(v_10 - v_00) +\n",
      "                T(in_x - in_x0) * T(v_01 - v_00) +\n",
      "                T(in_y - in_y0) * T(in_x - in_x0) * T(v_11 - v_01 - v_10 + v_00);\n",
      "\n",
      "            in_offset_r0 += in_image_size;\n",
      "            in_offset_r1 += in_image_size;\n",
      "            out_idx += out_image_size;\n",
      "        }\n",
      "    }\n",
      "}\n",
      "\n",
      "template <class T>\n",
      "void resize_image (\n",
      "  const int in_width,\n",
      "  const int in_height,\n",
      "  const int out_width,\n",
      "  const int out_height,\n",
      "  const int num_channels,\n",
      "  const int repeat,\n",
      "  const bool bilinear = false)\n",
      "{\n",
      "  size_t in_image_size = (size_t)in_height * in_width;\n",
      "  size_t in_size = num_channels * in_image_size;\n",
      "  size_t in_size_bytes = sizeof(T) * in_size;\n",
      "\n",
      "  size_t out_image_size = (size_t)out_height * out_width;\n",
      "  size_t out_size = num_channels * out_image_size;\n",
      "  size_t out_size_bytes = sizeof(T) * out_size;\n",
      "\n",
      "  T* in_images_h = (T*) malloc (in_size_bytes);\n",
      "  T* out_images_h = (T*) malloc (out_size_bytes);\n",
      "\n",
      "  for(size_t i = 0; i < in_size; i++) in_images_h[i] = static_cast<T>((i+1) % 13);\n",
      "\n",
      "  T *in_images_d, *out_images_d;\n",
      "  cudaMalloc((void**)&in_images_d, in_size_bytes);\n",
      "  cudaMemcpy(in_images_d, in_images_h, in_size_bytes, cudaMemcpyHostToDevice);\n",
      "\n",
      "  cudaMalloc((void**)&out_images_d, out_size_bytes);\n",
      "  cudaMemset(out_images_d, 0, out_size_bytes);\n",
      "\n",
      "  const float fx = in_width / out_width;\n",
      "  const float fy = in_height / out_height;\n",
      "\n",
      "  cudaDeviceSynchronize();\n",
      "  auto start = std::chrono::steady_clock::now();\n",
      "\n",
      "  // default grid size is 256 * 114\n",
      "  if (bilinear) {\n",
      "    for (int i = 0; i < repeat; i++) {\n",
      "      resize_bilinear<T, 8> <<<29184, 256>>> (\n",
      "        out_images_d, out_size, out_height, out_width,\n",
      "        in_images_d, in_height, in_width, fx, fy, true);\n",
      "    }\n",
      "  } else {\n",
      "    for (int i = 0; i < repeat; i++) {\n",
      "      resize<T, 8> <<<29184, 256>>> (\n",
      "        out_images_d, out_size, out_height, out_width,\n",
      "        in_images_d, in_height, in_width, fx, fy, true, true);\n",
      "    }\n",
      "  }\n",
      "\n",
      "  cudaDeviceSynchronize();\n",
      "  auto end = std::chrono::steady_clock::now();\n",
      "  auto time = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();\n",
      "  printf(\"Average kernel execution time: %lf (us)    Perf: %lf (GB/s)\\n\",\n",
      "         time * 1e-3 / repeat, (in_size_bytes + out_size_bytes) * repeat * 1.0 / time);\n",
      "  \n",
      "  cudaMemcpy(out_images_h, out_images_d, out_size_bytes, cudaMemcpyDeviceToHost);\n",
      "\n",
      "  cudaFree(in_images_d);\n",
      "  cudaFree(out_images_d);\n",
      "\n",
      "  free(in_images_h);\n",
      "  free(out_images_h);\n",
      "}\n",
      "\n",
      "int main(int argc, char* argv[]) {\n",
      "  if (argc != 7) {\n",
      "    printf(\"Usage: %s <input image width> <input image height>\\n\", argv[0]);\n",
      "    printf(\"          <output image width> <output image height>\\n\");\n",
      "    printf(\"          <image channels> <repeat>\\n\");\n",
      "    return 1;\n",
      "  }\n",
      "\n",
      "  const int in_width = atoi(argv[1]);\n",
      "  const int in_height = atoi(argv[2]);\n",
      "  const int out_width = atoi(argv[3]);\n",
      "  const int out_height = atoi(argv[4]);\n",
      "  const int num_channels = atoi(argv[5]);\n",
      "  const int repeat = atoi(argv[6]);\n",
      "\n",
      "  printf(\"Resize %d images from (%d x %d) to (%d x %d)\\n\",\n",
      "          num_channels, in_width, in_height, out_width, out_height);\n",
      "\n",
      "  printf(\"\\nThe size of each pixel is 1 byte\\n\");\n",
      "  resize_image<unsigned char>(in_width, in_height, out_width, out_height, num_channels, repeat);\n",
      "  printf(\"\\nBilinear resizing\\n\");\n",
      "  resize_image<unsigned char>(in_width, in_height, out_width, out_height, num_channels, repeat, true);\n",
      "\n",
      "  printf(\"\\nThe size of each pixel is 2 bytes\\n\");\n",
      "  resize_image<unsigned short>(in_width, in_height, out_width, out_height, num_channels, repeat);\n",
      "  printf(\"\\nBilinear resizing\\n\");\n",
      "  resize_image<unsigned short>(in_width, in_height, out_width, out_height, num_channels, repeat, true);\n",
      "\n",
      "  printf(\"\\nThe size of each pixel is 4 bytes\\n\");\n",
      "  resize_image<unsigned int>(in_width, in_height, out_width, out_height, num_channels, repeat);\n",
      "  printf(\"\\nBilinear resizing\\n\");\n",
      "  resize_image<unsigned int>(in_width, in_height, out_width, out_height, num_channels, repeat, true);\n",
      "\n",
      "  return 0;\n",
      "}\n",
      "\n",
      "\n",
      " \n",
      "---------- TextMessage (DummyInitialRequestAgent) ----------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gbolet/miniconda3/envs/autogen5.7/lib/python3.11/site-packages/autogen_agentchat/agents/_assistant_agent.py:955: UserWarning: Resolved model mismatch: openai/o4-mini != o4-mini-2025-04-16. Model mapping in autogen_ext.models.openai may be incorrect. Set the model to o4-mini-2025-04-16 to enhance token/cost estimation and suppress this warning.\n",
      "  model_result = await model_client.create(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- TextMessage (DummyInitialRequestAgent) ----------\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "RuntimeError: No available speakers found.\nTraceback:\nTraceback (most recent call last):\n\n  File \"/Users/gbolet/miniconda3/envs/autogen5.7/lib/python3.11/site-packages/autogen_agentchat/teams/_group_chat/_base_group_chat_manager.py\", line 165, in handle_agent_response\n    speaker_name = await speaker_name_future\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/Users/gbolet/miniconda3/envs/autogen5.7/lib/python3.11/site-packages/autogen_agentchat/teams/_group_chat/_graph/_digraph_group_chat.py\", line 357, in select_speaker\n    raise RuntimeError(\"No available speakers found.\")\n\nRuntimeError: No available speakers found.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m Console(result)\n",
      "File \u001b[0;32m~/miniconda3/envs/autogen5.7/lib/python3.11/site-packages/autogen_agentchat/ui/_console.py:117\u001b[0m, in \u001b[0;36mConsole\u001b[0;34m(stream, no_inline_images, output_stats, user_input_manager)\u001b[0m\n\u001b[1;32m    113\u001b[0m last_processed: Optional[T] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    115\u001b[0m streaming_chunks: List[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 117\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m stream:\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(message, TaskResult):\n\u001b[1;32m    119\u001b[0m         duration \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[0;32m~/miniconda3/envs/autogen5.7/lib/python3.11/site-packages/autogen_agentchat/teams/_group_chat/_base_group_chat.py:518\u001b[0m, in \u001b[0;36mBaseGroupChat.run_stream\u001b[0;34m(self, task, cancellation_token)\u001b[0m\n\u001b[1;32m    514\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(message, GroupChatTermination):\n\u001b[1;32m    515\u001b[0m     \u001b[38;5;66;03m# If the message contains an error, we need to raise it here.\u001b[39;00m\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;66;03m# This will stop the team and propagate the error.\u001b[39;00m\n\u001b[1;32m    517\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m message\u001b[38;5;241m.\u001b[39merror \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 518\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;28mstr\u001b[39m(message\u001b[38;5;241m.\u001b[39merror))\n\u001b[1;32m    519\u001b[0m     stop_reason \u001b[38;5;241m=\u001b[39m message\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\n\u001b[1;32m    520\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: RuntimeError: No available speakers found.\nTraceback:\nTraceback (most recent call last):\n\n  File \"/Users/gbolet/miniconda3/envs/autogen5.7/lib/python3.11/site-packages/autogen_agentchat/teams/_group_chat/_base_group_chat_manager.py\", line 165, in handle_agent_response\n    speaker_name = await speaker_name_future\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/Users/gbolet/miniconda3/envs/autogen5.7/lib/python3.11/site-packages/autogen_agentchat/teams/_group_chat/_graph/_digraph_group_chat.py\", line 357, in select_speaker\n    raise RuntimeError(\"No available speakers found.\")\n\nRuntimeError: No available speakers found.\n"
     ]
    }
   ],
   "source": [
    "await Console(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autogen5.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
