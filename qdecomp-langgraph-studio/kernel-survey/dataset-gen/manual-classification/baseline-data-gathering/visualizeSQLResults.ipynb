{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2ab1652d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: OPENAI_API_KEY=this-doesnt-matter\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "import sqlite3\n",
    "from pprint import pprint\n",
    "\n",
    "from io_cost import get_io_cost\n",
    "\n",
    "# im being really lazy here, just want to get something working\n",
    "%env OPENAI_API_KEY=this-doesnt-matter\n",
    "from agent import make_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397be559",
   "metadata": {},
   "source": [
    "# Baseline Experiments Results Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "483998a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing [openai-gpt-4.1-mini:fullPrompt:openrouter.sqlite]\n"
     ]
    }
   ],
   "source": [
    "db_dir = './checkpoints/'\n",
    "\n",
    "# get all the sqlite files in the db_dir\n",
    "sqlite_files = [f for f in os.listdir(db_dir) if f.endswith('.sqlite')]\n",
    "\n",
    "for sqlite_file in sqlite_files:\n",
    "    print(f\"Processing [{sqlite_file}]\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d746b68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "eaf90cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_thread_ids_from_sqlite(full_path: str) -> list[str]:\n",
    "    print(f\"Processing [{full_path}]\")\n",
    "    # open the sqlite file and read the column of thread_id\n",
    "    conn = sqlite3.connect(full_path)\n",
    "    cursor = conn.cursor()\n",
    "    # list the tables within the file\n",
    "    #cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "    #tables = cursor.fetchall()\n",
    "    #print(f\"Tables in the database: {tables}\")\n",
    "    \n",
    "    # there are duplicate thead_ids, we want the unique ones\n",
    "    cursor.execute(\"SELECT DISTINCT thread_id FROM checkpoints\")\n",
    "    thread_ids = cursor.fetchall()\n",
    "\n",
    "    unique_thread_ids = [str(id[0]) for id in thread_ids]\n",
    "\n",
    "    #print(thread_ids)\n",
    "    conn.close()\n",
    "    #print(f\"Found {len(thread_ids)} threads\")\n",
    "    return unique_thread_ids\n",
    "\n",
    "\n",
    "def split_thread_id_to_parts(thread_id: str) -> dict[str, str]:\n",
    "    # split the thread_id into model_name, prompt_type, provider_name\n",
    "    parts = thread_id.split(':')\n",
    "    if len(parts) != 10:\n",
    "        raise ValueError(f\"Invalid thread_id format: {thread_id}\")\n",
    "\n",
    "    # the different parts are:\n",
    "    # combined_name, model name, provider url, trial number, prompt type, variant type, nnz_flop_state, top_p, temp\n",
    "    to_return = {\n",
    "        'combined_name': parts[0],\n",
    "        'model_name': parts[1],\n",
    "        'provider': parts[2] + parts[3],\n",
    "        'trial_number': parts[4],\n",
    "        'prompt_type': parts[5],\n",
    "        'variant_type': parts[6],\n",
    "        'nnz_flop_state': parts[7],\n",
    "        'top_p': parts[8],\n",
    "        'temp': parts[9],\n",
    "    }\n",
    "    return to_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d19d3906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing [./checkpoints/openai-gpt-4.1-mini:fullPrompt:openrouter.sqlite]\n",
      "thread_id: (babelstream-cuda, init_kernel):openai/gpt-4.1-mini:https://openrouter.com/api/v1:1:full:OG:Zero SP + DP FLOP:0.1:0.2 has multiple cost values: [0.0023216, 0.0021408]\n",
      "\t input_tokens: [4648, 4648]\n",
      "\t output_tokens: [289, 176]\n",
      "tasks\n",
      "()\n",
      "metadata\n",
      "{'llm': 'openai',\n",
      " 'opr_model': 'openai/gpt-4.1-mini',\n",
      " 'opr_provider_api_key': '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',\n",
      " 'opr_provider_url': 'https://openrouter.com/api/v1',\n",
      " 'opr_temp': 0.2,\n",
      " 'opr_timeout': 120,\n",
      " 'opr_top_p': 0.1,\n",
      " 'parents': {},\n",
      " 'prompt_type': 'full',\n",
      " 'source': 'loop',\n",
      " 'step': 6,\n",
      " 'thread_id': '(babelstream-cuda, '\n",
      "              'init_kernel):openai/gpt-4.1-mini:https://openrouter.com/api/v1:1:full:OG:Zero '\n",
      "              'SP + DP FLOP:0.1:0.2',\n",
      " 'verbose_printing': True}\n",
      "--------------------------------------------\n",
      "\n",
      "thread_id: (babelstream-cuda, init_kernel):openai/gpt-4.1-mini:https://openrouter.com/api/v1:2:full:OG:Zero SP + DP FLOP:0.1:0.2 has multiple cost values: [0.0021008, 0.0020896]\n",
      "\t input_tokens: [4648, 4648]\n",
      "\t output_tokens: [151, 144]\n",
      "tasks\n",
      "()\n",
      "metadata\n",
      "{'llm': 'openai',\n",
      " 'opr_model': 'openai/gpt-4.1-mini',\n",
      " 'opr_provider_api_key': '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',\n",
      " 'opr_provider_url': 'https://openrouter.com/api/v1',\n",
      " 'opr_temp': 0.2,\n",
      " 'opr_timeout': 120,\n",
      " 'opr_top_p': 0.1,\n",
      " 'parents': {},\n",
      " 'prompt_type': 'full',\n",
      " 'source': 'loop',\n",
      " 'step': 6,\n",
      " 'thread_id': '(babelstream-cuda, '\n",
      "              'init_kernel):openai/gpt-4.1-mini:https://openrouter.com/api/v1:2:full:OG:Zero '\n",
      "              'SP + DP FLOP:0.1:0.2',\n",
      " 'verbose_printing': True}\n",
      "--------------------------------------------\n",
      "\n",
      "thread_id: (babelstream-cuda, init_kernel):openai/gpt-4.1-mini:https://openrouter.com/api/v1:3:full:OG:Zero SP + DP FLOP:0.1:0.2 has multiple cost values: [0.0021968, 0.0020768]\n",
      "\t input_tokens: [4648, 4648]\n",
      "\t output_tokens: [211, 136]\n",
      "tasks\n",
      "()\n",
      "metadata\n",
      "{'llm': 'openai',\n",
      " 'opr_model': 'openai/gpt-4.1-mini',\n",
      " 'opr_provider_api_key': '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',\n",
      " 'opr_provider_url': 'https://openrouter.com/api/v1',\n",
      " 'opr_temp': 0.2,\n",
      " 'opr_timeout': 120,\n",
      " 'opr_top_p': 0.1,\n",
      " 'parents': {},\n",
      " 'prompt_type': 'full',\n",
      " 'source': 'loop',\n",
      " 'step': 6,\n",
      " 'thread_id': '(babelstream-cuda, '\n",
      "              'init_kernel):openai/gpt-4.1-mini:https://openrouter.com/api/v1:3:full:OG:Zero '\n",
      "              'SP + DP FLOP:0.1:0.2',\n",
      " 'verbose_printing': True}\n",
      "--------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame()\n",
    "\n",
    "for sqlite_file in sqlite_files:\n",
    "    full_path = os.path.join(db_dir, sqlite_file)\n",
    "    thread_ids = get_thread_ids_from_sqlite(full_path)\n",
    "    graph = make_graph(full_path)\n",
    "\n",
    "    for thread_id in thread_ids:\n",
    "        #print(f\"Processing thread_id [{thread_id}]\")\n",
    "        parts = split_thread_id_to_parts(thread_id)\n",
    "\n",
    "        # get the state for the thread_id\n",
    "        config = {'configurable': {'thread_id': thread_id}}\n",
    "        state = graph.get_state(config)\n",
    "        #print(dir(state))\n",
    "        #print(state.next)\n",
    "\n",
    "        full_final_state_dict = parts | state.values\n",
    "\n",
    "        state_keys = list(state.values.keys())\n",
    "\n",
    "        # if there is no next state and no tasks left, then we had a successful execution\n",
    "        if (not state.next) and (not state.tasks):\n",
    "            full_final_state_dict['error'] = 'Success'\n",
    "            full_final_state_dict['state_of_failure'] = None\n",
    "            assert 'raw_flop_counts' in state_keys, f\"raw_flop_counts not in state keys: {state_keys} -- this was supposed to be a successful run...\"\n",
    "\n",
    "            if len(state.values['total_cost']) != 1:\n",
    "                print(f'thread_id: {thread_id} has multiple cost values: {state.values[\"total_cost\"]}')\n",
    "                print(f'\\t input_tokens: {state.values[\"input_tokens\"]}')\n",
    "                print(f'\\t output_tokens: {state.values[\"output_tokens\"]}')\n",
    "                print('tasks')\n",
    "                pprint(state.tasks)\n",
    "                print('metadata')\n",
    "                pprint(state.metadata)\n",
    "                print('--------------------------------------------\\n')\n",
    "        else:\n",
    "            # we had an error, it should be written in the tasks error\n",
    "            failure_task = state.tasks[0]\n",
    "            error = str(failure_task.error)\n",
    "            #print(state.tasks)\n",
    "            if 'JSONDecodeError' in error:\n",
    "                error = 'JSONDecodeError'\n",
    "            elif 'Invalid JSON' in error:\n",
    "                error = 'Invalid JSON'\n",
    "            full_final_state_dict['error'] = error\n",
    "            full_final_state_dict['state_of_failure'] = failure_task.name\n",
    "\n",
    "        #print(\"thread_id:\", thread_id)\n",
    "        dict_df = pd.DataFrame([full_final_state_dict])\n",
    "        df = pd.concat([df, dict_df], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "43616fd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['combined_name', 'model_name', 'provider', 'trial_number',\n",
      "       'prompt_type', 'variant_type', 'nnz_flop_state', 'top_p', 'temp',\n",
      "       'source_code', 'kernel_name', 'exec_args', 'grid_size', 'block_size',\n",
      "       'total_num_threads', 'empirical_sp_flop_count',\n",
      "       'empirical_dp_flop_count', 'raw_flop_counts', 'predicted_sp_flop_count',\n",
      "       'predicted_dp_flop_count', 'predicted_sp_flop_count_explanation',\n",
      "       'predicted_dp_flop_count_explanation', 'input_tokens', 'output_tokens',\n",
      "       'total_cost', 'error', 'state_of_failure', 'generic_model_name'],\n",
      "      dtype='object')\n",
      "                         combined_name           model_name  \\\n",
      "0    (ace-cuda, boundaryConditionsPhi)  openai/gpt-4.1-mini   \n",
      "1    (ace-cuda, boundaryConditionsPhi)  openai/gpt-4.1-mini   \n",
      "2    (ace-cuda, boundaryConditionsPhi)  openai/gpt-4.1-mini   \n",
      "3      (ace-cuda, boundaryConditionsU)  openai/gpt-4.1-mini   \n",
      "4      (ace-cuda, boundaryConditionsU)  openai/gpt-4.1-mini   \n",
      "..                                 ...                  ...   \n",
      "622     (word2vec-cuda, device_memset)  openai/gpt-4.1-mini   \n",
      "623     (word2vec-cuda, device_memset)  openai/gpt-4.1-mini   \n",
      "624     (zoom-cuda, zoom_out_edge_pad)  openai/gpt-4.1-mini   \n",
      "625     (zoom-cuda, zoom_out_edge_pad)  openai/gpt-4.1-mini   \n",
      "626     (zoom-cuda, zoom_out_edge_pad)  openai/gpt-4.1-mini   \n",
      "\n",
      "                         provider trial_number prompt_type     variant_type  \\\n",
      "0    https//openrouter.com/api/v1            1        full               OG   \n",
      "1    https//openrouter.com/api/v1            2        full               OG   \n",
      "2    https//openrouter.com/api/v1            3        full               OG   \n",
      "3    https//openrouter.com/api/v1            1        full               OG   \n",
      "4    https//openrouter.com/api/v1            1        full  Swapped FP Type   \n",
      "..                            ...          ...         ...              ...   \n",
      "622  https//openrouter.com/api/v1            2        full               OG   \n",
      "623  https//openrouter.com/api/v1            3        full               OG   \n",
      "624  https//openrouter.com/api/v1            1        full               OG   \n",
      "625  https//openrouter.com/api/v1            2        full               OG   \n",
      "626  https//openrouter.com/api/v1            3        full               OG   \n",
      "\n",
      "        nnz_flop_state top_p temp  \\\n",
      "0    Zero SP + DP FLOP   0.1  0.2   \n",
      "1    Zero SP + DP FLOP   0.1  0.2   \n",
      "2    Zero SP + DP FLOP   0.1  0.2   \n",
      "3     Non-zero DP FLOP   0.1  0.2   \n",
      "4     Non-zero SP FLOP   0.1  0.2   \n",
      "..                 ...   ...  ...   \n",
      "622  Zero SP + DP FLOP   0.1  0.2   \n",
      "623  Zero SP + DP FLOP   0.1  0.2   \n",
      "624  Zero SP + DP FLOP   0.1  0.2   \n",
      "625  Zero SP + DP FLOP   0.1  0.2   \n",
      "626  Zero SP + DP FLOP   0.1  0.2   \n",
      "\n",
      "                                           source_code  ...  \\\n",
      "0    -----------------------------------\\nreference...  ...   \n",
      "1    -----------------------------------\\nreference...  ...   \n",
      "2    -----------------------------------\\nreference...  ...   \n",
      "3    -----------------------------------\\nreference...  ...   \n",
      "4    -----------------------------------\\nreference...  ...   \n",
      "..                                                 ...  ...   \n",
      "622  -----------------------------------\\nword2vec....  ...   \n",
      "623  -----------------------------------\\nword2vec....  ...   \n",
      "624  -----------------------------------\\nmain.cu\\n...  ...   \n",
      "625  -----------------------------------\\nmain.cu\\n...  ...   \n",
      "626  -----------------------------------\\nmain.cu\\n...  ...   \n",
      "\n",
      "    predicted_sp_flop_count predicted_dp_flop_count  \\\n",
      "0              0.000000e+00            1.152000e+12   \n",
      "1              0.000000e+00            1.152000e+12   \n",
      "2              0.000000e+00            1.152000e+12   \n",
      "3              0.000000e+00            1.228800e+12   \n",
      "4              1.022976e+11            0.000000e+00   \n",
      "..                      ...                     ...   \n",
      "622            1.596928e+11            0.000000e+00   \n",
      "623            1.596928e+09            0.000000e+00   \n",
      "624            0.000000e+00            0.000000e+00   \n",
      "625            0.000000e+00            0.000000e+00   \n",
      "626            0.000000e+00            0.000000e+00   \n",
      "\n",
      "                   predicted_sp_flop_count_explanation  \\\n",
      "0    The kernel and all related computations use do...   \n",
      "1    The kernel and all device functions use double...   \n",
      "2    The kernel and all related computations use do...   \n",
      "3    The kernel and all device functions use double...   \n",
      "4    The kernel boundaryConditionsU is launched wit...   \n",
      "..                                                 ...   \n",
      "622  The kernel device_memset is launched with grid...   \n",
      "623  The kernel device_memset is launched with grid...   \n",
      "624  The kernel zoom_out_edge_pad performs only con...   \n",
      "625  The kernel zoom_out_edge_pad performs only con...   \n",
      "626  The kernel zoom_out_edge_pad performs only con...   \n",
      "\n",
      "                   predicted_dp_flop_count_explanation input_tokens  \\\n",
      "0    The kernel boundaryConditionsPhi performs no f...       [8720]   \n",
      "1    The kernel boundaryConditionsPhi is launched w...       [8720]   \n",
      "2    The target kernel is boundaryConditionsPhi, wh...       [8720]   \n",
      "3    The target kernel is boundaryConditionsU(doubl...       [8722]   \n",
      "4    The kernel boundaryConditionsU only performs c...       [8722]   \n",
      "..                                                 ...          ...   \n",
      "622  The device_memset kernel only sets array eleme...       [9780]   \n",
      "623  The device_memset kernel only sets each elemen...       [9780]   \n",
      "624  The kernel zoom_out_edge_pad uses only single-...       [5034]   \n",
      "625  The kernel zoom_out_edge_pad uses only single-...       [5034]   \n",
      "626  The kernel zoom_out_edge_pad uses only single-...       [5034]   \n",
      "\n",
      "     output_tokens               total_cost    error  state_of_failure  \\\n",
      "0            [363]              [0.0040688]  Success              None   \n",
      "1            [457]              [0.0042192]  Success              None   \n",
      "2            [458]              [0.0042208]  Success              None   \n",
      "3            [634]   [0.004503200000000001]  Success              None   \n",
      "4            [341]  [0.0040344000000000005]  Success              None   \n",
      "..             ...                      ...      ...               ...   \n",
      "622          [599]              [0.0048704]  Success              None   \n",
      "623          [251]   [0.004313600000000001]  Success              None   \n",
      "624          [143]  [0.0022424000000000003]  Success              None   \n",
      "625          [122]  [0.0022088000000000003]  Success              None   \n",
      "626          [157]  [0.0022648000000000004]  Success              None   \n",
      "\n",
      "     generic_model_name  \n",
      "0          gpt-4.1-mini  \n",
      "1          gpt-4.1-mini  \n",
      "2          gpt-4.1-mini  \n",
      "3          gpt-4.1-mini  \n",
      "4          gpt-4.1-mini  \n",
      "..                  ...  \n",
      "622        gpt-4.1-mini  \n",
      "623        gpt-4.1-mini  \n",
      "624        gpt-4.1-mini  \n",
      "625        gpt-4.1-mini  \n",
      "626        gpt-4.1-mini  \n",
      "\n",
      "[627 rows x 28 columns]\n"
     ]
    }
   ],
   "source": [
    "# print the dataframe data\n",
    "\n",
    "df['generic_model_name'] = df['model_name'].apply(lambda x: x.split('/')[1] if '/' in x else x)\n",
    "\n",
    "print(df.columns)\n",
    "print(df[df['error'] != 'None'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7fc81272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['combined_name', 'model_name', 'provider', 'trial_number',\n",
      "       'prompt_type', 'variant_type', 'nnz_flop_state', 'top_p', 'temp',\n",
      "       'source_code', 'kernel_name', 'exec_args', 'grid_size', 'block_size',\n",
      "       'total_num_threads', 'empirical_sp_flop_count',\n",
      "       'empirical_dp_flop_count', 'raw_flop_counts', 'predicted_sp_flop_count',\n",
      "       'predicted_dp_flop_count', 'predicted_sp_flop_count_explanation',\n",
      "       'predicted_dp_flop_count_explanation', 'input_tokens', 'output_tokens',\n",
      "       'total_cost', 'error', 'state_of_failure', 'generic_model_name'],\n",
      "      dtype='object')\n",
      "DF Size: (627, 28)\n",
      "error\n",
      "Success            624\n",
      "JSONDecodeError      2\n",
      "Invalid JSON         1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)\n",
    "\n",
    "print(f'DF Size: {df.shape}')\n",
    "\n",
    "# print the df datatypes\n",
    "# print(df.dtypes)\n",
    "\n",
    "print(df.error.value_counts( dropna=False ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21b5f8b",
   "metadata": {},
   "source": [
    "## Adjust some of the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5c931f2c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Expected a single cost value, got: [0.0023216, 0.0021408]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(cost_list) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected a single cost value, got: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcost_list\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cost_list[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m----> 5\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal_cost\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtotal_cost\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparse_total_cost\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotalQueryTime\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotalQueryTime\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mfloat\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#def parse_error(x):\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#    if pd.isna(x):\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m#        return \"Success\"\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m#df['error'] = df['error'].apply(parse_error)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/autogen5.7/lib/python3.11/site-packages/pandas/core/series.py:4924\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4800\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4918\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4922\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m-> 4924\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/autogen5.7/lib/python3.11/site-packages/pandas/core/apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[1;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[0;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/autogen5.7/lib/python3.11/site-packages/pandas/core/apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[1;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[1;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[1;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[1;32m   1509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/miniconda3/envs/autogen5.7/lib/python3.11/site-packages/pandas/core/base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[1;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[0;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/autogen5.7/lib/python3.11/site-packages/pandas/core/algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[1;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[1;32m   1747\u001b[0m     )\n",
      "File \u001b[0;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn[37], line 2\u001b[0m, in \u001b[0;36mparse_total_cost\u001b[0;34m(cost_list)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mparse_total_cost\u001b[39m(cost_list):\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(cost_list) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected a single cost value, got: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcost_list\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cost_list[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mAssertionError\u001b[0m: Expected a single cost value, got: [0.0023216, 0.0021408]"
     ]
    }
   ],
   "source": [
    "def parse_total_cost(cost_list):\n",
    "    assert len(cost_list) == 1, f\"Expected a single cost value, got: {cost_list}\"\n",
    "    return cost_list[0]\n",
    "\n",
    "df['total_cost'] = df['total_cost'].apply(parse_total_cost)\n",
    "df['totalQueryTime'] = df['totalQueryTime'].astype(float)\n",
    "\n",
    "#def parse_error(x):\n",
    "#    if pd.isna(x):\n",
    "#        return \"Success\"\n",
    "#    elif \"Invalid JSON\" in str(x):\n",
    "#        return \"Invalid\\nJSON\"\n",
    "#    elif \"NoneType\" in str(x):\n",
    "#        return \"NoneType\\nReturned\"\n",
    "#    elif \"TIMEOUT\" in str(x):\n",
    "#        return \"Query\\nTimeout\"\n",
    "#    return str(x)\n",
    "#\n",
    "#df['error'] = df['error'].apply(parse_error)\n",
    "df['has_nz_flops'] = df['nnz_flop_state'].apply(lambda x: 'No' if x == 'Zero SP + DP FLOP' else 'Yes')\n",
    "\n",
    "print(df['error'].value_counts())\n",
    "print(df[df['error'] != 'Success'])\n",
    "\n",
    "\n",
    "# percent difference, we add a small epsilon to avoid division by zero\n",
    "df['percent_diff_sp'] = 100*(df['predicted_sp_flop_count'] - df['empirical_sp_flop_count']) / (df['empirical_sp_flop_count'] + 1e-9)\n",
    "df['percent_diff_dp'] = 100*(df['predicted_dp_flop_count'] - df['empirical_dp_flop_count']) / (df['empirical_dp_flop_count'] + 1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2886d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we want to plot the prediction accuracy vs. the ground truth for each generic_model_name and prompt_type\n",
    "\n",
    "def plot_predictions_for_model_name(df, model_name='gpt-5-mini', prompt_type='simple'):\n",
    "    sub_df = df[(df['generic_model_name'] == model_name) & (df['prompt_type'] == prompt_type)]\n",
    "    success_df = sub_df[(sub_df['error'] == 'Success')]\n",
    "\n",
    "    print(f'Success DF Size: {success_df.shape}')\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(5, 4), dpi=200)\n",
    "\n",
    "    max_sp_dp_flop = max(\n",
    "        success_df['empirical_sp_flop_count'].max(),\n",
    "        success_df['empirical_dp_flop_count'].max(),\n",
    "        success_df['predicted_sp_flop_count'].max(),\n",
    "        success_df['predicted_dp_flop_count'].max()\n",
    "    )\n",
    "    min_sp_dp_flop = min(\n",
    "        success_df['empirical_sp_flop_count'].min(),\n",
    "        success_df['empirical_dp_flop_count'].min(),\n",
    "        success_df['predicted_sp_flop_count'].min(),\n",
    "        success_df['predicted_dp_flop_count'].min()\n",
    "    )\n",
    "\n",
    "    # plot the x-y line on the plot\n",
    "    ax.plot([min_sp_dp_flop, max_sp_dp_flop], [min_sp_dp_flop, max_sp_dp_flop], color='red', linestyle='--', alpha=0.5, label='perfect prediction')\n",
    "\n",
    "    sns.scatterplot(x='empirical_sp_flop_count', y='predicted_sp_flop_count', data=success_df, label='SP FLOP', alpha=0.5, ax=ax, color='orange')\n",
    "    sns.scatterplot(x='empirical_dp_flop_count', y='predicted_dp_flop_count', data=success_df, label='DP FLOP', alpha=0.5, ax=ax, color='blue')\n",
    "\n",
    "    ax.set_xscale('symlog')\n",
    "    ax.set_yscale('symlog')\n",
    "\n",
    "    ax.set_title(f'Baseline: LLM-Predicted vs Actual FLOP Counts \\n({model_name} -- {prompt_type} prompt)')\n",
    "\n",
    "    ax.set_xlabel('Actual/Profiled FLOP Count')\n",
    "    ax.set_ylabel('LLM-Predicted FLOP Count')\n",
    "    # change x ticks font size\n",
    "    ax.tick_params(axis='x', labelsize=6)\n",
    "    # adjust the legend font size to be smaller\n",
    "    ax.legend(fontsize=6)\n",
    "\n",
    "    # put the legend outside the plot\n",
    "    ax.legend(bbox_to_anchor=(1.01, 0.65), loc='upper left', fontsize=6)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    ##############################################\n",
    "    # let's print the CDF of the percent differences for SP and DP FLOP counts\n",
    "\n",
    "    nnz_sp_data = success_df['percent_diff_sp']\n",
    "    nnz_dp_data = success_df['percent_diff_dp']\n",
    "\n",
    "    # let's make a cdf plot\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(6, 4), dpi=200)\n",
    "\n",
    "    sns.ecdfplot(nnz_sp_data, label='SP FLOP', ax=ax, color='orange')\n",
    "    sns.ecdfplot(nnz_dp_data, label='DP FLOP', ax=ax, color='blue')\n",
    "\n",
    "    ax.set_xscale('symlog')\n",
    "    #ax.set_yscale('log')\n",
    "    ax.set_xlabel('Percent Difference (%)')\n",
    "    ax.set_ylabel('Proportion of Data <= X')\n",
    "    ax.legend()\n",
    "    ax.set_title(f'CDF of Percent Differences \\n({model_name} -- {prompt_type} prompt)')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # what this plot tells us is that we care mostly about the <-1 % and >+1% predictions\n",
    "    percents = [1, 5, 10, 50, 100]\n",
    "    for perc in percents:\n",
    "        num_sp_within_xpct = ((nnz_sp_data > -perc) & (nnz_sp_data < perc)).sum()\n",
    "        num_sp_outside_xpct = ((nnz_sp_data <= -perc) | (nnz_sp_data >= perc)).sum()\n",
    "\n",
    "        num_dp_within_xpct = ((nnz_dp_data > -perc) & (nnz_dp_data < perc)).sum()\n",
    "        num_dp_outside_xpct = ((nnz_dp_data <= -perc) | (nnz_dp_data >= perc)).sum()\n",
    "\n",
    "        total_sp = nnz_sp_data.shape[0]\n",
    "        total_dp = nnz_dp_data.shape[0]\n",
    "\n",
    "        print(f'SP FLOP: {num_sp_within_xpct} / {total_sp} ({100*num_sp_within_xpct/total_sp:.2f}%) within +/- {perc}%')\n",
    "        print(f'SP FLOP: {num_sp_outside_xpct} / {total_sp} ({100*num_sp_outside_xpct/total_sp:.2f}%) outside +/- {perc}%')\n",
    "        print(f'DP FLOP: {num_dp_within_xpct} / {total_dp} ({100*num_dp_within_xpct/total_dp:.2f}%) within +/- {perc}%')\n",
    "        print(f'DP FLOP: {num_dp_outside_xpct} / {total_dp} ({100*num_dp_outside_xpct/total_dp:.2f}%) outside +/- {perc}%')\n",
    "        print(\"\\n\")\n",
    "    ##############################################\n",
    "\n",
    "\n",
    "    # let's make histograms of the total_cost, totalQueryTime, and failure cases\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(15, 5), dpi=200)\n",
    "\n",
    "    sns.histplot(data=success_df, x='total_cost', hue='has_nz_flops', bins=50, ax=axs[0])\n",
    "    axs[0].set_title('Query Cost Distribution')\n",
    "    axs[0].set_xlabel('Query Cost ($ USD)')\n",
    "    axs[0].set_ylabel('Frequency')\n",
    "    axs[0].get_legend().set_title('Kernel has NZ FLOPs?')\n",
    "\n",
    "    # rotate the x ticks for the query cost plot\n",
    "    for label in axs[0].get_xticklabels():\n",
    "        label.set_rotation(45)\n",
    "        label.set_ha('right')\n",
    "\n",
    "\n",
    "    sns.histplot(data=success_df, x='totalQueryTime', hue='has_nz_flops', bins=50, ax=axs[1])\n",
    "    axs[1].set_title('Total Query Time Distribution')\n",
    "    axs[1].set_xlabel('Total Query Time (seconds)')\n",
    "    axs[1].set_ylabel('Frequency')\n",
    "    axs[1].get_legend().set_title('Kernel has NZ FLOPs?')\n",
    "\n",
    "    #error_value_counts = df['error'].value_counts()\n",
    "    sns.countplot(data=sub_df, x='error', ax=axs[2])\n",
    "    axs[2].set_title('Failure Cases Distribution')\n",
    "    axs[2].set_xlabel('Failure Cases')\n",
    "    axs[2].set_ylabel('Frequency')\n",
    "\n",
    "    # add countplot values on top of the bars\n",
    "    for p in axs[2].patches:\n",
    "        axs[2].annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()), ha='center', va='center', fontsize=8, color='black', xytext=(0, 5), textcoords='offset points')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2b3ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictions_for_model_name(df, 'gpt-5-mini', 'simple')\n",
    "plot_predictions_for_model_name(df, 'gpt-5-mini', 'full')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autogen5.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
